
/******************************************/
/* Begin Kernel                           */
/******************************************/
.amdgcn_target "amdgcn-amd-amdhsa--gfx942"
.text
.protected Custom_Cijk_Alik_Bljk_HHS_BH_Bias_SAV_UserArgs_MT128x128x64_MI16x16x1_SN_K1_MIWT2_8_STA_gfx942
.globl Custom_Cijk_Alik_Bljk_HHS_BH_Bias_SAV_UserArgs_MT128x128x64_MI16x16x1_SN_K1_MIWT2_8_STA_gfx942
.p2align 8
.type Custom_Cijk_Alik_Bljk_HHS_BH_Bias_SAV_UserArgs_MT128x128x64_MI16x16x1_SN_K1_MIWT2_8_STA_gfx942,@function
.section .rodata,#alloc
.p2align 6
.amdhsa_kernel Custom_Cijk_Alik_Bljk_HHS_BH_Bias_SAV_UserArgs_MT128x128x64_MI16x16x1_SN_K1_MIWT2_8_STA_gfx942
  .amdhsa_user_sgpr_kernarg_segment_ptr 1
  .amdhsa_accum_offset 192 // accvgpr offset
  .amdhsa_next_free_vgpr 256 // vgprs
  .amdhsa_next_free_sgpr 80 // sgprs
  .amdhsa_group_segment_fixed_size 16896 // lds bytes
  .amdhsa_private_segment_fixed_size 0
  .amdhsa_system_sgpr_workgroup_id_x 1
  .amdhsa_system_sgpr_workgroup_id_y 1
  .amdhsa_system_sgpr_workgroup_id_z 1
  .amdhsa_system_vgpr_workitem_id 0
  .amdhsa_float_denorm_mode_32 3
  .amdhsa_float_denorm_mode_16_64 3
  .amdhsa_user_sgpr_count 13
  .amdhsa_user_sgpr_kernarg_preload_length 11
  .amdhsa_user_sgpr_kernarg_preload_offset 0
.end_amdhsa_kernel
.text
/* Num VGPR   =192 */
/* Num AccVGPR=64 */
/* Num SGPR   =79 */

/******************************************/
/* Optimizations and Config:              */
/******************************************/
/* ThreadTile= 8 x 8 */
/* SubGroup= 16 x 16 */
/* VectorWidthA=1 */
/* VectorWidthB=8 */
/* GlobalReadVectorWidthA=8, GlobalReadVectorWidthB=8 */
/* DirectToLdsA=False */
/* DirectToLdsB=False */
/* UseSgprForGRO=0 */
.amdgpu_metadata
---
custom.config:
  ProblemType:
    OperationType: GEMM
    DataType: h
    DestDataType: h
    ComputeDataType: s
    HighPrecisionAccumulate: True
    TransposeA: True
    TransposeB: False
    UseBeta: True
    Batched: True
    Activation: False
    SwizzleTensorA: True
    SupportUserArgs: True
    UseBias: 1
    UseScaleAlphaVec: 1
  MatrixInstruction: [16, 16, 16, 1, 1, 2, 8, 4, 1]
  1LDSBuffer: 0
  ScheduleIterAlg: 3
  DepthU: 64
  StaggerU: 0
  WorkGroupMapping: 8
  WaveSeparateGlobalReadA: 1
  WaveSeparateGlobalReadB: 1
  GlobalReadVectorWidthA: 8
  GlobalReadVectorWidthB: 2
  AssertFree0ElementMultiple: 16
  AssertFree1ElementMultiple: 16
  AssertSummationElementMultiple: 32
  GlobalSplitU: 1
  GlobalSplitUAlgorithm: "MultipleBufferSingleKernel"
  NoReject: 1
  InternalSupportParams:
   KernArgsVersion: 2
   SupportUserGSU: True
   SupportCustomWGM: True
   SupportCustomStaggerU: True
   UseUniversalArgs: True
amdhsa.version:
  - 1
  - 1
amdhsa.kernels:
  - .name: Custom_Cijk_Alik_Bljk_HHS_BH_Bias_SAV_UserArgs_MT128x128x64_MI16x16x1_SN_K1_MIWT2_8_STA_gfx942
    .symbol: 'Custom_Cijk_Alik_Bljk_HHS_BH_Bias_SAV_UserArgs_MT128x128x64_MI16x16x1_SN_K1_MIWT2_8_STA_gfx942.kd'
    .language:                   OpenCL C
    .language_version:
      - 2
      - 0
    .args:
      - .name:            Gemm info
        .size:            4
        .offset:          0
        .value_kind:      by_value
        .value_type:      u32
      - .name:            kernel info0
        .size:            4
        .offset:          4
        .value_kind:      by_value
        .value_type:      u32
      - .name:            kernel info1
        .size:            4
        .offset:          8
        .value_kind:      by_value
        .value_type:      u32
      - .name:            numWG
        .size:            4
        .offset:          12
        .value_kind:      by_value
        .value_type:      u32
      - .name:            SizesFree0
        .size:            4
        .offset:          16
        .value_kind:      by_value
        .value_type:      u32
      - .name:            SizesFree1
        .size:            4
        .offset:          20
        .value_kind:      by_value
        .value_type:      u32
      - .name:            SizesFree2
        .size:            4
        .offset:          24
        .value_kind:      by_value
        .value_type:      u32
      - .name:            SizesSum0
        .size:            4
        .offset:          28
        .value_kind:      by_value
        .value_type:      u32
      - .name:            D
        .size:            8
        .offset:          32
        .value_kind:      global_buffer
        .value_type:      f16
        .address_space:   generic
      - .name:            C
        .size:            8
        .offset:          40
        .value_kind:      global_buffer
        .value_type:      f16
        .address_space:   generic
      - .name:            A
        .size:            8
        .offset:          48
        .value_kind:      global_buffer
        .value_type:      f16
        .address_space:   generic
      - .name:            B
        .size:            8
        .offset:          56
        .value_kind:      global_buffer
        .value_type:      f16
        .address_space:   generic
      - .name:            strideD0
        .size:            4
        .offset:          64
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideD1
        .size:            4
        .offset:          68
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideC0
        .size:            4
        .offset:          72
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideC1
        .size:            4
        .offset:          76
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideA0
        .size:            4
        .offset:          80
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideA1
        .size:            4
        .offset:          84
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideB0
        .size:            4
        .offset:          88
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideB1
        .size:            4
        .offset:          92
        .value_kind:      by_value
        .value_type:      u32
      - .name:            alpha
        .size:            4
        .offset:          96
        .value_kind:      by_value
        .value_type:      f32
      - .name:            beta
        .size:            4
        .offset:          100
        .value_kind:      by_value
        .value_type:      f32
      - .name:            AddressScaleAlphaVec
        .size:            8
        .offset:          104
        .value_kind:      global_buffer
        .value_type:      f32
        .address_space:   generic
      - .name:            bias
        .size:            8
        .offset:          112
        .value_kind:      global_buffer
        .value_type:      void
        .address_space:   generic
      - .name:            biasType
        .size:            4
        .offset:          120
        .value_kind:      by_value
        .value_type:      u32
      - .name:            StrideBias
        .size:            4
        .offset:          124
        .value_kind:      by_value
        .value_type:      u32
      - .name:            dstD
        .size:            8
        .offset:          128
        .value_kind:      global_buffer
        .value_type:      f16
        .address_space:   generic
      - .name:            Synchronizer
        .size:            8
        .offset:          136
        .value_kind:      global_buffer
        .value_type:      f32
        .address_space:   generic
      - .name:            GSUSync
        .size:            4
        .offset:          144
        .value_kind:      by_value
        .value_type:      u32
    .group_segment_fixed_size:   16896
    .kernarg_segment_align:      8
    .kernarg_segment_size:       152
    .max_flat_workgroup_size:    256
    .private_segment_fixed_size: 0
    .sgpr_count:                 79
    .sgpr_spill_count:           0
    .vgpr_count:                 192
    .vgpr_spill_count:           0
    .wavefront_size:             64
...
.end_amdgpu_metadata
Custom_Cijk_Alik_Bljk_HHS_BH_Bias_SAV_UserArgs_MT128x128x64_MI16x16x1_SN_K1_MIWT2_8_STA_gfx942:
label_ASM_Start:  /// Main body of the asm kernel

/* Magic div and mod functions */
.macro V_MAGIC_DIV dstIdx:req dividend:req magicNumber:req magicShift:req magicA:req
    v_mul_hi_u32 v[\dstIdx+1] \dividend \magicNumber
    v_mul_lo_u32 v[\dstIdx+0] \dividend \magicA
    v_add_u32 v[\dstIdx+0] v[\dstIdx+0] v[\dstIdx+1]
    v_lshrrev_b32 v[\dstIdx+0] \magicShift v[\dstIdx+0]
.endm

/******************************************/
/* VGPR Assignments                       */
/******************************************/
/* ValuC range: [0-0), serializedStore enabled */
.set vgprValuC, 0
/* ValuA/B   Xn=PLR buffer idx,  In=InnerUnroll idx */
.set vgprValuA_X0_I0, 0
.set vgprValuA_X1_I0, 4
.set vgprValuA_X2_I0, 8
.set vgprValuA_X3_I0, 12
.set vgprValuB_X0_I0, 16
.set vgprValuB_X1_I0, 32
.set vgprValuB_X2_I0, 48
.set vgprValuB_X3_I0, 64
.set vgprLocalWriteAddrB, 80
.set vgprGlobalReadOffsetA, 81
.set vgprGlobalReadOffsetB, 85
.set vgprG2LA, 90
.set vgprG2LA2, 106
.set vgprG2LB, 122
.set vgprLocalReadAddrB, 138
.set vgprSerial, 139

/******************************************/
/* SGPR Assignments                       */
/******************************************/
.set sgprKernArgAddress, 0
.set sgprWorkGroup0, 2
.set sgprWorkGroup1, 3
.set sgprWorkGroup2, 4
.set sgprArgType, 5
.set sgprGSUSumIdx, 6
.set sgprGSULog2BpeC, 8
.set sgprGSULog2BpeD, 9
.set sgprStaggerU, 10
.set sgprWGM, 11
.set sgprLoopCounterL, 12
.set sgprOrigLoopCounter, 13
.set sgprSrdD, 16
.set sgprSrdC, 20
.set sgprNumWorkGroups0, 14
.set sgprNumWorkGroups1, 15
.set sgprWSDstart, 24
.set sgprSizesFree, 28
.set sgprSizesSum, 31
.set sgprAddressD, 32
.set sgprAddressC, 34
.set sgprAddressA, 36
.set sgprAddressB, 38
.set sgprStridesD, 40
.set sgprStridesC, 42
.set sgprStridesA, 44
.set sgprStridesB, 46
.set sgprAlpha, 48
.set sgprBeta, 49
.set sgprAddressTD, 50
.set sgprSynchronizer, 52
.set sgprGSUSync, 54
.set sgprGSU, 55
.set sgprSwizzleStrideA, 79

/* Size Assignments */
.set sgprSizeI, sgprSizesFree+0
.set sgprSizeJ, sgprSizesFree+1
.set sgprSizeK, sgprSizesFree+2
.set sgprSizeL, sgprSizesSum+0

/* Stride Assignments */
.set constStrideD0I, 1
.set sgprStrideD1J, sgprStridesD+0
.set sgprStrideDK, sgprStridesD+1
.set constStrideC0I, 1
.set sgprStrideC1J, sgprStridesC+0
.set sgprStrideCK, sgprStridesC+1
.set constStrideAL, 1
.set sgprStrideA0I, sgprStridesA+0
.set sgprStrideAK, sgprStridesA+1
.set constStrideBL, 1
.set sgprStrideB1J, sgprStridesB+0
.set sgprStrideBK, sgprStridesB+1

.set MT0, 128
.set MT1, 128
.set DepthU, 64
.set BpeA, 2
.set BpeALog2, 1
.set BpeB, 2
.set BpeBLog2, 1
.set BpeAGR, 2
.set BpeAGRLog2, 1
.set BpeBGR, 2
.set BpeBGRLog2, 1
/* Number of elements to shift-left SRD */
.set SrdShiftLeftA, 8
.set SrdShiftLeftB, 8
/* 2GB limit - set offsets to -1 to exceed this and clamp */
.set BufferLimit, 0xffffffff
.set BufferOOB, 0x80000000

/******************************************/
/* Bits 127:96 of SRD.                    */
/* hex: 0x00020000                        */
/* dst_sel_x (3b): 0                      */
/* dst_sel_y (3b): 0                      */
/* dst_sel_z (3b): 0                      */
/* dst_sel_w (3b): 0                      */
/* num_format (3b): 0                     */
/* data_format (4b): 4                    */
/* user_vm_enable (1b): 0                 */
/* user_vm_mode (1b): 0                   */
/* index_stride (2b): 0                   */
/* add_tid_enable (1b): 0                 */
/* _unusedA (3b): 0                       */
/* nv (1b): 0                             */
/* _unusedB (2b): 0                       */
/* type (2b): 0                           */
/******************************************/
.set Srd127_96, 0x00020000

/* Global Offset A */
.macro GLOBAL_OFFSET_A vgprAddr:req vgprOffsetL:req vgprOffset0I:req vgprTmp:req
    v_mul_lo_u32 v[\vgprTmp+0] s[sgprStrideA0I] v[\vgprOffset0I] // mul d1 lower
    v_add_co_u32 v[\vgprAddr+0] vcc v[\vgprOffsetL] v[\vgprTmp+0] // accumulate K lower
    v_add_u32 v[\vgprAddr+0] 0x8 v[\vgprAddr+0]      // add prepad for pointer shift
    v_lshlrev_b32 v[\vgprAddr+0] 0x1 v[\vgprAddr+0]  // offset *= bytes/element
.endm

/* Global Swizzled Offset A */
.macro GLOBAL_SWIZZLED_OFFSET_A vgprAddr:req vgprOffsetL:req vgprOffset0I:req vgprTmp:req
    v_mul_lo_u32 v[\vgprTmp+0] s[sgprSwizzleStrideA] v[\vgprOffset0I] // mul d1 lower
    v_add_co_u32 v[\vgprAddr+0] vcc v[\vgprOffsetL] v[\vgprTmp+0] // accumulate K lower
    v_add_u32 v[\vgprAddr+0] 0x8 v[\vgprAddr+0]      // add prepad for pointer shift
    v_lshlrev_b32 v[\vgprAddr+0] 0x1 v[\vgprAddr+0]  // offset *= bytes/element
.endm

/* Global Offset B */
.macro GLOBAL_OFFSET_B vgprAddr:req vgprOffsetL:req vgprOffset1J:req vgprTmp:req
    v_mul_lo_u32 v[\vgprTmp+0] s[sgprStrideB1J] v[\vgprOffset1J] // mul d1 lower
    v_add_co_u32 v[\vgprAddr+0] vcc v[\vgprOffsetL] v[\vgprTmp+0] // accumulate K lower
    v_add_u32 v[\vgprAddr+0] 0x8 v[\vgprAddr+0]      // add prepad for pointer shift
    v_lshlrev_b32 v[\vgprAddr+0] 0x1 v[\vgprAddr+0]  // offset *= bytes/element
.endm

/* Dynamic Scalar Divide: vQuotient=vDividend/vDivisor; vRemainder=vDividend%vDivisor; */
.macro DYNAMIC_VECTOR_DIVIDE vQuotient vRemainder vDividend vDivisor vTmp0 vTmp1 sTmp
    v_cvt_f32_u32 v[\vQuotient] v[\vDivisor]
    v_rcp_f32 v[\vQuotient] v[\vQuotient]
    v_mul_f32 v[\vQuotient] 0x4f800000 v[\vQuotient]
    v_cvt_u32_f32 v[\vQuotient] v[\vQuotient]
    v_mul_lo_u32 v[\vRemainder] v[\vDivisor] v[\vQuotient]
    v_mul_hi_u32 v[\vTmp0] v[\vDivisor] v[\vQuotient]
    v_sub_co_u32 v[\vTmp1] vcc 0x0 v[\vRemainder]
    v_cmp_ne_i32 s[\sTmp:\sTmp+1] 0x0 v[\vTmp0]
    v_cndmask_b32 v[\vRemainder] v[\vTmp1] v[\vRemainder] s[\sTmp:\sTmp+1]
    v_mul_hi_u32 v[\vRemainder] v[\vRemainder] v[\vQuotient]
    v_sub_co_u32 v[\vTmp0] vcc v[\vQuotient] v[\vRemainder]
    v_add_co_u32 v[\vQuotient] vcc v[\vQuotient] v[\vRemainder]
    v_cndmask_b32 v[\vQuotient] v[\vQuotient] v[\vTmp0] s[\sTmp:\sTmp+1]
    v_mul_hi_u32 v[\vQuotient] v[\vQuotient] v[\vDividend]
    v_mul_lo_u32 v[\vRemainder] v[\vQuotient] v[\vDivisor]
    v_sub_co_u32 v[\vTmp0] vcc v[\vDividend] v[\vRemainder]
    v_cmp_ge_u32 s[\sTmp:\sTmp+1] v[\vDividend] v[\vRemainder]
    v_add_co_u32 v[\vRemainder] vcc 0x1 v[\vQuotient]
    v_add_co_u32 v[\vTmp1] vcc -1 v[\vQuotient]
    v_cmp_le_u32 vcc v[\vDivisor] v[\vTmp0]
    s_and_b64 vcc s[\sTmp:\sTmp+1] vcc
    v_cndmask_b32 v[\vQuotient] v[\vQuotient] v[\vRemainder] vcc
    v_cndmask_b32 v[\vQuotient] v[\vTmp1] v[\vQuotient] s[\sTmp:\sTmp+1]
    v_cmp_ne_i32 vcc 0x0 v[\vDivisor]
    v_cndmask_b32 v[\vQuotient] -1 v[\vQuotient] vcc // final result
    v_mul_lo_u32 v[\vRemainder] v[\vQuotient] v[\vDivisor]
    v_sub_co_u32 v[\vRemainder] vcc v[\vDividend] v[\vRemainder] // final result
.endm

/******************************************/
/* Allocate Resources                     */
/******************************************/

/* Load num of Gemms */
s_load_dword s26, s[sgprKernArgAddress:sgprKernArgAddress+1], 0x0

/* Load packed kernel args (StaggerU/GSU) */
s_load_dword s56, s[sgprKernArgAddress:sgprKernArgAddress+1], 0x4

/* Load WGM data */
s_load_dword s[sgprWGM], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x8

/* Load num of WGs */
s_load_dword s57, s[sgprKernArgAddress:sgprKernArgAddress+1], 0xc
s_waitcnt lgkmcnt(0)
s_lshr_b32 s27, s26, 0x1e                          // Get arg type
s_and_b32 s26, 0x3fffffff, s26                     // Get nums of gemm
/* Check if custom structure pointer is null */
s_cmp_eq_u32 s27, 2                                // ArgType == 2 ?
s_cbranch_scc0 label_LoadExternalEpilogueStruct

/* Grouped Gemm: Load address of external kernel arguments */
s_load_dwordx2 s[sgprAddressTD:sgprAddressTD+1], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x20
s_load_dwordx2 s[sgprSynchronizer:sgprSynchronizer+1], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x18
label_LoadExternalEpilogueStruct:
s_cmp_eq_u32 s27, 0                                // Is kernel args
s_cbranch_scc0 label_HBMArgs
s_add_u32 s[sgprKernArgAddress], s[sgprKernArgAddress], 0x10 // Shift common args
s_addc_u32 s[sgprKernArgAddress+1], s[sgprKernArgAddress+1], 0x0

/* Load Kernel Args */
s_load_dwordx16 s[28:43], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x0
s_load_dwordx4 s[44:47], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x40
s_load_dwordx2 s[48:49], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x50
s_waitcnt lgkmcnt(0)
s_branch label_LoadArgsEnd
label_HBMArgs:

/* Load address of kernel arguments */
s_load_dwordx2 s[sgprKernArgAddress:sgprKernArgAddress+1], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x10
s_waitcnt lgkmcnt(0)                               // wait for args to load
label_LoadArgsEnd:
s_branch label_common_kernel_entry

/* pad 31 snops to satisfy 0x100 code size for Preload Backward Compatibility Prologue */
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
label_Preload_Offset_Start:
s_and_b32 s26, 0x3fffffff, s2                      // Get nums of gemm
s_lshr_b32 s27, s2, 0x1e                           // Get arg type
/* Check if custom structure pointer is null */
s_cmp_eq_u32 s27, 2                                // ArgType == 2 ?
s_cbranch_scc0 label_LoadExternalEpilogueStruct_1
s_mov_b32 s[sgprSynchronizer+1], s9                // Load Synchronizer data
s_mov_b32 s[sgprSynchronizer], s8                  // Load Synchronizer data
s_mov_b32 s[sgprAddressTD+1], s11                  // Load AddressTD data
s_mov_b32 s[sgprAddressTD], s10                    // Load AddressTD data
label_LoadExternalEpilogueStruct_1:
s_mov_b32 s56, s3                                  // Preload internal args
s_cmp_eq_u32 s27, 0                                // Is kernel args
s_cbranch_scc0 label_Preload_HBMArgs
s_add_u32 s[sgprKernArgAddress], s[sgprKernArgAddress], 0x10 // Shift common args
s_addc_u32 s[sgprKernArgAddress+1], s[sgprKernArgAddress+1], 0x0

/* Load Kernel Args */
s_load_dword s35, s[sgprKernArgAddress:sgprKernArgAddress+1], 0x1c
s_load_dwordx8 s[36:43], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x20
s_load_dwordx4 s[44:47], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x40
s_load_dwordx2 s[48:49], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x50
s_mov_b32 s28, s6                                  // move preload data to correct sgpr
s_mov_b32 s29, s7                                  // move preload data to correct sgpr
s_mov_b32 s30, s8                                  // move preload data to correct sgpr
s_mov_b32 s31, s9                                  // move preload data to correct sgpr
s_mov_b32 s32, s10                                 // move preload data to correct sgpr
s_mov_b32 s33, s11                                 // move preload data to correct sgpr
s_mov_b32 s34, s12                                 // move preload data to correct sgpr
s_branch label_Preload_LoadArgsEnd
label_Preload_HBMArgs:
s_mov_b64 s[sgprKernArgAddress:sgprKernArgAddress+1], s[6:7] // Load address of kernel arguments
label_Preload_LoadArgsEnd:
s_mov_b32 s[sgprWGM], s4                           // Preload internal args2
s_mov_b32 s57, s5                                  // Load num of WGs
label_common_kernel_entry:  /// for both preload/non-preload common code
s_mov_b32 s[sgprWorkGroup0+0], s13                 // restore workgroup id
s_mov_b32 s[sgprWorkGroup0+1], s14                 // restore workgroup id
s_mov_b32 s[sgprWorkGroup0+2], s15                 // restore workgroup id
s_and_b32 s[sgprStaggerU], s56, 0xffff0000         // Restore StaggerU related vars
s_lshr_b32 s[sgprStaggerU], s[sgprStaggerU], 0x10
s_and_b32 s[sgprGSU], s56, 0xffff                  // Restore GSUConfig and GSU
s_mov_b32 s[sgprArgType], s27
s_mov_b32 m0, 0x4200                               // LDS clamp at 16896 bytes
v_mov_b32 v[vgprSerial], v0                        // thread serial id

/* remap workgroup to XCCs */
s_lshr_b32 s62, s[sgprWGM], 0x10                   // Get WGMXCC
s_and_b32 s62, 0x3f, s62                           // Get WGMXCC
s_lshr_b32 s63, s[sgprWGM], 0x16                   // Get CU_Count
/* remap WGs if WGMXCC > 1 */
s_cmp_gt_u32 s62, 1
s_cbranch_scc0 label_skip_WGMXCC
/* only remap WGs in the range */
v_cvt_f32_u32 v6, s62                              // s59 = s57 / s62
v_rcp_iflag_f32 v6, v6                             // s59 = s57 / s62
v_cvt_f32_u32 v7, s57                              // s59 = s57 / s62
v_mul_f32 v6, v6, v7                               // s59 = s57 / s62
v_cvt_u32_f32 v6, v6                               // s59 = s57 / s62
v_mul_u32_u24 v7, v6, s62                          // s59 = s57 / s62
v_sub_u32 v7, s57, v7                              // s59 = s57 / s62
v_cmpx_eq_u32 exec, v7, s62                        // s59 = s57 / s62
v_add_u32 v6, 1, v6                                // s59 = s57 / s62
s_mov_b64 exec, -1                                 // s59 = s57 / s62
v_readfirstlane_b32 s59, v6                        // quotient
s_mul_i32 s59, s59, s62
s_cmp_ge_u32 s[sgprWorkGroup0], s59
s_cbranch_scc1 label_skip_WGMXCC
s_cmp_eq_u32 s63, 0                                // CU_Count == 0 ?
s_cbranch_scc0 label_XCCG_nonzero
v_cvt_f32_u32 v6, s62                              // s59 = s[sgprWorkGroup0] / s62
v_rcp_iflag_f32 v6, v6                             // s59 = s[sgprWorkGroup0] / s62
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // s59 = s[sgprWorkGroup0] / s62
v_mul_f32 v6, v6, v7                               // s59 = s[sgprWorkGroup0] / s62
v_cvt_u32_f32 v6, v6                               // s59 = s[sgprWorkGroup0] / s62
v_mul_u32_u24 v7, v6, s62                          // s59 = s[sgprWorkGroup0] / s62
v_sub_u32 v7, s[sgprWorkGroup0], v7                // s59 = s[sgprWorkGroup0] / s62
v_cmpx_eq_u32 exec, v7, s62                        // s59 = s[sgprWorkGroup0] / s62
v_add_u32 v6, 1, v6                                // s59 = s[sgprWorkGroup0] / s62
v_mov_b32 v7, 0                                    // s60 = s[sgprWorkGroup0] % s62
s_mov_b64 exec, -1                                 // s59 = s[sgprWorkGroup0] / s62
v_readfirstlane_b32 s59, v6                        // quotient
v_readfirstlane_b32 s60, v7                        // remainder
v_cvt_f32_u32 v6, s62                              // s61 = s57 / s62
v_rcp_iflag_f32 v6, v6                             // s61 = s57 / s62
v_cvt_f32_u32 v7, s57                              // s61 = s57 / s62
v_mul_f32 v6, v6, v7                               // s61 = s57 / s62
v_cvt_u32_f32 v6, v6                               // s61 = s57 / s62
v_mul_u32_u24 v7, v6, s62                          // s61 = s57 / s62
v_sub_u32 v7, s57, v7                              // s61 = s57 / s62
v_cmpx_eq_u32 exec, v7, s62                        // s61 = s57 / s62
v_add_u32 v6, 1, v6                                // s61 = s57 / s62
s_mov_b64 exec, -1                                 // s61 = s57 / s62
v_readfirstlane_b32 s61, v6                        // quotient
s_mul_i32 s60, s60, s61
s_add_u32 s[sgprWorkGroup0], s59, s60
s_branch label_skip_WGMXCC
label_XCCG_nonzero:
/* temp0 = (wg//CU_Count)*CU_Count */
v_cvt_f32_u32 v6, s63                              // wg//CU_Count
v_rcp_iflag_f32 v6, v6                             // wg//CU_Count
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // wg//CU_Count
v_mul_f32 v6, v6, v7                               // wg//CU_Count
v_cvt_u32_f32 v6, v6                               // wg//CU_Count
v_mul_u32_u24 v7, v6, s63                          // wg//CU_Count
v_sub_u32 v7, s[sgprWorkGroup0], v7                // wg//CU_Count
v_cmpx_eq_u32 exec, v7, s63                        // wg//CU_Count
v_add_u32 v6, 1, v6                                // wg//CU_Count
v_mov_b32 v7, 0                                    // wg//CU_Count
s_mov_b64 exec, -1                                 // wg//CU_Count
v_readfirstlane_b32 s59, v6                        // quotient
v_readfirstlane_b32 s60, v7                        // remainder
s_mul_i32 s59, s59, s63
/* temp1 = (wg%CU_Count)//WGMXCC */
v_cvt_f32_u32 v6, s62                              // s60 = s60 / s62
v_rcp_iflag_f32 v6, v6                             // s60 = s60 / s62
v_cvt_f32_u32 v7, s60                              // s60 = s60 / s62
v_mul_f32 v6, v6, v7                               // s60 = s60 / s62
v_cvt_u32_f32 v6, v6                               // s60 = s60 / s62
v_mul_u32_u24 v7, v6, s62                          // s60 = s60 / s62
v_sub_u32 v7, s60, v7                              // s60 = s60 / s62
v_cmpx_eq_u32 exec, v7, s62                        // s60 = s60 / s62
v_add_u32 v6, 1, v6                                // s60 = s60 / s62
s_mov_b64 exec, -1                                 // s60 = s60 / s62
v_readfirstlane_b32 s60, v6                        // quotient
/* temp0 = temp0 + temp1 */
s_add_u32 s59, s59, s60
/* temp1 = (wg%WGMXCC) * ((WGs - (WGs//CU_Count) * CU_Count) if (wg > (WGs//CU_Count) * CU_Count) else CU_Count)//WGMXCC */
v_cvt_f32_u32 v6, s63                              // WGs//CU_Count
v_rcp_iflag_f32 v6, v6                             // WGs//CU_Count
v_cvt_f32_u32 v7, s57                              // WGs//CU_Count
v_mul_f32 v6, v6, v7                               // WGs//CU_Count
v_cvt_u32_f32 v6, v6                               // WGs//CU_Count
v_mul_u32_u24 v7, v6, s63                          // WGs//CU_Count
v_sub_u32 v7, s57, v7                              // WGs//CU_Count
v_cmpx_eq_u32 exec, v7, s63                        // WGs//CU_Count
v_add_u32 v6, 1, v6                                // WGs//CU_Count
s_mov_b64 exec, -1                                 // WGs//CU_Count
v_readfirstlane_b32 s60, v6                        // quotient
s_mul_i32 s60, s60, s63
s_sub_u32 s61, s57, s60
s_cmp_gt_u32 s[sgprWorkGroup0], s60
s_cselect_b32 s60, s61, s63
v_cvt_f32_u32 v6, s62                              // s60 = s60 / s62
v_rcp_iflag_f32 v6, v6                             // s60 = s60 / s62
v_cvt_f32_u32 v7, s60                              // s60 = s60 / s62
v_mul_f32 v6, v6, v7                               // s60 = s60 / s62
v_cvt_u32_f32 v6, v6                               // s60 = s60 / s62
v_mul_u32_u24 v7, v6, s62                          // s60 = s60 / s62
v_sub_u32 v7, s60, v7                              // s60 = s60 / s62
v_cmpx_eq_u32 exec, v7, s62                        // s60 = s60 / s62
v_add_u32 v6, 1, v6                                // s60 = s60 / s62
s_mov_b64 exec, -1                                 // s60 = s60 / s62
v_readfirstlane_b32 s60, v6                        // quotient
v_cvt_f32_u32 v6, s62                              // s58 = s[sgprWorkGroup0] / s62
v_rcp_iflag_f32 v6, v6                             // s58 = s[sgprWorkGroup0] / s62
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // s58 = s[sgprWorkGroup0] / s62
v_mul_f32 v6, v6, v7                               // s58 = s[sgprWorkGroup0] / s62
v_cvt_u32_f32 v6, v6                               // s58 = s[sgprWorkGroup0] / s62
v_mul_u32_u24 v7, v6, s62                          // s58 = s[sgprWorkGroup0] / s62
v_sub_u32 v7, s[sgprWorkGroup0], v7                // s58 = s[sgprWorkGroup0] / s62
v_cmpx_eq_u32 exec, v7, s62                        // s58 = s[sgprWorkGroup0] / s62
v_add_u32 v6, 1, v6                                // s58 = s[sgprWorkGroup0] / s62
v_mov_b32 v7, 0                                    // s61 = s[sgprWorkGroup0] % s62
s_mov_b64 exec, -1                                 // s58 = s[sgprWorkGroup0] / s62
v_readfirstlane_b32 s58, v6                        // quotient
v_readfirstlane_b32 s61, v7                        // remainder
s_mul_i32 s60, s60, s61
/* WorkGroup0 = temp0 + temp1 */
s_add_u32 s[sgprWorkGroup0], s59, s60
label_skip_WGMXCC:  /// skip WGMXCC if no enough WGs to remap
s_cmp_eq_u32 s27, 0
s_cbranch_scc0 label_MultiGemm
/* init: add vgpr [0...80) to pool */
/* init: add vgpr [0...0) to pool */
/* init: add agpr [0...64) to pool */

/******************************************/
/* Local Read Addresses                   */
/******************************************/

/* local read addresses: tile assignments a/b */
/* lr1J */
v_and_b32 v1, 63, v[vgprSerial]                    // 0. thread id in wave: wtid = tid % wavelength(64)
v_and_b32 v0, 15, v1                               // 1. N offset: nIdx = wtid % MI_N(16)
v_lshlrev_b32 v0, 0x6, v0                          // 1. N offset: nOffset = nIdx * nStride(64)
/* Skip. 2. block offset: bnOffset = 0 when num1DBlocks = 1 */
v_lshlrev_b32 v0, 0x3, v0                          // 4. apply VectorWidth: bnOffset = bnOffset * vw(8)
v_lshrrev_b32 v1, 4, v1                            // 5. K offset: kIdx = wtid / (MIN(16) * MIBB(1))
v_lshlrev_b32 v1, 0x3, v1                          // 5. K offset: lrKOffset = kIdx * mStride(8)
v_add_u32 v0, v1, v0                               // 6. offset in wave: lrOffset = bnOffset + lrKOffset

/* local read addresses: final offsets a */

/* local read addresses: final offsets b */
v_lshrrev_b32 v1, 6, v[vgprSerial]                 // v1 = v[vgprSerial] / 64
v_lshrrev_b32 v1, 2, v1                            // LSU offset: Get LSU wave_id
s_mov_b32 s56, 64                                  // LSU offset: stride = lsuStride(64) when umlds==True
v_mul_lo_u32 v1, s56, v1                           // LSU offset: lsuoffset = wave_id*lsuStride*(MT1+PAD)
v_add_lshl_u32 v[vgprLocalReadAddrB], v1, v0, 0x1  // Final Offset: offset = (lro1+lsuoffset)*bpeDS
v_lshrrev_b32 v2, 10, v[vgprLocalReadAddrB]        // Final Offset: padding 32 per block 1024
v_lshlrev_b32 v2, 0x5, v2                          // Final Offset: padding 32 per block 1024
v_add_u32 v[vgprLocalReadAddrB], v2, v[vgprLocalReadAddrB] // Final Offset: add padding 32 per block 1024

/* local read addresses: declare addresses a */
/* N/A */

/* local read addresses: declare addresses b */

/******************************************/
/* Local Write Addresses                  */
/******************************************/
/* LVCA = 4 */
/* v1 = A-unroll = serial/LVCA */
/* TileAssignment for DirectToVgprA */
v_and_b32 v1, 63, v[vgprSerial]                    // 0. thread id in wave: wtid = tid % wavelength(64)
//v_and_b32 v0, 15, v1                               // 1. N offset: nIdx = wtid % MI_N(16)
v_lshrrev_b32 v0, 6, v[vgprSerial]                 // 7. wave offset in N dimen: wtid = tid / dividedForWaveId(64)
v_lshrrev_b32 v4, 5, s[sgprSizesSum] // numKr = DimK / 32
v_mul_u32_u24 v0, v4, v0 // wtid * numKr
                                                   // 1. N offset: nOffset = nIdx * nStride(1) (multiplier is 1, do nothing)
/* Skip. 2. block offset: bnOffset = 0 when num1DBlocks = 1 */
                                                   // 4. apply VectorWidth: bnOffset = bnOffset * vw(1) (multiplier is 1, do nothing)
//v_lshrrev_b32 v1, 4, v1                            // 5. K offset: kIdx = wtid / (MIN(16) * MIBB(1))
//v_lshrrev_b32 v4, 6, v[vgprSerial]                 // 7. wave offset in N dimen: wtid = tid / dividedForWaveId(64)
//v_and_b32 v4, 3, v4                                // 7. wave offset in M dimen: wtid0 = wtid / num1DWaves(4)
//v_lshlrev_b32 v4, 0x4, v4                          // 7. wave offset in M dimen: wOffset = wtid0 * W0Stride(16)
//v_add_u32 v0, v4, v0                               // 7. final local read offset: flrOffset = lrOffset + WOffset
/* unroll *= glvw */
v_lshlrev_b32 v1, 0x3, v1                          // v1 = v1 * 8
v_mov_b32 v4, v1                                   // copy for GlobalSplitU
/* LVCB = 8 */
/* v3 = B-unroll = serial%LVCB */
v_lshrrev_b32 v2, 3, v[vgprSerial]                 // v2 = v[vgprSerial] / 8
v_and_b32 v3, 7, v[vgprSerial]                     // v3 = v[vgprSerial] % 8
/* unroll *= glvw */
v_lshlrev_b32 v3, 0x3, v3                          // v3 = v3 * 8
v_mov_b32 v5, v3                                   // copy for GlobalSplitU
/* lwaUnrollAssignmentA = v4 */
/* lwaUnrollAssignmentB = v5 */

/* local write addresses: first offset a */

/* local write addresses: first offset b */
v_mul_u32_u24 v[vgprLocalWriteAddrB], 0x40, v2     // lwBL**(DepthU_Compute + PAD)
v_add_lshl_u32 v[vgprLocalWriteAddrB], v5, v[vgprLocalWriteAddrB], 0x1 // lwFOB = (lwBB + lwBL*(DepthU+PAD))*bpeDS
v_lshrrev_b32 v6, 10, v[vgprLocalWriteAddrB]       // padding 32 per block 1024
v_lshlrev_b32 v6, 0x5, v6                          // padding 32 per block 1024
v_add_u32 v[vgprLocalWriteAddrB], v6, v[vgprLocalWriteAddrB] // add padding 32 per block 1024
v_mov_b32 v8, MT0                                  // set MT0 into sgpr
v_mov_b32 v7, s[sgprSizesFree+0]                   // set Free0 size
v_cvt_f32_u32 v6, v8                               // v6 = ceil(v7 / v8)
v_rcp_iflag_f32 v6, v6                             // v6 = ceil(v7 / v8)
v_cvt_f32_u32 v9, v7                               // v6 = ceil(v7 / v8)
v_mul_f32 v6, v6, v9                               // v6 = ceil(v7 / v8)
v_cvt_u32_f32 v6, v6                               // v6 = ceil(v7 / v8)
v_mul_u32_u24 v9, v6, v8                           // v6 = ceil(v7 / v8)
v_sub_u32 v9, v7, v9                               // v6 = ceil(v7 / v8)
v_cmp_ne_u32 vcc, v9, 0                            // v6 = ceil(v7 / v8)
v_addc_co_u32 v6, vcc, v6, 0, vcc                  // ceil
v_mov_b32 v8, MT1                                  // set MT1 into sgpr
v_mov_b32 v7, s[sgprSizesFree+1]                   // set Free1 size
v_readfirstlane_b32 s[sgprNumWorkGroups0], v6      // set back to numWorkGroup0
v_cvt_f32_u32 v6, v8                               // v6 = ceil(v7 / v8)
v_rcp_iflag_f32 v6, v6                             // v6 = ceil(v7 / v8)
v_cvt_f32_u32 v9, v7                               // v6 = ceil(v7 / v8)
v_mul_f32 v6, v6, v9                               // v6 = ceil(v7 / v8)
v_cvt_u32_f32 v6, v6                               // v6 = ceil(v7 / v8)
v_mul_u32_u24 v9, v6, v8                           // v6 = ceil(v7 / v8)
v_sub_u32 v9, v7, v9                               // v6 = ceil(v7 / v8)
v_cmp_ne_u32 vcc, v9, 0                            // v6 = ceil(v7 / v8)
v_addc_co_u32 v6, vcc, v6, 0, vcc                  // ceil
s_nop 0                                            // 1 wait states
v_readfirstlane_b32 s[sgprNumWorkGroups1], v6      // set back to numWorkGroup1
s_waitcnt lgkmcnt(0)                               // wait for 44/0 bytes of kern args

/* remap wg from 1D(idxWG012) to 3D(wg2,wg1,wg0) */
/* wg2 = idxWG012 * smallMagicNumber(1/(numWG0*numWG1)) */
s_mul_i32 s56, s[sgprNumWorkGroups0], s[sgprNumWorkGroups1]
s_and_b32 s57, s[sgprGSU], 0x3fff                  // Restore GSU
s_mul_i32 s56, s56, s57
v_cvt_f32_u32 v6, s56                              // s56 = s[sgprWorkGroup0] / s56
v_rcp_iflag_f32 v6, v6                             // s56 = s[sgprWorkGroup0] / s56
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // s56 = s[sgprWorkGroup0] / s56
v_mul_f32 v6, v6, v7                               // s56 = s[sgprWorkGroup0] / s56
v_cvt_u32_f32 v6, v6                               // s56 = s[sgprWorkGroup0] / s56
v_mul_u32_u24 v7, v6, s56                          // s56 = s[sgprWorkGroup0] / s56
v_sub_u32 v7, s[sgprWorkGroup0], v7                // s56 = s[sgprWorkGroup0] / s56
v_cmpx_eq_u32 exec, v7, s56                        // s56 = s[sgprWorkGroup0] / s56
v_add_u32 v6, 1, v6                                // s56 = s[sgprWorkGroup0] / s56
s_mov_b64 exec, -1                                 // s56 = s[sgprWorkGroup0] / s56
v_readfirstlane_b32 s56, v6                        // quotient
s_mov_b32 s[sgprWorkGroup2], s56
/* idxWG01 = idxWG012 - wg2 * numWG0 * numWG1 */
s_mul_i32 s56, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s56, s56, s[sgprWorkGroup2]
s_mul_i32 s56, s56, s57
s_sub_u32 s[sgprWorkGroup0], s[sgprWorkGroup0], s56
/* wg1 = idxWG01 * smallMagicNumber(1/numWG0) */
v_cvt_f32_u32 v6, s[sgprNumWorkGroups0]            // s56 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_rcp_iflag_f32 v6, v6                             // s56 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // s56 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_mul_f32 v6, v6, v7                               // s56 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_cvt_u32_f32 v6, v6                               // s56 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_mul_u32_u24 v7, v6, s[sgprNumWorkGroups0]        // s56 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_sub_u32 v7, s[sgprWorkGroup0], v7                // s56 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_cmpx_eq_u32 exec, v7, s[sgprNumWorkGroups0]      // s56 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_add_u32 v6, 1, v6                                // s56 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
s_mov_b64 exec, -1                                 // s56 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_readfirstlane_b32 s56, v6                        // quotient
s_mov_b32 s[sgprWorkGroup1], s56
/* wg0 = idxWG01 - wg1 * numWG0 */
s_mul_i32 s56, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_sub_u32 s[sgprWorkGroup0], s[sgprWorkGroup0], s56
s_branch label_MultiGemmEnd
label_MultiGemm:

/* Check if custom structure pointer is null */
s_cmp_eq_u32 s[sgprArgType], 2                     // ArgType == 2 ?
s_cbranch_scc1 label_IsExternalValid               // branch if ArgType == 2
s_mov_b32 s15, 132
s_mul_i32 s62, s26, 4
s_mov_b64 s[56:57], s[sgprKernArgAddress:sgprKernArgAddress+1]
s_branch label_IsExternalValidEnd
label_IsExternalValid:
s_mov_b32 s15, 196
s_mov_b32 s62, 0x0
s_mov_b64 s[56:57], s[sgprKernArgAddress:sgprKernArgAddress+1]
label_IsExternalValidEnd:

/* Grouped Gemm:: prefetch 1 arg load */
s_mov_b32 s14, 1
s_mov_b32 s63, 0
s_load_dwordx4 s[28:31], s[56:57], s62
s_cmpk_eq_u32 s26, 1                               // if gemm_count is 1?
s_cbranch_scc1 label_wgTable_noLoadLoop

/* Grouped Gemm:: accumulate numTiles for each gemm */
/* Grouped Gemm:: loop start */
label_Loop_GemmCount:
s_waitcnt lgkmcnt(0)
s_lshr_b32 s60, s28, 7                             // s60 = s28 / 128
s_and_b32 s58, 127, s28                            // s58 = s28 % 128
s_addc_u32 s60, s60, 0x0
s_lshr_b32 s61, s29, 7                             // s61 = s29 / 128
s_and_b32 s58, 127, s29                            // s58 = s29 % 128
s_addc_u32 s61, s61, 0x0
s_mul_i32 s60, s60, s61
s_mul_i32 s60, s60, s30
s_and_b32 s61, s[sgprGSU], 0x3fff                  // Restore GSU
s_mul_i32 s60, s60, s61
s_add_u32 s63, s63, s60
s_cmp_lt_u32 s[sgprWorkGroup0], s63
s_cbranch_scc1 label_FOUND
/* Check if custom structure pointer is null */
s_cmp_eq_u32 s[sgprArgType], 2                     // ArgType == 2 ?
s_cbranch_scc0 label_LoadExternalEpilogueStruct_2
s_mul_i32 s58, s28, s29
s_and_b32 s60, s[sgprGSU], 0x3fff                  // Restore GSU
s_mul_i32 s58, s58, s60
s_lshl_b32 s58, s58, 2
s_add_u32 s[sgprAddressTD], s[sgprAddressTD], s58
s_addc_u32 s[sgprAddressTD+1], s[sgprAddressTD+1], 0x0
s_add_u32 s[sgprSynchronizer], s[sgprSynchronizer], 0x28000
s_addc_u32 s[sgprSynchronizer+1], s[sgprSynchronizer+1], 0x0
label_LoadExternalEpilogueStruct_2:
s_add_u32 s62, s62, s15
s_load_dwordx4 s[28:31], s[56:57], s62
s_add_u32 s14, s14, 1
s_cmp_lt_u32 s14, s26
s_cbranch_scc1 label_Loop_GemmCount

/* Grouped Gemm:: noLoadLoop */
label_wgTable_noLoadLoop:
s_waitcnt lgkmcnt(0)
s_lshr_b32 s60, s28, 7                             // s60 = s28 / 128
s_and_b32 s58, 127, s28                            // s58 = s28 % 128
s_addc_u32 s60, s60, 0x0
s_lshr_b32 s61, s29, 7                             // s61 = s29 / 128
s_and_b32 s58, 127, s29                            // s58 = s29 % 128
s_addc_u32 s61, s61, 0x0
s_mul_i32 s60, s60, s61
s_mul_i32 s60, s60, s30
s_and_b32 s56, s[sgprGSU], 0x3fff                  // Restore GSU
s_mul_i32 s60, s60, s56
s_add_u32 s63, s63, s60

/* Grouped Gemm:: gemmIndex found */
label_FOUND:
s_sub_u32 s57, s14, 1
s_sub_u32 s56, s63, s60
s_sub_u32 s[sgprWorkGroup0], s[sgprWorkGroup0], s56
/* Check if custom structure pointer is null */
s_cmp_eq_u32 s[sgprArgType], 2                     // ArgType == 2 ?
s_cbranch_scc1 label_LoadExternalStruct            // branch if ArgType == 2

/* Grouped Gemm: offset argument address to gemm */
/* Grouped Gemm: offset address from wg_table_start to args_start */
s_lshl2_add_u32 s[sgprKernArgAddress], s26, s[sgprKernArgAddress]
s_addc_u32 s[sgprKernArgAddress+1], s[sgprKernArgAddress+1], 0x0
/* Grouped Gemm: offset address from args_start to gemm_start */
s_mul_i32 s57, s57, 132
s_add_u32 s[sgprKernArgAddress], s[sgprKernArgAddress], s57
s_addc_u32 s[sgprKernArgAddress+1], s[sgprKernArgAddress+1], 0x0

/* Load Kernel Args */
s_load_dwordx16 s[32:47], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x10
s_load_dwordx2 s[48:49], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x50
s_branch label_LoadExternalStructEnd
label_LoadExternalStruct:
/* Grouped Gemm: offset address from args_start to gemm_start */
s_mul_i32 s57, s57, 196
s_add_u32 s[sgprKernArgAddress], s[sgprKernArgAddress], s57
s_addc_u32 s[sgprKernArgAddress+1], s[sgprKernArgAddress+1], 0x0
s_load_dwordx16 s[32:47], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x10
s_load_dword s48, s[sgprKernArgAddress:sgprKernArgAddress+1], 0x50
// Read Beta
s_load_dword s49, s[sgprKernArgAddress:sgprKernArgAddress+1], 0x60
label_LoadExternalStructEnd:
/* init: add vgpr [0...80) to pool */
/* init: add vgpr [0...0) to pool */
/* init: add agpr [0...64) to pool */

/******************************************/
/* Local Read Addresses                   */
/******************************************/

/* local read addresses: tile assignments a/b */
/* lr1J */
v_and_b32 v1, 63, v[vgprSerial]                    // 0. thread id in wave: wtid = tid % wavelength(64)
v_and_b32 v0, 15, v1                               // 1. N offset: nIdx = wtid % MI_N(16)
v_lshlrev_b32 v0, 0x6, v0                          // 1. N offset: nOffset = nIdx * nStride(64)
/* Skip. 2. block offset: bnOffset = 0 when num1DBlocks = 1 */
v_lshlrev_b32 v0, 0x3, v0                          // 4. apply VectorWidth: bnOffset = bnOffset * vw(8)
v_lshrrev_b32 v1, 4, v1                            // 5. K offset: kIdx = wtid / (MIN(16) * MIBB(1))
v_lshlrev_b32 v1, 0x3, v1                          // 5. K offset: lrKOffset = kIdx * mStride(8)
v_add_u32 v0, v1, v0                               // 6. offset in wave: lrOffset = bnOffset + lrKOffset

/* local read addresses: final offsets a */

/* local read addresses: final offsets b */
v_lshrrev_b32 v1, 6, v[vgprSerial]                 // v1 = v[vgprSerial] / 64
v_lshrrev_b32 v1, 2, v1                            // LSU offset: Get LSU wave_id
s_mov_b32 s56, 64                                  // LSU offset: stride = lsuStride(64) when umlds==True
v_mul_lo_u32 v1, s56, v1                           // LSU offset: lsuoffset = wave_id*lsuStride*(MT1+PAD)
v_add_lshl_u32 v[vgprLocalReadAddrB], v1, v0, 0x1  // Final Offset: offset = (lro1+lsuoffset)*bpeDS
v_lshrrev_b32 v2, 10, v[vgprLocalReadAddrB]        // Final Offset: padding 32 per block 1024
v_lshlrev_b32 v2, 0x5, v2                          // Final Offset: padding 32 per block 1024
v_add_u32 v[vgprLocalReadAddrB], v2, v[vgprLocalReadAddrB] // Final Offset: add padding 32 per block 1024

/* local read addresses: declare addresses a */
/* N/A */

/* local read addresses: declare addresses b */

/******************************************/
/* Local Write Addresses                  */
/******************************************/
/* LVCA = 4 */
/* v1 = A-unroll = serial/LVCA */
/* TileAssignment for DirectToVgprA */
v_and_b32 v1, 63, v[vgprSerial]                    // 0. thread id in wave: wtid = tid % wavelength(64)
v_lshrrev_b32 v0, 6, v[vgprSerial]                 // 7. wave offset in N dimen: wtid = tid / dividedForWaveId(64)
v_lshrrev_b32 v4, 5, s[sgprSizesSum]               // numKr = DimK / 32
v_mul_u32_u24 v0, v4, v0                           // wtid * numKr
                                                   // 1. N offset: nOffset = nIdx * nStride(1) (multiplier is 1, do nothing)
/* Skip. 2. block offset: bnOffset = 0 when num1DBlocks = 1 */
                                                   // 4. apply VectorWidth: bnOffset = bnOffset * vw(1) (multiplier is 1, do nothing)
//v_lshrrev_b32 v1, 4, v1                            // 5. K offset: kIdx = wtid / (MIN(16) * MIBB(1))
//v_lshrrev_b32 v4, 6, v[vgprSerial]                 // 7. wave offset in N dimen: wtid = tid / dividedForWaveId(64)
//v_and_b32 v4, 3, v4                                // 7. wave offset in M dimen: wtid0 = wtid / num1DWaves(4)
//v_lshlrev_b32 v4, 0x4, v4                          // 7. wave offset in M dimen: wOffset = wtid0 * W0Stride(16)
//v_add_u32 v0, v4, v0                               // 7. final local read offset: flrOffset = lrOffset + WOffset
/* unroll *= glvw */
v_lshlrev_b32 v1, 0x3, v1                          // v1 = v1 * 8
v_mov_b32 v4, v1                                   // copy for GlobalSplitU
/* LVCB = 8 */
/* v3 = B-unroll = serial%LVCB */
v_lshrrev_b32 v2, 3, v[vgprSerial]                 // v2 = v[vgprSerial] / 8
v_and_b32 v3, 7, v[vgprSerial]                     // v3 = v[vgprSerial] % 8
/* unroll *= glvw */
v_lshlrev_b32 v3, 0x3, v3                          // v3 = v3 * 8
v_mov_b32 v5, v3                                   // copy for GlobalSplitU
/* lwaUnrollAssignmentA = v4 */
/* lwaUnrollAssignmentB = v5 */

/* local write addresses: first offset a */

/* local write addresses: first offset b */
v_mul_u32_u24 v[vgprLocalWriteAddrB], 0x40, v2     // lwBL**(DepthU_Compute + PAD)
v_add_lshl_u32 v[vgprLocalWriteAddrB], v5, v[vgprLocalWriteAddrB], 0x1 // lwFOB = (lwBB + lwBL*(DepthU+PAD))*bpeDS
v_lshrrev_b32 v6, 10, v[vgprLocalWriteAddrB]       // padding 32 per block 1024
v_lshlrev_b32 v6, 0x5, v6                          // padding 32 per block 1024
v_add_u32 v[vgprLocalWriteAddrB], v6, v[vgprLocalWriteAddrB] // add padding 32 per block 1024
v_mov_b32 v8, MT0                                  // set MT0 into sgpr
v_mov_b32 v7, s[sgprSizesFree+0]                   // set Free0 size
v_cvt_f32_u32 v6, v8                               // v6 = ceil(v7 / v8)
v_rcp_iflag_f32 v6, v6                             // v6 = ceil(v7 / v8)
v_cvt_f32_u32 v9, v7                               // v6 = ceil(v7 / v8)
v_mul_f32 v6, v6, v9                               // v6 = ceil(v7 / v8)
v_cvt_u32_f32 v6, v6                               // v6 = ceil(v7 / v8)
v_mul_u32_u24 v9, v6, v8                           // v6 = ceil(v7 / v8)
v_sub_u32 v9, v7, v9                               // v6 = ceil(v7 / v8)
v_cmp_ne_u32 vcc, v9, 0                            // v6 = ceil(v7 / v8)
v_addc_co_u32 v6, vcc, v6, 0, vcc                  // ceil
v_mov_b32 v8, MT1                                  // set MT1 into sgpr
v_mov_b32 v7, s[sgprSizesFree+1]                   // set Free1 size
v_readfirstlane_b32 s[sgprNumWorkGroups0], v6      // set back to numWorkGroup0
v_cvt_f32_u32 v6, v8                               // v6 = ceil(v7 / v8)
v_rcp_iflag_f32 v6, v6                             // v6 = ceil(v7 / v8)
v_cvt_f32_u32 v9, v7                               // v6 = ceil(v7 / v8)
v_mul_f32 v6, v6, v9                               // v6 = ceil(v7 / v8)
v_cvt_u32_f32 v6, v6                               // v6 = ceil(v7 / v8)
v_mul_u32_u24 v9, v6, v8                           // v6 = ceil(v7 / v8)
v_sub_u32 v9, v7, v9                               // v6 = ceil(v7 / v8)
v_cmp_ne_u32 vcc, v9, 0                            // v6 = ceil(v7 / v8)
v_addc_co_u32 v6, vcc, v6, 0, vcc                  // ceil
s_nop 0                                            // 1 wait states
v_readfirstlane_b32 s[sgprNumWorkGroups1], v6      // set back to numWorkGroup1
s_waitcnt lgkmcnt(0)                               // wait for 44/0 bytes of kern args

/* Early stop if N(SizeFreeJ) == 0 */
s_cmp_eq_u32 s[sgprSizeJ], 0x0
s_cbranch_scc0 label_NoEarlyStop_N0
label_EarlyStop_if_N_is_0:
s_endpgm
label_NoEarlyStop_N0:

/* remap wg from 1D(idxWG012) to 3D(wg2,wg1,wg0) */
/* wg2 = idxWG012 * smallMagicNumber(1/(numWG0*numWG1)) */
s_mul_i32 s56, s[sgprNumWorkGroups0], s[sgprNumWorkGroups1]
s_and_b32 s57, s[sgprGSU], 0x3fff                  // Restore GSU
s_mul_i32 s56, s56, s57
v_cvt_f32_u32 v6, s56                              // s56 = s[sgprWorkGroup0] / s56
v_rcp_iflag_f32 v6, v6                             // s56 = s[sgprWorkGroup0] / s56
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // s56 = s[sgprWorkGroup0] / s56
v_mul_f32 v6, v6, v7                               // s56 = s[sgprWorkGroup0] / s56
v_cvt_u32_f32 v6, v6                               // s56 = s[sgprWorkGroup0] / s56
v_mul_u32_u24 v7, v6, s56                          // s56 = s[sgprWorkGroup0] / s56
v_sub_u32 v7, s[sgprWorkGroup0], v7                // s56 = s[sgprWorkGroup0] / s56
v_cmpx_eq_u32 exec, v7, s56                        // s56 = s[sgprWorkGroup0] / s56
v_add_u32 v6, 1, v6                                // s56 = s[sgprWorkGroup0] / s56
s_mov_b64 exec, -1                                 // s56 = s[sgprWorkGroup0] / s56
v_readfirstlane_b32 s56, v6                        // quotient
s_mov_b32 s[sgprWorkGroup2], s56
/* idxWG01 = idxWG012 - wg2 * numWG0 * numWG1 */
s_mul_i32 s56, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s56, s56, s[sgprWorkGroup2]
s_mul_i32 s56, s56, s57
s_sub_u32 s[sgprWorkGroup0], s[sgprWorkGroup0], s56
/* wg1 = idxWG01 * smallMagicNumber(1/numWG0) */
v_cvt_f32_u32 v6, s[sgprNumWorkGroups0]            // s56 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_rcp_iflag_f32 v6, v6                             // s56 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // s56 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_mul_f32 v6, v6, v7                               // s56 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_cvt_u32_f32 v6, v6                               // s56 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_mul_u32_u24 v7, v6, s[sgprNumWorkGroups0]        // s56 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_sub_u32 v7, s[sgprWorkGroup0], v7                // s56 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_cmpx_eq_u32 exec, v7, s[sgprNumWorkGroups0]      // s56 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_add_u32 v6, 1, v6                                // s56 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
s_mov_b64 exec, -1                                 // s56 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_readfirstlane_b32 s56, v6                        // quotient
s_mov_b32 s[sgprWorkGroup1], s56
/* wg0 = idxWG01 - wg1 * numWG0 */
s_mul_i32 s56, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_sub_u32 s[sgprWorkGroup0], s[sgprWorkGroup0], s56

/* Early stop if wg exceed */
s_cmp_ge_u32 s[sgprWorkGroup2], s[sgprSizesFree+2]
s_cbranch_scc0 label_NoEarlyStop_wgExceed
label_EarlyStop_if_wg_exceed:
s_endpgm
label_NoEarlyStop_wgExceed:

label_MultiGemmEnd:
.set sgprSrdA, 56
.set sgprSrdB, 60
.set sgprShadowLimitA, 26
.set sgprShadowLimitB, 64
.set sgprStaggerUIter, 66
.set sgprStaggerUIterDTV, 67
.set sgprWrapUA, 68
.set sgprWrapUB, 70
.set sgprGlobalReadIncsA, 72
.set sgprGlobalReadIncsB, 73
s_sub_u32 s[sgprAddressA+0], s[sgprAddressA+0], 16 // pre-pad to make room for possible pointer shift
s_subb_u32 s[sgprAddressA+1], s[sgprAddressA+1], 0 // pre-pad to make room for possible pointer shift
s_sub_u32 s[sgprAddressB+0], s[sgprAddressB+0], 16 // pre-pad to make room for possible pointer shift
s_subb_u32 s[sgprAddressB+1], s[sgprAddressB+1], 0 // pre-pad to make room for possible pointer shift

/* Short circuit condition if Alpha == 0, then sumDims=0 */
v_cmp_eq_f32 vcc, s[sgprAlpha], 0.0                // s[Alpha] == 0.0f ?
s_cbranch_vccz label_AlphaNonZero                  // branch if s[Alpha] != 0
s_mov_b32 s[sgprSizesSum+0], 0x0                   // Set summation dim=0 if Alpha == 0
label_AlphaNonZero:

/******************************************/
/* Begin setupNewTile                     */
/******************************************/

/* global read addresses: work-group */
/* graWorkGroup mapping */
s_and_b32 s74, s[sgprGSU], 0x3fff                  // Restore GSU
s_cmp_eq_u32 s74, 1                                // GSU == 1 ?
s_cbranch_scc1 label_GSU                           // branch if GSU == 1
/* Check if custom structure pointer is null */
s_cmp_eq_u32 s[sgprArgType], 2                     // ArgType == 2 ?
s_cbranch_scc0 label_LoadExternalEpilogueStruct_3
s_mov_b64 s[sgprWSDstart:sgprWSDstart+1], s[sgprAddressD:sgprAddressD+1]
s_mov_b64 s[sgprAddressD:sgprAddressD+1], s[sgprAddressTD:sgprAddressTD+1]
s_mov_b64 s[sgprAddressTD:sgprAddressTD+1], s[sgprWSDstart:sgprWSDstart+1]
label_LoadExternalEpilogueStruct_3:
// GSU-not-WGMapRR :nwg1 = (size1J + MT1J - 1) / MT1J;
s_and_b32 s74, s[sgprGSU], 0x4000                  // SCC = (GSUWGMRR == 1) ?
s_cbranch_scc1 label_GSUWGMRR                      // branch if GSUWGMRR == 1
s_and_b32 s74, s[sgprGSU], 0x3fff                  // Restore GSU
v_cvt_f32_u32 v6, s74                              // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s74
v_rcp_iflag_f32 v6, v6                             // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s74
v_cvt_f32_u32 v7, s[sgprWorkGroup1]                // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s74
v_mul_f32 v6, v6, v7                               // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s74
v_cvt_u32_f32 v6, v6                               // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s74
v_mul_u32_u24 v7, v6, s74                          // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s74
v_sub_u32 v7, s[sgprWorkGroup1], v7                // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s74
v_cmpx_eq_u32 exec, v7, s74                        // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s74
v_add_u32 v6, 1, v6                                // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s74
v_mov_b32 v7, 0                                    // s[sgprGSUSumIdx] = s[sgprWorkGroup1] % s74
s_mov_b64 exec, -1                                 // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s74
v_readfirstlane_b32 s[sgprWorkGroup1], v6          // quotient
v_readfirstlane_b32 s[sgprGSUSumIdx], v7           // remainder
s_branch label_GSUWGMRR_End
label_GSUWGMRR:
v_cvt_f32_u32 v6, s[sgprNumWorkGroups1]            // s[sgprGSUSumIdx] = s[sgprWorkGroup1] / s[sgprNumWorkGroups1]
v_rcp_iflag_f32 v6, v6                             // s[sgprGSUSumIdx] = s[sgprWorkGroup1] / s[sgprNumWorkGroups1]
v_cvt_f32_u32 v7, s[sgprWorkGroup1]                // s[sgprGSUSumIdx] = s[sgprWorkGroup1] / s[sgprNumWorkGroups1]
v_mul_f32 v6, v6, v7                               // s[sgprGSUSumIdx] = s[sgprWorkGroup1] / s[sgprNumWorkGroups1]
v_cvt_u32_f32 v6, v6                               // s[sgprGSUSumIdx] = s[sgprWorkGroup1] / s[sgprNumWorkGroups1]
v_mul_u32_u24 v7, v6, s[sgprNumWorkGroups1]        // s[sgprGSUSumIdx] = s[sgprWorkGroup1] / s[sgprNumWorkGroups1]
v_sub_u32 v7, s[sgprWorkGroup1], v7                // s[sgprGSUSumIdx] = s[sgprWorkGroup1] / s[sgprNumWorkGroups1]
v_cmpx_eq_u32 exec, v7, s[sgprNumWorkGroups1]      // s[sgprGSUSumIdx] = s[sgprWorkGroup1] / s[sgprNumWorkGroups1]
v_add_u32 v6, 1, v6                                // s[sgprGSUSumIdx] = s[sgprWorkGroup1] / s[sgprNumWorkGroups1]
v_mov_b32 v7, 0                                    // s[sgprWorkGroup1] = s[sgprWorkGroup1] % s[sgprNumWorkGroups1]
s_mov_b64 exec, -1                                 // s[sgprGSUSumIdx] = s[sgprWorkGroup1] / s[sgprNumWorkGroups1]
v_readfirstlane_b32 s[sgprGSUSumIdx], v6           // quotient
v_readfirstlane_b32 s[sgprWorkGroup1], v7          // remainder
label_GSUWGMRR_End:
s_mov_b32 s[sgprGSULog2BpeC], 1
s_mov_b32 s[sgprGSULog2BpeD], 2
s_branch label_GSU_End
label_GSU:
s_mov_b64 s[sgprGSUSumIdx:sgprGSUSumIdx+1], 0      // Set GSUSumIdx to 0
s_mov_b32 s[sgprGSULog2BpeC], 1
s_mov_b32 s[sgprGSULog2BpeD], 1
label_GSU_End:
s_sext_i32_i16 s[sgprWGM], s[sgprWGM]              // Restore WGM
s_cmp_gt_i32 s[sgprWGM], 1                         // WGM > 1 ?
s_cbranch_scc1 label_WGMPositive                   // branch if WGM > 1
s_cmp_ge_i32 s[sgprWGM], 0                         // WGM >= 0 ?
s_cbranch_scc1 label_WGM                           // branch if WGM >= 0
s_abs_i32 s[sgprWGM], s[sgprWGM]                   // abs(WGM)
v_cvt_f32_u32 v6, s[sgprWGM]                       // WGM
v_rcp_iflag_f32 v6, v6                             // WGM
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // WGM
v_mul_f32 v6, v6, v7                               // WGM
v_cvt_u32_f32 v6, v6                               // WGM
v_mul_u32_u24 v7, v6, s[sgprWGM]                   // WGM
v_sub_u32 v7, s[sgprWorkGroup0], v7                // WGM
v_cmpx_eq_u32 exec, v7, s[sgprWGM]                 // WGM
v_add_u32 v6, 1, v6                                // WGM
s_mov_b64 exec, -1                                 // WGM
v_readfirstlane_b32 s76, v6                        // quotient
s_mul_i32 s77, s76, s[sgprWGM]                     // quotient * non-magic divisor
s_sub_u32 s77, s[sgprWorkGroup0], s77              // WorkGroup0=remainder
s_mul_i32 s77, s77, s[sgprNumWorkGroups1]          // (wg1 % WGM)*NumWorkGroups1
s_add_u32 s77, s77, s[sgprWorkGroup1]              // wgSerial = wg0 + (wg1 % WGM)*NumWorkGroups1
v_cvt_f32_u32 v6, s[sgprWGM]                       // WGM
v_rcp_iflag_f32 v6, v6                             // WGM
v_cvt_f32_u32 v7, s[sgprNumWorkGroups0]            // WGM
v_mul_f32 v6, v6, v7                               // WGM
v_cvt_u32_f32 v6, v6                               // WGM
v_mul_u32_u24 v7, v6, s[sgprWGM]                   // WGM
v_sub_u32 v7, s[sgprNumWorkGroups0], v7            // WGM
v_cmpx_eq_u32 exec, v7, s[sgprWGM]                 // WGM
v_add_u32 v6, 1, v6                                // WGM
s_mov_b64 exec, -1                                 // WGM
v_readfirstlane_b32 s74, v6                        // quotient
s_mul_i32 s75, s[sgprWGM], s74                     // quotient * non-magic divisor
s_sub_u32 s75, s[sgprNumWorkGroups0], s75          // NumWorkGroups0=remainder
s_cmp_eq_u32 s75, 0                                // remainder == 0 ?
s_cmov_b32 s75, s[sgprWGM]                         // remainder = WGM if remainder == 0
s_cmp_ge_u32 s76, s74                              // blockId >= numFullBlocks ?
s_cselect_b32 s74, s75, s[sgprWGM]
v_cvt_f32_u32 v6, s74                              // s[sgprWorkGroup1] = s77 / s74
v_rcp_iflag_f32 v6, v6                             // s[sgprWorkGroup1] = s77 / s74
v_cvt_f32_u32 v7, s77                              // s[sgprWorkGroup1] = s77 / s74
v_mul_f32 v6, v6, v7                               // s[sgprWorkGroup1] = s77 / s74
v_cvt_u32_f32 v6, v6                               // s[sgprWorkGroup1] = s77 / s74
v_mul_u32_u24 v7, v6, s74                          // s[sgprWorkGroup1] = s77 / s74
v_sub_u32 v7, s77, v7                              // s[sgprWorkGroup1] = s77 / s74
v_cmpx_eq_u32 exec, v7, s74                        // s[sgprWorkGroup1] = s77 / s74
v_add_u32 v6, 1, v6                                // s[sgprWorkGroup1] = s77 / s74
v_mov_b32 v7, 0                                    // s[sgprWorkGroup0] = s77 % s74
s_mov_b64 exec, -1                                 // s[sgprWorkGroup1] = s77 / s74
v_readfirstlane_b32 s[sgprWorkGroup1], v6          // quotient
v_readfirstlane_b32 s[sgprWorkGroup0], v7          // remainder
s_mul_i32 s[sgprWorkGroup0], s[sgprWorkGroup1], s74 // quotient * non-magic divisor
s_sub_u32 s[sgprWorkGroup0], s77, s[sgprWorkGroup0] // WorkGroup0=remainder
s_mul_i32 s76, s76, s[sgprWGM]                     // blockId * WGM
s_add_u32 s[sgprWorkGroup0], s[sgprWorkGroup0], s76 // wg1 += blockId * WGM
s_branch label_WGM
label_WGMPositive:
v_cvt_f32_u32 v6, s[sgprWGM]                       // WGM
v_rcp_iflag_f32 v6, v6                             // WGM
v_cvt_f32_u32 v7, s[sgprWorkGroup1]                // WGM
v_mul_f32 v6, v6, v7                               // WGM
v_cvt_u32_f32 v6, v6                               // WGM
v_mul_u32_u24 v7, v6, s[sgprWGM]                   // WGM
v_sub_u32 v7, s[sgprWorkGroup1], v7                // WGM
v_cmpx_eq_u32 exec, v7, s[sgprWGM]                 // WGM
v_add_u32 v6, 1, v6                                // WGM
s_mov_b64 exec, -1                                 // WGM
v_readfirstlane_b32 s76, v6                        // quotient
s_mul_i32 s77, s76, s[sgprWGM]                     // quotient * non-magic divisor
s_sub_u32 s77, s[sgprWorkGroup1], s77              // WorkGroup1=remainder
s_mul_i32 s77, s77, s[sgprNumWorkGroups0]          // (wg1 % WGM)*NumWorkGroups0
s_add_u32 s77, s77, s[sgprWorkGroup0]              // wgSerial = wg0 + (wg1 % WGM)*NumWorkGroups0
v_cvt_f32_u32 v6, s[sgprWGM]                       // WGM
v_rcp_iflag_f32 v6, v6                             // WGM
v_cvt_f32_u32 v7, s[sgprNumWorkGroups1]            // WGM
v_mul_f32 v6, v6, v7                               // WGM
v_cvt_u32_f32 v6, v6                               // WGM
v_mul_u32_u24 v7, v6, s[sgprWGM]                   // WGM
v_sub_u32 v7, s[sgprNumWorkGroups1], v7            // WGM
v_cmpx_eq_u32 exec, v7, s[sgprWGM]                 // WGM
v_add_u32 v6, 1, v6                                // WGM
s_mov_b64 exec, -1                                 // WGM
v_readfirstlane_b32 s74, v6                        // quotient
s_mul_i32 s75, s[sgprWGM], s74                     // quotient * non-magic divisor
s_sub_u32 s75, s[sgprNumWorkGroups1], s75          // NumWorkGroups1=remainder
s_cmp_eq_u32 s75, 0                                // remainder == 0 ?
s_cmov_b32 s75, s[sgprWGM]                         // remainder = WGM if remainder == 0
s_cmp_ge_u32 s76, s74                              // blockId >= numFullBlocks ?
s_cselect_b32 s74, s75, s[sgprWGM]
v_cvt_f32_u32 v6, s74                              // s[sgprWorkGroup0] = s77 / s74
v_rcp_iflag_f32 v6, v6                             // s[sgprWorkGroup0] = s77 / s74
v_cvt_f32_u32 v7, s77                              // s[sgprWorkGroup0] = s77 / s74
v_mul_f32 v6, v6, v7                               // s[sgprWorkGroup0] = s77 / s74
v_cvt_u32_f32 v6, v6                               // s[sgprWorkGroup0] = s77 / s74
v_mul_u32_u24 v7, v6, s74                          // s[sgprWorkGroup0] = s77 / s74
v_sub_u32 v7, s77, v7                              // s[sgprWorkGroup0] = s77 / s74
v_cmpx_eq_u32 exec, v7, s74                        // s[sgprWorkGroup0] = s77 / s74
v_add_u32 v6, 1, v6                                // s[sgprWorkGroup0] = s77 / s74
v_mov_b32 v7, 0                                    // s[sgprWorkGroup1] = s77 % s74
s_mov_b64 exec, -1                                 // s[sgprWorkGroup0] = s77 / s74
v_readfirstlane_b32 s[sgprWorkGroup0], v6          // quotient
v_readfirstlane_b32 s[sgprWorkGroup1], v7          // remainder
s_mul_i32 s[sgprWorkGroup1], s[sgprWorkGroup0], s74 // quotient * non-magic divisor
s_sub_u32 s[sgprWorkGroup1], s77, s[sgprWorkGroup1] // WorkGroup1=remainder
s_mul_i32 s76, s76, s[sgprWGM]                     // blockId * WGM
s_add_u32 s[sgprWorkGroup1], s[sgprWorkGroup1], s76 // wg1 += blockId * WGM
label_WGM:

/* global read addresses: tile offset assignment a */
/* graTileAssignmentA = v0 */

/* global read addresses: tile offset assignment b */
/* graTileAssignmentB = v2 */

/* global read addresses: unroll assignment a */
/* v1 */

/* global read addresses: unroll assignment b */
/* v3 */

/* global read addresses: other free assignments */
/* s[sgprWorkGroup2] */

/* global read addresses: tile offsets a */
v_mov_b32 v6, v0                                   // groA0I_0
s_lshr_b32 s79, s[sgprSizesSum], 5                 // # of Kr
s_lshl_b32 s79, s79, 2
v_add_co_u32 v7, vcc, s79, v6                       // groA0I_1 += LSPA
//v_add_co_u32 v7, vcc, 64, v6                       // groA0I_1 += LSPA

/* global read addresses: tile offsets b */
v_mov_b32 v8, v2                                   // groB1J_0
v_add_co_u32 v9, vcc, 32, v8                       // groB1J_1 += LSPB
v_add_co_u32 v10, vcc, 32, v9                      // groB1J_2 += LSPB
v_add_co_u32 v11, vcc, 32, v10                     // groB1J_3 += LSPB

/* global read addresses: unroll offsets a */
v_mov_b32 v12, v1                                  // groAL_0
v_add_co_u32 v13, vcc, 16*32, v12                     // groAL_1 + LSCA

/* global read addresses: unroll offsets b */
v_mov_b32 v14, v3                                  // groBL_0

/* global read addresses: final offsets a */
//GLOBAL_OFFSET_A vgprGlobalReadOffsetA+0, 12,  6, 15 // gROA_0_0_0_0
//GLOBAL_OFFSET_A vgprGlobalReadOffsetA+1, 12,  7, 15 // gROA_0_0_1_0
//GLOBAL_OFFSET_A vgprGlobalReadOffsetA+2, 13,  6, 15 // gROA_1_0_0_0
//GLOBAL_OFFSET_A vgprGlobalReadOffsetA+3, 13,  7, 15 // gROA_1_0_1_0
s_mov_b32 s[sgprSwizzleStrideA] 16*32
GLOBAL_SWIZZLED_OFFSET_A vgprGlobalReadOffsetA+0, 12,  6, 15 // gROA_0_0_0_0
GLOBAL_SWIZZLED_OFFSET_A vgprGlobalReadOffsetA+1, 12,  7, 15 // gROA_0_0_1_0
GLOBAL_SWIZZLED_OFFSET_A vgprGlobalReadOffsetA+2, 13,  6, 15 // gROA_1_0_0_0
GLOBAL_SWIZZLED_OFFSET_A vgprGlobalReadOffsetA+3, 13,  7, 15 // gROA_1_0_1_0

/* global read addresses: final offsets b */
GLOBAL_OFFSET_B vgprGlobalReadOffsetB+0, 14,  8, 15 // gROB_0_0_0_0
GLOBAL_OFFSET_B vgprGlobalReadOffsetB+1, 14,  9, 15 // gROB_0_0_1_0
GLOBAL_OFFSET_B vgprGlobalReadOffsetB+2, 14, 10, 15 // gROB_0_0_2_0
GLOBAL_OFFSET_B vgprGlobalReadOffsetB+3, 14, 11, 15 // gROB_0_0_3_0

/* global read addresses: addresses a */
/* max read offset = size[n] * stride[n-1] */
s_mul_hi_u32 s77, s[sgprWorkGroup0], 128           // WorkGroup[01] * MT
s_mul_i32 s76, s[sgprWorkGroup0], 128              // WorkGroup[01] * MT
s_mul_hi_u32 s77, s76, s[sgprStrideA0I]            // tlu=0, scaled tile-offset by stride
s_mul_i32 s76, s76, s[sgprStrideA0I]               // tlu=0, scaled tile-offset by stride
s_and_b32 s74, s[sgprGSU], 0x8000                  // SCC = (GSUC == 1) ?
s_cbranch_scc1 label_GSUC_A                        // branch if GSUC == 1
s_mul_hi_u32 s75, 64, s[sgprGSUSumIdx]             // gsuOffset = DepthU*GSUSumIdx
s_mul_i32 s74, 64, s[sgprGSUSumIdx]                // gsuOffset = DepthU*GSUSumIdx
s_branch label_GSUC_A_End
label_GSUC_A:
s_lshr_b32 s[sgprLoopCounterL], s[sgprSizesSum], 6 // s[LoopCounterL] = s[sgprSizesSum] / 64
s_and_b32 s[sgprGSUSumIdx+1], s[sgprGSU], 0x3fff   // Restore GSU
v_cvt_f32_u32 v0, s[sgprGSUSumIdx+1]               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_rcp_iflag_f32 v0, v0                             // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_cvt_f32_u32 v1, s[sgprLoopCounterL]              // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_mul_f32 v0, v0, v1                               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_cvt_u32_f32 v0, v0                               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_mul_u32_u24 v1, v0, s[sgprGSUSumIdx+1]           // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_sub_u32 v1, s[sgprLoopCounterL], v1              // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_cmpx_eq_u32 exec, v1, s[sgprGSUSumIdx+1]         // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_add_u32 v0, 1, v0                                // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_mov_b32 v1, 0                                    // s[sgprGSUSumIdx+1] = s[sgprLoopCounterL] % s[sgprGSUSumIdx+1]
s_mov_b64 exec, -1                                 // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_readfirstlane_b32 s[sgprLoopCounterL], v0        // quotient
v_readfirstlane_b32 s[sgprGSUSumIdx+1], v1         // remainder
s_mul_i32 s75, s[sgprLoopCounterL], s[sgprGSUSumIdx] // quotient*GSUSumIdx
s_add_u32 s74, 1, s[sgprLoopCounterL]              // quotient+1
s_add_u32 s75, s75, s[sgprGSUSumIdx+1]             // quotient*GSUSumIdx+remainder
s_mul_i32 s74, s74, s[sgprGSUSumIdx]               // (quotient+1)*GSUSumIdx
s_cmp_lt_u32 s[sgprGSUSumIdx], s[sgprGSUSumIdx+1]  // gsuSumIdx < numIterPerWgRemainder
s_cselect_b32 s74, s74, s75                        // (quotient+1)*GSUSumIdx if needed
s_mul_hi_u32 s75, s74, 64                          // gsuOffset = DepthU*accumulatedNumOfLoopCounterL
s_mul_i32 s74, s74, 64                             // gsuOffset = DepthU*accumulatedNumOfLoopCounterL
label_GSUC_A_End:
s_add_u32 s76, s76, s74                            // accum GsuOffset term to tilestart
s_addc_u32 s77, s77, s75                           // accum GsuOffset term to tilestart
s_mov_b32 s[sgprShadowLimitA+0], 1                 // Init tensor size
s_mov_b32 s[sgprShadowLimitA+1], 0                 // init tensor size
s_sub_u32 s74, s[sgprSizeL], 1                     // (size-1)
s_mul_hi_u32 s75, constStrideAL, s74               // stride x (size-1)
s_mul_i32 s74, constStrideAL, s74                  // stride x (size-1)
s_add_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s74 // sum tensor size
s_addc_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s75 // sum tensor size
s_sub_u32 s74, s[sgprSizeI], 1                     // (size-1)
s_mul_hi_u32 s75, s[sgprStrideA0I], s74            // stride x (size-1)
s_mul_i32 s74, s[sgprStrideA0I], s74               // stride x (size-1)
s_add_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s74 // sum tensor size
s_addc_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s75 // sum tensor size
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s76 // sub tileStart
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s77 // sub tileStart
s_lshl_b64 s[sgprShadowLimitA:sgprShadowLimitA+1], s[sgprShadowLimitA:sgprShadowLimitA+1], 0x1 // Set limit to use bytes
s_add_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], 16 // extend limit for pre-pad
s_addc_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], 0 // extend limit for pre-pad
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
s_mul_hi_u32 s75, s[sgprStrideAK], s[sgprWorkGroup2] // Stride*WG
s_mul_i32 s74, s[sgprStrideAK], s[sgprWorkGroup2]  // Stride*WG
s_add_u32 s76, s76, s74                            // accum wg term to tilestart
s_addc_u32 s77, s77, s75                           // accum wg term to tilestart
s_lshl_b64 s[76:77], s[76:77], 0x1                 // tileStart *= BPE
s_add_u32 s[sgprSrdA+0], s[sgprAddressA+0], s76    // SRD base = Address+ tileStart0
s_addc_u32 s[sgprSrdA+1], s[sgprAddressA+1], s77   // SRD base = Address+ tileStart1
s_mov_b32 s[sgprSrdA+3], Srd127_96                 // Set bits 127_96 in SRD

/* global read addresses: addresses b */
/* max read offset = size[n] * stride[n-1] */
s_mul_hi_u32 s77, s[sgprWorkGroup1], 128           // WorkGroup[01] * MT
s_mul_i32 s76, s[sgprWorkGroup1], 128              // WorkGroup[01] * MT
s_mul_hi_u32 s77, s76, s[sgprStrideB1J]            // tlu=0, scaled tile-offset by stride
s_mul_i32 s76, s76, s[sgprStrideB1J]               // tlu=0, scaled tile-offset by stride
s_and_b32 s74, s[sgprGSU], 0x8000                  // SCC = (GSUC == 1) ?
s_cbranch_scc1 label_GSUC_B                        // branch if GSUC == 1
s_mul_hi_u32 s75, 64*16, s[sgprGSUSumIdx]             // gsuOffset = DepthU*GSUSumIdx
s_mul_i32 s74, 64*16, s[sgprGSUSumIdx]                // gsuOffset = DepthU*GSUSumIdx
s_branch label_GSUC_B_End
label_GSUC_B:
s_lshr_b32 s[sgprLoopCounterL], s[sgprSizesSum], 6 // s[LoopCounterL] = s[sgprSizesSum] / 64
s_and_b32 s[sgprGSUSumIdx+1], s[sgprGSU], 0x3fff   // Restore GSU
v_cvt_f32_u32 v0, s[sgprGSUSumIdx+1]               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_rcp_iflag_f32 v0, v0                             // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_cvt_f32_u32 v1, s[sgprLoopCounterL]              // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_mul_f32 v0, v0, v1                               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_cvt_u32_f32 v0, v0                               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_mul_u32_u24 v1, v0, s[sgprGSUSumIdx+1]           // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_sub_u32 v1, s[sgprLoopCounterL], v1              // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_cmpx_eq_u32 exec, v1, s[sgprGSUSumIdx+1]         // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_add_u32 v0, 1, v0                                // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_mov_b32 v1, 0                                    // s[sgprGSUSumIdx+1] = s[sgprLoopCounterL] % s[sgprGSUSumIdx+1]
s_mov_b64 exec, -1                                 // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_readfirstlane_b32 s[sgprLoopCounterL], v0        // quotient
v_readfirstlane_b32 s[sgprGSUSumIdx+1], v1         // remainder
s_mul_i32 s75, s[sgprLoopCounterL], s[sgprGSUSumIdx] // quotient*GSUSumIdx
s_add_u32 s74, 1, s[sgprLoopCounterL]              // quotient+1
s_add_u32 s75, s75, s[sgprGSUSumIdx+1]             // quotient*GSUSumIdx+remainder
s_mul_i32 s74, s74, s[sgprGSUSumIdx]               // (quotient+1)*GSUSumIdx
s_cmp_lt_u32 s[sgprGSUSumIdx], s[sgprGSUSumIdx+1]  // gsuSumIdx < numIterPerWgRemainder
s_cselect_b32 s74, s74, s75                        // (quotient+1)*GSUSumIdx if needed
s_mul_hi_u32 s75, s74, 64                          // gsuOffset = DepthU*accumulatedNumOfLoopCounterL
s_mul_i32 s74, s74, 64                             // gsuOffset = DepthU*accumulatedNumOfLoopCounterL
label_GSUC_B_End:
s_add_u32 s76, s76, s74                            // accum GsuOffset term to tilestart
s_addc_u32 s77, s77, s75                           // accum GsuOffset term to tilestart
s_mov_b32 s[sgprShadowLimitB+0], 1                 // Init tensor size
s_mov_b32 s[sgprShadowLimitB+1], 0                 // init tensor size
s_sub_u32 s74, s[sgprSizeL], 1                     // (size-1)
s_mul_hi_u32 s75, constStrideBL, s74               // stride x (size-1)
s_mul_i32 s74, constStrideBL, s74                  // stride x (size-1)
s_add_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s74 // sum tensor size
s_addc_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s75 // sum tensor size
s_sub_u32 s74, s[sgprSizeJ], 1                     // (size-1)
s_mul_hi_u32 s75, s[sgprStrideB1J], s74            // stride x (size-1)
s_mul_i32 s74, s[sgprStrideB1J], s74               // stride x (size-1)
s_add_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s74 // sum tensor size
s_addc_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s75 // sum tensor size
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s76 // sub tileStart
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s77 // sub tileStart
s_lshl_b64 s[sgprShadowLimitB:sgprShadowLimitB+1], s[sgprShadowLimitB:sgprShadowLimitB+1], 0x1 // Set limit to use bytes
s_add_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], 16 // extend limit for pre-pad
s_addc_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], 0 // extend limit for pre-pad
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
s_mul_hi_u32 s75, s[sgprStrideBK], s[sgprWorkGroup2] // Stride*WG
s_mul_i32 s74, s[sgprStrideBK], s[sgprWorkGroup2]  // Stride*WG
s_add_u32 s76, s76, s74                            // accum wg term to tilestart
s_addc_u32 s77, s77, s75                           // accum wg term to tilestart
s_lshl_b64 s[76:77], s[76:77], 0x1                 // tileStart *= BPE
s_add_u32 s[sgprSrdB+0], s[sgprAddressB+0], s76    // SRD base = Address+ tileStart0
s_addc_u32 s[sgprSrdB+1], s[sgprAddressB+1], s77   // SRD base = Address+ tileStart1
s_mov_b32 s[sgprSrdB+3], Srd127_96                 // Set bits 127_96 in SRD

/* global read addresses: increments a */
s_and_b32 s75, s[sgprGSU], 0x3fff                  // Restore GSU
s_mul_i32 s75, s75, DepthU*BpeAGR                  // GSU*DepthU*Bpe
s_mul_i32 s75, 16, s75
s_and_b32 s74, s[sgprGSU], 0x8000                  // SCC = (GSUC == 1) ?
s_cselect_b32 s[sgprGlobalReadIncsA+0], DepthU*BpeAGR, s75 // incrA (unrollIdx)

/* global read addresses: increments b */
s_and_b32 s75, s[sgprGSU], 0x3fff                  // Restore GSU
s_mul_i32 s75, s75, DepthU*BpeBGR                  // GSU*DepthU*Bpe
s_and_b32 s74, s[sgprGSU], 0x8000                  // SCC = (GSUC == 1) ?
s_cselect_b32 s[sgprGlobalReadIncsB+0], DepthU*BpeBGR, s75 // incrB (unrollIdx)
/* declare loop num iterations */
s_lshr_b32 s[sgprLoopCounterL], s[sgprSizesSum+0], 6 // s[sgprLoopCounterL] = s[sgprSizesSum+0] / 64
s_and_b32 s74, s[sgprGSU], 0x3fff                  // Restore GSU
s_cmp_eq_u32 s74, 1                                // GSU == 1 ?
s_cbranch_scc1 label_GSU_1                         // branch if GSU == 1
s_and_b32 s[sgprGSUSumIdx+1], s[sgprGSU], 0x3fff   // Restore GSU
v_cvt_f32_u32 v0, s[sgprGSUSumIdx+1]               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_rcp_iflag_f32 v0, v0                             // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_cvt_f32_u32 v1, s[sgprLoopCounterL]              // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_mul_f32 v0, v0, v1                               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_cvt_u32_f32 v0, v0                               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_mul_u32_u24 v1, v0, s[sgprGSUSumIdx+1]           // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_sub_u32 v1, s[sgprLoopCounterL], v1              // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_cmpx_eq_u32 exec, v1, s[sgprGSUSumIdx+1]         // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_add_u32 v0, 1, v0                                // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_mov_b32 v1, 0                                    // s[sgprGSUSumIdx+1] = s[sgprLoopCounterL] % s[sgprGSUSumIdx+1]
s_mov_b64 exec, -1                                 // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_readfirstlane_b32 s[sgprLoopCounterL], v0        // quotient
v_readfirstlane_b32 s[sgprGSUSumIdx+1], v1         // remainder
s_add_u32 s74, 1, s[sgprLoopCounterL]              // tmp<-numIterMyWg+1
s_cmp_lt_u32 s[sgprGSUSumIdx], s[sgprGSUSumIdx+1]  // gsuSumIdx < numIterPerWgRemainder
s_cmov_b32 s[sgprLoopCounterL], s74                // numIterMyWg++ if needed
label_GSU_1:
s_mov_b32 s[sgprOrigLoopCounter], s[sgprLoopCounterL] // copy loop counter
s_and_b32 s76, s[sgprStaggerU], 0x1f00
s_lshr_b32 s76, s76, 0x8
s_and_b32 s77, s[sgprStaggerU], 0xe000
s_and_b32 s[sgprStaggerU], s[sgprStaggerU], 0xff
s_mov_b32 s74, s[sgprStaggerU]                     // init staggerU
label_beginStaggerUIter:
s_lshl_b32 s75, s74, s76                           // shift by StaggerUStride
s_cmp_ge_u32 s[sgprOrigLoopCounter], s75           // loopCount >= current shift Count
s_cbranch_scc1 label_endStaggerUIter               // jump to end
s_lshr_b32 s74, s74, 1                             // step down to smaller stagger
s_branch label_beginStaggerUIter                   // jump to begin
label_endStaggerUIter:
s_sub_u32 s75, s74, 1                              // staggerU mask
s_cmp_ge_u32 s74, 1                                // if current staggerU >= 1
s_cselect_b32 s[sgprStaggerUIter], s75, 0          // set Mask
s_cmp_eq_u32 s77, 0x0
s_cbranch_scc1 label_StaggerUMapping_1
s_mov_b32 s74, s[sgprWorkGroup0]
s_branch label_staggerInputEnd
label_StaggerUMapping_1:
s_cmp_eq_u32 s77, 0x2000
s_cbranch_scc1 label_StaggerUMapping_2
s_mov_b32 s74, s[sgprWorkGroup1]
s_branch label_staggerInputEnd
label_StaggerUMapping_2:
s_cmp_eq_u32 s77, 0x4000
s_cbranch_scc1 label_StaggerUMapping_3
s_mov_b32 s74, -0x1
s_branch label_staggerInputEnd
label_StaggerUMapping_3:
s_cmp_eq_u32 s77, 0x6000
s_cbranch_scc1 label_StaggerUMapping_4
s_mul_i32 s75, s[sgprNumWorkGroups0], s[sgprWorkGroup1]
s_add_u32 s74, s74, s75
s_add_u32 s74, s74, s[sgprWorkGroup0]
s_branch label_staggerInputEnd
label_StaggerUMapping_4:
s_cmp_eq_u32 s77, 0x8000
s_cbranch_scc1 label_staggerInputEnd
s_mov_b32 s74, -0x1
s_branch label_staggerInputEnd
label_staggerInputEnd:
s_and_b32 s[sgprStaggerUIter], s[sgprStaggerUIter], s74 // Compute actual stagger start for this tile
s_lshl_b32 s[sgprStaggerUIter], s[sgprStaggerUIter], s76 // shift by StaggerUStride

/* SRDs += (StaggerUIter) * GlobalReadIncsA+0 */
s_mul_hi_i32 s75, s[sgprStaggerUIter], s[sgprGlobalReadIncsA+0] //  stagger byte offset
s_mul_i32 s74, s[sgprStaggerUIter], s[sgprGlobalReadIncsA+0] //  stagger byte offset
s_mul_hi_i32 s[sgprWrapUA+1], s[sgprLoopCounterL], s[sgprGlobalReadIncsA+0] // Number of bytes accessed by the unroll loop
s_mul_i32 s[sgprWrapUA+0], s[sgprLoopCounterL], s[sgprGlobalReadIncsA+0] // Number of bytes accessed by the unroll loop
s_sub_u32 s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0], s[sgprWrapUA+0] // remove one iteration
s_subb_u32 s[sgprWrapUA+1], 0, s[sgprWrapUA+1]     // remove one iteration
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s74        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s75       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s74 // limit -= inc)
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s75 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32

/* SRDs += (StaggerUIter) * GlobalReadIncsB+0 */
s_mul_hi_i32 s75, s[sgprStaggerUIter], s[sgprGlobalReadIncsB+0] //  stagger byte offset
s_mul_i32 s74, s[sgprStaggerUIter], s[sgprGlobalReadIncsB+0] //  stagger byte offset
s_mul_hi_i32 s[sgprWrapUB+1], s[sgprLoopCounterL], s[sgprGlobalReadIncsB+0] // Number of bytes accessed by the unroll loop
s_mul_i32 s[sgprWrapUB+0], s[sgprLoopCounterL], s[sgprGlobalReadIncsB+0] // Number of bytes accessed by the unroll loop
s_sub_u32 s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0], s[sgprWrapUB+0] // remove one iteration
s_subb_u32 s[sgprWrapUB+1], 0, s[sgprWrapUB+1]     // remove one iteration
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s74        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s75       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s74 // limit -= inc)
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s75 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
s_add_u32 s[sgprStaggerUIterDTV], s[sgprStaggerUIter], 1 // Subtract (PGR-1); StaggerUIter now contains target iteration to wrap
s_add_u32 s[sgprStaggerUIter], s[sgprStaggerUIter], 2 // Subtract (PGR-1); StaggerUIter now contains target iteration to wrap
/* local read addresses: init pointers a */
/* local read addresses: init pointers b */

/* localReadInitPointers */

/* prefetch: global -> local */
s_cmp_eq_u32 s[sgprLoopCounterL], 0                // at last iteration?
s_cbranch_scc1 label_ShadowInitStart               // skip to ShadowInitStart iter b/c numIter==0
buffer_load_dwordx4 v[vgprG2LB+0:vgprG2LB+0+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_0_0
buffer_load_dwordx4 v[vgprG2LB+4:vgprG2LB+4+3], v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_1_0
buffer_load_dwordx4 v[vgprG2LB+8:vgprG2LB+8+3], v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_2_0
buffer_load_dwordx4 v[vgprG2LB+12:vgprG2LB+12+3], v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_3_0
buffer_load_dwordx4 v[vgprG2LA+0:vgprG2LA+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_0_0
buffer_load_dwordx4 v[vgprG2LA+4:vgprG2LA+4+3], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 1_0_0_0
buffer_load_dwordx4 v[vgprG2LA+8:vgprG2LA+8+3], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_1_0
buffer_load_dwordx4 v[vgprG2LA+12:vgprG2LA+12+3], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 1_0_1_0

/* global read inc B loopL */
s_add_u32 s76, s[sgprLoopCounterL], 1              // remove pf(1)
s_cmp_eq_u32 s[sgprStaggerUIter], s76              // Is this wrapIter? (pf)
s_cselect_b32 s74, s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0] // incLower <- ?
s_cselect_b32 s75, s[sgprWrapUB+1], 0              // incUpper <- ?
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s74        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s75       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s74 // limit -= inc)
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s75 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32

/******************************************/
/* End setupNewTile                       */
/******************************************/
label_ShadowInitStart:
s_mov_b32 s[sgprSrdD+0], s[sgprAddressD+0]         // init SRD base address (lower)
s_mov_b32 s[sgprSrdD+1], s[sgprAddressD+1]         // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdD+2], 0x80000000
s_mov_b32 s[sgprSrdD+3], Srd127_96                 // Set bits 127_96 in post-loop SRD

s_mov_b32 s[sgprSrdC+0], s[sgprAddressC+0]         // init SRD base address (lower)
s_mov_b32 s[sgprSrdC+1], s[sgprAddressC+1]         // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdC+2], 0x80000000
s_mov_b32 s[sgprSrdC+3], Srd127_96                 // Set bits 127_96 in post-loop SRD


s_mul_i32 s76, MT1, s[sgprWorkGroup1]              // <- wg1*MT1
s_mul_hi_u32 s75, s76, s[sgprStrideC1J]            // ScaleC s76 by Stride
s_mul_i32 s74, s76, s[sgprStrideC1J]               // ScaleC s76 by Stride
s_lshl_b64 s[74:75], s[74:75], s[sgprGSULog2BpeC]  // scale by bpe
s_add_u32 s[sgprSrdC+0], s[sgprAddressC+0], s74    // add lo to SRD
s_addc_u32 s[sgprSrdC+1], s[sgprAddressC+1], s75   // add hi to SRD
s_mul_hi_u32 s75, s76, s[sgprStrideD1J]            // ScaleD s76 by Stride
s_mul_i32 s74, s76, s[sgprStrideD1J]               // ScaleD s76 by Stride
s_lshl_b64 s[74:75], s[74:75], s[sgprGSULog2BpeD]  // scale by bpe
s_add_u32 s[sgprSrdD+0], s[sgprAddressD+0], s74    // add lo to SRD
s_addc_u32 s[sgprSrdD+1], s[sgprAddressD+1], s75   // add hi to SRD

s_mul_hi_u32 s75, s[sgprWorkGroup2], s[sgprStrideCK] // ScaleC s[sgprWorkGroup2] by Stride
s_mul_i32 s74, s[sgprWorkGroup2], s[sgprStrideCK]  // ScaleC s[sgprWorkGroup2] by Stride
s_lshl_b64 s[74:75], s[74:75], s[sgprGSULog2BpeC]  // scale by bpe
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s74        // add lo to SRD
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], s75       // add hi to SRD
s_mul_hi_u32 s75, s[sgprWorkGroup2], s[sgprStrideDK] // ScaleD s[sgprWorkGroup2] by Stride
s_mul_i32 s74, s[sgprWorkGroup2], s[sgprStrideDK]  // ScaleD s[sgprWorkGroup2] by Stride
s_lshl_b64 s[74:75], s[74:75], s[sgprGSULog2BpeD]  // scale by bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s74        // add lo to SRD
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], s75       // add hi to SRD

// backup workspace start
s_mov_b32 s[sgprWSDstart+0], s[sgprSrdD+0]         // recode workspace start
s_mov_b32 s[sgprWSDstart+1], s[sgprSrdD+1]         // recode workspace start
s_and_b32 s74, s[sgprGSU], 0x3fff                  // Restore GSU
s_cmp_eq_u32 s74, 1                                // GSU == 1 ?
s_cbranch_scc1 label_GSU_2                         // branch if GSU == 1
// GSU Output Buffer offset: Free0 + (Free1-1)*StrideC1J + (Free2-1)*StrideCK * GSUIdx * bpe%s
s_mul_hi_u32 s75, s[sgprSizesFree+0], s[sgprGSUSumIdx] // Free0
s_mul_i32 s74, s[sgprSizesFree+0], s[sgprGSUSumIdx] // Free0
s_sub_u32 s76, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s76, s76, s[sgprGSUSumIdx]               // Free1
s_mul_hi_u32 s77, s76, s[sgprStrideC1J]            // Free1
s_mul_i32 s76, s76, s[sgprStrideC1J]               // Free1
s_add_u32 s74, s74, s76                            // Free1
s_addc_u32 s75, s75, s77                           // Free1
s_sub_u32 s76, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s76, s76, s[sgprGSUSumIdx]               // Free2
s_mul_hi_u32 s77, s76, s[sgprStrideCK]             // Free2
s_mul_i32 s76, s76, s[sgprStrideCK]                // Free2
s_add_u32 s74, s74, s76                            // Free2
s_addc_u32 s75, s75, s77                           // Free2
s_lshl_b64 s[74:75], s[74:75], 2                   // scale by bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s74        // add lo GSU offset to SRD
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], s75       // add hi GSU offset to SRD
label_GSU_2:
.set sgprGSULog2BpeC, UNDEF
.set sgprAddressC, UNDEF
.set sgprAddressD, UNDEF

/* initC: remove ValuC vgpr buffer [0...0) from pool */

/* initC: remove acc vgpr buffer [0...64) from pool */

/* initC: remove ValuA/B vgpr buffer [0...80) from pool */
v_accvgpr_write acc0, 0x0                          // initC
v_accvgpr_write acc1, 0x0                          // initC
v_accvgpr_write acc2, 0x0                          // initC
v_accvgpr_write acc3, 0x0                          // initC
v_accvgpr_write acc4, 0x0                          // initC
v_accvgpr_write acc5, 0x0                          // initC
v_accvgpr_write acc6, 0x0                          // initC
v_accvgpr_write acc7, 0x0                          // initC
v_accvgpr_write acc8, 0x0                          // initC
v_accvgpr_write acc9, 0x0                          // initC
v_accvgpr_write acc10, 0x0                         // initC
v_accvgpr_write acc11, 0x0                         // initC
v_accvgpr_write acc12, 0x0                         // initC
v_accvgpr_write acc13, 0x0                         // initC
v_accvgpr_write acc14, 0x0                         // initC
v_accvgpr_write acc15, 0x0                         // initC
v_accvgpr_write acc16, 0x0                         // initC
v_accvgpr_write acc17, 0x0                         // initC
v_accvgpr_write acc18, 0x0                         // initC
v_accvgpr_write acc19, 0x0                         // initC
v_accvgpr_write acc20, 0x0                         // initC
v_accvgpr_write acc21, 0x0                         // initC
v_accvgpr_write acc22, 0x0                         // initC
v_accvgpr_write acc23, 0x0                         // initC
v_accvgpr_write acc24, 0x0                         // initC
v_accvgpr_write acc25, 0x0                         // initC
v_accvgpr_write acc26, 0x0                         // initC
v_accvgpr_write acc27, 0x0                         // initC
v_accvgpr_write acc28, 0x0                         // initC
v_accvgpr_write acc29, 0x0                         // initC
v_accvgpr_write acc30, 0x0                         // initC
v_accvgpr_write acc31, 0x0                         // initC
v_accvgpr_write acc32, 0x0                         // initC
v_accvgpr_write acc33, 0x0                         // initC
v_accvgpr_write acc34, 0x0                         // initC
v_accvgpr_write acc35, 0x0                         // initC
v_accvgpr_write acc36, 0x0                         // initC
v_accvgpr_write acc37, 0x0                         // initC
v_accvgpr_write acc38, 0x0                         // initC
v_accvgpr_write acc39, 0x0                         // initC
v_accvgpr_write acc40, 0x0                         // initC
v_accvgpr_write acc41, 0x0                         // initC
v_accvgpr_write acc42, 0x0                         // initC
v_accvgpr_write acc43, 0x0                         // initC
v_accvgpr_write acc44, 0x0                         // initC
v_accvgpr_write acc45, 0x0                         // initC
v_accvgpr_write acc46, 0x0                         // initC
v_accvgpr_write acc47, 0x0                         // initC
v_accvgpr_write acc48, 0x0                         // initC
v_accvgpr_write acc49, 0x0                         // initC
v_accvgpr_write acc50, 0x0                         // initC
v_accvgpr_write acc51, 0x0                         // initC
v_accvgpr_write acc52, 0x0                         // initC
v_accvgpr_write acc53, 0x0                         // initC
v_accvgpr_write acc54, 0x0                         // initC
v_accvgpr_write acc55, 0x0                         // initC
v_accvgpr_write acc56, 0x0                         // initC
v_accvgpr_write acc57, 0x0                         // initC
v_accvgpr_write acc58, 0x0                         // initC
v_accvgpr_write acc59, 0x0                         // initC
v_accvgpr_write acc60, 0x0                         // initC
v_accvgpr_write acc61, 0x0                         // initC
v_accvgpr_write acc62, 0x0                         // initC
v_accvgpr_write acc63, 0x0                         // initC
s_cmp_eq_u32 s[sgprLoopCounterL], 0                // at last iteration?

/* after InitC, skip to end of prefetch last iter if numIter==0 */
s_cbranch_scc0 label_NoBranch_VBQ7NFIZE7TH0KCP_0   // Only branch on scc1
s_getpc_b64 s[32:33]                               // addr of next instr
s_add_i32 s34, label_PrefetchGlobalLastIterEnd, 0x4 // target branch offset
s_add_u32 s32, s32, s34                            // add target branch offset
s_addc_u32 s33, s33, 0                             // add high and carry
s_setpc_b64 s[32:33]                               // branch to label_PrefetchGlobalLastIterEnd
label_NoBranch_VBQ7NFIZE7TH0KCP_0:
s_waitcnt vmcnt(4)                                 // 8wait for global read

/* local write a */

/* local write b */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+3] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+4:vgprG2LB+4+3] offset:4224 // lwoB_0_0_1_0 = (0*LSCB)*(MT1J+PAD) + (1*LSPB) = 4224
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+8:vgprG2LB+8+3] offset:8448 // lwoB_0_0_2_0 = (0*LSCB)*(MT1J+PAD) + (2*LSPB) = 8448
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+12:vgprG2LB+12+3] offset:12672 // lwoB_0_0_3_0 = (0*LSCB)*(MT1J+PAD) + (3*LSPB) = 12672

/* local write swap a */

/* local write swap b */
s_cmp_eq_u32 s[sgprLoopCounterL], 0x1              // PGR=2 but only 1 loop
s_cbranch_scc1 label_skipPGR2_0                    // PGR=2 but only 1 loop
buffer_load_dwordx4 v[vgprG2LB+0:vgprG2LB+0+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_0_0
buffer_load_dwordx4 v[vgprG2LB+4:vgprG2LB+4+3], v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_1_0
buffer_load_dwordx4 v[vgprG2LB+8:vgprG2LB+8+3], v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_2_0
buffer_load_dwordx4 v[vgprG2LB+12:vgprG2LB+12+3], v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_3_0
label_skipPGR2_0:
s_waitcnt lgkmcnt(0)                               // 0prefetch wait for local write
// Skip force waitcnt0
s_barrier

/* local read prefetch a */

/* local read prefetch b */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+3], v[vgprLocalReadAddrB] offset:128 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+3], v[vgprLocalReadAddrB] offset:256 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+3], v[vgprLocalReadAddrB] offset:384 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+3], v[vgprLocalReadAddrB] offset:512 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+3], v[vgprLocalReadAddrB] offset:640 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+3], v[vgprLocalReadAddrB] offset:768 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+28:vgprValuB_X0_I0+28+3], v[vgprLocalReadAddrB] offset:896 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0

/* local read inc a */

/* local read inc b */
/* N/A, lro->32 */
/* self.localReadDoCntA 1 self.localReadDoCntB 1 */

/******************************************/
/* Unrolled Loop(s) - Begin               */
/******************************************/
label_openLoopL:
s_cmp_eq_u32 s[sgprLoopCounterL], 0x1              // LoopCounterL < EndCounter
s_cbranch_scc1 label_toPGR1_0                      // PGR=2 but only 1 loop, toPGR1
s_cmp_le_u32 s[sgprLoopCounterL], 0x2              // LoopCounterL < EndCounter
s_cbranch_scc1 label_LoopEndL                      // do not enter LoopL
label_LoopBeginL:

/******************************************/
/* Unrolled Loop 1/2 - Begin              */
/******************************************/

/* Begin Each Unroll: Check VGPR.checkin for INT8 LW */

/* iter 0 */
s_waitcnt vmcnt(6)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:18, lwStartMfmaIndex:22, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:0  */
s_waitcnt lgkmcnt(7)                               // wait for prior local read local write old=0, new=7 newLW=0 newLR=7 for iteration == 0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
ds_read_b128 v[vgprValuB_X2_I0+0:vgprValuB_X2_I0+0+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0

/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIterDTV] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
ds_read_b128 v[vgprValuB_X2_I0+4:vgprValuB_X2_I0+4+3], v[vgprLocalReadAddrB] offset:192 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s32, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
s_waitcnt lgkmcnt(2)                               // wait for prior local read local write
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
ds_read_b128 v[vgprValuB_X2_I0+8:vgprValuB_X2_I0+8+3], v[vgprLocalReadAddrB] offset:320 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s33, s[sgprWrapUA+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
ds_read_b128 v[vgprValuB_X2_I0+12:vgprValuB_X2_I0+12+3], v[vgprLocalReadAddrB] offset:448 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=2 iui=0
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s32        // gra SRD += inc(lower)
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
ds_read_b128 v[vgprValuB_X2_I0+16:vgprValuB_X2_I0+16+3], v[vgprLocalReadAddrB] offset:576 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=2 iui=0
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s33       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
ds_read_b128 v[vgprValuB_X2_I0+20:vgprValuB_X2_I0+20+3], v[vgprLocalReadAddrB] offset:704 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=2 iui=0
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s32 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
ds_read_b128 v[vgprValuB_X2_I0+24:vgprValuB_X2_I0+24+3], v[vgprLocalReadAddrB] offset:832 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=2 iui=0
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s33 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
ds_read_b128 v[vgprValuB_X2_I0+28:vgprValuB_X2_I0+28+3], v[vgprLocalReadAddrB] offset:960 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=2 iui=0
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
/* localReadsVacancy: latencyLeft 2 */

/* global read inc B loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIter] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s32, s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0] // incLower <- ?
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s33, s[sgprWrapUB+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
/* localReadsVacancy: latencyLeft 2 */
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s32        // gra SRD += inc(lower)
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
/* localReadsVacancy: latencyLeft 2 */
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s33       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
/* localReadsVacancy: latencyLeft 2 */
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s32 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=2 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=8 */

/* iter 1 */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:22, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:16  */
/* localReadsVacancy: latencyLeft 2 */
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s33 // limit -= inc)
s_waitcnt lgkmcnt(8)                               // wait for prior local read local write old=0, new=8 newLW=0 newLR=8
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:17  */
/* localReadsVacancy: latencyLeft 2 */
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:18  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:21  */
/* schedule remaining localreads for 1LDSB */
/* localReadsVacancy: latencyLeft 2 */
/* 1 LDS buffer: read-sync-write */
s_waitcnt lgkmcnt(0)
s_barrier
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:22  */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:23  */
buffer_load_dwordx4 v[vgprG2LA2+0:vgprG2LA2+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_0_0
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:24  */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:25  */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:26  */
buffer_load_dwordx4 v[vgprG2LA2+4:vgprG2LA2+4+3], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 1_0_0_0
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:27  */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:28  */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:29  */
buffer_load_dwordx4 v[vgprG2LA2+8:vgprG2LA2+8+3], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:30  */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:31  */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=2 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=8 */

/* iter 2 (reset local read pointers iteration)  (swap and reset local write pointers iteration)  (swap local read pointers iteration)  */
s_waitcnt vmcnt(7)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:18, lwStartMfmaIndex:22, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:32  */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:33  */
buffer_load_dwordx4 v[vgprG2LA2+12:vgprG2LA2+12+3], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 1_0_1_0
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:34  */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:35  */
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(7)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+3] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:36  */
buffer_load_dwordx4 v[vgprG2LB+0:vgprG2LB+0+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_0_0
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:37  */
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:38  */
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(7)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+4:vgprG2LB+4+3] offset:4224 // lwoB_0_0_1_0 = (0*LSCB)*(MT1J+PAD) + (1*LSPB) = 4224
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:39  */
buffer_load_dwordx4 v[vgprG2LB+4:vgprG2LB+4+3], v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:40  */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:41  */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:42  */
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(7)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+8:vgprG2LB+8+3] offset:8448 // lwoB_0_0_2_0 = (0*LSCB)*(MT1J+PAD) + (2*LSPB) = 8448
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:43  */
buffer_load_dwordx4 v[vgprG2LB+8:vgprG2LB+8+3], v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_2_0
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:44  */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:45  */
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(7)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+12:vgprG2LB+12+3] offset:12672 // lwoB_0_0_3_0 = (0*LSCB)*(MT1J+PAD) + (3*LSPB) = 12672
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:46  */
buffer_load_dwordx4 v[vgprG2LB+12:vgprG2LB+12+3], v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_3_0

/* local write swap offsets a */

/* local write swap offsets b */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:47  */

/* local read swap offsets a */

/* local read swap offsets b */

/* local read init pointers a */

/* local read init pointers b */

/* localReadInitPointers */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=2 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=8 */

/* iter 3 */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:22, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:48  */
s_waitcnt lgkmcnt(0)                               // 3wait for local write
// Skip force waitcnt0
s_barrier
s_waitcnt lgkmcnt(4)                               // wait for prior local read local write old=0, new=4 newLW=4 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:49  */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:50  */
ds_read_b128 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+3], v[vgprLocalReadAddrB] offset:128 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:51  */
ds_read_b128 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+3], v[vgprLocalReadAddrB] offset:256 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:52  */
ds_read_b128 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+3], v[vgprLocalReadAddrB] offset:384 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:53  */
ds_read_b128 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+3], v[vgprLocalReadAddrB] offset:512 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:54  */
ds_read_b128 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+3], v[vgprLocalReadAddrB] offset:640 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:55  */
ds_read_b128 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+3], v[vgprLocalReadAddrB] offset:768 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:56  */
ds_read_b128 v[vgprValuB_X0_I0+28:vgprValuB_X0_I0+28+3], v[vgprLocalReadAddrB] offset:896 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:57  */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:58  */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:59  */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:60  */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:61  */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:62  */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:63  */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=1 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=2 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=8 */

/******************************************/
/* Unrolled Loop - End 1/2                */
/******************************************/
s_sub_u32 s[sgprLoopCounterL], s[sgprLoopCounterL], 1 // dec counterL
s_cmp_le_u32 s[sgprLoopCounterL], 0x2              // counteL<=2
s_cbranch_scc1 label_NoGRloopAfterABLoop           // exit LoopL

/******************************************/
/* Unrolled Loop 2/2 - Begin              */
/******************************************/

/* Begin Each Unroll: Check VGPR.checkin for INT8 LW */

/* iter 0 */
s_waitcnt vmcnt(6)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:18, lwStartMfmaIndex:22, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:0  */
s_waitcnt lgkmcnt(7)                               // wait for prior local read local write old=0, new=7 newLW=0 newLR=7 for iteration == 0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
ds_read_b128 v[vgprValuB_X2_I0+0:vgprValuB_X2_I0+0+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0

/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIterDTV] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
ds_read_b128 v[vgprValuB_X2_I0+4:vgprValuB_X2_I0+4+3], v[vgprLocalReadAddrB] offset:192 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s32, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
s_waitcnt lgkmcnt(2)                               // wait for prior local read local write
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
ds_read_b128 v[vgprValuB_X2_I0+8:vgprValuB_X2_I0+8+3], v[vgprLocalReadAddrB] offset:320 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s33, s[sgprWrapUA+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
ds_read_b128 v[vgprValuB_X2_I0+12:vgprValuB_X2_I0+12+3], v[vgprLocalReadAddrB] offset:448 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=2 iui=0
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s32        // gra SRD += inc(lower)
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
ds_read_b128 v[vgprValuB_X2_I0+16:vgprValuB_X2_I0+16+3], v[vgprLocalReadAddrB] offset:576 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=2 iui=0
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s33       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
ds_read_b128 v[vgprValuB_X2_I0+20:vgprValuB_X2_I0+20+3], v[vgprLocalReadAddrB] offset:704 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=2 iui=0
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s32 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
ds_read_b128 v[vgprValuB_X2_I0+24:vgprValuB_X2_I0+24+3], v[vgprLocalReadAddrB] offset:832 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=2 iui=0
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s33 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
ds_read_b128 v[vgprValuB_X2_I0+28:vgprValuB_X2_I0+28+3], v[vgprLocalReadAddrB] offset:960 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=2 iui=0
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
/* localReadsVacancy: latencyLeft 2 */

/* global read inc B loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIter] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s32, s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0] // incLower <- ?
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s33, s[sgprWrapUB+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
/* localReadsVacancy: latencyLeft 2 */
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s32        // gra SRD += inc(lower)
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
/* localReadsVacancy: latencyLeft 2 */
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s33       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
/* localReadsVacancy: latencyLeft 2 */
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s32 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=2 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=8 */

/* iter 1 */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:22, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:16  */
/* localReadsVacancy: latencyLeft 2 */
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s33 // limit -= inc)
s_waitcnt lgkmcnt(8)                               // wait for prior local read local write old=0, new=8 newLW=0 newLR=8
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:17  */
/* localReadsVacancy: latencyLeft 2 */
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:18  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:21  */
/* schedule remaining localreads for 1LDSB */
/* localReadsVacancy: latencyLeft 2 */
/* 1 LDS buffer: read-sync-write */
s_waitcnt lgkmcnt(0)
s_barrier
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:22  */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:23  */
buffer_load_dwordx4 v[vgprG2LA+0:vgprG2LA+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_0_0
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:24  */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:25  */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:26  */
buffer_load_dwordx4 v[vgprG2LA+4:vgprG2LA+4+3], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 1_0_0_0
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:27  */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:28  */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:29  */
buffer_load_dwordx4 v[vgprG2LA+8:vgprG2LA+8+3], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:30  */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:31  */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=2 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=8 */

/* iter 2 (reset local read pointers iteration)  (swap and reset local write pointers iteration)  (swap local read pointers iteration)  */
s_waitcnt vmcnt(7)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:18, lwStartMfmaIndex:22, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:32  */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:33  */
buffer_load_dwordx4 v[vgprG2LA+12:vgprG2LA+12+3], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 1_0_1_0
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:34  */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:35  */
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(7)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+3] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:36  */
buffer_load_dwordx4 v[vgprG2LB+0:vgprG2LB+0+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_0_0
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:37  */
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:38  */
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(7)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+4:vgprG2LB+4+3] offset:4224 // lwoB_0_0_1_0 = (0*LSCB)*(MT1J+PAD) + (1*LSPB) = 4224
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:39  */
buffer_load_dwordx4 v[vgprG2LB+4:vgprG2LB+4+3], v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:40  */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:41  */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:42  */
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(7)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+8:vgprG2LB+8+3] offset:8448 // lwoB_0_0_2_0 = (0*LSCB)*(MT1J+PAD) + (2*LSPB) = 8448
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:43  */
buffer_load_dwordx4 v[vgprG2LB+8:vgprG2LB+8+3], v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_2_0
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:44  */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:45  */
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(7)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+12:vgprG2LB+12+3] offset:12672 // lwoB_0_0_3_0 = (0*LSCB)*(MT1J+PAD) + (3*LSPB) = 12672
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:46  */
buffer_load_dwordx4 v[vgprG2LB+12:vgprG2LB+12+3], v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_3_0

/* local write swap offsets a */

/* local write swap offsets b */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:47  */

/* local read swap offsets a */

/* local read swap offsets b */

/* local read init pointers a */

/* local read init pointers b */

/* localReadInitPointers */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=2 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=8 */

/* iter 3 */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:22, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:48  */
s_waitcnt lgkmcnt(0)                               // 3wait for local write
// Skip force waitcnt0
s_barrier
s_waitcnt lgkmcnt(4)                               // wait for prior local read local write old=0, new=4 newLW=4 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:49  */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:50  */
ds_read_b128 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+3], v[vgprLocalReadAddrB] offset:128 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:51  */
ds_read_b128 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+3], v[vgprLocalReadAddrB] offset:256 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:52  */
ds_read_b128 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+3], v[vgprLocalReadAddrB] offset:384 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:53  */
ds_read_b128 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+3], v[vgprLocalReadAddrB] offset:512 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:54  */
ds_read_b128 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+3], v[vgprLocalReadAddrB] offset:640 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:55  */
ds_read_b128 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+3], v[vgprLocalReadAddrB] offset:768 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:56  */
ds_read_b128 v[vgprValuB_X0_I0+28:vgprValuB_X0_I0+28+3], v[vgprLocalReadAddrB] offset:896 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:57  */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:58  */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:59  */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:60  */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:61  */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:62  */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:63  */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=1 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=2 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=8 */

/******************************************/
/* Unrolled Loop - End 2/2 (final)        */
/******************************************/

/* closeLoop loopL finalLoop=1 tailLoop=0 */
s_sub_u32 s[sgprLoopCounterL], s[sgprLoopCounterL], 1 // dec counterL
s_cmp_eq_i32 s[sgprLoopCounterL], 0x2              // counterL==2
s_cbranch_scc0 label_LoopBeginL                    // restart LoopL
label_LoopEndL:

/* Before NLL: Check VGPR.checkin for INT8 LW */

/******************************************/
/* Ord. NoGlobalLoadLoop - Begin 1/2      */
/******************************************/

/* iter 0 */
s_waitcnt vmcnt(6)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:18, lwStartMfmaIndex:22, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:0  */
s_waitcnt lgkmcnt(7)                               // wait for prior local read local write old=0, new=7 newLW=0 newLR=7 for iteration == 0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
ds_read_b128 v[vgprValuB_X2_I0+0:vgprValuB_X2_I0+0+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0

/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIterDTV] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
ds_read_b128 v[vgprValuB_X2_I0+4:vgprValuB_X2_I0+4+3], v[vgprLocalReadAddrB] offset:192 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s32, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
s_waitcnt lgkmcnt(2)                               // wait for prior local read local write
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
ds_read_b128 v[vgprValuB_X2_I0+8:vgprValuB_X2_I0+8+3], v[vgprLocalReadAddrB] offset:320 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s33, s[sgprWrapUA+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
ds_read_b128 v[vgprValuB_X2_I0+12:vgprValuB_X2_I0+12+3], v[vgprLocalReadAddrB] offset:448 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=2 iui=0
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s32        // gra SRD += inc(lower)
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
ds_read_b128 v[vgprValuB_X2_I0+16:vgprValuB_X2_I0+16+3], v[vgprLocalReadAddrB] offset:576 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=2 iui=0
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s33       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
ds_read_b128 v[vgprValuB_X2_I0+20:vgprValuB_X2_I0+20+3], v[vgprLocalReadAddrB] offset:704 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=2 iui=0
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s32 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
ds_read_b128 v[vgprValuB_X2_I0+24:vgprValuB_X2_I0+24+3], v[vgprLocalReadAddrB] offset:832 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=2 iui=0
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s33 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
ds_read_b128 v[vgprValuB_X2_I0+28:vgprValuB_X2_I0+28+3], v[vgprLocalReadAddrB] offset:960 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=2 iui=0
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
/* localReadsVacancy: latencyLeft 2 */

/* global read inc B loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIter] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s32, s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0] // incLower <- ?
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s33, s[sgprWrapUB+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
/* localReadsVacancy: latencyLeft 2 */
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s32        // gra SRD += inc(lower)
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
/* localReadsVacancy: latencyLeft 2 */
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s33       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
/* localReadsVacancy: latencyLeft 2 */
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s32 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=2 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=8 */

/* iter 1 */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:22, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:16  */
/* localReadsVacancy: latencyLeft 2 */
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s33 // limit -= inc)
s_waitcnt lgkmcnt(8)                               // wait for prior local read local write old=0, new=8 newLW=0 newLR=8
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:17  */
/* localReadsVacancy: latencyLeft 2 */
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:18  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:21  */
/* schedule remaining localreads for 1LDSB */
/* localReadsVacancy: latencyLeft 2 */
/* 1 LDS buffer: read-sync-write */
s_waitcnt lgkmcnt(0)
s_barrier
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:22  */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:23  */
buffer_load_dwordx4 v[vgprG2LA2+0:vgprG2LA2+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_0_0
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:24  */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:25  */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:26  */
buffer_load_dwordx4 v[vgprG2LA2+4:vgprG2LA2+4+3], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 1_0_0_0
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:27  */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:28  */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:29  */
buffer_load_dwordx4 v[vgprG2LA2+8:vgprG2LA2+8+3], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:30  */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:31  */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=2 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=8 */

/* iter 2 (reset local read pointers iteration)  (swap and reset local write pointers iteration)  (swap local read pointers iteration)  */
s_waitcnt vmcnt(7)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:18, lwStartMfmaIndex:22, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:32  */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:33  */
buffer_load_dwordx4 v[vgprG2LA2+12:vgprG2LA2+12+3], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 1_0_1_0
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:34  */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:35  */
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(7)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+3] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:36  */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:37  */
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:38  */
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(6)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+4:vgprG2LB+4+3] offset:4224 // lwoB_0_0_1_0 = (0*LSCB)*(MT1J+PAD) + (1*LSPB) = 4224
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:39  */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:40  */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:41  */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:42  */
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(5)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+8:vgprG2LB+8+3] offset:8448 // lwoB_0_0_2_0 = (0*LSCB)*(MT1J+PAD) + (2*LSPB) = 8448
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:43  */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:44  */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:45  */
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(4)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+12:vgprG2LB+12+3] offset:12672 // lwoB_0_0_3_0 = (0*LSCB)*(MT1J+PAD) + (3*LSPB) = 12672
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:46  */

/* local write swap offsets a */

/* local write swap offsets b */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:47  */

/* local read swap offsets a */

/* local read swap offsets b */

/* local read init pointers a */

/* local read init pointers b */

/* localReadInitPointers */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=2 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=8 */

/* iter 3 */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:22, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:48  */
s_waitcnt lgkmcnt(0)                               // 3wait for local write
// Skip force waitcnt0
s_barrier
s_waitcnt lgkmcnt(4)                               // wait for prior local read local write old=0, new=4 newLW=4 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:49  */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:50  */
ds_read_b128 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+3], v[vgprLocalReadAddrB] offset:128 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:51  */
ds_read_b128 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+3], v[vgprLocalReadAddrB] offset:256 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:52  */
ds_read_b128 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+3], v[vgprLocalReadAddrB] offset:384 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:53  */
ds_read_b128 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+3], v[vgprLocalReadAddrB] offset:512 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:54  */
ds_read_b128 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+3], v[vgprLocalReadAddrB] offset:640 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:55  */
ds_read_b128 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+3], v[vgprLocalReadAddrB] offset:768 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:56  */
ds_read_b128 v[vgprValuB_X0_I0+28:vgprValuB_X0_I0+28+3], v[vgprLocalReadAddrB] offset:896 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:57  */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:58  */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:59  */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:60  */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:61  */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:62  */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:63  */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=1 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=2 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=8 */
s_sub_u32 s[sgprLoopCounterL], s[sgprLoopCounterL], 1 // dec counterL
s_branch label_toPGR1_0                            // Branch to toPGR1
label_NoGRloopAfterABLoop:

/******************************************/
/* Ord. NoGlobalLoadLoop - Begin 2/2      */
/******************************************/

/* iter 0 */
s_waitcnt vmcnt(6)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:18, lwStartMfmaIndex:22, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:0  */
s_waitcnt lgkmcnt(7)                               // wait for prior local read local write old=0, new=7 newLW=0 newLR=7 for iteration == 0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
ds_read_b128 v[vgprValuB_X2_I0+0:vgprValuB_X2_I0+0+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0

/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIterDTV] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
ds_read_b128 v[vgprValuB_X2_I0+4:vgprValuB_X2_I0+4+3], v[vgprLocalReadAddrB] offset:192 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s32, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
s_waitcnt lgkmcnt(2)                               // wait for prior local read local write
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
ds_read_b128 v[vgprValuB_X2_I0+8:vgprValuB_X2_I0+8+3], v[vgprLocalReadAddrB] offset:320 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s33, s[sgprWrapUA+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
ds_read_b128 v[vgprValuB_X2_I0+12:vgprValuB_X2_I0+12+3], v[vgprLocalReadAddrB] offset:448 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=2 iui=0
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s32        // gra SRD += inc(lower)
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
ds_read_b128 v[vgprValuB_X2_I0+16:vgprValuB_X2_I0+16+3], v[vgprLocalReadAddrB] offset:576 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=2 iui=0
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s33       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
ds_read_b128 v[vgprValuB_X2_I0+20:vgprValuB_X2_I0+20+3], v[vgprLocalReadAddrB] offset:704 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=2 iui=0
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s32 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
ds_read_b128 v[vgprValuB_X2_I0+24:vgprValuB_X2_I0+24+3], v[vgprLocalReadAddrB] offset:832 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=2 iui=0
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s33 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
ds_read_b128 v[vgprValuB_X2_I0+28:vgprValuB_X2_I0+28+3], v[vgprLocalReadAddrB] offset:960 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=2 iui=0
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
/* localReadsVacancy: latencyLeft 2 */

/* global read inc B loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIter] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s32, s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0] // incLower <- ?
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s33, s[sgprWrapUB+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
/* localReadsVacancy: latencyLeft 2 */
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s32        // gra SRD += inc(lower)
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
/* localReadsVacancy: latencyLeft 2 */
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s33       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
/* localReadsVacancy: latencyLeft 2 */
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s32 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=2 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=8 */

/* iter 1 */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:22, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:16  */
/* localReadsVacancy: latencyLeft 2 */
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s33 // limit -= inc)
s_waitcnt lgkmcnt(8)                               // wait for prior local read local write old=0, new=8 newLW=0 newLR=8
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:17  */
/* localReadsVacancy: latencyLeft 2 */
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:18  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:21  */
/* schedule remaining localreads for 1LDSB */
/* localReadsVacancy: latencyLeft 2 */
/* 1 LDS buffer: read-sync-write */
s_waitcnt lgkmcnt(0)
s_barrier
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:22  */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:23  */
buffer_load_dwordx4 v[vgprG2LA+0:vgprG2LA+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_0_0
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:24  */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:25  */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:26  */
buffer_load_dwordx4 v[vgprG2LA+4:vgprG2LA+4+3], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 1_0_0_0
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:27  */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:28  */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:29  */
buffer_load_dwordx4 v[vgprG2LA+8:vgprG2LA+8+3], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:30  */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:31  */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=2 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=8 */

/* iter 2 (reset local read pointers iteration)  (swap and reset local write pointers iteration)  (swap local read pointers iteration)  */
s_waitcnt vmcnt(7)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:18, lwStartMfmaIndex:22, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:32  */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:33  */
buffer_load_dwordx4 v[vgprG2LA+12:vgprG2LA+12+3], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 1_0_1_0
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:34  */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:35  */
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(7)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+3] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:36  */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:37  */
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:38  */
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(6)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+4:vgprG2LB+4+3] offset:4224 // lwoB_0_0_1_0 = (0*LSCB)*(MT1J+PAD) + (1*LSPB) = 4224
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:39  */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:40  */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:41  */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:42  */
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(5)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+8:vgprG2LB+8+3] offset:8448 // lwoB_0_0_2_0 = (0*LSCB)*(MT1J+PAD) + (2*LSPB) = 8448
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:43  */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:44  */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:45  */
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(4)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+12:vgprG2LB+12+3] offset:12672 // lwoB_0_0_3_0 = (0*LSCB)*(MT1J+PAD) + (3*LSPB) = 12672
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:46  */

/* local write swap offsets a */

/* local write swap offsets b */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:47  */

/* local read swap offsets a */

/* local read swap offsets b */

/* local read init pointers a */

/* local read init pointers b */

/* localReadInitPointers */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=2 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=8 */

/* iter 3 */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:22, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:48  */
s_waitcnt lgkmcnt(0)                               // 3wait for local write
// Skip force waitcnt0
s_barrier
s_waitcnt lgkmcnt(4)                               // wait for prior local read local write old=0, new=4 newLW=4 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:49  */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:50  */
ds_read_b128 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+3], v[vgprLocalReadAddrB] offset:128 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:51  */
ds_read_b128 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+3], v[vgprLocalReadAddrB] offset:256 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:52  */
ds_read_b128 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+3], v[vgprLocalReadAddrB] offset:384 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:53  */
ds_read_b128 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+3], v[vgprLocalReadAddrB] offset:512 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:54  */
ds_read_b128 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+3], v[vgprLocalReadAddrB] offset:640 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:55  */
ds_read_b128 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+3], v[vgprLocalReadAddrB] offset:768 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:56  */
ds_read_b128 v[vgprValuB_X0_I0+28:vgprValuB_X0_I0+28+3], v[vgprLocalReadAddrB] offset:896 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:57  */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:58  */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:59  */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:60  */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:61  */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:62  */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:63  */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=1 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=2 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=8 */
s_sub_u32 s[sgprLoopCounterL], s[sgprLoopCounterL], 1 // dec counterL
label_toPGR1_0:
s_and_b32 s8, s[sgprGSU], 0x3fff                   // Restore GSU
s_cmp_eq_u32 s8, 1                                 // GSU == 1 ?
s_cbranch_scc0 label_GSU_3                         // branch if GSU != 1

/******************************************/
/* Opt. NoLoadLoop - Begin 1/2            */
/******************************************/
s_cmpk_eq_u32 s[sgprBeta], 0x0                     // Beta == 0
s_cbranch_scc0 label_OptNLL_End                    // Branch if Beta is not zero

s_cmp_eq_u32 s[sgprAlpha], 1.0                     // Alpha == 1.0 ?
s_cbranch_scc0 label_OptNLL_End                    // branch if alpha != 1

s_and_b32 s32, 127, s[sgprSizeI]                   // s32 = s[sgprSizeI] % 128
s_add_u32 s33, -0x1, s[sgprNumWorkGroups0]
s_cmp_ge_u32 s[sgprWorkGroup0], s33                // wg0 >= nwg0-1 ?
s_cselect_b32 s32, s32, 0                          // set rMT0
s_cmpk_gt_u32 s32, 0x0                             // rMT0 > 0
s_cbranch_scc1 label_OptNLL_End                    // jump if edges required
s_and_b32 s32, 127, s[sgprSizeJ]                   // s32 = s[sgprSizeJ] % 128
s_add_u32 s33, -0x1, s[sgprNumWorkGroups1]
s_cmp_ge_u32 s[sgprWorkGroup1], s33                // wg1 >= nwg1-1
s_cselect_b32 s32, s32, 0                          // set rMT1
s_cmpk_gt_u32 s32, 0x0                             // rMT1 > 0
s_cbranch_scc1 label_OptNLL_End                    // jump if edges required

s_and_b32 s33, 63, s[sgprSizesSum+0]               // s33 = s[sgprSizesSum+0] % 64
s_cmp_eq_u32 s33, 0x0                              // numIterL == 0
s_cbranch_scc0 label_OptNLL_End                    // skip if tail loop required
s_bitcmp1_b32 s[sgprOrigLoopCounter], 0x0          // test if OrigLoopCounter is Odd ?
s_cbranch_scc1 label_OptNLL_second                 // jump to second NoLoadLoop

/* iter 0 (last unrolled loop) */
s_waitcnt vmcnt(2)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:9, lwStartMfmaIndex:46, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:0  */
s_waitcnt lgkmcnt(7)                               // wait for prior local read local write old=0, new=7 newLW=0 newLR=7 for iteration == 0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
ds_read_b128 v[vgprValuB_X2_I0+0:vgprValuB_X2_I0+0+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0

/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIterDTV] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
ds_read_b128 v[vgprValuB_X2_I0+4:vgprValuB_X2_I0+4+3], v[vgprLocalReadAddrB] offset:192 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s32, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
s_waitcnt lgkmcnt(2)                               // wait for prior local read local write
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
ds_read_b128 v[vgprValuB_X2_I0+8:vgprValuB_X2_I0+8+3], v[vgprLocalReadAddrB] offset:320 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s33, s[sgprWrapUA+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
ds_read_b128 v[vgprValuB_X2_I0+12:vgprValuB_X2_I0+12+3], v[vgprLocalReadAddrB] offset:448 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=2 iui=0
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s32        // gra SRD += inc(lower)
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
ds_read_b128 v[vgprValuB_X2_I0+16:vgprValuB_X2_I0+16+3], v[vgprLocalReadAddrB] offset:576 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=2 iui=0
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s33       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
ds_read_b128 v[vgprValuB_X2_I0+20:vgprValuB_X2_I0+20+3], v[vgprLocalReadAddrB] offset:704 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=2 iui=0
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s32 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
ds_read_b128 v[vgprValuB_X2_I0+24:vgprValuB_X2_I0+24+3], v[vgprLocalReadAddrB] offset:832 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=2 iui=0
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s33 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
ds_read_b128 v[vgprValuB_X2_I0+28:vgprValuB_X2_I0+28+3], v[vgprLocalReadAddrB] offset:960 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=2 iui=0
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=2 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=8 */

/* iter 1 (last unrolled loop) */
/*  grEndMfmaIndex:9, lwStartMfmaIndex:46, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:16  */
/* localReadsVacancy: latencyLeft 2 */
s_waitcnt lgkmcnt(8)                               // wait for prior local read local write old=0, new=8 newLW=0 newLR=8
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:17  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:18  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:21  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:22  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:23  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:24  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:25  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:26  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:27  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:28  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:29  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:30  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:31  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=2 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=8 */

/* iter 2 (last unrolled loop) */
s_waitcnt vmcnt(0)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:9, lwStartMfmaIndex:46, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:32  */
/* localReadsVacancy: latencyLeft 2 */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:33  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:34  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:35  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:36  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:37  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:38  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:39  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:40  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:41  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:42  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:43  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:44  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:45  */
/* schedule remaining localreads for 1LDSB */
/* localReadsVacancy: latencyLeft 2 */
/* 1 LDS buffer: read-sync-write */
s_waitcnt lgkmcnt(0)
s_barrier
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:46  */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:47  */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=2 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=8 */

/* iter 3 (last unrolled loop) */
/*  grEndMfmaIndex:9, lwStartMfmaIndex:46, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:48  */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:49  */
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:50  */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:51  */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:52  */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:53  */
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:54  */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:55  */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:56  */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:57  */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:58  */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:59  */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:60  */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:61  */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:62  */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:63  */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=2 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=8 */
s_branch label_toPGR1end_OptNLL_0                  // Branch to toPGR1end

/******************************************/
/* Opt. NoLoadLoop - Begin 2/2            */
/******************************************/
label_OptNLL_second:  /// second Opt NoLoadLoop entry

/* iter 0 (last unrolled loop) */
s_waitcnt vmcnt(2)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:9, lwStartMfmaIndex:46, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:0  */
s_waitcnt lgkmcnt(7)                               // wait for prior local read local write old=0, new=7 newLW=0 newLR=7 for iteration == 0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
ds_read_b128 v[vgprValuB_X2_I0+0:vgprValuB_X2_I0+0+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0

/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIterDTV] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
ds_read_b128 v[vgprValuB_X2_I0+4:vgprValuB_X2_I0+4+3], v[vgprLocalReadAddrB] offset:192 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s32, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
s_waitcnt lgkmcnt(2)                               // wait for prior local read local write
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
ds_read_b128 v[vgprValuB_X2_I0+8:vgprValuB_X2_I0+8+3], v[vgprLocalReadAddrB] offset:320 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s33, s[sgprWrapUA+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
ds_read_b128 v[vgprValuB_X2_I0+12:vgprValuB_X2_I0+12+3], v[vgprLocalReadAddrB] offset:448 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=2 iui=0
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s32        // gra SRD += inc(lower)
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
ds_read_b128 v[vgprValuB_X2_I0+16:vgprValuB_X2_I0+16+3], v[vgprLocalReadAddrB] offset:576 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=2 iui=0
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s33       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
ds_read_b128 v[vgprValuB_X2_I0+20:vgprValuB_X2_I0+20+3], v[vgprLocalReadAddrB] offset:704 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=2 iui=0
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s32 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
ds_read_b128 v[vgprValuB_X2_I0+24:vgprValuB_X2_I0+24+3], v[vgprLocalReadAddrB] offset:832 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=2 iui=0
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s33 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
ds_read_b128 v[vgprValuB_X2_I0+28:vgprValuB_X2_I0+28+3], v[vgprLocalReadAddrB] offset:960 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=2 iui=0
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=2 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=8 */

/* iter 1 (last unrolled loop) */
/*  grEndMfmaIndex:9, lwStartMfmaIndex:46, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:16  */
/* localReadsVacancy: latencyLeft 2 */
s_waitcnt lgkmcnt(8)                               // wait for prior local read local write old=0, new=8 newLW=0 newLR=8
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:17  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:18  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:21  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:22  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:23  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:24  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:25  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:26  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:27  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:28  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:29  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:30  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:31  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=2 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=8 */

/* iter 2 (last unrolled loop) */
s_waitcnt vmcnt(0)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:9, lwStartMfmaIndex:46, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:32  */
/* localReadsVacancy: latencyLeft 2 */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:33  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:34  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:35  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:36  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:37  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:38  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:39  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:40  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:41  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:42  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:43  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:44  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:45  */
/* schedule remaining localreads for 1LDSB */
/* localReadsVacancy: latencyLeft 2 */
/* 1 LDS buffer: read-sync-write */
s_waitcnt lgkmcnt(0)
s_barrier
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:46  */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:47  */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=2 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=8 */

/* iter 3 (last unrolled loop) */
/*  grEndMfmaIndex:9, lwStartMfmaIndex:46, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:48  */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:49  */
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:50  */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:51  */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:52  */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:53  */
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:54  */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:55  */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:56  */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:57  */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:58  */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:59  */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:60  */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:61  */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:62  */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:63  */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=2 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=8 */
label_toPGR1end_OptNLL_0:
/* Stores for OptNLL */
label_Summation_End_OptNLL:
/* endSummation: add vgpr [0...139) to pool */
/* load store sgprs */
.set sgprAddressScaleAlphaVec, 32
.set sgprAddressBias, 34
.set sgprBiasType, 36
.set sgprBiasStride, 37
/* Check if custom structure pointer is null */
s_cmp_eq_u32 s[sgprArgType], 2                     // ArgType == 2 ?
s_cbranch_scc1 label_LoadExternalEpilogueStruct_4  // branch if ArgType == 2
s_load_dwordx4 s[32:35], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x58
s_load_dwordx2 s[36:37], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x68
s_branch label_LoadExternalEpilogueStructEnd
label_LoadExternalEpilogueStruct_4:
s_load_dwordx4 s[32:35], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x90
s_load_dwordx2 s[36:37], s[sgprKernArgAddress:sgprKernArgAddress+1], 0xa0
label_LoadExternalEpilogueStructEnd:
/* load store sgprs2 */
.set sgprAddressTD, 50
.set sgprSynchronizer, 52
/* Check if custom structure pointer is null */
s_cmp_eq_u32 s[sgprArgType], 2                     // ArgType == 2 ?
s_cbranch_scc1 label_LoadExternalEpilogueStruct_5
s_load_dwordx2 s[sgprAddressTD:sgprAddressTD+1], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x70
s_load_dwordx2 s[sgprSynchronizer:sgprSynchronizer+1], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x78
label_LoadExternalEpilogueStruct_5:
.set sgprSrdScaleAlphaVec, 44
.set sgprSrdBias, 56
.set sgprSrdTD, 60
.set sgprGSUSync, 8

/* Mapping of Acc register -> C Vgpr register */
/* computeStoreVgprs */
v_lshrrev_b32 v4, 6, v[vgprSerial]                 // v4 = v[vgprSerial] / 64
v_lshrrev_b32 v5, 2, v4                            // v5 = v4 / 4
v_mul_lo_u32 v1, 0x10, v5                          // wave coordination offset 1
v_and_b32 v5, 15, v[vgprSerial]                    // v5 = v[vgprSerial] % 16
v_add_lshl_u32 v1, v5, v1, 3                       // coordination 1 = vwB *(wave_id1 + tid1)
v_mul_lo_u32 v2, v1, s[sgprStrideC1J]              //  offset 1
v_mul_lo_u32 v3, v1, s[sgprStrideD1J]              //  offset 1
v_and_b32 v5, 3, v4                                // v5 = v4 % 4
v_mul_lo_u32 v5, 0x10, v5                          // wave coordination offset 0
v_and_b32 v0, 63, v[vgprSerial]                    // v0 = v[vgprSerial] % 64
v_lshrrev_b32 v0, 4, v0                            // v0 = v0 / 16
v_lshlrev_b32 v0, 0x2, v0                          // thread0 * continuous_output
v_add_lshl_u32 v0, v5, v0, 0                       // coordination 0 = vwA *(wave_id0 + tid0)
s_mul_i32 s11, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_add_u32 v0, s11, v0                              // coord 0 = (tid0/MI_m)*4 + waveG0*MIB_m + MT0*SG0
s_mul_i32 s11, 128, s[sgprWorkGroup1]              // wgp1 * MT1
v_add_u32 v1, s11, v1                              // coord 1 = (tid0%MI_m) + waveG1*MIB_n + MT1*SG1

/******************************************/
/* Global Write Elements                  */
/******************************************/
s_waitcnt lgkmcnt(0)                               // wait for 40 bytes of kern args.
s_mov_b32 s[sgprSrdScaleAlphaVec+0], s[sgprAddressScaleAlphaVec+0] // init SRD base address (lower)
s_mov_b32 s[sgprSrdScaleAlphaVec+1], s[sgprAddressScaleAlphaVec+1] // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdScaleAlphaVec+3], Srd127_96     // Set bits 127_96 in post-loop SRD
s_cmp_eq_u64 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], 0 // s[AddressScaleAlphaVec] == 0 ?
s_cbranch_scc0 label_ScaleAlphaVecAddrValid        // branch if s[AddressScaleAlphaVec] != 0
s_mov_b32 s[sgprSrdScaleAlphaVec+2], 0
s_branch label_ScaleAlphaVecAddrValid_End
label_ScaleAlphaVecAddrValid:
s_mov_b32 s[sgprSrdScaleAlphaVec+2], s[sgprSizeI]
label_ScaleAlphaVecAddrValid_End:

s_mul_i32 s[sgprSrdScaleAlphaVec+2], 0x4, s[sgprSrdScaleAlphaVec+2] // ScaleAlphaVec scaled by BPE
s_add_u32 s11, s[sgprWorkGroup2], 0x1
s_mul_i32 s11, s[sgprBiasStride], s11              // stride * (wg+1)
s_cmp_eq_u32 s11, 0x0                              // bias stride = 0?
s_cselect_b32 s11, s[sgprSizeI], s11
s_mov_b32 s[sgprSrdBias+0], s[sgprAddressBias+0]   // init SRD base address (lower)
s_mov_b32 s[sgprSrdBias+1], s[sgprAddressBias+1]   // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdBias+3], Srd127_96              // Set bits 127_96 in post-loop SRD
s_cmp_eq_u64 s[sgprAddressBias:sgprAddressBias+1], 0 // s[AddressBias] == 0 ?
s_cbranch_scc0 label_BiasAddrValid                 // branch if s[AddressBias] != 0
s_mov_b32 s[sgprSrdBias+2], 0
s_branch label_BiasAddrValid_End
label_BiasAddrValid:
s_mov_b32 s[sgprSrdBias+2], s11
label_BiasAddrValid_End:

label_Load_Biasf32_0:
s_cmpk_lg_u32 s[sgprBiasType], 0                   // BiasType != 0
s_cbranch_scc1 label_Load_Biasf16_0                // Branch if true

/******************************************/
/* Read vector to LDS                     */
/******************************************/
s_mul_i32 s64, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_add_u32 v8, s64, v[vgprSerial]                   // coord 0 = wgp0 * MT0 + thread offset
s_mul_i32 s[sgprSrdBias+2], 0x4, s[sgprSrdBias+2]  // scaled by BPE
s_mul_i32 s64, s[sgprBiasStride], s[sgprWorkGroup2] // Stride * WG
v_add_u32 v6, s64, v8                              // coord 0 = wgp0 * MT0 + thread offset + Stride * WG
v_lshlrev_b32 v6, 0x2, v6                          // Global bias address scaled by BPE
v_lshlrev_b32 v7, 0x2, v8                          // Global scaleAlpha address scaled by BPE
s_mul_i32 s64, 128, s[sgprWorkGroup1]              // wgp1 * MT1
v_add_u32 v8, s64, v[vgprSerial]                   // coord 1 = wgp1 * MT1 + thread offset
buffer_load_dword v4, v6, s[sgprSrdBias:sgprSrdBias+3], 0 offen offset:0 // Load Bias
buffer_load_dword v5, v7, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // Load ScaleAlphaVec
v_lshlrev_b32 v8, 0x2, v[vgprSerial]               // Local address scaled by BPE
s_barrier                                          // wait for all global loads.
s_waitcnt vmcnt(1)                                 // wait for global load
ds_write_b32 v8, v4 offset:0                       // store bias
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
s_waitcnt vmcnt(0)                                 // wait for global load
v_cndmask_b32 v5, 1.0, v5, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
ds_write_b32 v8, v5 offset:1024                    // store scaleAlpha
s_branch label_Load_Bias_End                       // Branch to load bias end
label_Load_Biasf16_0:
s_cmpk_lg_u32 s[sgprBiasType], 4                   // BiasType != 4
s_cbranch_scc1 label_Load_Bias_End                 // Branch if true

/******************************************/
/* Read vector to LDS                     */
/******************************************/
s_mul_i32 s64, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_add_u32 v8, s64, v[vgprSerial]                   // coord 0 = wgp0 * MT0 + thread offset
s_mul_i32 s[sgprSrdBias+2], 0x2, s[sgprSrdBias+2]  // scaled by BPE
s_mul_i32 s64, s[sgprBiasStride], s[sgprWorkGroup2] // Stride * WG
v_add_u32 v6, s64, v8                              // coord 0 = wgp0 * MT0 + thread offset + Stride * WG
v_lshlrev_b32 v6, 0x1, v6                          // Global bias address scaled by BPE
v_lshlrev_b32 v7, 0x2, v8                          // Global scaleAlpha address scaled by BPE
s_mul_i32 s64, 128, s[sgprWorkGroup1]              // wgp1 * MT1
v_add_u32 v8, s64, v[vgprSerial]                   // coord 1 = wgp1 * MT1 + thread offset
buffer_load_short_d16 v4, v6, s[sgprSrdBias:sgprSrdBias+3], 0 offen offset:0 // Load Bias
buffer_load_dword v5, v7, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // Load ScaleAlphaVec
v_lshlrev_b32 v8, 0x2, v[vgprSerial]               // Local address scaled by BPE
s_barrier                                          // wait for all global loads.
s_waitcnt vmcnt(1)                                 // wait for global load
v_cvt_f32_f16 v4, v4                               // convert to FP32
ds_write_b32 v8, v4 offset:0                       // store bias
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
s_waitcnt vmcnt(0)                                 // wait for global load
v_cndmask_b32 v5, 1.0, v5, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
ds_write_b32 v8, v5 offset:1024                    // store scaleAlpha
s_branch label_Load_Bias_End                       // Branch to load bias end
label_Load_Bias_End:
.set sgprAddressScaleAlphaVec, UNDEF
.set sgprSrdScaleAlphaVec, UNDEF
label_GW_B0_E0:

/* edge=0, allocate 2 sgpr. perBatchTmpS=2 perBatchMaskS=0 perElementMaskS=0 elementsPerBatch=14 */
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,1,0,0:vw4); (0,0,1,0:vw4); (0,1,1,0:vw4); (0,0,2,0:vw4); (0,1,2,0:vw4); (0,0,3,0:vw4); (0,1,3,0:vw4); (0,0,4,0:vw4); (0,1,4,0:vw4); (0,0,5,0:vw4); (0,1,5,0:vw4); (0,0,6,0:vw4); (0,1,6,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s12, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s12
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for LDS write
s_barrier                                          // LDS write barrier
ds_read_b128 v[12:15], v8 offset:0                 // load Bias
v_add_u32 v9, 1024, v8                             // add ScaleAlphaVec offset (1)
ds_read_b128 v[16:19], v9 offset:0                 // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
ds_read_b128 v[24:27], v8 offset:256               // load Bias
ds_read_b128 v[28:31], v9 offset:256               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
v_add_lshl_u32 v6, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc1           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+40], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+41], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+42], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+43], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+44], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+45], acc17          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+46], acc18          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+47], acc19          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+48], acc20          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+49], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+50], acc22          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+51], acc23          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+52], acc24          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+53], acc25          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+54], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+55], acc27          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+56], acc28          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+57], acc29          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+58], acc30          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+59], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+60], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+61], acc33          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+62], acc34          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+63], acc35          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+64], acc36          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+65], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+66], acc38          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+67], acc39          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+68], acc40          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+69], acc41          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+70], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+71], acc43          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+72], acc44          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+73], acc45          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+74], acc46          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+75], acc47          // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+76], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+77], acc49          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+78], acc50          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+79], acc51          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+80], acc52          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+81], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+82], acc54          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+83], acc55          // copy acc to vreg[55]

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(2)                               // lgkmcnt(2) = 4 - 1 (bias) - 1 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D

s_waitcnt lgkmcnt(0)                               // lgkmcnt(0) = 4 - 2 (bias) - 2 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[28:29], v[vgprValuC+40:vgprValuC+40+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[30:31], v[vgprValuC+42:vgprValuC+42+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[24:25], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[26:27], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
buffer_store_dwordx2 v[40:41], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[28:29], v[vgprValuC+48:vgprValuC+48+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[30:31], v[vgprValuC+50:vgprValuC+50+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[24:25], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[26:27], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[16:17], v[vgprValuC+52:vgprValuC+52+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[18:19], v[vgprValuC+54:vgprValuC+54+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+52:vgprValuC+52+1], v[12:13], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[vgprValuC+54:vgprValuC+54+1], v[14:15], v[vgprValuC+54:vgprValuC+54+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+52], v[vgprValuC+52]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+53], v[vgprValuC+53]     // convert C to fp16
v_pack_b32_f16 v52, v[vgprValuC+52], v[vgprValuC+53] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+54], v[vgprValuC+54]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+55], v[vgprValuC+55]     // convert C to fp16
v_pack_b32_f16 v53, v[vgprValuC+54], v[vgprValuC+55] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[52:53], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[28:29], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[30:31], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[24:25], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[26:27], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[12:13], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[14:15], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[60:61], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[28:29], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[30:31], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[16:17], v[vgprValuC+68:vgprValuC+68+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[18:19], v[vgprValuC+70:vgprValuC+70+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+68:vgprValuC+68+1], v[12:13], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[vgprValuC+70:vgprValuC+70+1], v[14:15], v[vgprValuC+70:vgprValuC+70+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+68], v[vgprValuC+68]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+69], v[vgprValuC+69]     // convert C to fp16
v_pack_b32_f16 v68, v[vgprValuC+68], v[vgprValuC+69] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+70], v[vgprValuC+70]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+71], v[vgprValuC+71]     // convert C to fp16
v_pack_b32_f16 v69, v[vgprValuC+70], v[vgprValuC+71] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[68:69], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[28:29], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[30:31], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[24:25], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[26:27], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[16:17], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[18:19], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[12:13], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[14:15], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[76:77], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[28:29], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[30:31], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,0,7,0:vw4); (0,1,7,0:vw4)        */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
ds_read_b128 v[12:15], v8 offset:0                 // load Bias
ds_read_b128 v[16:19], v9 offset:0                 // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
ds_read_b128 v[24:27], v8 offset:256               // load Bias
ds_read_b128 v[28:31], v9 offset:256               // load scaleAlpha
v_accvgpr_read_b32 v[vgprValuC+20], acc56          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+21], acc57          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+22], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+23], acc59          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+32], acc60          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+33], acc61          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+34], acc62          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+35], acc63          // copy acc to vreg[63]

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(2)                               // lgkmcnt(2) = 4 - 1 (bias) - 1 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D

s_waitcnt lgkmcnt(0)                               // lgkmcnt(0) = 4 - 2 (bias) - 2 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End                              // jump to end
label_GW_End:

s_endpgm                                           // Kernel End
label_OptNLL_End:
label_GSU_3:

/******************************************/
/* Ord. NoLoadLoop - Begin 1/2            */
/******************************************/
s_bitcmp1_b32 s[sgprOrigLoopCounter], 0x0          // test if OrigLoopCounter is Odd ?
s_cbranch_scc1 label_OrdNLL_second                 // jump to second NoLoadLoop

/* iter 0 (last unrolled loop) */
s_waitcnt vmcnt(2)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:9, lwStartMfmaIndex:46, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:0  */
s_waitcnt lgkmcnt(7)                               // wait for prior local read local write old=0, new=7 newLW=0 newLR=7 for iteration == 0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
ds_read_b128 v[vgprValuB_X2_I0+0:vgprValuB_X2_I0+0+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0

/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIterDTV] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
ds_read_b128 v[vgprValuB_X2_I0+4:vgprValuB_X2_I0+4+3], v[vgprLocalReadAddrB] offset:192 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s32, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
s_waitcnt lgkmcnt(2)                               // wait for prior local read local write
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
ds_read_b128 v[vgprValuB_X2_I0+8:vgprValuB_X2_I0+8+3], v[vgprLocalReadAddrB] offset:320 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s33, s[sgprWrapUA+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
ds_read_b128 v[vgprValuB_X2_I0+12:vgprValuB_X2_I0+12+3], v[vgprLocalReadAddrB] offset:448 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=2 iui=0
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s32        // gra SRD += inc(lower)
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
ds_read_b128 v[vgprValuB_X2_I0+16:vgprValuB_X2_I0+16+3], v[vgprLocalReadAddrB] offset:576 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=2 iui=0
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s33       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
ds_read_b128 v[vgprValuB_X2_I0+20:vgprValuB_X2_I0+20+3], v[vgprLocalReadAddrB] offset:704 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=2 iui=0
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s32 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
ds_read_b128 v[vgprValuB_X2_I0+24:vgprValuB_X2_I0+24+3], v[vgprLocalReadAddrB] offset:832 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=2 iui=0
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s33 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
ds_read_b128 v[vgprValuB_X2_I0+28:vgprValuB_X2_I0+28+3], v[vgprLocalReadAddrB] offset:960 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=2 iui=0
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=2 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=8 */

/* iter 1 (last unrolled loop) */
/*  grEndMfmaIndex:9, lwStartMfmaIndex:46, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:16  */
/* localReadsVacancy: latencyLeft 2 */
s_waitcnt lgkmcnt(8)                               // wait for prior local read local write old=0, new=8 newLW=0 newLR=8
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:17  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:18  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:21  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:22  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:23  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:24  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:25  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:26  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:27  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:28  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:29  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:30  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:31  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=2 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=8 */

/* iter 2 (last unrolled loop) */
s_waitcnt vmcnt(0)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:9, lwStartMfmaIndex:46, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:32  */
/* localReadsVacancy: latencyLeft 2 */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:33  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:34  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:35  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:36  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:37  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:38  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:39  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:40  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:41  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:42  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:43  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:44  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:45  */
/* schedule remaining localreads for 1LDSB */
/* localReadsVacancy: latencyLeft 2 */
/* 1 LDS buffer: read-sync-write */
s_waitcnt lgkmcnt(0)
s_barrier
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:46  */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:47  */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=2 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=8 */

/* iter 3 (last unrolled loop) */
/*  grEndMfmaIndex:9, lwStartMfmaIndex:46, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:48  */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:49  */
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:50  */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:51  */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:52  */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:53  */
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:54  */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:55  */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:56  */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:57  */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:58  */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:59  */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:60  */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:61  */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:62  */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:63  */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=2 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=8 */
s_branch label_toPGR1end_OrdNLL_0                  // Branch to toPGR1end

/******************************************/
/* Ord. NoLoadLoop - Begin 2/2            */
/******************************************/
label_OrdNLL_second:  /// second Ord NoLoadLoop entry

/* iter 0 (last unrolled loop) */
s_waitcnt vmcnt(2)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:9, lwStartMfmaIndex:46, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:0  */
s_waitcnt lgkmcnt(7)                               // wait for prior local read local write old=0, new=7 newLW=0 newLR=7 for iteration == 0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
ds_read_b128 v[vgprValuB_X2_I0+0:vgprValuB_X2_I0+0+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0

/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIterDTV] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
ds_read_b128 v[vgprValuB_X2_I0+4:vgprValuB_X2_I0+4+3], v[vgprLocalReadAddrB] offset:192 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s32, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
s_waitcnt lgkmcnt(2)                               // wait for prior local read local write
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
ds_read_b128 v[vgprValuB_X2_I0+8:vgprValuB_X2_I0+8+3], v[vgprLocalReadAddrB] offset:320 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s33, s[sgprWrapUA+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
ds_read_b128 v[vgprValuB_X2_I0+12:vgprValuB_X2_I0+12+3], v[vgprLocalReadAddrB] offset:448 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=2 iui=0
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s32        // gra SRD += inc(lower)
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
ds_read_b128 v[vgprValuB_X2_I0+16:vgprValuB_X2_I0+16+3], v[vgprLocalReadAddrB] offset:576 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=2 iui=0
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s33       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
ds_read_b128 v[vgprValuB_X2_I0+20:vgprValuB_X2_I0+20+3], v[vgprLocalReadAddrB] offset:704 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=2 iui=0
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s32 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
ds_read_b128 v[vgprValuB_X2_I0+24:vgprValuB_X2_I0+24+3], v[vgprLocalReadAddrB] offset:832 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=2 iui=0
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s33 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
ds_read_b128 v[vgprValuB_X2_I0+28:vgprValuB_X2_I0+28+3], v[vgprLocalReadAddrB] offset:960 // L -> Reg lro=32 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=2 iui=0
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=2 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=8 */

/* iter 1 (last unrolled loop) */
/*  grEndMfmaIndex:9, lwStartMfmaIndex:46, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:16  */
/* localReadsVacancy: latencyLeft 2 */
s_waitcnt lgkmcnt(8)                               // wait for prior local read local write old=0, new=8 newLW=0 newLR=8
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:17  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:18  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:21  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:22  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:23  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:24  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:25  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:26  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:27  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:28  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:29  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:30  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:31  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=2 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=8 */

/* iter 2 (last unrolled loop) */
s_waitcnt vmcnt(0)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:9, lwStartMfmaIndex:46, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:32  */
/* localReadsVacancy: latencyLeft 2 */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:33  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:34  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:35  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:36  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:37  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:38  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:39  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:40  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:41  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:42  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:43  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:44  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:45  */
/* schedule remaining localreads for 1LDSB */
/* localReadsVacancy: latencyLeft 2 */
/* 1 LDS buffer: read-sync-write */
s_waitcnt lgkmcnt(0)
s_barrier
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:46  */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:47  */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=2 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=8 */

/* iter 3 (last unrolled loop) */
/*  grEndMfmaIndex:9, lwStartMfmaIndex:46, lwEndMfmaIndex:46  */
/*  numMfmaForLR:15, syncPlrMfmaIndex:48  */
/*  mfmaIndex:48  */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:49  */
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:50  */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:51  */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:52  */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:53  */
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:54  */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:55  */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:56  */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:57  */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:58  */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:59  */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:60  */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:61  */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:62  */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:63  */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=2 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=8 */
label_toPGR1end_OrdNLL_0:
label_PrefetchGlobalLastIterEnd:

/******************************************/
/* Tail Loop                              */
/******************************************/

/* Tail: add ValuA/B vgpr buffer [0...80) to pool */

/* local write reset offsets a */

/* local write reset offsets b */

// numIterL = LOCAL_SPLITU * min(sizeL % LOCAL_DEPTHU, DEPTHU / LOCAL_SPLITU)
s_and_b32 s[sgprLoopCounterL], 63, s[sgprSizesSum+0] // s[sgprLoopCounterL] = s[sgprSizesSum+0] % 64
s_and_b32 s32, s[sgprGSU], 0x8000                  // SCC = (GSUC == 1) ?
s_cbranch_scc1 label_GSUC_TL                       // branch if GSUC == 1
s_cmp_lg_u32 s[sgprGSUSumIdx], s[sgprGSUSumIdx+1]  // gsuSumIdx == numIterPerWgRemainder
s_cmov_b32 s[sgprLoopCounterL], 0x0                // numIter=0 if gsuSimIdx != numIterPerWgRemainder
s_branch label_GSUC_TL_End
label_GSUC_TL:
s_lshr_b32 s33, s[sgprSizesSum], 6                 // s33 = s[sgprSizesSum] / 64
s_and_b32 s34, s[sgprGSU], 0x3fff                  // Restore GSU
v_cvt_f32_u32 v0, s34                              // s32 = s33 / s34
v_rcp_iflag_f32 v0, v0                             // s32 = s33 / s34
v_cvt_f32_u32 v1, s33                              // s32 = s33 / s34
v_mul_f32 v0, v0, v1                               // s32 = s33 / s34
v_cvt_u32_f32 v0, v0                               // s32 = s33 / s34
v_mul_u32_u24 v1, v0, s34                          // s32 = s33 / s34
v_sub_u32 v1, s33, v1                              // s32 = s33 / s34
v_cmpx_eq_u32 exec, v1, s34                        // s32 = s33 / s34
v_add_u32 v0, 1, v0                                // s32 = s33 / s34
v_mov_b32 v1, 0                                    // s[sgprGSUSumIdx+1] = s33 % s34
s_mov_b64 exec, -1                                 // s32 = s33 / s34
v_readfirstlane_b32 s32, v0                        // quotient
v_readfirstlane_b32 s[sgprGSUSumIdx+1], v1         // remainder
s_sub_u32 s33, s34, 1                              // GSU-1
s_cmp_eq_u32 s32, 0                                // quotient == 0
s_cselect_b32 s32, s[sgprGSUSumIdx+1], s33         // lastWg = (quotient==0) ? numIterPerWgRemainder : GSU-1
s_cmp_lg_u32 s[sgprGSUSumIdx], s32                 // gsuSumIdx == lastWg
s_cmov_b32 s[sgprLoopCounterL], 0x0                // numIter=0 if gsuSumIdx != lastWg
label_GSUC_TL_End:
s_cmp_eq_u32 s[sgprLoopCounterL], 0x0              // numIterL == 0
s_mov_b32 s[sgprOrigLoopCounter], 0                // repurpose to count each localRead increment
s_cbranch_scc1 label_SkipTailLoopL                 // skip to end of tail loop b/c numIter==0

/* remove stagger offsets for tail loop */
s_sub_i32 s32, 3, s[sgprStaggerUIter]
s_mul_hi_i32 s33, s32, s[sgprGlobalReadIncsA+0]    // start offset S in bytes
s_mul_i32 s32, s32, s[sgprGlobalReadIncsA+0]       // start offset S in bytes
s_sub_u32 s32, s32, s[sgprWrapUA]                  // S - WrapU
s_subb_u32 s33, s33, s[sgprWrapUA+1]               // S - WrapU
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s32        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s33       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s32 // limit -= inc)
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s33 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
s_sub_i32 s32, 3, s[sgprStaggerUIter]
s_mul_hi_i32 s33, s32, s[sgprGlobalReadIncsB+0]    // start offset S in bytes
s_mul_i32 s32, s32, s[sgprGlobalReadIncsB+0]       // start offset S in bytes
s_sub_u32 s32, s32, s[sgprWrapUB]                  // S - WrapU
s_subb_u32 s33, s33, s[sgprWrapUB+1]               // S - WrapU
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s32        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s33       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s32 // limit -= inc)
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s33 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32

/* Update M0 for DTLDS */

/* global read A */
/* g2l=0, load component 0 */
buffer_load_short_d16 v[vgprG2LA+0+0], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // load one buffer value
/* g2l=0, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+0+0], v[vgprG2LA+0+0], v0      // HasEccHalf: pack
/* g2l=0, load component 2 */
buffer_load_short_d16 v[vgprG2LA+0+1], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:4 // load one buffer value
/* g2l=0, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+0+1], v[vgprG2LA+0+1], v0      // HasEccHalf: pack
/* g2l=0, load component 4 */
buffer_load_short_d16 v[vgprG2LA+0+2], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:8 // load one buffer value
/* g2l=0, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+0+2], v[vgprG2LA+0+2], v0      // HasEccHalf: pack
/* g2l=0, load component 6 */
buffer_load_short_d16 v[vgprG2LA+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:12 // load one buffer value
/* g2l=0, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+0+3], v[vgprG2LA+0+3], v0      // HasEccHalf: pack
/* g2l=4, load component 0 */
buffer_load_short_d16 v[vgprG2LA+4+0], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // load one buffer value
/* g2l=4, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+4+0], v[vgprG2LA+4+0], v0      // HasEccHalf: pack
/* g2l=4, load component 2 */
buffer_load_short_d16 v[vgprG2LA+4+1], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:4 // load one buffer value
/* g2l=4, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+4+1], v[vgprG2LA+4+1], v0      // HasEccHalf: pack
/* g2l=4, load component 4 */
buffer_load_short_d16 v[vgprG2LA+4+2], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:8 // load one buffer value
/* g2l=4, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+4+2], v[vgprG2LA+4+2], v0      // HasEccHalf: pack
/* g2l=4, load component 6 */
buffer_load_short_d16 v[vgprG2LA+4+3], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:12 // load one buffer value
/* g2l=4, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+4+3], v[vgprG2LA+4+3], v0      // HasEccHalf: pack
/* g2l=8, load component 0 */
buffer_load_short_d16 v[vgprG2LA+8+0], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // load one buffer value
/* g2l=8, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+8+0], v[vgprG2LA+8+0], v0      // HasEccHalf: pack
/* g2l=8, load component 2 */
buffer_load_short_d16 v[vgprG2LA+8+1], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:4 // load one buffer value
/* g2l=8, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+8+1], v[vgprG2LA+8+1], v0      // HasEccHalf: pack
/* g2l=8, load component 4 */
buffer_load_short_d16 v[vgprG2LA+8+2], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:8 // load one buffer value
/* g2l=8, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+8+2], v[vgprG2LA+8+2], v0      // HasEccHalf: pack
/* g2l=8, load component 6 */
buffer_load_short_d16 v[vgprG2LA+8+3], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:12 // load one buffer value
/* g2l=8, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+8+3], v[vgprG2LA+8+3], v0      // HasEccHalf: pack
/* g2l=12, load component 0 */
buffer_load_short_d16 v[vgprG2LA+12+0], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // load one buffer value
/* g2l=12, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+12+0], v[vgprG2LA+12+0], v0    // HasEccHalf: pack
/* g2l=12, load component 2 */
buffer_load_short_d16 v[vgprG2LA+12+1], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:4 // load one buffer value
/* g2l=12, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+12+1], v[vgprG2LA+12+1], v0    // HasEccHalf: pack
/* g2l=12, load component 4 */
buffer_load_short_d16 v[vgprG2LA+12+2], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:8 // load one buffer value
/* g2l=12, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+12+2], v[vgprG2LA+12+2], v0    // HasEccHalf: pack
/* g2l=12, load component 6 */
buffer_load_short_d16 v[vgprG2LA+12+3], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:12 // load one buffer value
/* g2l=12, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+12+3], v[vgprG2LA+12+3], v0    // HasEccHalf: pack

/* Update M0 for DTLDS */

/* global read B */
/* g2l=0, load component 0 */
buffer_load_short_d16 v[vgprG2LB+0+0], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // load one buffer value
/* g2l=0, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+0+0], v[vgprG2LB+0+0], v0      // HasEccHalf: pack
/* g2l=0, load component 2 */
buffer_load_short_d16 v[vgprG2LB+0+1], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:4 // load one buffer value
/* g2l=0, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+0+1], v[vgprG2LB+0+1], v0      // HasEccHalf: pack
/* g2l=0, load component 4 */
buffer_load_short_d16 v[vgprG2LB+0+2], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:8 // load one buffer value
/* g2l=0, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+0+2], v[vgprG2LB+0+2], v0      // HasEccHalf: pack
/* g2l=0, load component 6 */
buffer_load_short_d16 v[vgprG2LB+0+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:12 // load one buffer value
/* g2l=0, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+0+3], v[vgprG2LB+0+3], v0      // HasEccHalf: pack
/* g2l=4, load component 0 */
buffer_load_short_d16 v[vgprG2LB+4+0], v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // load one buffer value
/* g2l=4, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+4+0], v[vgprG2LB+4+0], v0      // HasEccHalf: pack
/* g2l=4, load component 2 */
buffer_load_short_d16 v[vgprG2LB+4+1], v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:4 // load one buffer value
/* g2l=4, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+4+1], v[vgprG2LB+4+1], v0      // HasEccHalf: pack
/* g2l=4, load component 4 */
buffer_load_short_d16 v[vgprG2LB+4+2], v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:8 // load one buffer value
/* g2l=4, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+4+2], v[vgprG2LB+4+2], v0      // HasEccHalf: pack
/* g2l=4, load component 6 */
buffer_load_short_d16 v[vgprG2LB+4+3], v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:12 // load one buffer value
/* g2l=4, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+4+3], v[vgprG2LB+4+3], v0      // HasEccHalf: pack
/* g2l=8, load component 0 */
buffer_load_short_d16 v[vgprG2LB+8+0], v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // load one buffer value
/* g2l=8, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+8+0], v[vgprG2LB+8+0], v0      // HasEccHalf: pack
/* g2l=8, load component 2 */
buffer_load_short_d16 v[vgprG2LB+8+1], v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:4 // load one buffer value
/* g2l=8, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+8+1], v[vgprG2LB+8+1], v0      // HasEccHalf: pack
/* g2l=8, load component 4 */
buffer_load_short_d16 v[vgprG2LB+8+2], v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:8 // load one buffer value
/* g2l=8, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+8+2], v[vgprG2LB+8+2], v0      // HasEccHalf: pack
/* g2l=8, load component 6 */
buffer_load_short_d16 v[vgprG2LB+8+3], v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:12 // load one buffer value
/* g2l=8, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+8+3], v[vgprG2LB+8+3], v0      // HasEccHalf: pack
/* g2l=12, load component 0 */
buffer_load_short_d16 v[vgprG2LB+12+0], v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // load one buffer value
/* g2l=12, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+12+0], v[vgprG2LB+12+0], v0    // HasEccHalf: pack
/* g2l=12, load component 2 */
buffer_load_short_d16 v[vgprG2LB+12+1], v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:4 // load one buffer value
/* g2l=12, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+12+1], v[vgprG2LB+12+1], v0    // HasEccHalf: pack
/* g2l=12, load component 4 */
buffer_load_short_d16 v[vgprG2LB+12+2], v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:8 // load one buffer value
/* g2l=12, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+12+2], v[vgprG2LB+12+2], v0    // HasEccHalf: pack
/* g2l=12, load component 6 */
buffer_load_short_d16 v[vgprG2LB+12+3], v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:12 // load one buffer value
/* g2l=12, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+12+3], v[vgprG2LB+12+3], v0    // HasEccHalf: pack
s_waitcnt vmcnt(0)                                 // 2wait for global read
// Skip force waitcnt0
s_barrier

/* local write a */

/* local write b */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+3] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+4:vgprG2LB+4+3] offset:4224 // lwoB_0_0_1_0 = (0*LSCB)*(MT1J+PAD) + (1*LSPB) = 4224
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+8:vgprG2LB+8+3] offset:8448 // lwoB_0_0_2_0 = (0*LSCB)*(MT1J+PAD) + (2*LSPB) = 8448
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+12:vgprG2LB+12+3] offset:12672 // lwoB_0_0_3_0 = (0*LSCB)*(MT1J+PAD) + (3*LSPB) = 12672
s_waitcnt lgkmcnt(0)                               // 5wait for local write
// Skip force waitcnt0
s_barrier

/* local read reset offsets a */

/* local read reset offsets b */

/* local read init pointers a */

/* local read init pointers b */

/* localReadInitPointers */

/* tail loop: macs */
label_TailLoopBeginL:

/* Tail: remove ValuA/B vgpr buffer [0...80) from pool */

/* Tail: add address/G2L vgpr [80...139) to pool */

/* tail loop unroll iter 0 */

/* local read a */

/* local read b */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+3], v[vgprLocalReadAddrB] offset:128 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+3], v[vgprLocalReadAddrB] offset:256 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+3], v[vgprLocalReadAddrB] offset:384 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+3], v[vgprLocalReadAddrB] offset:512 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+3], v[vgprLocalReadAddrB] offset:640 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+3], v[vgprLocalReadAddrB] offset:768 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+28:vgprValuB_X0_I0+28+3], v[vgprLocalReadAddrB] offset:896 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0

/* local read inc a */

/* local read inc b */
s_mov_b32 s8, 0x40                                 // inc
v_add_co_u32 v[vgprLocalReadAddrB], vcc, s8, v[vgprLocalReadAddrB] // lrB += 64 (bpeDS)
s_waitcnt lgkmcnt(0)                               // 4wait for local read
v_and_b32 v80, 63, v[vgprSerial]                   // v80 = v[vgprSerial] % 64
v_lshrrev_b32 v80, 4, v80                          // v80 = v80 / 16
v_lshlrev_b32 v80, 0x3, v80                        // v80 = v80 * 8
v_cmp_ge_i32 s[32:33], v80, s[sgprLoopCounterL]    // check K index >= Size L
v_cndmask_b32 v[vgprG2LA+0+0+0], v[vgprG2LA+0+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+4+0+0], v[vgprG2LA+4+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+0+0+1], v[vgprG2LA+0+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+4+0+1], v[vgprG2LA+4+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+0+0+0+0], v[vgprValuB_X0_I0+0+0+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+4+0+0+0], v[vgprValuB_X0_I0+4+0+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+8+0+0+0], v[vgprValuB_X0_I0+8+0+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+12+0+0+0], v[vgprValuB_X0_I0+12+0+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+16+0+0+0], v[vgprValuB_X0_I0+16+0+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+20+0+0+0], v[vgprValuB_X0_I0+20+0+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+24+0+0+0], v[vgprValuB_X0_I0+24+0+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+28+0+0+0], v[vgprValuB_X0_I0+28+0+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+0+0+0+1], v[vgprValuB_X0_I0+0+0+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+4+0+0+1], v[vgprValuB_X0_I0+4+0+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+8+0+0+1], v[vgprValuB_X0_I0+8+0+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+12+0+0+1], v[vgprValuB_X0_I0+12+0+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+16+0+0+1], v[vgprValuB_X0_I0+16+0+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+20+0+0+1], v[vgprValuB_X0_I0+20+0+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+24+0+0+1], v[vgprValuB_X0_I0+24+0+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+28+0+0+1], v[vgprValuB_X0_I0+28+0+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_sub_u32 v80, s[sgprLoopCounterL], v80            // get distance between size and k index
v_cmp_lt_i32 s[32:33], v80, 4                      // set partial 0 if distance less than input per thread
s_and_b32 s34, s[sgprLoopCounterL], 3              // get inputs for edge thread
s_sub_u32 s34, 4, s34                              // use shift to fill 0 for outside element
s_lshl_b32 s34, s34, 4                             // use shift to fill 0 for outside element
v_lshlrev_b64 v[82:83], s34, v[vgprG2LA+0+0:vgprG2LA+0+0+1]
v_cndmask_b32 v[vgprG2LA+0+0+0], v[vgprG2LA+0+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprG2LA+0+0+1], v[vgprG2LA+0+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprG2LA+4+0:vgprG2LA+4+0+1]
v_cndmask_b32 v[vgprG2LA+4+0+0], v[vgprG2LA+4+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprG2LA+4+0+1], v[vgprG2LA+4+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+0+0+0+0], v[vgprValuB_X0_I0+0+0+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X0_I0+0+0+0+1], v[vgprValuB_X0_I0+0+0+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+4+0+0+0], v[vgprValuB_X0_I0+4+0+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X0_I0+4+0+0+1], v[vgprValuB_X0_I0+4+0+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+8+0+0+0], v[vgprValuB_X0_I0+8+0+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X0_I0+8+0+0+1], v[vgprValuB_X0_I0+8+0+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+12+0+0+0], v[vgprValuB_X0_I0+12+0+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X0_I0+12+0+0+1], v[vgprValuB_X0_I0+12+0+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+16+0+0+0], v[vgprValuB_X0_I0+16+0+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X0_I0+16+0+0+1], v[vgprValuB_X0_I0+16+0+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+20+0+0+0], v[vgprValuB_X0_I0+20+0+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X0_I0+20+0+0+1], v[vgprValuB_X0_I0+20+0+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+24+0+0+0], v[vgprValuB_X0_I0+24+0+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X0_I0+24+0+0+1], v[vgprValuB_X0_I0+24+0+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+28+0+0+0], v[vgprValuB_X0_I0+28+0+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X0_I0+28+0+0+1], v[vgprValuB_X0_I0+28+0+0+1], v83, s[32:33]
s_nop 1
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[60:63] // left value = acc[60+0:63+0]

/* tail loop unroll iter 1 */

/* local read a */

/* local read b */
ds_read_b128 v[vgprValuB_X2_I0+0:vgprValuB_X2_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
ds_read_b128 v[vgprValuB_X2_I0+4:vgprValuB_X2_I0+4+3], v[vgprLocalReadAddrB] offset:128 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=2 iui=0
ds_read_b128 v[vgprValuB_X2_I0+8:vgprValuB_X2_I0+8+3], v[vgprLocalReadAddrB] offset:256 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=2 iui=0
ds_read_b128 v[vgprValuB_X2_I0+12:vgprValuB_X2_I0+12+3], v[vgprLocalReadAddrB] offset:384 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=2 iui=0
ds_read_b128 v[vgprValuB_X2_I0+16:vgprValuB_X2_I0+16+3], v[vgprLocalReadAddrB] offset:512 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=2 iui=0
ds_read_b128 v[vgprValuB_X2_I0+20:vgprValuB_X2_I0+20+3], v[vgprLocalReadAddrB] offset:640 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=2 iui=0
ds_read_b128 v[vgprValuB_X2_I0+24:vgprValuB_X2_I0+24+3], v[vgprLocalReadAddrB] offset:768 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=2 iui=0
ds_read_b128 v[vgprValuB_X2_I0+28:vgprValuB_X2_I0+28+3], v[vgprLocalReadAddrB] offset:896 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=2 iui=0

/* local read inc a */

/* local read inc b */
s_mov_b32 s8, 0x40                                 // inc
v_add_co_u32 v[vgprLocalReadAddrB], vcc, s8, v[vgprLocalReadAddrB] // lrB += 64 (bpeDS)
s_waitcnt lgkmcnt(0)                               // 4wait for local read
v_and_b32 v80, 63, v[vgprSerial]                   // v80 = v[vgprSerial] % 64
v_lshrrev_b32 v80, 4, v80                          // v80 = v80 / 16
v_lshlrev_b32 v80, 0x3, v80                        // v80 = v80 * 8
v_add_u32 v80, 0x4, v80                            // k += (u%%numReadsIterCoalesced) * numMIInput
v_cmp_ge_i32 s[32:33], v80, s[sgprLoopCounterL]    // check K index >= Size L
v_cndmask_b32 v[vgprG2LA+0+2+0], v[vgprG2LA+0+2+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+4+2+0], v[vgprG2LA+4+2+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+0+2+1], v[vgprG2LA+0+2+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+4+2+1], v[vgprG2LA+4+2+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+0+2+0+0], v[vgprValuB_X0_I0+0+2+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+4+2+0+0], v[vgprValuB_X0_I0+4+2+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+8+2+0+0], v[vgprValuB_X0_I0+8+2+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+12+2+0+0], v[vgprValuB_X0_I0+12+2+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+16+2+0+0], v[vgprValuB_X0_I0+16+2+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+20+2+0+0], v[vgprValuB_X0_I0+20+2+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+24+2+0+0], v[vgprValuB_X0_I0+24+2+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+28+2+0+0], v[vgprValuB_X0_I0+28+2+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+0+2+0+1], v[vgprValuB_X0_I0+0+2+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+4+2+0+1], v[vgprValuB_X0_I0+4+2+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+8+2+0+1], v[vgprValuB_X0_I0+8+2+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+12+2+0+1], v[vgprValuB_X0_I0+12+2+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+16+2+0+1], v[vgprValuB_X0_I0+16+2+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+20+2+0+1], v[vgprValuB_X0_I0+20+2+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+24+2+0+1], v[vgprValuB_X0_I0+24+2+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+28+2+0+1], v[vgprValuB_X0_I0+28+2+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_sub_u32 v80, s[sgprLoopCounterL], v80            // get distance between size and k index
v_cmp_lt_i32 s[32:33], v80, 4                      // set partial 0 if distance less than input per thread
s_and_b32 s34, s[sgprLoopCounterL], 3              // get inputs for edge thread
s_sub_u32 s34, 4, s34                              // use shift to fill 0 for outside element
s_lshl_b32 s34, s34, 4                             // use shift to fill 0 for outside element
v_lshlrev_b64 v[82:83], s34, v[vgprG2LA+0+2:vgprG2LA+0+2+1]
v_cndmask_b32 v[vgprG2LA+0+2+0], v[vgprG2LA+0+2+0], v82, s[32:33]
v_cndmask_b32 v[vgprG2LA+0+2+1], v[vgprG2LA+0+2+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprG2LA+4+2:vgprG2LA+4+2+1]
v_cndmask_b32 v[vgprG2LA+4+2+0], v[vgprG2LA+4+2+0], v82, s[32:33]
v_cndmask_b32 v[vgprG2LA+4+2+1], v[vgprG2LA+4+2+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+0+2+0+0], v[vgprValuB_X0_I0+0+2+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X0_I0+0+2+0+1], v[vgprValuB_X0_I0+0+2+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+4+2+0+0], v[vgprValuB_X0_I0+4+2+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X0_I0+4+2+0+1], v[vgprValuB_X0_I0+4+2+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+8+2+0+0], v[vgprValuB_X0_I0+8+2+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X0_I0+8+2+0+1], v[vgprValuB_X0_I0+8+2+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+12+2+0+0], v[vgprValuB_X0_I0+12+2+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X0_I0+12+2+0+1], v[vgprValuB_X0_I0+12+2+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+16+2+0+0], v[vgprValuB_X0_I0+16+2+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X0_I0+16+2+0+1], v[vgprValuB_X0_I0+16+2+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+20+2+0+0], v[vgprValuB_X0_I0+20+2+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X0_I0+20+2+0+1], v[vgprValuB_X0_I0+20+2+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+24+2+0+0], v[vgprValuB_X0_I0+24+2+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X0_I0+24+2+0+1], v[vgprValuB_X0_I0+24+2+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+28+2+0+0], v[vgprValuB_X0_I0+28+2+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X0_I0+28+2+0+1], v[vgprValuB_X0_I0+28+2+0+1], v83, s[32:33]
s_nop 1
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[60:63] // left value = acc[60+0:63+0]

/* closeLoop loopL finalLoop=0 tailLoop=1 */
s_sub_i32 s[sgprLoopCounterL], s[sgprLoopCounterL], 0x20 // dec counterL (tailLoop)
s_add_u32 s[sgprOrigLoopCounter], s[sgprOrigLoopCounter], 0x20 // inc counterL
s_cmp_le_i32 s[sgprLoopCounterL], 0x0              // counterL<=0
s_cbranch_scc1 label_TailLoopEndL                  // exit LoopL

/* tail loop unroll iter 2 */

/* local read inc a */

/* local read inc b */
s_mov_b32 s8, 0x40                                 // inc
v_add_co_u32 v[vgprLocalReadAddrB], vcc, s8, v[vgprLocalReadAddrB] // lrB += 64 (bpeDS)
s_waitcnt lgkmcnt(0)                               // 4wait for local read
v_and_b32 v80, 63, v[vgprSerial]                   // v80 = v[vgprSerial] % 64
v_lshrrev_b32 v80, 4, v80                          // v80 = v80 / 16
v_lshlrev_b32 v80, 0x3, v80                        // v80 = v80 * 8
v_cmp_ge_i32 s[32:33], v80, s[sgprLoopCounterL]    // check K index >= Size L
v_cndmask_b32 v[vgprG2LA+8+0+0], v[vgprG2LA+8+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+12+0+0], v[vgprG2LA+12+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+8+0+1], v[vgprG2LA+8+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+12+0+1], v[vgprG2LA+12+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+0+0+0+0], v[vgprValuB_X2_I0+0+0+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+4+0+0+0], v[vgprValuB_X2_I0+4+0+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+8+0+0+0], v[vgprValuB_X2_I0+8+0+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+12+0+0+0], v[vgprValuB_X2_I0+12+0+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+16+0+0+0], v[vgprValuB_X2_I0+16+0+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+20+0+0+0], v[vgprValuB_X2_I0+20+0+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+24+0+0+0], v[vgprValuB_X2_I0+24+0+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+28+0+0+0], v[vgprValuB_X2_I0+28+0+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+0+0+0+1], v[vgprValuB_X2_I0+0+0+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+4+0+0+1], v[vgprValuB_X2_I0+4+0+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+8+0+0+1], v[vgprValuB_X2_I0+8+0+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+12+0+0+1], v[vgprValuB_X2_I0+12+0+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+16+0+0+1], v[vgprValuB_X2_I0+16+0+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+20+0+0+1], v[vgprValuB_X2_I0+20+0+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+24+0+0+1], v[vgprValuB_X2_I0+24+0+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+28+0+0+1], v[vgprValuB_X2_I0+28+0+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_sub_u32 v80, s[sgprLoopCounterL], v80            // get distance between size and k index
v_cmp_lt_i32 s[32:33], v80, 4                      // set partial 0 if distance less than input per thread
s_and_b32 s34, s[sgprLoopCounterL], 3              // get inputs for edge thread
s_sub_u32 s34, 4, s34                              // use shift to fill 0 for outside element
s_lshl_b32 s34, s34, 4                             // use shift to fill 0 for outside element
v_lshlrev_b64 v[82:83], s34, v[vgprG2LA+8+0:vgprG2LA+8+0+1]
v_cndmask_b32 v[vgprG2LA+8+0+0], v[vgprG2LA+8+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprG2LA+8+0+1], v[vgprG2LA+8+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprG2LA+12+0:vgprG2LA+12+0+1]
v_cndmask_b32 v[vgprG2LA+12+0+0], v[vgprG2LA+12+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprG2LA+12+0+1], v[vgprG2LA+12+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1]
v_cndmask_b32 v[vgprValuB_X2_I0+0+0+0+0], v[vgprValuB_X2_I0+0+0+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X2_I0+0+0+0+1], v[vgprValuB_X2_I0+0+0+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1]
v_cndmask_b32 v[vgprValuB_X2_I0+4+0+0+0], v[vgprValuB_X2_I0+4+0+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X2_I0+4+0+0+1], v[vgprValuB_X2_I0+4+0+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1]
v_cndmask_b32 v[vgprValuB_X2_I0+8+0+0+0], v[vgprValuB_X2_I0+8+0+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X2_I0+8+0+0+1], v[vgprValuB_X2_I0+8+0+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1]
v_cndmask_b32 v[vgprValuB_X2_I0+12+0+0+0], v[vgprValuB_X2_I0+12+0+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X2_I0+12+0+0+1], v[vgprValuB_X2_I0+12+0+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1]
v_cndmask_b32 v[vgprValuB_X2_I0+16+0+0+0], v[vgprValuB_X2_I0+16+0+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X2_I0+16+0+0+1], v[vgprValuB_X2_I0+16+0+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1]
v_cndmask_b32 v[vgprValuB_X2_I0+20+0+0+0], v[vgprValuB_X2_I0+20+0+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X2_I0+20+0+0+1], v[vgprValuB_X2_I0+20+0+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1]
v_cndmask_b32 v[vgprValuB_X2_I0+24+0+0+0], v[vgprValuB_X2_I0+24+0+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X2_I0+24+0+0+1], v[vgprValuB_X2_I0+24+0+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1]
v_cndmask_b32 v[vgprValuB_X2_I0+28+0+0+0], v[vgprValuB_X2_I0+28+0+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X2_I0+28+0+0+1], v[vgprValuB_X2_I0+28+0+0+1], v83, s[32:33]
s_nop 1
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], acc[60:63] // left value = acc[60+0:63+0]

/* tail loop unroll iter 3 */

/* local read inc a */

/* local read inc b */
s_mov_b32 s8, 0x40                                 // inc
v_add_co_u32 v[vgprLocalReadAddrB], vcc, s8, v[vgprLocalReadAddrB] // lrB += 64 (bpeDS)
s_waitcnt lgkmcnt(0)                               // 4wait for local read
v_and_b32 v80, 63, v[vgprSerial]                   // v80 = v[vgprSerial] % 64
v_lshrrev_b32 v80, 4, v80                          // v80 = v80 / 16
v_lshlrev_b32 v80, 0x3, v80                        // v80 = v80 * 8
v_add_u32 v80, 0x4, v80                            // k += (u%%numReadsIterCoalesced) * numMIInput
v_cmp_ge_i32 s[32:33], v80, s[sgprLoopCounterL]    // check K index >= Size L
v_cndmask_b32 v[vgprG2LA+8+2+0], v[vgprG2LA+8+2+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+12+2+0], v[vgprG2LA+12+2+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+8+2+1], v[vgprG2LA+8+2+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+12+2+1], v[vgprG2LA+12+2+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+0+2+0+0], v[vgprValuB_X2_I0+0+2+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+4+2+0+0], v[vgprValuB_X2_I0+4+2+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+8+2+0+0], v[vgprValuB_X2_I0+8+2+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+12+2+0+0], v[vgprValuB_X2_I0+12+2+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+16+2+0+0], v[vgprValuB_X2_I0+16+2+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+20+2+0+0], v[vgprValuB_X2_I0+20+2+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+24+2+0+0], v[vgprValuB_X2_I0+24+2+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+28+2+0+0], v[vgprValuB_X2_I0+28+2+0+0], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+0+2+0+1], v[vgprValuB_X2_I0+0+2+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+4+2+0+1], v[vgprValuB_X2_I0+4+2+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+8+2+0+1], v[vgprValuB_X2_I0+8+2+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+12+2+0+1], v[vgprValuB_X2_I0+12+2+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+16+2+0+1], v[vgprValuB_X2_I0+16+2+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+20+2+0+1], v[vgprValuB_X2_I0+20+2+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+24+2+0+1], v[vgprValuB_X2_I0+24+2+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X2_I0+28+2+0+1], v[vgprValuB_X2_I0+28+2+0+1], 0x0, s[32:33] // set 0 if K_idx >= sizeL
v_sub_u32 v80, s[sgprLoopCounterL], v80            // get distance between size and k index
v_cmp_lt_i32 s[32:33], v80, 4                      // set partial 0 if distance less than input per thread
s_and_b32 s34, s[sgprLoopCounterL], 3              // get inputs for edge thread
s_sub_u32 s34, 4, s34                              // use shift to fill 0 for outside element
s_lshl_b32 s34, s34, 4                             // use shift to fill 0 for outside element
v_lshlrev_b64 v[82:83], s34, v[vgprG2LA+8+2:vgprG2LA+8+2+1]
v_cndmask_b32 v[vgprG2LA+8+2+0], v[vgprG2LA+8+2+0], v82, s[32:33]
v_cndmask_b32 v[vgprG2LA+8+2+1], v[vgprG2LA+8+2+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprG2LA+12+2:vgprG2LA+12+2+1]
v_cndmask_b32 v[vgprG2LA+12+2+0], v[vgprG2LA+12+2+0], v82, s[32:33]
v_cndmask_b32 v[vgprG2LA+12+2+1], v[vgprG2LA+12+2+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1]
v_cndmask_b32 v[vgprValuB_X2_I0+0+2+0+0], v[vgprValuB_X2_I0+0+2+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X2_I0+0+2+0+1], v[vgprValuB_X2_I0+0+2+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1]
v_cndmask_b32 v[vgprValuB_X2_I0+4+2+0+0], v[vgprValuB_X2_I0+4+2+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X2_I0+4+2+0+1], v[vgprValuB_X2_I0+4+2+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1]
v_cndmask_b32 v[vgprValuB_X2_I0+8+2+0+0], v[vgprValuB_X2_I0+8+2+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X2_I0+8+2+0+1], v[vgprValuB_X2_I0+8+2+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1]
v_cndmask_b32 v[vgprValuB_X2_I0+12+2+0+0], v[vgprValuB_X2_I0+12+2+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X2_I0+12+2+0+1], v[vgprValuB_X2_I0+12+2+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1]
v_cndmask_b32 v[vgprValuB_X2_I0+16+2+0+0], v[vgprValuB_X2_I0+16+2+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X2_I0+16+2+0+1], v[vgprValuB_X2_I0+16+2+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1]
v_cndmask_b32 v[vgprValuB_X2_I0+20+2+0+0], v[vgprValuB_X2_I0+20+2+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X2_I0+20+2+0+1], v[vgprValuB_X2_I0+20+2+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1]
v_cndmask_b32 v[vgprValuB_X2_I0+24+2+0+0], v[vgprValuB_X2_I0+24+2+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X2_I0+24+2+0+1], v[vgprValuB_X2_I0+24+2+0+1], v83, s[32:33]
v_lshlrev_b64 v[82:83], s34, v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1]
v_cndmask_b32 v[vgprValuB_X2_I0+28+2+0+0], v[vgprValuB_X2_I0+28+2+0+0], v82, s[32:33]
v_cndmask_b32 v[vgprValuB_X2_I0+28+2+0+1], v[vgprValuB_X2_I0+28+2+0+1], v83, s[32:33]
s_nop 1
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], acc[60:63] // left value = acc[60+0:63+0]

/* closeLoop loopL finalLoop=1 tailLoop=1 */
s_sub_i32 s[sgprLoopCounterL], s[sgprLoopCounterL], 0x20 // dec counterL (tailLoop)
s_add_u32 s[sgprOrigLoopCounter], s[sgprOrigLoopCounter], 0x20 // inc counterL
s_cmp_le_i32 s[sgprLoopCounterL], 0x0              // counterL<=0
s_cbranch_scc0 label_TailLoopBeginL                // restart LoopL
label_TailLoopEndL:
label_SkipTailLoopL:

/* Tail: remove address/G2L [80...139) from pool */
label_Summation_End_S3F8GKEC0GZ9PKCV_0:
/* endSummation: add vgpr [0...139) to pool */
.set sgprWGM, UNDEF
.set sgprLoopCounterL, UNDEF
.set sgprOrigLoopCounter, UNDEF
.set sgprShadowLimitA, UNDEF
.set sgprAddressA, UNDEF
.set sgprAddressB, UNDEF
.set sgprStridesA, UNDEF
.set sgprStridesB, UNDEF
.set sgprSrdA, UNDEF
.set sgprSrdB, UNDEF
.set sgprShadowLimitB, UNDEF
.set sgprStaggerUIter, UNDEF
.set sgprStaggerUIterDTV, UNDEF
.set sgprWrapUA, UNDEF
.set sgprWrapUB, UNDEF
.set sgprGlobalReadIncsA, UNDEF
.set sgprGlobalReadIncsB, UNDEF
/* load store sgprs */
.set sgprAddressScaleAlphaVec, 32
.set sgprAddressBias, 34
.set sgprBiasType, 36
.set sgprBiasStride, 37
s_and_b32 s8, s[sgprGSU], 0x3fff                   // Restore GSU
s_cmp_eq_u32 s8, 1                                 // GSU == 1 ?
/* Check if custom structure pointer is null */
s_cmp_eq_u32 s[sgprArgType], 2                     // ArgType == 2 ?
s_cbranch_scc1 label_LoadExternalEpilogueStruct_6  // branch if ArgType == 2
s_load_dwordx4 s[32:35], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x58
s_load_dwordx2 s[36:37], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x68
s_branch label_LoadExternalEpilogueStructEnd_1
label_LoadExternalEpilogueStruct_6:
s_load_dwordx4 s[32:35], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x90
s_load_dwordx2 s[36:37], s[sgprKernArgAddress:sgprKernArgAddress+1], 0xa0
label_LoadExternalEpilogueStructEnd_1:
label_GSU_4:
/* load store sgprs2 */
.set sgprAddressTD, 50
.set sgprSynchronizer, 52
/* Check if custom structure pointer is null */
s_cmp_eq_u32 s[sgprArgType], 2                     // ArgType == 2 ?
s_cbranch_scc1 label_LoadExternalEpilogueStruct_7
s_load_dwordx2 s[sgprAddressTD:sgprAddressTD+1], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x70
s_load_dwordx2 s[sgprSynchronizer:sgprSynchronizer+1], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x78
label_LoadExternalEpilogueStruct_7:
.set sgprSrdScaleAlphaVec, 44
.set sgprSrdBias, 56
.set sgprSrdTD, 60
.set sgprGSUSync, 8
.set sgprSrdSync, 64

/* Mapping of Acc register -> C Vgpr register */

/* not-LocalSplitU: global write indices */
/* computeStoreVgprs */
v_lshrrev_b32 v4, 6, v[vgprSerial]                 // v4 = v[vgprSerial] / 64
v_lshrrev_b32 v5, 2, v4                            // v5 = v4 / 4
v_mul_lo_u32 v1, 0x10, v5                          // wave coordination offset 1
v_and_b32 v5, 15, v[vgprSerial]                    // v5 = v[vgprSerial] % 16
v_add_lshl_u32 v1, v5, v1, 3                       // coordination 1 = vwB *(wave_id1 + tid1)
v_mul_lo_u32 v2, v1, s[sgprStrideC1J]              //  offset 1
v_mul_lo_u32 v3, v1, s[sgprStrideD1J]              //  offset 1
v_and_b32 v5, 3, v4                                // v5 = v4 % 4
v_mul_lo_u32 v5, 0x10, v5                          // wave coordination offset 0
v_and_b32 v0, 63, v[vgprSerial]                    // v0 = v[vgprSerial] % 64
v_lshrrev_b32 v0, 4, v0                            // v0 = v0 / 16
v_lshlrev_b32 v0, 0x2, v0                          // thread0 * continuous_output
v_add_lshl_u32 v0, v5, v0, 0                       // coordination 0 = vwA *(wave_id0 + tid0)
s_mul_i32 s11, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_add_u32 v0, s11, v0                              // coord 0 = (tid0/MI_m)*4 + waveG0*MIB_m + MT0*SG0
s_mul_i32 s11, 128, s[sgprWorkGroup1]              // wgp1 * MT1
v_add_u32 v1, s11, v1                              // coord 1 = (tid0%MI_m) + waveG1*MIB_n + MT1*SG1

/* not-LocalSplitU: global write */

/******************************************/
/* Global Write Elements                  */
/******************************************/
s_waitcnt lgkmcnt(0)                               // wait for 40 bytes of kern args.
s_and_b32 s11, s[sgprGSU], 0x3fff                  // Restore GSU
s_cmp_eq_u32 s11, 1                                // GSU == 1 ?
s_cbranch_scc0 label_NoBranch_RGB0UOKR6XZP42OO_0   // Only branch on scc1
// long branch if GSU == 1
s_getpc_b64 s[68:69]                               // addr of next instr
s_add_i32 s70, label_GSU_5, 0x4                    // target branch offset
s_add_u32 s68, s68, s70                            // add target branch offset
s_addc_u32 s69, s69, 0                             // add high and carry
s_setpc_b64 s[68:69]                               // branch to label_GSU_5
label_NoBranch_RGB0UOKR6XZP42OO_0:
s_mov_b32 s[sgprSrdScaleAlphaVec+0], s[sgprAddressScaleAlphaVec+0] // init SRD base address (lower)
s_mov_b32 s[sgprSrdScaleAlphaVec+1], s[sgprAddressScaleAlphaVec+1] // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdScaleAlphaVec+3], Srd127_96     // Set bits 127_96 in post-loop SRD
s_cmp_eq_u64 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], 0 // s[AddressScaleAlphaVec] == 0 ?
s_cbranch_scc0 label_ScaleAlphaVec_1AddrValid      // branch if s[AddressScaleAlphaVec] != 0
s_mov_b32 s[sgprSrdScaleAlphaVec+2], 0
s_branch label_ScaleAlphaVec_1AddrValid_End
label_ScaleAlphaVec_1AddrValid:
s_mov_b32 s[sgprSrdScaleAlphaVec+2], s[sgprSizeI]
label_ScaleAlphaVec_1AddrValid_End:

s_mul_i32 s[sgprSrdScaleAlphaVec+2], 0x4, s[sgprSrdScaleAlphaVec+2] // ScaleAlphaVec scaled by BPE

s_mov_b32 s[sgprSrdTD+3], Srd127_96                // Set bits 127_96 in post-loop SRD
s_mov_b32 s[sgprSrdTD+2], 0x80000000
s_mul_i32 s11, MT1, s[sgprWorkGroup1]
s_mul_hi_u32 s13, s11, s[sgprStrideC1J]
s_mul_i32 s12, s11, s[sgprStrideC1J]
s_lshl_b64 s[12:13], s[12:13], 1                   // scale by bpe
s_add_u32 s[sgprSrdTD+0], s[sgprAddressTD+0], s12
s_addc_u32 s[sgprSrdTD+1], s[sgprAddressTD+1], s13
s_mul_hi_u32 s13, s[sgprStrideCK], s[sgprWorkGroup2]
s_mul_i32 s12, s[sgprStrideCK], s[sgprWorkGroup2]
s_lshl_b64 s[12:13], s[12:13], 1                   // scale by bpe
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s12
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], s13
s_add_u32 s11, s[sgprWorkGroup2], 0x1
s_mul_i32 s11, s[sgprBiasStride], s11              // stride * (wg+1)
s_cmp_eq_u32 s11, 0x0                              // bias stride = 0?
s_cselect_b32 s11, s[sgprSizeI], s11
s_mov_b32 s[sgprSrdBias+0], s[sgprAddressBias+0]   // init SRD base address (lower)
s_mov_b32 s[sgprSrdBias+1], s[sgprAddressBias+1]   // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdBias+3], Srd127_96              // Set bits 127_96 in post-loop SRD
s_cmp_eq_u64 s[sgprAddressBias:sgprAddressBias+1], 0 // s[AddressBias] == 0 ?
s_cbranch_scc0 label_Bias_1AddrValid               // branch if s[AddressBias] != 0
s_mov_b32 s[sgprSrdBias+2], 0
s_branch label_Bias_1AddrValid_End
label_Bias_1AddrValid:
s_mov_b32 s[sgprSrdBias+2], s11
label_Bias_1AddrValid_End:

label_Load_Biasf32_0_1:
s_cmpk_lg_u32 s[sgprBiasType], 0                   // BiasType != 0
s_cbranch_scc1 label_Load_Biasf16_0_1              // Branch if true

/******************************************/
/* Read vector to LDS                     */
/******************************************/
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_add_u32 v8, s68, v[vgprSerial]                   // coord 0 = wgp0 * MT0 + thread offset
s_mul_i32 s[sgprSrdBias+2], 0x4, s[sgprSrdBias+2]  // scaled by BPE
s_mul_i32 s68, s[sgprBiasStride], s[sgprWorkGroup2] // Stride * WG
v_add_u32 v6, s68, v8                              // coord 0 = wgp0 * MT0 + thread offset + Stride * WG
v_lshlrev_b32 v6, 0x2, v6                          // Global bias address scaled by BPE
v_lshlrev_b32 v7, 0x2, v8                          // Global scaleAlpha address scaled by BPE
s_mul_i32 s68, 128, s[sgprWorkGroup1]              // wgp1 * MT1
v_add_u32 v8, s68, v[vgprSerial]                   // coord 1 = wgp1 * MT1 + thread offset
buffer_load_dword v4, v6, s[sgprSrdBias:sgprSrdBias+3], 0 offen offset:0 // Load Bias
buffer_load_dword v5, v7, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // Load ScaleAlphaVec
v_lshlrev_b32 v8, 0x2, v[vgprSerial]               // Local address scaled by BPE
s_barrier                                          // wait for all global loads.
s_waitcnt vmcnt(1)                                 // wait for global load
ds_write_b32 v8, v4 offset:0                       // store bias
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
s_waitcnt vmcnt(0)                                 // wait for global load
v_cndmask_b32 v5, 1.0, v5, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
ds_write_b32 v8, v5 offset:1024                    // store scaleAlpha
s_branch label_Load_Bias_End_1                     // Branch to load bias end
label_Load_Biasf16_0_1:
s_cmpk_lg_u32 s[sgprBiasType], 4                   // BiasType != 4
s_cbranch_scc1 label_Load_Bias_End_1               // Branch if true

/******************************************/
/* Read vector to LDS                     */
/******************************************/
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_add_u32 v8, s68, v[vgprSerial]                   // coord 0 = wgp0 * MT0 + thread offset
s_mul_i32 s[sgprSrdBias+2], 0x2, s[sgprSrdBias+2]  // scaled by BPE
s_mul_i32 s68, s[sgprBiasStride], s[sgprWorkGroup2] // Stride * WG
v_add_u32 v6, s68, v8                              // coord 0 = wgp0 * MT0 + thread offset + Stride * WG
v_lshlrev_b32 v6, 0x1, v6                          // Global bias address scaled by BPE
v_lshlrev_b32 v7, 0x2, v8                          // Global scaleAlpha address scaled by BPE
s_mul_i32 s68, 128, s[sgprWorkGroup1]              // wgp1 * MT1
v_add_u32 v8, s68, v[vgprSerial]                   // coord 1 = wgp1 * MT1 + thread offset
buffer_load_short_d16 v4, v6, s[sgprSrdBias:sgprSrdBias+3], 0 offen offset:0 // Load Bias
buffer_load_dword v5, v7, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // Load ScaleAlphaVec
v_lshlrev_b32 v8, 0x2, v[vgprSerial]               // Local address scaled by BPE
s_barrier                                          // wait for all global loads.
s_waitcnt vmcnt(1)                                 // wait for global load
v_cvt_f32_f16 v4, v4                               // convert to FP32
ds_write_b32 v8, v4 offset:0                       // store bias
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
s_waitcnt vmcnt(0)                                 // wait for global load
v_cndmask_b32 v5, 1.0, v5, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
ds_write_b32 v8, v5 offset:1024                    // store scaleAlpha
s_branch label_Load_Bias_End_1                     // Branch to load bias end
label_Load_Bias_End_1:
.set sgprAddressScaleAlphaVec, UNDEF
.set sgprSrdScaleAlphaVec, UNDEF
s_cmpk_eq_u32 s[sgprBeta], 0x0                     // Beta == 0
s_cbranch_scc0 label_GW_Beta_1                     // Branch if Beta is not zero

s_and_b32 s44, 127, s[sgprSizeI]                   // s44 = s[sgprSizeI] % 128
s_add_u32 s45, -0x1, s[sgprNumWorkGroups0]
s_cmp_ge_u32 s[sgprWorkGroup0], s45                // wg0 >= nwg0-1 ?
s_cselect_b32 s44, s44, 0                          // set rMT0
s_cmpk_gt_u32 s44, 0x0                             // rMT0 > 0
s_cbranch_scc1 label_GW_B0_E1_M                    // jump if edges required
s_and_b32 s44, 127, s[sgprSizeJ]                   // s44 = s[sgprSizeJ] % 128
s_add_u32 s45, -0x1, s[sgprNumWorkGroups1]
s_cmp_ge_u32 s[sgprWorkGroup1], s45                // wg1 >= nwg1-1
s_cselect_b32 s44, s44, 0                          // set rMT1
s_cmpk_gt_u32 s44, 0x0                             // rMT1 > 0
s_cbranch_scc1 label_GW_B0_E1_N                    // jump if edges required
label_GW_B0_E0_1:

/* edge=0, allocate 2 sgpr. perBatchTmpS=2 perBatchMaskS=0 perElementMaskS=0 elementsPerBatch=14 */
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,1,0,0:vw4); (0,0,1,0:vw4); (0,1,1,0:vw4); (0,0,2,0:vw4); (0,1,2,0:vw4); (0,0,3,0:vw4); (0,1,3,0:vw4); (0,0,4,0:vw4); (0,1,4,0:vw4); (0,0,5,0:vw4); (0,1,5,0:vw4); (0,0,6,0:vw4); (0,1,6,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s12, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v9, v0, s12
v_lshlrev_b32 v9, 0x2, v9                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for LDS write
s_barrier                                          // LDS write barrier
ds_read_b128 v[12:15], v9 offset:0                 // load Bias
v_add_u32 v10, 1024, v9                            // add ScaleAlphaVec offset (1)
ds_read_b128 v[16:19], v10 offset:0                // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
ds_read_b128 v[24:27], v9 offset:256               // load Bias
ds_read_b128 v[28:31], v10 offset:256              // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_add_lshl_u32 v7, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc1           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+40], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+41], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+42], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+43], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+44], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+45], acc17          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+46], acc18          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+47], acc19          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+48], acc20          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+49], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+50], acc22          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+51], acc23          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+52], acc24          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+53], acc25          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+54], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+55], acc27          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+56], acc28          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+57], acc29          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+58], acc30          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+59], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+60], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+61], acc33          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+62], acc34          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+63], acc35          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+64], acc36          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+65], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+66], acc38          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+67], acc39          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+68], acc40          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+69], acc41          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+70], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+71], acc43          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+72], acc44          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+73], acc45          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+74], acc46          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+75], acc47          // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+76], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+77], acc49          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+78], acc50          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+79], acc51          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+80], acc52          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+81], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+82], acc54          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+83], acc55          // copy acc to vreg[55]

/* store after Acc, GSU: 2 */

/* store after Acc, GSU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 20

buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256, sc0 sc1 // store D 32

s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[36:39], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 36

buffer_store_dwordx4 v[40:43], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256, sc0 sc1 // store D 40

s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[44:47], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 44

buffer_store_dwordx4 v[48:51], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256, sc0 sc1 // store D 48

s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[52:55], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 52

buffer_store_dwordx4 v[56:59], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256, sc0 sc1 // store D 56

s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[60:63], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 60

buffer_store_dwordx4 v[64:67], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256, sc0 sc1 // store D 64

s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[68:71], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 68

buffer_store_dwordx4 v[72:75], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256, sc0 sc1 // store D 72

s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[76:79], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 76

buffer_store_dwordx4 v[80:83], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256, sc0 sc1 // store D 80
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//sourece store done, GSU:2

// check done start
// synchronizer offset cal
s_mul_i32 s27, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s11, s27, s[sgprWorkGroup2]
s_mul_i32 s26, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s26, s26, s[sgprWorkGroup0]
s_add_u32 s26, s26, s11
v_readfirstlane_b32 s11, v[vgprSerial]
s_mul_i32 s27, s27, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s11, s11, 0x6
s_mul_i32 s11, s27, s11                            // wave offset at batch
s_add_u32 s26, s11, s26
s_lshl_b32 s26, s26, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s26
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_and_b32 s11, s[sgprGSU], 0x3fff                  // Restore GSU
s_sub_u32 s11, s11, 0x1
s_atomic_dec s11, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s69, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s68, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s72, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s72, s72, 1                              // Free1
s_mul_hi_u32 s71, s72, s[sgprStrideC1J]            // Free1
s_mul_i32 s70, s72, s[sgprStrideC1J]               // Free1
s_add_u32 s68, s68, s70                            // Free1
s_addc_u32 s69, s69, s71                           // Free1
s_sub_u32 s72, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s72, s72, 1                              // Free2
s_mul_hi_u32 s71, s72, s[sgprStrideCK]             // Free2
s_mul_i32 s70, s72, s[sgprStrideCK]                // Free2
s_add_u32 s68, s68, s70                            // Free2
s_addc_u32 s69, s69, s71                           // Free2
s_lshl_b64 s[32:33], s[68:69], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s11, 0x1
s_cbranch_scc1 label_NoBranch_BJFAZRNAR5SOLB70_0   // Only branch on scc0
// long branch sync
s_getpc_b64 s[44:45]                               // addr of next instr
s_add_i32 s46, label_Sync_EDN_0, 0x4               // target branch offset
s_add_u32 s44, s44, s46                            // add target branch offset
s_addc_u32 s45, s45, 0                             // add high and carry
s_setpc_b64 s[44:45]                               // branch to label_Sync_EDN_0
label_NoBranch_BJFAZRNAR5SOLB70_0:
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 20
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_176 // SyncAddbranchhere
buffer_load_dwordx4 v[88:91], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_176:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[88:89]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[90:91]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_176 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v84, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[88:91], v84, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_176     // Syncbranchhere

label_Synchronizer_read_add_end_1_176:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_176:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[32:35], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU D 0 32
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_177 // SyncAddbranchhere
buffer_load_dwordx4 v[88:91], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_177:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[88:89]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[90:91]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_177 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v84, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[88:91], v84, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_177     // Syncbranchhere

label_Synchronizer_read_add_end_1_177:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_177:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
s_lshl_b32 s38, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s44, s44, s38                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s45, s45, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s38
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[36:39], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 36
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_178 // SyncAddbranchhere
buffer_load_dwordx4 v[88:91], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_178:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[88:89]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[90:91]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_178 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v84, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[88:91], v84, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_178     // Syncbranchhere

label_Synchronizer_read_add_end_1_178:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_178:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[40:43], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU D 0 40
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_179 // SyncAddbranchhere
buffer_load_dwordx4 v[88:91], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_179:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[88:89]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[90:91]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_179 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v84, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[88:91], v84, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_179     // Syncbranchhere

label_Synchronizer_read_add_end_1_179:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_179:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
s_lshl_b32 s38, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s44, s44, s38                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s45, s45, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s38
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[44:47], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 44
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_180 // SyncAddbranchhere
buffer_load_dwordx4 v[88:91], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_180:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[88:89]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[90:91]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_180 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v84, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[88:91], v84, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_180     // Syncbranchhere

label_Synchronizer_read_add_end_1_180:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_180:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[48:51], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU D 0 48
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_181 // SyncAddbranchhere
buffer_load_dwordx4 v[88:91], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_181:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[88:89]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[90:91]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_181 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v84, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[88:91], v84, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_181     // Syncbranchhere

label_Synchronizer_read_add_end_1_181:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_181:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
s_lshl_b32 s38, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s44, s44, s38                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s45, s45, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s38
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[52:55], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 52
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_182 // SyncAddbranchhere
buffer_load_dwordx4 v[88:91], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_182:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[52:53], v[52:53], v[88:89]          // buffer pk
v_pk_add_f32 v[54:55], v[54:55], v[90:91]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_182 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v84, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[88:91], v84, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_182     // Syncbranchhere

label_Synchronizer_read_add_end_1_182:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_182:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[56:59], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU D 0 56
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_183 // SyncAddbranchhere
buffer_load_dwordx4 v[88:91], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_183:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[56:57], v[56:57], v[88:89]          // buffer pk
v_pk_add_f32 v[58:59], v[58:59], v[90:91]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_183 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v84, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[88:91], v84, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_183     // Syncbranchhere

label_Synchronizer_read_add_end_1_183:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_183:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
s_lshl_b32 s38, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s44, s44, s38                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s45, s45, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s38
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[60:63], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 60
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_184 // SyncAddbranchhere
buffer_load_dwordx4 v[88:91], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_184:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[88:89]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[90:91]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_184 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v84, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[88:91], v84, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_184     // Syncbranchhere

label_Synchronizer_read_add_end_1_184:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_184:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[64:67], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU D 0 64
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_185 // SyncAddbranchhere
buffer_load_dwordx4 v[88:91], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_185:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[64:65], v[64:65], v[88:89]          // buffer pk
v_pk_add_f32 v[66:67], v[66:67], v[90:91]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_185 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v84, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[88:91], v84, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_185     // Syncbranchhere

label_Synchronizer_read_add_end_1_185:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_185:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
s_lshl_b32 s38, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s44, s44, s38                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s45, s45, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s38
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[68:71], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 68
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_186 // SyncAddbranchhere
buffer_load_dwordx4 v[88:91], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_186:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[68:69], v[68:69], v[88:89]          // buffer pk
v_pk_add_f32 v[70:71], v[70:71], v[90:91]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_186 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v84, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[88:91], v84, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_186     // Syncbranchhere

label_Synchronizer_read_add_end_1_186:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_186:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[72:75], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU D 0 72
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_187 // SyncAddbranchhere
buffer_load_dwordx4 v[88:91], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_187:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[72:73], v[72:73], v[88:89]          // buffer pk
v_pk_add_f32 v[74:75], v[74:75], v[90:91]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_187 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v84, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[88:91], v84, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_187     // Syncbranchhere

label_Synchronizer_read_add_end_1_187:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_187:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
s_lshl_b32 s38, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s44, s44, s38                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s45, s45, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s38
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[76:79], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 76
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_188 // SyncAddbranchhere
buffer_load_dwordx4 v[88:91], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_188:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[76:77], v[76:77], v[88:89]          // buffer pk
v_pk_add_f32 v[78:79], v[78:79], v[90:91]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_188 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v84, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[88:91], v84, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_188     // Syncbranchhere

label_Synchronizer_read_add_end_1_188:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_188:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[80:83], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU D 0 80
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_189 // SyncAddbranchhere
buffer_load_dwordx4 v[88:91], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_189:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[80:81], v[80:81], v[88:89]          // buffer pk
v_pk_add_f32 v[82:83], v[82:83], v[90:91]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_189 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v84, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[88:91], v84, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_189     // Syncbranchhere

label_Synchronizer_read_add_end_1_189:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_189:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0), (0, 1, 1, 0), (0, 0, 2, 0), (0, 1, 2, 0), (0, 0, 3, 0), (0, 1, 3, 0), (0, 0, 4, 0), (0, 1, 4, 0), (0, 0, 5, 0), (0, 1, 5, 0), (0, 0, 6, 0), (0, 1, 6, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha

/* apply mask, calc new C and issue writes */

v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth

v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:128, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s12      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[28:29], v[vgprValuC+40:vgprValuC+40+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[30:31], v[vgprValuC+42:vgprValuC+42+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[24:25], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[26:27], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
buffer_store_dwordx2 v[40:41], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:128, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s12      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[28:29], v[vgprValuC+48:vgprValuC+48+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[30:31], v[vgprValuC+50:vgprValuC+50+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[24:25], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[26:27], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:128, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[16:17], v[vgprValuC+52:vgprValuC+52+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[18:19], v[vgprValuC+54:vgprValuC+54+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+52:vgprValuC+52+1], v[12:13], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[vgprValuC+54:vgprValuC+54+1], v[14:15], v[vgprValuC+54:vgprValuC+54+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+52], v[vgprValuC+52]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+53], v[vgprValuC+53]     // convert C to fp16
v_pack_b32_f16 v52, v[vgprValuC+52], v[vgprValuC+53] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+54], v[vgprValuC+54]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+55], v[vgprValuC+55]     // convert C to fp16
v_pack_b32_f16 v53, v[vgprValuC+54], v[vgprValuC+55] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s12      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[52:53], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[28:29], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[30:31], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[24:25], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[26:27], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:128, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[12:13], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[14:15], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s12      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[60:61], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[28:29], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[30:31], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:128, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[16:17], v[vgprValuC+68:vgprValuC+68+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[18:19], v[vgprValuC+70:vgprValuC+70+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+68:vgprValuC+68+1], v[12:13], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[vgprValuC+70:vgprValuC+70+1], v[14:15], v[vgprValuC+70:vgprValuC+70+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+68], v[vgprValuC+68]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+69], v[vgprValuC+69]     // convert C to fp16
v_pack_b32_f16 v68, v[vgprValuC+68], v[vgprValuC+69] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+70], v[vgprValuC+70]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+71], v[vgprValuC+71]     // convert C to fp16
v_pack_b32_f16 v69, v[vgprValuC+70], v[vgprValuC+71] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s12      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[68:69], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[28:29], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[30:31], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[24:25], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[26:27], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:128, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[16:17], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[18:19], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[12:13], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[14:15], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s12      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[76:77], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[28:29], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[30:31], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:128, sc0 sc1 // store TD not StoreRemapVectorWidth
label_Sync_EDN_0:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//GW end
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,0,7,0:vw4); (0,1,7,0:vw4)        */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
ds_read_b128 v[12:15], v9 offset:0                 // load Bias
ds_read_b128 v[16:19], v10 offset:0                // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
ds_read_b128 v[24:27], v9 offset:256               // load Bias
ds_read_b128 v[28:31], v10 offset:256              // load scaleAlpha
v_accvgpr_read_b32 v[vgprValuC+20], acc56          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+21], acc57          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+22], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+23], acc59          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+32], acc60          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+33], acc61          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+34], acc62          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+35], acc63          // copy acc to vreg[63]

/* store after Acc, GSU: 2 */

/* store after Acc, GSU: 2 */

s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 20

buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256, sc0 sc1 // store D 32
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//sourece store done, GSU:2

// check done start
// synchronizer offset cal
s_mul_i32 s27, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s11, s27, s[sgprWorkGroup2]
s_mul_i32 s26, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s26, s26, s[sgprWorkGroup0]
s_add_u32 s26, s26, s11
v_readfirstlane_b32 s11, v[vgprSerial]
s_mul_i32 s27, s27, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s11, s11, 0x6
s_mul_i32 s11, s27, s11                            // wave offset at batch
s_add_u32 s26, s11, s26
s_mul_i32 s27, s27, 4                              // cal a batch offset
s_mul_i32 s27, s27, 1                              // this batch offset
s_add_u32 s26, s26, s27
s_lshl_b32 s26, s26, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s26
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_and_b32 s11, s[sgprGSU], 0x3fff                  // Restore GSU
s_sub_u32 s11, s11, 0x1
s_atomic_dec s11, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s69, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s68, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s72, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s72, s72, 1                              // Free1
s_mul_hi_u32 s71, s72, s[sgprStrideC1J]            // Free1
s_mul_i32 s70, s72, s[sgprStrideC1J]               // Free1
s_add_u32 s68, s68, s70                            // Free1
s_addc_u32 s69, s69, s71                           // Free1
s_sub_u32 s72, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s72, s72, 1                              // Free2
s_mul_hi_u32 s71, s72, s[sgprStrideCK]             // Free2
s_mul_i32 s70, s72, s[sgprStrideCK]                // Free2
s_add_u32 s68, s68, s70                            // Free2
s_addc_u32 s69, s69, s71                           // Free2
s_lshl_b64 s[32:33], s[68:69], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s11, 0x1
s_cbranch_scc1 label_NoBranch_EKRAP10FUM5ODJBY_0   // Only branch on scc0
// long branch sync
s_getpc_b64 s[44:45]                               // addr of next instr
s_add_i32 s46, label_Sync_EDN_1, 0x4               // target branch offset
s_add_u32 s44, s44, s46                            // add target branch offset
s_addc_u32 s45, s45, 0                             // add high and carry
s_setpc_b64 s[44:45]                               // branch to label_Sync_EDN_1
label_NoBranch_EKRAP10FUM5ODJBY_0:
// check done end

// buffer load start
s_lshl_b32 s38, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s44, s44, s38                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s45, s45, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s38
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[20:23], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 20
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_190 // SyncAddbranchhere
buffer_load_dwordx4 v[40:43], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_190:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[40:41]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[42:43]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_190 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[40:43], v36, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_190     // Syncbranchhere

label_Synchronizer_read_add_end_1_190:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_190:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[32:35], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU D 0 32
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_191 // SyncAddbranchhere
buffer_load_dwordx4 v[40:43], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_191:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[40:41]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[42:43]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_191 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[40:43], v36, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_191     // Syncbranchhere

label_Synchronizer_read_add_end_1_191:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_191:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 7, 0), (0, 1, 7, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha

/* apply mask, calc new C and issue writes */

v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s12      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth

v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:128, sc0 sc1 // store TD not StoreRemapVectorWidth
label_Sync_EDN_1:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//GW end
s_branch label_GW_End_1                            // jump to end
label_GW_B0_E1_N:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=10 */
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,1,0,0:vw4); (0,0,1,0:vw4); (0,1,1,0:vw4); (0,0,2,0:vw4); (0,1,2,0:vw4); (0,0,3,0:vw4); (0,1,3,0:vw4); (0,0,4,0:vw4); (0,1,4,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v98, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s68
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for LDS write
s_barrier                                          // LDS write barrier
ds_read_b128 v[12:15], v8 offset:0                 // load Bias
v_add_u32 v9, 1024, v8                             // add ScaleAlphaVec offset (3)
ds_read_b128 v[16:19], v9 offset:0                 // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v98, v6, s[72:73]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v98, v7, s[72:73]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v4, s68
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
ds_read_b128 v[28:31], v24 offset:0                // load Bias
v_add_u32 v25, 1024, v24                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[32:35], v25 offset:0                // load scaleAlpha
v_add_lshl_u32 v10, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v98, v10, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v98, v11, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s68
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_add_u32 v41, 1024, v40                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v98, v26, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v27, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v98, v27, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v48, v4, s68
v_lshlrev_b32 v48, 0x2, v48                        // Bias address scaled by BPE
v_add_u32 v49, 1024, v48                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v42, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v98, v42, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v43, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v98, v43, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v56, v0, s68
v_lshlrev_b32 v56, 0x2, v56                        // Bias address scaled by BPE
v_add_u32 v57, 1024, v56                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v50, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v50, v98, v50, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v51, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v98, v51, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v64, v4, s68
v_lshlrev_b32 v64, 0x2, v64                        // Bias address scaled by BPE
v_add_u32 v65, 1024, v64                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v58, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v58, v98, v58, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v59, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v59, v98, v59, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v0, s68
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_add_u32 v73, 1024, v72                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v66, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v98, v66, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v67, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v67, v98, v67, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v80, v4, s68
v_lshlrev_b32 v80, 0x2, v80                        // Bias address scaled by BPE
v_add_u32 v81, 1024, v80                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v74, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v74, v98, v74, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v75, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v75, v98, v75, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v88, v0, s68
v_lshlrev_b32 v88, 0x2, v88                        // Bias address scaled by BPE
v_add_u32 v89, 1024, v88                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v82, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v82, v98, v82, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v83, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v98, v83, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v96, v4, s68
v_lshlrev_b32 v96, 0x2, v96                        // Bias address scaled by BPE
v_add_u32 v97, 1024, v96                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v90, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v98, v90, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v91, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v98, v91, s[72:73]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc1           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+36], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+37], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+38], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+39], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+44], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+45], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+46], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+47], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+52], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+53], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+54], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+55], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+60], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+61], acc17          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+62], acc18          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+63], acc19          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+68], acc20          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+69], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+70], acc22          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+71], acc23          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+76], acc24          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+77], acc25          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+78], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+79], acc27          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+84], acc28          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+85], acc29          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+86], acc30          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+87], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+92], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+93], acc33          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+94], acc34          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+95], acc35          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+100], acc36         // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+101], acc37         // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+102], acc38         // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+103], acc39         // copy acc to vreg[39]

/* store after Acc, GSU: 2 */

/* store after Acc, GSU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 20

buffer_store_dwordx4 v[36:39], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 36

buffer_store_dwordx4 v[44:47], v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 44

buffer_store_dwordx4 v[52:55], v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 52

buffer_store_dwordx4 v[60:63], v50, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 60

buffer_store_dwordx4 v[68:71], v58, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 68

buffer_store_dwordx4 v[76:79], v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 76

buffer_store_dwordx4 v[84:87], v74, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 84

buffer_store_dwordx4 v[92:95], v82, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 92

buffer_store_dwordx4 v[100:103], v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 100
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//sourece store done, GSU:2

// check done start
// synchronizer offset cal
s_mul_i32 s13, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s11, s13, s[sgprWorkGroup2]
s_mul_i32 s12, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s12, s12, s[sgprWorkGroup0]
s_add_u32 s12, s12, s11
v_readfirstlane_b32 s11, v[vgprSerial]
s_mul_i32 s13, s13, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s11, s11, 0x6
s_mul_i32 s11, s13, s11                            // wave offset at batch
s_add_u32 s12, s11, s12
s_lshl_b32 s12, s12, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s12
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_and_b32 s11, s[sgprGSU], 0x3fff                  // Restore GSU
s_sub_u32 s11, s11, 0x1
s_atomic_dec s11, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s75, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s74, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s78, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s78, s78, 1                              // Free1
s_mul_hi_u32 s77, s78, s[sgprStrideC1J]            // Free1
s_mul_i32 s76, s78, s[sgprStrideC1J]               // Free1
s_add_u32 s74, s74, s76                            // Free1
s_addc_u32 s75, s75, s77                           // Free1
s_sub_u32 s78, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s78, s78, 1                              // Free2
s_mul_hi_u32 s77, s78, s[sgprStrideCK]             // Free2
s_mul_i32 s76, s78, s[sgprStrideCK]                // Free2
s_add_u32 s74, s74, s76                            // Free2
s_addc_u32 s75, s75, s77                           // Free2
s_lshl_b64 s[26:27], s[74:75], 2                   // scale by bpe

v_mov_b32 v98, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s11, 0x1
s_cbranch_scc1 label_NoBranch_4PAKCGJHRVW144HH_0   // Only branch on scc0
// long branch sync
s_getpc_b64 s[44:45]                               // addr of next instr
s_add_i32 s46, label_Sync_EDN_Edge_3, 0x4          // target branch offset
s_add_u32 s44, s44, s46                            // add target branch offset
s_addc_u32 s45, s45, 0                             // add high and carry
s_setpc_b64 s[44:45]                               // branch to label_Sync_EDN_Edge_3
label_NoBranch_4PAKCGJHRVW144HH_0:
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 20
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_160 // SyncAddbranchhere
buffer_load_dwordx4 v[104:107], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_160:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[104:105]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[106:107]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_160 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v99, v6, v98, s[32:33]               // protect if OOB
buffer_load_dwordx4 v[104:107], v99, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_160     // Syncbranchhere

label_Synchronizer_read_add_end_1_160:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_160:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v98, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[36:39], v10, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 36
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_161 // SyncAddbranchhere
buffer_load_dwordx4 v[104:107], v10, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_161:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[104:105]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[106:107]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_161 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v99, v10, v98, s[32:33]              // protect if OOB
buffer_load_dwordx4 v[104:107], v99, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_161     // Syncbranchhere

label_Synchronizer_read_add_end_1_161:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_161:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v98, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[44:47], v26, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 44
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_162 // SyncAddbranchhere
buffer_load_dwordx4 v[104:107], v26, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_162:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[104:105]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[106:107]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_162 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v99, v26, v98, s[32:33]              // protect if OOB
buffer_load_dwordx4 v[104:107], v99, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_162     // Syncbranchhere

label_Synchronizer_read_add_end_1_162:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_162:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v98, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[52:55], v42, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 52
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_163 // SyncAddbranchhere
buffer_load_dwordx4 v[104:107], v42, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_163:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[52:53], v[52:53], v[104:105]        // buffer pk
v_pk_add_f32 v[54:55], v[54:55], v[106:107]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_163 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v99, v42, v98, s[32:33]              // protect if OOB
buffer_load_dwordx4 v[104:107], v99, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_163     // Syncbranchhere

label_Synchronizer_read_add_end_1_163:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_163:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v98, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[60:63], v50, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 60
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_164 // SyncAddbranchhere
buffer_load_dwordx4 v[104:107], v50, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_164:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[104:105]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[106:107]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_164 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v99, v50, v98, s[32:33]              // protect if OOB
buffer_load_dwordx4 v[104:107], v99, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_164     // Syncbranchhere

label_Synchronizer_read_add_end_1_164:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_164:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v98, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[68:71], v58, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 68
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_165 // SyncAddbranchhere
buffer_load_dwordx4 v[104:107], v58, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_165:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[68:69], v[68:69], v[104:105]        // buffer pk
v_pk_add_f32 v[70:71], v[70:71], v[106:107]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_165 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v99, v58, v98, s[32:33]              // protect if OOB
buffer_load_dwordx4 v[104:107], v99, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_165     // Syncbranchhere

label_Synchronizer_read_add_end_1_165:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_165:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v98, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[76:79], v66, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 76
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_166 // SyncAddbranchhere
buffer_load_dwordx4 v[104:107], v66, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_166:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[76:77], v[76:77], v[104:105]        // buffer pk
v_pk_add_f32 v[78:79], v[78:79], v[106:107]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_166 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v99, v66, v98, s[32:33]              // protect if OOB
buffer_load_dwordx4 v[104:107], v99, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_166     // Syncbranchhere

label_Synchronizer_read_add_end_1_166:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_166:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v98, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[84:87], v74, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 84
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_167 // SyncAddbranchhere
buffer_load_dwordx4 v[104:107], v74, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_167:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[84:85], v[84:85], v[104:105]        // buffer pk
v_pk_add_f32 v[86:87], v[86:87], v[106:107]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_167 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v99, v74, v98, s[32:33]              // protect if OOB
buffer_load_dwordx4 v[104:107], v99, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_167     // Syncbranchhere

label_Synchronizer_read_add_end_1_167:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_167:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v98, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[92:95], v82, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 92
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_168 // SyncAddbranchhere
buffer_load_dwordx4 v[104:107], v82, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_168:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[92:93], v[92:93], v[104:105]        // buffer pk
v_pk_add_f32 v[94:95], v[94:95], v[106:107]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_168 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v99, v82, v98, s[32:33]              // protect if OOB
buffer_load_dwordx4 v[104:107], v99, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_168     // Syncbranchhere

label_Synchronizer_read_add_end_1_168:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_168:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v98, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[100:103], v90, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 100
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_169 // SyncAddbranchhere
buffer_load_dwordx4 v[104:107], v90, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_169:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[100:101], v[100:101], v[104:105]    // buffer pk
v_pk_add_f32 v[102:103], v[102:103], v[106:107]    // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_169 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v99, v90, v98, s[32:33]              // protect if OOB
buffer_load_dwordx4 v[104:107], v99, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_169     // Syncbranchhere

label_Synchronizer_read_add_end_1_169:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_169:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0), (0, 1, 1, 0), (0, 0, 2, 0), (0, 1, 2, 0), (0, 0, 3, 0), (0, 1, 3, 0), (0, 0, 4, 0), (0, 1, 4, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[32:33], v[vgprValuC+36:vgprValuC+36+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[34:35], v[vgprValuC+38:vgprValuC+38+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[28:29], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[30:31], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v27, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[32:33], v[vgprValuC+52:vgprValuC+52+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[34:35], v[vgprValuC+54:vgprValuC+54+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_pk_add_f32 v[vgprValuC+52:vgprValuC+52+1], v[28:29], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[vgprValuC+54:vgprValuC+54+1], v[30:31], v[vgprValuC+54:vgprValuC+54+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+52], v[vgprValuC+52]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+53], v[vgprValuC+53]     // convert C to fp16
v_pack_b32_f16 v52, v[vgprValuC+52], v[vgprValuC+53] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+54], v[vgprValuC+54]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+55], v[vgprValuC+55]     // convert C to fp16
v_pack_b32_f16 v53, v[vgprValuC+54], v[vgprValuC+55] // Pack with neighbor
buffer_store_dwordx2 v[52:53], v43, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[12:13], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[14:15], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
buffer_store_dwordx2 v[60:61], v51, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[32:33], v[vgprValuC+68:vgprValuC+68+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[34:35], v[vgprValuC+70:vgprValuC+70+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_pk_add_f32 v[vgprValuC+68:vgprValuC+68+1], v[28:29], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[vgprValuC+70:vgprValuC+70+1], v[30:31], v[vgprValuC+70:vgprValuC+70+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+68], v[vgprValuC+68]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+69], v[vgprValuC+69]     // convert C to fp16
v_pack_b32_f16 v68, v[vgprValuC+68], v[vgprValuC+69] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+70], v[vgprValuC+70]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+71], v[vgprValuC+71]     // convert C to fp16
v_pack_b32_f16 v69, v[vgprValuC+70], v[vgprValuC+71] // Pack with neighbor
buffer_store_dwordx2 v[68:69], v59, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[16:17], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[18:19], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[12:13], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[14:15], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
buffer_store_dwordx2 v[76:77], v67, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[32:33], v[vgprValuC+84:vgprValuC+84+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[34:35], v[vgprValuC+86:vgprValuC+86+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_pk_add_f32 v[vgprValuC+84:vgprValuC+84+1], v[28:29], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[vgprValuC+86:vgprValuC+86+1], v[30:31], v[vgprValuC+86:vgprValuC+86+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+84], v[vgprValuC+84]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+85], v[vgprValuC+85]     // convert C to fp16
v_pack_b32_f16 v84, v[vgprValuC+84], v[vgprValuC+85] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+86], v[vgprValuC+86]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+87], v[vgprValuC+87]     // convert C to fp16
v_pack_b32_f16 v85, v[vgprValuC+86], v[vgprValuC+87] // Pack with neighbor
buffer_store_dwordx2 v[84:85], v75, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[16:17], v[vgprValuC+92:vgprValuC+92+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[18:19], v[vgprValuC+94:vgprValuC+94+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+92:vgprValuC+92+1], v[12:13], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[vgprValuC+94:vgprValuC+94+1], v[14:15], v[vgprValuC+94:vgprValuC+94+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+92], v[vgprValuC+92]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+93], v[vgprValuC+93]     // convert C to fp16
v_pack_b32_f16 v92, v[vgprValuC+92], v[vgprValuC+93] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+94], v[vgprValuC+94]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+95], v[vgprValuC+95]     // convert C to fp16
v_pack_b32_f16 v93, v[vgprValuC+94], v[vgprValuC+95] // Pack with neighbor
buffer_store_dwordx2 v[92:93], v83, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[32:33], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[34:35], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[28:29], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[30:31], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
buffer_store_dwordx2 v[100:101], v91, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
label_Sync_EDN_Edge_3:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//GW end
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,0,5,0:vw4); (0,1,5,0:vw4); (0,0,6,0:vw4); (0,1,6,0:vw4); (0,0,7,0:vw4); (0,1,7,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v66, BufferOOB
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s68
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
ds_read_b128 v[12:15], v8 offset:0                 // load Bias
v_add_u32 v9, 1024, v8                             // add ScaleAlphaVec offset (3)
ds_read_b128 v[16:19], v9 offset:0                 // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v66, v6, s[72:73]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v66, v7, s[72:73]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v4, s68
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
ds_read_b128 v[28:31], v24 offset:0                // load Bias
v_add_u32 v25, 1024, v24                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[32:35], v25 offset:0                // load scaleAlpha
v_add_lshl_u32 v10, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v66, v10, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v66, v11, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s68
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_add_u32 v41, 1024, v40                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v66, v26, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v27, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v66, v27, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v48, v4, s68
v_lshlrev_b32 v48, 0x2, v48                        // Bias address scaled by BPE
v_add_u32 v49, 1024, v48                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v42, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v66, v42, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v43, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v66, v43, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v56, v0, s68
v_lshlrev_b32 v56, 0x2, v56                        // Bias address scaled by BPE
v_add_u32 v57, 1024, v56                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v50, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v50, v66, v50, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v51, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v66, v51, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v64, v4, s68
v_lshlrev_b32 v64, 0x2, v64                        // Bias address scaled by BPE
v_add_u32 v65, 1024, v64                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v58, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v58, v66, v58, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v59, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v59, v66, v59, s[72:73]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc40          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+21], acc41          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+22], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+23], acc43          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+36], acc44          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+37], acc45          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+38], acc46          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+39], acc47          // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+44], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+45], acc49          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+46], acc50          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+47], acc51          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+52], acc52          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+53], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+54], acc54          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+55], acc55          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+60], acc56          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+61], acc57          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+62], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+63], acc59          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+68], acc60          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+69], acc61          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+70], acc62          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+71], acc63          // copy acc to vreg[63]

/* store after Acc, GSU: 2 */

/* store after Acc, GSU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 20

buffer_store_dwordx4 v[36:39], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 36

buffer_store_dwordx4 v[44:47], v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 44

buffer_store_dwordx4 v[52:55], v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 52

buffer_store_dwordx4 v[60:63], v50, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 60

buffer_store_dwordx4 v[68:71], v58, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 68
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//sourece store done, GSU:2

// check done start
// synchronizer offset cal
s_mul_i32 s13, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s11, s13, s[sgprWorkGroup2]
s_mul_i32 s12, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s12, s12, s[sgprWorkGroup0]
s_add_u32 s12, s12, s11
v_readfirstlane_b32 s11, v[vgprSerial]
s_mul_i32 s13, s13, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s11, s11, 0x6
s_mul_i32 s11, s13, s11                            // wave offset at batch
s_add_u32 s12, s11, s12
s_mul_i32 s13, s13, 4                              // cal a batch offset
s_mul_i32 s13, s13, 1                              // this batch offset
s_add_u32 s12, s12, s13
s_lshl_b32 s12, s12, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s12
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_and_b32 s11, s[sgprGSU], 0x3fff                  // Restore GSU
s_sub_u32 s11, s11, 0x1
s_atomic_dec s11, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s75, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s74, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s78, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s78, s78, 1                              // Free1
s_mul_hi_u32 s77, s78, s[sgprStrideC1J]            // Free1
s_mul_i32 s76, s78, s[sgprStrideC1J]               // Free1
s_add_u32 s74, s74, s76                            // Free1
s_addc_u32 s75, s75, s77                           // Free1
s_sub_u32 s78, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s78, s78, 1                              // Free2
s_mul_hi_u32 s77, s78, s[sgprStrideCK]             // Free2
s_mul_i32 s76, s78, s[sgprStrideCK]                // Free2
s_add_u32 s74, s74, s76                            // Free2
s_addc_u32 s75, s75, s77                           // Free2
s_lshl_b64 s[26:27], s[74:75], 2                   // scale by bpe

v_mov_b32 v66, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s11, 0x1
s_cbranch_scc1 label_NoBranch_N1EPQX651TFNETZZ_0   // Only branch on scc0
// long branch sync
s_getpc_b64 s[44:45]                               // addr of next instr
s_add_i32 s46, label_Sync_EDN_Edge_4, 0x4          // target branch offset
s_add_u32 s44, s44, s46                            // add target branch offset
s_addc_u32 s45, s45, 0                             // add high and carry
s_setpc_b64 s[44:45]                               // branch to label_Sync_EDN_Edge_4
label_NoBranch_N1EPQX651TFNETZZ_0:
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 20
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_170 // SyncAddbranchhere
buffer_load_dwordx4 v[72:75], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_170:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[72:73]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[74:75]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_170 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v67, v6, v66, s[32:33]               // protect if OOB
buffer_load_dwordx4 v[72:75], v67, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_170     // Syncbranchhere

label_Synchronizer_read_add_end_1_170:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_170:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v66, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[36:39], v10, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 36
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_171 // SyncAddbranchhere
buffer_load_dwordx4 v[72:75], v10, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_171:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[72:73]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[74:75]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_171 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v67, v10, v66, s[32:33]              // protect if OOB
buffer_load_dwordx4 v[72:75], v67, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_171     // Syncbranchhere

label_Synchronizer_read_add_end_1_171:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_171:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v66, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[44:47], v26, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 44
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_172 // SyncAddbranchhere
buffer_load_dwordx4 v[72:75], v26, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_172:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[72:73]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[74:75]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_172 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v67, v26, v66, s[32:33]              // protect if OOB
buffer_load_dwordx4 v[72:75], v67, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_172     // Syncbranchhere

label_Synchronizer_read_add_end_1_172:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_172:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v66, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[52:55], v42, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 52
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_173 // SyncAddbranchhere
buffer_load_dwordx4 v[72:75], v42, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_173:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[52:53], v[52:53], v[72:73]          // buffer pk
v_pk_add_f32 v[54:55], v[54:55], v[74:75]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_173 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v67, v42, v66, s[32:33]              // protect if OOB
buffer_load_dwordx4 v[72:75], v67, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_173     // Syncbranchhere

label_Synchronizer_read_add_end_1_173:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_173:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v66, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[60:63], v50, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 60
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_174 // SyncAddbranchhere
buffer_load_dwordx4 v[72:75], v50, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_174:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[72:73]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[74:75]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_174 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v67, v50, v66, s[32:33]              // protect if OOB
buffer_load_dwordx4 v[72:75], v67, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_174     // Syncbranchhere

label_Synchronizer_read_add_end_1_174:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_174:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v66, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[68:71], v58, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 68
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_175 // SyncAddbranchhere
buffer_load_dwordx4 v[72:75], v58, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_175:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[68:69], v[68:69], v[72:73]          // buffer pk
v_pk_add_f32 v[70:71], v[70:71], v[74:75]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_175 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v67, v58, v66, s[32:33]              // protect if OOB
buffer_load_dwordx4 v[72:75], v67, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_175     // Syncbranchhere

label_Synchronizer_read_add_end_1_175:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_175:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 5, 0), (0, 1, 5, 0), (0, 0, 6, 0), (0, 1, 6, 0), (0, 0, 7, 0), (0, 1, 7, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[32:33], v[vgprValuC+36:vgprValuC+36+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[34:35], v[vgprValuC+38:vgprValuC+38+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[28:29], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[30:31], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v27, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[32:33], v[vgprValuC+52:vgprValuC+52+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[34:35], v[vgprValuC+54:vgprValuC+54+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_pk_add_f32 v[vgprValuC+52:vgprValuC+52+1], v[28:29], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[vgprValuC+54:vgprValuC+54+1], v[30:31], v[vgprValuC+54:vgprValuC+54+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+52], v[vgprValuC+52]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+53], v[vgprValuC+53]     // convert C to fp16
v_pack_b32_f16 v52, v[vgprValuC+52], v[vgprValuC+53] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+54], v[vgprValuC+54]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+55], v[vgprValuC+55]     // convert C to fp16
v_pack_b32_f16 v53, v[vgprValuC+54], v[vgprValuC+55] // Pack with neighbor
buffer_store_dwordx2 v[52:53], v43, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[12:13], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[14:15], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
buffer_store_dwordx2 v[60:61], v51, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[32:33], v[vgprValuC+68:vgprValuC+68+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[34:35], v[vgprValuC+70:vgprValuC+70+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_pk_add_f32 v[vgprValuC+68:vgprValuC+68+1], v[28:29], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[vgprValuC+70:vgprValuC+70+1], v[30:31], v[vgprValuC+70:vgprValuC+70+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+68], v[vgprValuC+68]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+69], v[vgprValuC+69]     // convert C to fp16
v_pack_b32_f16 v68, v[vgprValuC+68], v[vgprValuC+69] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+70], v[vgprValuC+70]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+71], v[vgprValuC+71]     // convert C to fp16
v_pack_b32_f16 v69, v[vgprValuC+70], v[vgprValuC+71] // Pack with neighbor
buffer_store_dwordx2 v[68:69], v59, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
label_Sync_EDN_Edge_4:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//GW end
s_branch label_GW_End_1                            // jump to end
label_GW_B0_E1_M:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=30 */
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw1); (0,0,0,1:vw1); (0,0,0,2:vw1); (0,0,0,3:vw1); (0,1,0,0:vw1); (0,1,0,1:vw1); (0,1,0,2:vw1); (0,1,0,3:vw1); (0,0,1,0:vw1); (0,0,1,1:vw1); (0,0,1,2:vw1); (0,0,1,3:vw1); (0,1,1,0:vw1); (0,1,1,1:vw1); (0,1,1,2:vw1); (0,1,1,3:vw1); (0,0,2,0:vw1); (0,0,2,1:vw1); (0,0,2,2:vw1); (0,0,2,3:vw1); (0,1,2,0:vw1); (0,1,2,1:vw1); (0,1,2,2:vw1); (0,1,2,3:vw1); (0,0,3,0:vw1); (0,0,3,1:vw1); (0,0,3,2:vw1); (0,0,3,3:vw1); (0,1,3,0:vw1); (0,1,3,1:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v173, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s68
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for LDS write
s_barrier                                          // LDS write barrier
ds_read_b32 v10, v8 offset:0                       // load Bias
v_add_u32 v9, 1024, v8                             // add ScaleAlphaVec offset (3)
ds_read_b32 v11, v9 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v173, v6, s[72:73]               // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v173, v7, s[72:73]               // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v4, s68
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
ds_read_b32 v17, v15 offset:0                      // load Bias
v_add_u32 v16, 1024, v15                           // add ScaleAlphaVec offset (3)
ds_read_b32 v18, v16 offset:0                      // load scaleAlpha
v_add_lshl_u32 v13, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v173, v13, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v14, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v173, v14, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v22, v4, s68
v_lshlrev_b32 v22, 0x2, v22                        // Bias address scaled by BPE
ds_read_b32 v24, v22 offset:0                      // load Bias
v_add_u32 v23, 1024, v22                           // add ScaleAlphaVec offset (3)
ds_read_b32 v25, v23 offset:0                      // load scaleAlpha
v_add_lshl_u32 v20, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v173, v20, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v21, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v21, v173, v21, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v29, v4, s68
v_lshlrev_b32 v29, 0x2, v29                        // Bias address scaled by BPE
ds_read_b32 v31, v29 offset:0                      // load Bias
v_add_u32 v30, 1024, v29                           // add ScaleAlphaVec offset (3)
ds_read_b32 v32, v30 offset:0                      // load scaleAlpha
v_add_lshl_u32 v27, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v173, v27, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v28, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v173, v28, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v36, v4, s68
v_lshlrev_b32 v36, 0x2, v36                        // Bias address scaled by BPE
ds_read_b32 v38, v36 offset:0                      // load Bias
v_add_u32 v37, 1024, v36                           // add ScaleAlphaVec offset (3)
ds_read_b32 v39, v37 offset:0                      // load scaleAlpha
v_add_lshl_u32 v34, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v173, v34, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v35, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v35, v173, v35, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v43, v4, s68
v_lshlrev_b32 v43, 0x2, v43                        // Bias address scaled by BPE
ds_read_b32 v45, v43 offset:0                      // load Bias
v_add_u32 v44, 1024, v43                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v44 offset:0                      // load scaleAlpha
v_add_lshl_u32 v41, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v173, v41, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v42, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v173, v42, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v50, v4, s68
v_lshlrev_b32 v50, 0x2, v50                        // Bias address scaled by BPE
ds_read_b32 v52, v50 offset:0                      // load Bias
v_add_u32 v51, 1024, v50                           // add ScaleAlphaVec offset (3)
ds_read_b32 v53, v51 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v173, v48, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v49, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v49, v173, v49, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v57, v4, s68
v_lshlrev_b32 v57, 0x2, v57                        // Bias address scaled by BPE
ds_read_b32 v59, v57 offset:0                      // load Bias
v_add_u32 v58, 1024, v57                           // add ScaleAlphaVec offset (3)
ds_read_b32 v60, v58 offset:0                      // load scaleAlpha
v_add_lshl_u32 v55, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v173, v55, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v56, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v173, v56, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v64, v0, s68
v_lshlrev_b32 v64, 0x2, v64                        // Bias address scaled by BPE
v_add_u32 v65, 1024, v64                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v62, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v173, v62, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v63, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v63, v173, v63, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v69, v4, s68
v_lshlrev_b32 v69, 0x2, v69                        // Bias address scaled by BPE
v_add_u32 v70, 1024, v69                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v67, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v67, v173, v67, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v68, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v68, v173, v68, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v74, v4, s68
v_lshlrev_b32 v74, 0x2, v74                        // Bias address scaled by BPE
v_add_u32 v75, 1024, v74                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v72, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v72, v173, v72, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v73, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v73, v173, v73, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v79, v4, s68
v_lshlrev_b32 v79, 0x2, v79                        // Bias address scaled by BPE
v_add_u32 v80, 1024, v79                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v77, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v173, v77, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v78, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v78, v173, v78, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v84, v4, s68
v_lshlrev_b32 v84, 0x2, v84                        // Bias address scaled by BPE
v_add_u32 v85, 1024, v84                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v82, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v82, v173, v82, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v83, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v173, v83, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v89, v4, s68
v_lshlrev_b32 v89, 0x2, v89                        // Bias address scaled by BPE
v_add_u32 v90, 1024, v89                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v87, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v87, v173, v87, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v88, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v88, v173, v88, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v94, v4, s68
v_lshlrev_b32 v94, 0x2, v94                        // Bias address scaled by BPE
v_add_u32 v95, 1024, v94                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v92, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v92, v173, v92, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v93, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v93, v173, v93, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v99, v4, s68
v_lshlrev_b32 v99, 0x2, v99                        // Bias address scaled by BPE
v_add_u32 v100, 1024, v99                          // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v97, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v173, v97, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v98, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v98, v173, v98, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v104, v0, s68
v_lshlrev_b32 v104, 0x2, v104                      // Bias address scaled by BPE
v_add_u32 v105, 1024, v104                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v102, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v102, v173, v102, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v103, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v103, v173, v103, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v109, v4, s68
v_lshlrev_b32 v109, 0x2, v109                      // Bias address scaled by BPE
v_add_u32 v110, 1024, v109                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v107, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v107, v173, v107, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v108, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v108, v173, v108, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v114, v4, s68
v_lshlrev_b32 v114, 0x2, v114                      // Bias address scaled by BPE
v_add_u32 v115, 1024, v114                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v112, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v112, v173, v112, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v113, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v113, v173, v113, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v4, s68
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v120, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v117, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v117, v173, v117, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v118, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v173, v118, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v124, v4, s68
v_lshlrev_b32 v124, 0x2, v124                      // Bias address scaled by BPE
v_add_u32 v125, 1024, v124                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v122, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v122, v173, v122, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v123, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v123, v173, v123, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v129, v4, s68
v_lshlrev_b32 v129, 0x2, v129                      // Bias address scaled by BPE
v_add_u32 v130, 1024, v129                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v127, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v127, v173, v127, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v128, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v173, v128, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v134, v4, s68
v_lshlrev_b32 v134, 0x2, v134                      // Bias address scaled by BPE
v_add_u32 v135, 1024, v134                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v132, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v132, v173, v132, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v133, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v133, v173, v133, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v140, v4, s68
v_lshlrev_b32 v140, 0x2, v140                      // Bias address scaled by BPE
v_add_u32 v141, 1024, v140                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v137, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v137, v173, v137, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v138, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v173, v138, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v145, v0, s68
v_lshlrev_b32 v145, 0x2, v145                      // Bias address scaled by BPE
v_add_u32 v146, 1024, v145                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v143, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v143, v173, v143, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v144, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v144, v173, v144, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v150, v4, s68
v_lshlrev_b32 v150, 0x2, v150                      // Bias address scaled by BPE
v_add_u32 v151, 1024, v150                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v148, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v148, v173, v148, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v149, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v173, v149, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v155, v4, s68
v_lshlrev_b32 v155, 0x2, v155                      // Bias address scaled by BPE
v_add_u32 v156, 1024, v155                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v153, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v153, v173, v153, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v154, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v154, v173, v154, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v160, v4, s68
v_lshlrev_b32 v160, 0x2, v160                      // Bias address scaled by BPE
v_add_u32 v161, 1024, v160                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v158, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v158, v173, v158, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v159, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v173, v159, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v165, v4, s68
v_lshlrev_b32 v165, 0x2, v165                      // Bias address scaled by BPE
v_add_u32 v166, 1024, v165                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v163, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v163, v173, v163, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v164, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v164, v173, v164, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v170, v4, s68
v_lshlrev_b32 v170, 0x2, v170                      // Bias address scaled by BPE
v_add_u32 v171, 1024, v170                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v168, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v168, v173, v168, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v169, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v169, v173, v169, s[72:73]           // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+12], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+19], acc1           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+33], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+40], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+47], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+54], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+61], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+66], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+71], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+76], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+81], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+86], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+91], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+96], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+101], acc15         // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+106], acc16         // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+111], acc17         // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+116], acc18         // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+121], acc19         // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+126], acc20         // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+131], acc21         // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+136], acc22         // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+142], acc23         // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+147], acc24         // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+152], acc25         // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+157], acc26         // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+162], acc27         // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+167], acc28         // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+172], acc29         // copy acc to vreg[29]

/* store after Acc, GSU: 2 */

/* store after Acc, GSU: 2 */

buffer_store_dword v12, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 12

buffer_store_dword v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 19

buffer_store_dword v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 26

buffer_store_dword v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 33

buffer_store_dword v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 40

buffer_store_dword v47, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 47

buffer_store_dword v54, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 54

buffer_store_dword v61, v55, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 61

buffer_store_dword v66, v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 66

buffer_store_dword v71, v67, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 71

buffer_store_dword v76, v72, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 76

buffer_store_dword v81, v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 81

buffer_store_dword v86, v82, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 86

buffer_store_dword v91, v87, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 91

buffer_store_dword v96, v92, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 96

buffer_store_dword v101, v97, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 101

buffer_store_dword v106, v102, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 106

buffer_store_dword v111, v107, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 111

buffer_store_dword v116, v112, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 116

buffer_store_dword v121, v117, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 121

buffer_store_dword v126, v122, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 126

buffer_store_dword v131, v127, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 131

buffer_store_dword v136, v132, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 136

buffer_store_dword v142, v137, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 142

buffer_store_dword v147, v143, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 147

buffer_store_dword v152, v148, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 152

buffer_store_dword v157, v153, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 157

buffer_store_dword v162, v158, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 162

buffer_store_dword v167, v163, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 167

buffer_store_dword v172, v168, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 172
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//sourece store done, GSU:2

// check done start
// synchronizer offset cal
s_mul_i32 s13, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s11, s13, s[sgprWorkGroup2]
s_mul_i32 s12, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s12, s12, s[sgprWorkGroup0]
s_add_u32 s12, s12, s11
v_readfirstlane_b32 s11, v[vgprSerial]
s_mul_i32 s13, s13, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s11, s11, 0x6
s_mul_i32 s11, s13, s11                            // wave offset at batch
s_add_u32 s12, s11, s12
s_lshl_b32 s12, s12, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s12
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_and_b32 s11, s[sgprGSU], 0x3fff                  // Restore GSU
s_sub_u32 s11, s11, 0x1
s_atomic_dec s11, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s75, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s74, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s78, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s78, s78, 1                              // Free1
s_mul_hi_u32 s77, s78, s[sgprStrideC1J]            // Free1
s_mul_i32 s76, s78, s[sgprStrideC1J]               // Free1
s_add_u32 s74, s74, s76                            // Free1
s_addc_u32 s75, s75, s77                           // Free1
s_sub_u32 s78, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s78, s78, 1                              // Free2
s_mul_hi_u32 s77, s78, s[sgprStrideCK]             // Free2
s_mul_i32 s76, s78, s[sgprStrideCK]                // Free2
s_add_u32 s74, s74, s76                            // Free2
s_addc_u32 s75, s75, s77                           // Free2
s_lshl_b64 s[26:27], s[74:75], 2                   // scale by bpe

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s11, 0x1
s_cbranch_scc1 label_NoBranch_S4E4MY8N8MB2NVUR_0   // Only branch on scc0
// long branch sync
s_getpc_b64 s[44:45]                               // addr of next instr
s_add_i32 s46, label_Sync_EDN_Edge_0, 0x4          // target branch offset
s_add_u32 s44, s44, s46                            // add target branch offset
s_addc_u32 s45, s45, 0                             // add high and carry
s_setpc_b64 s[44:45]                               // branch to label_Sync_EDN_Edge_0
label_NoBranch_S4E4MY8N8MB2NVUR_0:
// check done end

// buffer load start
buffer_load_dword v12, v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 12
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_96 // SyncAddbranchhere
buffer_load_dword v176, v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_96:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v12, v12, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_96 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v6, v173, s[32:33]             // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_96      // Syncbranchhere

label_Synchronizer_read_add_end_1_96:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_96:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v19, v13, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 19
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_97 // SyncAddbranchhere
buffer_load_dword v176, v13, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_97:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v19, v19, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_97 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v13, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_97      // Syncbranchhere

label_Synchronizer_read_add_end_1_97:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_97:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v26, v20, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 26
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_98 // SyncAddbranchhere
buffer_load_dword v176, v20, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_98:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v26, v26, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_98 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v20, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_98      // Syncbranchhere

label_Synchronizer_read_add_end_1_98:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_98:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v33, v27, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 33
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_99 // SyncAddbranchhere
buffer_load_dword v176, v27, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_99:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v33, v33, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_99 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v27, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_99      // Syncbranchhere

label_Synchronizer_read_add_end_1_99:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_99:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v40, v34, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 40
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_100 // SyncAddbranchhere
buffer_load_dword v176, v34, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_100:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v40, v40, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_100 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v34, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_100     // Syncbranchhere

label_Synchronizer_read_add_end_1_100:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_100:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v47, v41, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 47
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_101 // SyncAddbranchhere
buffer_load_dword v176, v41, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_101:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v47, v47, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_101 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v41, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_101     // Syncbranchhere

label_Synchronizer_read_add_end_1_101:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_101:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v54, v48, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 54
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_102 // SyncAddbranchhere
buffer_load_dword v176, v48, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_102:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v54, v54, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_102 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v48, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_102     // Syncbranchhere

label_Synchronizer_read_add_end_1_102:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_102:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v61, v55, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 61
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_103 // SyncAddbranchhere
buffer_load_dword v176, v55, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_103:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v61, v61, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_103 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v55, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_103     // Syncbranchhere

label_Synchronizer_read_add_end_1_103:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_103:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v66, v62, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 66
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_104 // SyncAddbranchhere
buffer_load_dword v176, v62, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_104:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v66, v66, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_104 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v62, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_104     // Syncbranchhere

label_Synchronizer_read_add_end_1_104:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_104:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v71, v67, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 71
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_105 // SyncAddbranchhere
buffer_load_dword v176, v67, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_105:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v71, v71, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_105 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v67, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_105     // Syncbranchhere

label_Synchronizer_read_add_end_1_105:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_105:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v76, v72, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 76
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_106 // SyncAddbranchhere
buffer_load_dword v176, v72, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_106:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v76, v76, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_106 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v72, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_106     // Syncbranchhere

label_Synchronizer_read_add_end_1_106:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_106:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v81, v77, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 81
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_107 // SyncAddbranchhere
buffer_load_dword v176, v77, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_107:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v81, v81, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_107 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v77, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_107     // Syncbranchhere

label_Synchronizer_read_add_end_1_107:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_107:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v86, v82, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 86
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_108 // SyncAddbranchhere
buffer_load_dword v176, v82, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_108:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v86, v86, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_108 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v82, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_108     // Syncbranchhere

label_Synchronizer_read_add_end_1_108:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_108:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v91, v87, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 91
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_109 // SyncAddbranchhere
buffer_load_dword v176, v87, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_109:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v91, v91, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_109 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v87, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_109     // Syncbranchhere

label_Synchronizer_read_add_end_1_109:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_109:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v96, v92, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 96
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_110 // SyncAddbranchhere
buffer_load_dword v176, v92, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_110:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v96, v96, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_110 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v92, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_110     // Syncbranchhere

label_Synchronizer_read_add_end_1_110:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_110:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v101, v97, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 101
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_111 // SyncAddbranchhere
buffer_load_dword v176, v97, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_111:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v101, v101, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_111 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v97, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_111     // Syncbranchhere

label_Synchronizer_read_add_end_1_111:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_111:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v106, v102, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 106
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_112 // SyncAddbranchhere
buffer_load_dword v176, v102, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_112:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v106, v106, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_112 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v102, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_112     // Syncbranchhere

label_Synchronizer_read_add_end_1_112:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_112:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v111, v107, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 111
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_113 // SyncAddbranchhere
buffer_load_dword v176, v107, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_113:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v111, v111, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_113 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v107, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_113     // Syncbranchhere

label_Synchronizer_read_add_end_1_113:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_113:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v116, v112, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 116
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_114 // SyncAddbranchhere
buffer_load_dword v176, v112, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_114:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v116, v116, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_114 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v112, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_114     // Syncbranchhere

label_Synchronizer_read_add_end_1_114:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_114:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v121, v117, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 121
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_115 // SyncAddbranchhere
buffer_load_dword v176, v117, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_115:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v121, v121, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_115 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v117, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_115     // Syncbranchhere

label_Synchronizer_read_add_end_1_115:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_115:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v126, v122, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 126
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_116 // SyncAddbranchhere
buffer_load_dword v176, v122, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_116:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v126, v126, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_116 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v122, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_116     // Syncbranchhere

label_Synchronizer_read_add_end_1_116:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_116:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v131, v127, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 131
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_117 // SyncAddbranchhere
buffer_load_dword v176, v127, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_117:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v131, v131, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_117 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v127, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_117     // Syncbranchhere

label_Synchronizer_read_add_end_1_117:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_117:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v136, v132, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 136
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_118 // SyncAddbranchhere
buffer_load_dword v176, v132, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_118:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v136, v136, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_118 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v132, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_118     // Syncbranchhere

label_Synchronizer_read_add_end_1_118:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_118:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v142, v137, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 142
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_119 // SyncAddbranchhere
buffer_load_dword v176, v137, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_119:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v142, v142, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_119 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v137, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_119     // Syncbranchhere

label_Synchronizer_read_add_end_1_119:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_119:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v147, v143, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 147
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_120 // SyncAddbranchhere
buffer_load_dword v176, v143, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_120:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v147, v147, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_120 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v143, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_120     // Syncbranchhere

label_Synchronizer_read_add_end_1_120:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_120:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v152, v148, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 152
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_121 // SyncAddbranchhere
buffer_load_dword v176, v148, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_121:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v152, v152, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_121 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v148, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_121     // Syncbranchhere

label_Synchronizer_read_add_end_1_121:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_121:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v157, v153, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 157
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_122 // SyncAddbranchhere
buffer_load_dword v176, v153, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_122:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v157, v157, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_122 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v153, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_122     // Syncbranchhere

label_Synchronizer_read_add_end_1_122:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_122:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v162, v158, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 162
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_123 // SyncAddbranchhere
buffer_load_dword v176, v158, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_123:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v162, v162, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_123 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v158, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_123     // Syncbranchhere

label_Synchronizer_read_add_end_1_123:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_123:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v167, v163, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 167
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_124 // SyncAddbranchhere
buffer_load_dword v176, v163, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_124:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v167, v167, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_124 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v163, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_124     // Syncbranchhere

label_Synchronizer_read_add_end_1_124:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_124:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v172, v168, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 172
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_125 // SyncAddbranchhere
buffer_load_dword v176, v168, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_125:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v172, v172, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_125 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v168, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_125     // Syncbranchhere

label_Synchronizer_read_add_end_1_125:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_125:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 0, 1), (0, 0, 0, 2), (0, 0, 0, 3), (0, 1, 0, 0), (0, 1, 0, 1), (0, 1, 0, 2), (0, 1, 0, 3), (0, 0, 1, 0), (0, 0, 1, 1), (0, 0, 1, 2), (0, 0, 1, 3), (0, 1, 1, 0), (0, 1, 1, 1), (0, 1, 1, 2), (0, 1, 1, 3), (0, 0, 2, 0), (0, 0, 2, 1), (0, 0, 2, 2), (0, 0, 2, 3), (0, 1, 2, 0), (0, 1, 2, 1), (0, 1, 2, 2), (0, 1, 2, 3), (0, 0, 3, 0), (0, 0, 3, 1), (0, 0, 3, 2), (0, 0, 3, 3), (0, 1, 3, 0), (0, 1, 3, 1)] */
v_mul_f32 v[vgprValuC+12], s[sgprAlpha], v[vgprValuC+12] // *= alpha
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+121], s[sgprAlpha], v[vgprValuC+121] // *= alpha
v_mul_f32 v[vgprValuC+126], s[sgprAlpha], v[vgprValuC+126] // *= alpha
v_mul_f32 v[vgprValuC+131], s[sgprAlpha], v[vgprValuC+131] // *= alpha
v_mul_f32 v[vgprValuC+136], s[sgprAlpha], v[vgprValuC+136] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+147], s[sgprAlpha], v[vgprValuC+147] // *= alpha
v_mul_f32 v[vgprValuC+152], s[sgprAlpha], v[vgprValuC+152] // *= alpha
v_mul_f32 v[vgprValuC+157], s[sgprAlpha], v[vgprValuC+157] // *= alpha
v_mul_f32 v[vgprValuC+162], s[sgprAlpha], v[vgprValuC+162] // *= alpha
v_mul_f32 v[vgprValuC+167], s[sgprAlpha], v[vgprValuC+167] // *= alpha
v_mul_f32 v[vgprValuC+172], s[sgprAlpha], v[vgprValuC+172] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+12], v11, v[vgprValuC+12]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+12], v10, v[vgprValuC+12]    // C += bias
v_cvt_f16_f32 v12, v[vgprValuC+12]                 // convert C to fp16
buffer_store_short v12, v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+19], v17, v[vgprValuC+19]    // C += bias
v_cvt_f16_f32 v19, v[vgprValuC+19]                 // convert C to fp16
buffer_store_short v19, v14, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+26], v24, v[vgprValuC+26]    // C += bias
v_cvt_f16_f32 v26, v[vgprValuC+26]                 // convert C to fp16
buffer_store_short v26, v21, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+33], v31, v[vgprValuC+33]    // C += bias
v_cvt_f16_f32 v33, v[vgprValuC+33]                 // convert C to fp16
buffer_store_short v33, v28, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+40], v38, v[vgprValuC+40]    // C += bias
v_cvt_f16_f32 v40, v[vgprValuC+40]                 // convert C to fp16
buffer_store_short v40, v35, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v42, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+54], v53, v[vgprValuC+54]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+54], v52, v[vgprValuC+54]    // C += bias
v_cvt_f16_f32 v54, v[vgprValuC+54]                 // convert C to fp16
buffer_store_short v54, v49, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+61], v60, v[vgprValuC+61]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+61], v59, v[vgprValuC+61]    // C += bias
v_cvt_f16_f32 v61, v[vgprValuC+61]                 // convert C to fp16
buffer_store_short v61, v56, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+66], v11, v[vgprValuC+66]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+66], v10, v[vgprValuC+66]    // C += bias
v_cvt_f16_f32 v66, v[vgprValuC+66]                 // convert C to fp16
buffer_store_short v66, v63, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+71], v18, v[vgprValuC+71]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+71], v17, v[vgprValuC+71]    // C += bias
v_cvt_f16_f32 v71, v[vgprValuC+71]                 // convert C to fp16
buffer_store_short v71, v68, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+76], v25, v[vgprValuC+76]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+76], v24, v[vgprValuC+76]    // C += bias
v_cvt_f16_f32 v76, v[vgprValuC+76]                 // convert C to fp16
buffer_store_short v76, v73, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+81], v32, v[vgprValuC+81]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+81], v31, v[vgprValuC+81]    // C += bias
v_cvt_f16_f32 v81, v[vgprValuC+81]                 // convert C to fp16
buffer_store_short v81, v78, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+86], v39, v[vgprValuC+86]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+86], v38, v[vgprValuC+86]    // C += bias
v_cvt_f16_f32 v86, v[vgprValuC+86]                 // convert C to fp16
buffer_store_short v86, v83, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+91], v46, v[vgprValuC+91]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+91], v45, v[vgprValuC+91]    // C += bias
v_cvt_f16_f32 v91, v[vgprValuC+91]                 // convert C to fp16
buffer_store_short v91, v88, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+96], v53, v[vgprValuC+96]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+96], v52, v[vgprValuC+96]    // C += bias
v_cvt_f16_f32 v96, v[vgprValuC+96]                 // convert C to fp16
buffer_store_short v96, v93, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+101], v60, v[vgprValuC+101]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+101], v59, v[vgprValuC+101]  // C += bias
v_cvt_f16_f32 v101, v[vgprValuC+101]               // convert C to fp16
buffer_store_short v101, v98, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+106], v11, v[vgprValuC+106]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+106], v10, v[vgprValuC+106]  // C += bias
v_cvt_f16_f32 v106, v[vgprValuC+106]               // convert C to fp16
buffer_store_short v106, v103, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+111], v18, v[vgprValuC+111]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+111], v17, v[vgprValuC+111]  // C += bias
v_cvt_f16_f32 v111, v[vgprValuC+111]               // convert C to fp16
buffer_store_short v111, v108, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+116], v25, v[vgprValuC+116]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+116], v24, v[vgprValuC+116]  // C += bias
v_cvt_f16_f32 v116, v[vgprValuC+116]               // convert C to fp16
buffer_store_short v116, v113, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+121], v32, v[vgprValuC+121]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+121], v31, v[vgprValuC+121]  // C += bias
v_cvt_f16_f32 v121, v[vgprValuC+121]               // convert C to fp16
buffer_store_short v121, v118, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+126], v39, v[vgprValuC+126]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+126], v38, v[vgprValuC+126]  // C += bias
v_cvt_f16_f32 v126, v[vgprValuC+126]               // convert C to fp16
buffer_store_short v126, v123, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+131], v46, v[vgprValuC+131]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+131], v45, v[vgprValuC+131]  // C += bias
v_cvt_f16_f32 v131, v[vgprValuC+131]               // convert C to fp16
buffer_store_short v131, v128, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+136], v53, v[vgprValuC+136]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+136], v52, v[vgprValuC+136]  // C += bias
v_cvt_f16_f32 v136, v[vgprValuC+136]               // convert C to fp16
buffer_store_short v136, v133, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+142], v60, v[vgprValuC+142]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+142], v59, v[vgprValuC+142]  // C += bias
v_cvt_f16_f32 v142, v[vgprValuC+142]               // convert C to fp16
buffer_store_short v142, v138, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+147], v11, v[vgprValuC+147]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+147], v10, v[vgprValuC+147]  // C += bias
v_cvt_f16_f32 v147, v[vgprValuC+147]               // convert C to fp16
buffer_store_short v147, v144, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+152], v18, v[vgprValuC+152]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+152], v17, v[vgprValuC+152]  // C += bias
v_cvt_f16_f32 v152, v[vgprValuC+152]               // convert C to fp16
buffer_store_short v152, v149, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+157], v25, v[vgprValuC+157]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+157], v24, v[vgprValuC+157]  // C += bias
v_cvt_f16_f32 v157, v[vgprValuC+157]               // convert C to fp16
buffer_store_short v157, v154, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+162], v32, v[vgprValuC+162]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+162], v31, v[vgprValuC+162]  // C += bias
v_cvt_f16_f32 v162, v[vgprValuC+162]               // convert C to fp16
buffer_store_short v162, v159, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+167], v39, v[vgprValuC+167]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+167], v38, v[vgprValuC+167]  // C += bias
v_cvt_f16_f32 v167, v[vgprValuC+167]               // convert C to fp16
buffer_store_short v167, v164, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+172], v46, v[vgprValuC+172]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+172], v45, v[vgprValuC+172]  // C += bias
v_cvt_f16_f32 v172, v[vgprValuC+172]               // convert C to fp16
buffer_store_short v172, v169, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
label_Sync_EDN_Edge_0:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//GW end
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,1,3,2:vw1); (0,1,3,3:vw1); (0,0,4,0:vw1); (0,0,4,1:vw1); (0,0,4,2:vw1); (0,0,4,3:vw1); (0,1,4,0:vw1); (0,1,4,1:vw1); (0,1,4,2:vw1); (0,1,4,3:vw1); (0,0,5,0:vw1); (0,0,5,1:vw1); (0,0,5,2:vw1); (0,0,5,3:vw1); (0,1,5,0:vw1); (0,1,5,1:vw1); (0,1,5,2:vw1); (0,1,5,3:vw1); (0,0,6,0:vw1); (0,0,6,1:vw1); (0,0,6,2:vw1); (0,0,6,3:vw1); (0,1,6,0:vw1); (0,1,6,1:vw1); (0,1,6,2:vw1); (0,1,6,3:vw1); (0,0,7,0:vw1); (0,0,7,1:vw1); (0,0,7,2:vw1); (0,0,7,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v173, BufferOOB
/* (d1,vc1,d0,vc0)=(0,3,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v4, s68
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
ds_read_b32 v10, v8 offset:0                       // load Bias
v_add_u32 v9, 1024, v8                             // add ScaleAlphaVec offset (3)
ds_read_b32 v11, v9 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v4, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v173, v6, s[72:73]               // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v173, v7, s[72:73]               // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v4, s68
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
ds_read_b32 v17, v15 offset:0                      // load Bias
v_add_u32 v16, 1024, v15                           // add ScaleAlphaVec offset (3)
ds_read_b32 v18, v16 offset:0                      // load scaleAlpha
v_add_lshl_u32 v13, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v173, v13, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v14, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v173, v14, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v22, v0, s68
v_lshlrev_b32 v22, 0x2, v22                        // Bias address scaled by BPE
ds_read_b32 v24, v22 offset:0                      // load Bias
v_add_u32 v23, 1024, v22                           // add ScaleAlphaVec offset (3)
ds_read_b32 v25, v23 offset:0                      // load scaleAlpha
v_add_lshl_u32 v20, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v173, v20, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v21, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v21, v173, v21, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v29, v4, s68
v_lshlrev_b32 v29, 0x2, v29                        // Bias address scaled by BPE
ds_read_b32 v31, v29 offset:0                      // load Bias
v_add_u32 v30, 1024, v29                           // add ScaleAlphaVec offset (3)
ds_read_b32 v32, v30 offset:0                      // load scaleAlpha
v_add_lshl_u32 v27, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v173, v27, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v28, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v173, v28, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v36, v4, s68
v_lshlrev_b32 v36, 0x2, v36                        // Bias address scaled by BPE
ds_read_b32 v38, v36 offset:0                      // load Bias
v_add_u32 v37, 1024, v36                           // add ScaleAlphaVec offset (3)
ds_read_b32 v39, v37 offset:0                      // load scaleAlpha
v_add_lshl_u32 v34, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v173, v34, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v35, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v35, v173, v35, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v43, v4, s68
v_lshlrev_b32 v43, 0x2, v43                        // Bias address scaled by BPE
ds_read_b32 v45, v43 offset:0                      // load Bias
v_add_u32 v44, 1024, v43                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v44 offset:0                      // load scaleAlpha
v_add_lshl_u32 v41, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v173, v41, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v42, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v173, v42, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v50, v4, s68
v_lshlrev_b32 v50, 0x2, v50                        // Bias address scaled by BPE
ds_read_b32 v52, v50 offset:0                      // load Bias
v_add_u32 v51, 1024, v50                           // add ScaleAlphaVec offset (3)
ds_read_b32 v53, v51 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v173, v48, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v49, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v49, v173, v49, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v57, v4, s68
v_lshlrev_b32 v57, 0x2, v57                        // Bias address scaled by BPE
ds_read_b32 v59, v57 offset:0                      // load Bias
v_add_u32 v58, 1024, v57                           // add ScaleAlphaVec offset (3)
ds_read_b32 v60, v58 offset:0                      // load scaleAlpha
v_add_lshl_u32 v55, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v173, v55, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v56, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v173, v56, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v64, v4, s68
v_lshlrev_b32 v64, 0x2, v64                        // Bias address scaled by BPE
v_add_u32 v65, 1024, v64                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v62, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v173, v62, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v63, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v63, v173, v63, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v69, v4, s68
v_lshlrev_b32 v69, 0x2, v69                        // Bias address scaled by BPE
v_add_u32 v70, 1024, v69                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v67, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v67, v173, v67, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v68, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v68, v173, v68, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v74, v0, s68
v_lshlrev_b32 v74, 0x2, v74                        // Bias address scaled by BPE
v_add_u32 v75, 1024, v74                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v72, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v72, v173, v72, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v73, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v73, v173, v73, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v79, v4, s68
v_lshlrev_b32 v79, 0x2, v79                        // Bias address scaled by BPE
v_add_u32 v80, 1024, v79                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v77, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v173, v77, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v78, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v78, v173, v78, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v84, v4, s68
v_lshlrev_b32 v84, 0x2, v84                        // Bias address scaled by BPE
v_add_u32 v85, 1024, v84                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v82, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v82, v173, v82, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v83, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v173, v83, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v89, v4, s68
v_lshlrev_b32 v89, 0x2, v89                        // Bias address scaled by BPE
v_add_u32 v90, 1024, v89                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v87, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v87, v173, v87, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v88, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v88, v173, v88, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v94, v4, s68
v_lshlrev_b32 v94, 0x2, v94                        // Bias address scaled by BPE
v_add_u32 v95, 1024, v94                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v92, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v92, v173, v92, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v93, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v93, v173, v93, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v99, v4, s68
v_lshlrev_b32 v99, 0x2, v99                        // Bias address scaled by BPE
v_add_u32 v100, 1024, v99                          // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v97, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v173, v97, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v98, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v98, v173, v98, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v104, v4, s68
v_lshlrev_b32 v104, 0x2, v104                      // Bias address scaled by BPE
v_add_u32 v105, 1024, v104                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v102, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v102, v173, v102, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v103, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v103, v173, v103, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v109, v4, s68
v_lshlrev_b32 v109, 0x2, v109                      // Bias address scaled by BPE
v_add_u32 v110, 1024, v109                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v107, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v107, v173, v107, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v108, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v108, v173, v108, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v114, v0, s68
v_lshlrev_b32 v114, 0x2, v114                      // Bias address scaled by BPE
v_add_u32 v115, 1024, v114                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v112, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v112, v173, v112, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v113, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v113, v173, v113, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v4, s68
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v120, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v117, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v117, v173, v117, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v118, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v173, v118, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v124, v4, s68
v_lshlrev_b32 v124, 0x2, v124                      // Bias address scaled by BPE
v_add_u32 v125, 1024, v124                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v122, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v122, v173, v122, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v123, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v123, v173, v123, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v129, v4, s68
v_lshlrev_b32 v129, 0x2, v129                      // Bias address scaled by BPE
v_add_u32 v130, 1024, v129                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v127, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v127, v173, v127, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v128, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v173, v128, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v134, v4, s68
v_lshlrev_b32 v134, 0x2, v134                      // Bias address scaled by BPE
v_add_u32 v135, 1024, v134                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v132, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v132, v173, v132, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v133, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v133, v173, v133, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v140, v4, s68
v_lshlrev_b32 v140, 0x2, v140                      // Bias address scaled by BPE
v_add_u32 v141, 1024, v140                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v137, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v137, v173, v137, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v138, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v173, v138, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v145, v4, s68
v_lshlrev_b32 v145, 0x2, v145                      // Bias address scaled by BPE
v_add_u32 v146, 1024, v145                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v143, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v143, v173, v143, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v144, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v144, v173, v144, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v150, v4, s68
v_lshlrev_b32 v150, 0x2, v150                      // Bias address scaled by BPE
v_add_u32 v151, 1024, v150                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v148, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v148, v173, v148, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v149, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v173, v149, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v155, v0, s68
v_lshlrev_b32 v155, 0x2, v155                      // Bias address scaled by BPE
v_add_u32 v156, 1024, v155                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v153, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v153, v173, v153, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v154, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v154, v173, v154, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v160, v4, s68
v_lshlrev_b32 v160, 0x2, v160                      // Bias address scaled by BPE
v_add_u32 v161, 1024, v160                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v158, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v158, v173, v158, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v159, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v173, v159, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v165, v4, s68
v_lshlrev_b32 v165, 0x2, v165                      // Bias address scaled by BPE
v_add_u32 v166, 1024, v165                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v163, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v163, v173, v163, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v164, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v164, v173, v164, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v170, v4, s68
v_lshlrev_b32 v170, 0x2, v170                      // Bias address scaled by BPE
v_add_u32 v171, 1024, v170                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v168, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v168, v173, v168, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v169, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v169, v173, v169, s[72:73]           // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+12], acc30          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+19], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+26], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+33], acc33          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+40], acc34          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+47], acc35          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+54], acc36          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+61], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+66], acc38          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+71], acc39          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+76], acc40          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+81], acc41          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+86], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+91], acc43          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+96], acc44          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+101], acc45         // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+106], acc46         // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+111], acc47         // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+116], acc48         // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+121], acc49         // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+126], acc50         // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+131], acc51         // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+136], acc52         // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+142], acc53         // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+147], acc54         // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+152], acc55         // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+157], acc56         // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+162], acc57         // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+167], acc58         // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+172], acc59         // copy acc to vreg[59]

/* store after Acc, GSU: 2 */

/* store after Acc, GSU: 2 */

buffer_store_dword v12, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 12

buffer_store_dword v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 19

buffer_store_dword v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 26

buffer_store_dword v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 33

buffer_store_dword v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 40

buffer_store_dword v47, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 47

buffer_store_dword v54, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 54

buffer_store_dword v61, v55, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 61

buffer_store_dword v66, v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 66

buffer_store_dword v71, v67, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 71

buffer_store_dword v76, v72, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 76

buffer_store_dword v81, v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 81

buffer_store_dword v86, v82, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 86

buffer_store_dword v91, v87, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 91

buffer_store_dword v96, v92, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 96

buffer_store_dword v101, v97, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 101

buffer_store_dword v106, v102, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 106

buffer_store_dword v111, v107, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 111

buffer_store_dword v116, v112, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 116

buffer_store_dword v121, v117, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 121

buffer_store_dword v126, v122, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 126

buffer_store_dword v131, v127, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 131

buffer_store_dword v136, v132, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 136

buffer_store_dword v142, v137, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 142

buffer_store_dword v147, v143, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 147

buffer_store_dword v152, v148, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 152

buffer_store_dword v157, v153, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 157

buffer_store_dword v162, v158, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 162

buffer_store_dword v167, v163, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 167

buffer_store_dword v172, v168, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 172
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//sourece store done, GSU:2

// check done start
// synchronizer offset cal
s_mul_i32 s13, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s11, s13, s[sgprWorkGroup2]
s_mul_i32 s12, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s12, s12, s[sgprWorkGroup0]
s_add_u32 s12, s12, s11
v_readfirstlane_b32 s11, v[vgprSerial]
s_mul_i32 s13, s13, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s11, s11, 0x6
s_mul_i32 s11, s13, s11                            // wave offset at batch
s_add_u32 s12, s11, s12
s_mul_i32 s13, s13, 4                              // cal a batch offset
s_mul_i32 s13, s13, 1                              // this batch offset
s_add_u32 s12, s12, s13
s_lshl_b32 s12, s12, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s12
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_and_b32 s11, s[sgprGSU], 0x3fff                  // Restore GSU
s_sub_u32 s11, s11, 0x1
s_atomic_dec s11, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s75, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s74, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s78, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s78, s78, 1                              // Free1
s_mul_hi_u32 s77, s78, s[sgprStrideC1J]            // Free1
s_mul_i32 s76, s78, s[sgprStrideC1J]               // Free1
s_add_u32 s74, s74, s76                            // Free1
s_addc_u32 s75, s75, s77                           // Free1
s_sub_u32 s78, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s78, s78, 1                              // Free2
s_mul_hi_u32 s77, s78, s[sgprStrideCK]             // Free2
s_mul_i32 s76, s78, s[sgprStrideCK]                // Free2
s_add_u32 s74, s74, s76                            // Free2
s_addc_u32 s75, s75, s77                           // Free2
s_lshl_b64 s[26:27], s[74:75], 2                   // scale by bpe

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s11, 0x1
s_cbranch_scc1 label_NoBranch_NK5U8O85T43QFXGG_0   // Only branch on scc0
// long branch sync
s_getpc_b64 s[44:45]                               // addr of next instr
s_add_i32 s46, label_Sync_EDN_Edge_1, 0x4          // target branch offset
s_add_u32 s44, s44, s46                            // add target branch offset
s_addc_u32 s45, s45, 0                             // add high and carry
s_setpc_b64 s[44:45]                               // branch to label_Sync_EDN_Edge_1
label_NoBranch_NK5U8O85T43QFXGG_0:
// check done end

// buffer load start
buffer_load_dword v12, v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 12
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_126 // SyncAddbranchhere
buffer_load_dword v176, v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_126:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v12, v12, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_126 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v6, v173, s[32:33]             // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_126     // Syncbranchhere

label_Synchronizer_read_add_end_1_126:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_126:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v19, v13, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 19
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_127 // SyncAddbranchhere
buffer_load_dword v176, v13, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_127:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v19, v19, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_127 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v13, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_127     // Syncbranchhere

label_Synchronizer_read_add_end_1_127:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_127:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v26, v20, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 26
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_128 // SyncAddbranchhere
buffer_load_dword v176, v20, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_128:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v26, v26, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_128 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v20, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_128     // Syncbranchhere

label_Synchronizer_read_add_end_1_128:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_128:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v33, v27, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 33
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_129 // SyncAddbranchhere
buffer_load_dword v176, v27, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_129:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v33, v33, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_129 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v27, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_129     // Syncbranchhere

label_Synchronizer_read_add_end_1_129:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_129:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v40, v34, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 40
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_130 // SyncAddbranchhere
buffer_load_dword v176, v34, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_130:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v40, v40, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_130 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v34, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_130     // Syncbranchhere

label_Synchronizer_read_add_end_1_130:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_130:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v47, v41, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 47
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_131 // SyncAddbranchhere
buffer_load_dword v176, v41, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_131:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v47, v47, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_131 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v41, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_131     // Syncbranchhere

label_Synchronizer_read_add_end_1_131:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_131:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v54, v48, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 54
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_132 // SyncAddbranchhere
buffer_load_dword v176, v48, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_132:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v54, v54, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_132 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v48, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_132     // Syncbranchhere

label_Synchronizer_read_add_end_1_132:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_132:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v61, v55, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 61
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_133 // SyncAddbranchhere
buffer_load_dword v176, v55, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_133:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v61, v61, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_133 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v55, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_133     // Syncbranchhere

label_Synchronizer_read_add_end_1_133:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_133:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v66, v62, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 66
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_134 // SyncAddbranchhere
buffer_load_dword v176, v62, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_134:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v66, v66, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_134 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v62, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_134     // Syncbranchhere

label_Synchronizer_read_add_end_1_134:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_134:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v71, v67, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 71
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_135 // SyncAddbranchhere
buffer_load_dword v176, v67, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_135:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v71, v71, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_135 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v67, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_135     // Syncbranchhere

label_Synchronizer_read_add_end_1_135:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_135:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v76, v72, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 76
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_136 // SyncAddbranchhere
buffer_load_dword v176, v72, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_136:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v76, v76, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_136 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v72, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_136     // Syncbranchhere

label_Synchronizer_read_add_end_1_136:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_136:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v81, v77, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 81
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_137 // SyncAddbranchhere
buffer_load_dword v176, v77, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_137:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v81, v81, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_137 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v77, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_137     // Syncbranchhere

label_Synchronizer_read_add_end_1_137:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_137:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v86, v82, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 86
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_138 // SyncAddbranchhere
buffer_load_dword v176, v82, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_138:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v86, v86, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_138 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v82, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_138     // Syncbranchhere

label_Synchronizer_read_add_end_1_138:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_138:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v91, v87, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 91
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_139 // SyncAddbranchhere
buffer_load_dword v176, v87, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_139:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v91, v91, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_139 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v87, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_139     // Syncbranchhere

label_Synchronizer_read_add_end_1_139:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_139:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v96, v92, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 96
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_140 // SyncAddbranchhere
buffer_load_dword v176, v92, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_140:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v96, v96, v176                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_140 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v92, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_140     // Syncbranchhere

label_Synchronizer_read_add_end_1_140:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_140:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v101, v97, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 101
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_141 // SyncAddbranchhere
buffer_load_dword v176, v97, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_141:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v101, v101, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_141 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v97, v173, s[32:33]            // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_141     // Syncbranchhere

label_Synchronizer_read_add_end_1_141:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_141:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v106, v102, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 106
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_142 // SyncAddbranchhere
buffer_load_dword v176, v102, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_142:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v106, v106, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_142 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v102, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_142     // Syncbranchhere

label_Synchronizer_read_add_end_1_142:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_142:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v111, v107, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 111
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_143 // SyncAddbranchhere
buffer_load_dword v176, v107, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_143:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v111, v111, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_143 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v107, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_143     // Syncbranchhere

label_Synchronizer_read_add_end_1_143:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_143:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v116, v112, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 116
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_144 // SyncAddbranchhere
buffer_load_dword v176, v112, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_144:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v116, v116, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_144 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v112, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_144     // Syncbranchhere

label_Synchronizer_read_add_end_1_144:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_144:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v121, v117, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 121
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_145 // SyncAddbranchhere
buffer_load_dword v176, v117, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_145:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v121, v121, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_145 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v117, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_145     // Syncbranchhere

label_Synchronizer_read_add_end_1_145:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_145:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v126, v122, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 126
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_146 // SyncAddbranchhere
buffer_load_dword v176, v122, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_146:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v126, v126, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_146 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v122, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_146     // Syncbranchhere

label_Synchronizer_read_add_end_1_146:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_146:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v131, v127, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 131
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_147 // SyncAddbranchhere
buffer_load_dword v176, v127, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_147:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v131, v131, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_147 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v127, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_147     // Syncbranchhere

label_Synchronizer_read_add_end_1_147:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_147:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v136, v132, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 136
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_148 // SyncAddbranchhere
buffer_load_dword v176, v132, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_148:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v136, v136, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_148 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v132, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_148     // Syncbranchhere

label_Synchronizer_read_add_end_1_148:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_148:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v142, v137, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 142
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_149 // SyncAddbranchhere
buffer_load_dword v176, v137, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_149:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v142, v142, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_149 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v137, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_149     // Syncbranchhere

label_Synchronizer_read_add_end_1_149:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_149:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v147, v143, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 147
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_150 // SyncAddbranchhere
buffer_load_dword v176, v143, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_150:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v147, v147, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_150 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v143, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_150     // Syncbranchhere

label_Synchronizer_read_add_end_1_150:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_150:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v152, v148, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 152
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_151 // SyncAddbranchhere
buffer_load_dword v176, v148, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_151:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v152, v152, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_151 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v148, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_151     // Syncbranchhere

label_Synchronizer_read_add_end_1_151:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_151:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v157, v153, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 157
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_152 // SyncAddbranchhere
buffer_load_dword v176, v153, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_152:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v157, v157, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_152 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v153, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_152     // Syncbranchhere

label_Synchronizer_read_add_end_1_152:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_152:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v162, v158, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 162
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_153 // SyncAddbranchhere
buffer_load_dword v176, v158, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_153:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v162, v162, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_153 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v158, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_153     // Syncbranchhere

label_Synchronizer_read_add_end_1_153:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_153:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v167, v163, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 167
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_154 // SyncAddbranchhere
buffer_load_dword v176, v163, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_154:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v167, v167, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_154 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v163, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_154     // Syncbranchhere

label_Synchronizer_read_add_end_1_154:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_154:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v173, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v172, v168, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 172
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_155 // SyncAddbranchhere
buffer_load_dword v176, v168, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_155:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v172, v172, v176                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_155 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v174, v168, v173, s[32:33]           // protect if OOB
buffer_load_dword v176, v174, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_155     // Syncbranchhere

label_Synchronizer_read_add_end_1_155:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_155:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 1, 3, 2), (0, 1, 3, 3), (0, 0, 4, 0), (0, 0, 4, 1), (0, 0, 4, 2), (0, 0, 4, 3), (0, 1, 4, 0), (0, 1, 4, 1), (0, 1, 4, 2), (0, 1, 4, 3), (0, 0, 5, 0), (0, 0, 5, 1), (0, 0, 5, 2), (0, 0, 5, 3), (0, 1, 5, 0), (0, 1, 5, 1), (0, 1, 5, 2), (0, 1, 5, 3), (0, 0, 6, 0), (0, 0, 6, 1), (0, 0, 6, 2), (0, 0, 6, 3), (0, 1, 6, 0), (0, 1, 6, 1), (0, 1, 6, 2), (0, 1, 6, 3), (0, 0, 7, 0), (0, 0, 7, 1), (0, 0, 7, 2), (0, 0, 7, 3)] */
v_mul_f32 v[vgprValuC+12], s[sgprAlpha], v[vgprValuC+12] // *= alpha
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+121], s[sgprAlpha], v[vgprValuC+121] // *= alpha
v_mul_f32 v[vgprValuC+126], s[sgprAlpha], v[vgprValuC+126] // *= alpha
v_mul_f32 v[vgprValuC+131], s[sgprAlpha], v[vgprValuC+131] // *= alpha
v_mul_f32 v[vgprValuC+136], s[sgprAlpha], v[vgprValuC+136] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+147], s[sgprAlpha], v[vgprValuC+147] // *= alpha
v_mul_f32 v[vgprValuC+152], s[sgprAlpha], v[vgprValuC+152] // *= alpha
v_mul_f32 v[vgprValuC+157], s[sgprAlpha], v[vgprValuC+157] // *= alpha
v_mul_f32 v[vgprValuC+162], s[sgprAlpha], v[vgprValuC+162] // *= alpha
v_mul_f32 v[vgprValuC+167], s[sgprAlpha], v[vgprValuC+167] // *= alpha
v_mul_f32 v[vgprValuC+172], s[sgprAlpha], v[vgprValuC+172] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+12], v11, v[vgprValuC+12]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+12], v10, v[vgprValuC+12]    // C += bias
v_cvt_f16_f32 v12, v[vgprValuC+12]                 // convert C to fp16
buffer_store_short v12, v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+19], v17, v[vgprValuC+19]    // C += bias
v_cvt_f16_f32 v19, v[vgprValuC+19]                 // convert C to fp16
buffer_store_short v19, v14, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+26], v24, v[vgprValuC+26]    // C += bias
v_cvt_f16_f32 v26, v[vgprValuC+26]                 // convert C to fp16
buffer_store_short v26, v21, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+33], v31, v[vgprValuC+33]    // C += bias
v_cvt_f16_f32 v33, v[vgprValuC+33]                 // convert C to fp16
buffer_store_short v33, v28, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+40], v38, v[vgprValuC+40]    // C += bias
v_cvt_f16_f32 v40, v[vgprValuC+40]                 // convert C to fp16
buffer_store_short v40, v35, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v42, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+54], v53, v[vgprValuC+54]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+54], v52, v[vgprValuC+54]    // C += bias
v_cvt_f16_f32 v54, v[vgprValuC+54]                 // convert C to fp16
buffer_store_short v54, v49, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+61], v60, v[vgprValuC+61]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+61], v59, v[vgprValuC+61]    // C += bias
v_cvt_f16_f32 v61, v[vgprValuC+61]                 // convert C to fp16
buffer_store_short v61, v56, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+66], v11, v[vgprValuC+66]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+66], v10, v[vgprValuC+66]    // C += bias
v_cvt_f16_f32 v66, v[vgprValuC+66]                 // convert C to fp16
buffer_store_short v66, v63, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+71], v18, v[vgprValuC+71]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+71], v17, v[vgprValuC+71]    // C += bias
v_cvt_f16_f32 v71, v[vgprValuC+71]                 // convert C to fp16
buffer_store_short v71, v68, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+76], v25, v[vgprValuC+76]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+76], v24, v[vgprValuC+76]    // C += bias
v_cvt_f16_f32 v76, v[vgprValuC+76]                 // convert C to fp16
buffer_store_short v76, v73, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+81], v32, v[vgprValuC+81]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+81], v31, v[vgprValuC+81]    // C += bias
v_cvt_f16_f32 v81, v[vgprValuC+81]                 // convert C to fp16
buffer_store_short v81, v78, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+86], v39, v[vgprValuC+86]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+86], v38, v[vgprValuC+86]    // C += bias
v_cvt_f16_f32 v86, v[vgprValuC+86]                 // convert C to fp16
buffer_store_short v86, v83, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+91], v46, v[vgprValuC+91]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+91], v45, v[vgprValuC+91]    // C += bias
v_cvt_f16_f32 v91, v[vgprValuC+91]                 // convert C to fp16
buffer_store_short v91, v88, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+96], v53, v[vgprValuC+96]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+96], v52, v[vgprValuC+96]    // C += bias
v_cvt_f16_f32 v96, v[vgprValuC+96]                 // convert C to fp16
buffer_store_short v96, v93, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+101], v60, v[vgprValuC+101]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+101], v59, v[vgprValuC+101]  // C += bias
v_cvt_f16_f32 v101, v[vgprValuC+101]               // convert C to fp16
buffer_store_short v101, v98, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+106], v11, v[vgprValuC+106]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+106], v10, v[vgprValuC+106]  // C += bias
v_cvt_f16_f32 v106, v[vgprValuC+106]               // convert C to fp16
buffer_store_short v106, v103, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+111], v18, v[vgprValuC+111]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+111], v17, v[vgprValuC+111]  // C += bias
v_cvt_f16_f32 v111, v[vgprValuC+111]               // convert C to fp16
buffer_store_short v111, v108, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+116], v25, v[vgprValuC+116]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+116], v24, v[vgprValuC+116]  // C += bias
v_cvt_f16_f32 v116, v[vgprValuC+116]               // convert C to fp16
buffer_store_short v116, v113, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+121], v32, v[vgprValuC+121]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+121], v31, v[vgprValuC+121]  // C += bias
v_cvt_f16_f32 v121, v[vgprValuC+121]               // convert C to fp16
buffer_store_short v121, v118, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+126], v39, v[vgprValuC+126]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+126], v38, v[vgprValuC+126]  // C += bias
v_cvt_f16_f32 v126, v[vgprValuC+126]               // convert C to fp16
buffer_store_short v126, v123, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+131], v46, v[vgprValuC+131]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+131], v45, v[vgprValuC+131]  // C += bias
v_cvt_f16_f32 v131, v[vgprValuC+131]               // convert C to fp16
buffer_store_short v131, v128, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+136], v53, v[vgprValuC+136]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+136], v52, v[vgprValuC+136]  // C += bias
v_cvt_f16_f32 v136, v[vgprValuC+136]               // convert C to fp16
buffer_store_short v136, v133, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+142], v60, v[vgprValuC+142]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+142], v59, v[vgprValuC+142]  // C += bias
v_cvt_f16_f32 v142, v[vgprValuC+142]               // convert C to fp16
buffer_store_short v142, v138, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+147], v11, v[vgprValuC+147]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+147], v10, v[vgprValuC+147]  // C += bias
v_cvt_f16_f32 v147, v[vgprValuC+147]               // convert C to fp16
buffer_store_short v147, v144, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+152], v18, v[vgprValuC+152]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+152], v17, v[vgprValuC+152]  // C += bias
v_cvt_f16_f32 v152, v[vgprValuC+152]               // convert C to fp16
buffer_store_short v152, v149, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+157], v25, v[vgprValuC+157]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+157], v24, v[vgprValuC+157]  // C += bias
v_cvt_f16_f32 v157, v[vgprValuC+157]               // convert C to fp16
buffer_store_short v157, v154, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+162], v32, v[vgprValuC+162]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+162], v31, v[vgprValuC+162]  // C += bias
v_cvt_f16_f32 v162, v[vgprValuC+162]               // convert C to fp16
buffer_store_short v162, v159, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+167], v39, v[vgprValuC+167]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+167], v38, v[vgprValuC+167]  // C += bias
v_cvt_f16_f32 v167, v[vgprValuC+167]               // convert C to fp16
buffer_store_short v167, v164, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+172], v46, v[vgprValuC+172]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+172], v45, v[vgprValuC+172]  // C += bias
v_cvt_f16_f32 v172, v[vgprValuC+172]               // convert C to fp16
buffer_store_short v172, v169, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
label_Sync_EDN_Edge_1:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//GW end
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #2 (d1,d0,vc1,vc0) = */
/*    (0,1,7,0:vw1); (0,1,7,1:vw1); (0,1,7,2:vw1); (0,1,7,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v34, BufferOOB
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v4, s68
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
ds_read_b32 v10, v8 offset:0                       // load Bias
v_add_u32 v9, 1024, v8                             // add ScaleAlphaVec offset (3)
ds_read_b32 v11, v9 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v4, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v34, v6, s[72:73]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v34, v7, s[72:73]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v4, s68
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
ds_read_b32 v17, v15 offset:0                      // load Bias
v_add_u32 v16, 1024, v15                           // add ScaleAlphaVec offset (3)
ds_read_b32 v18, v16 offset:0                      // load scaleAlpha
v_add_lshl_u32 v13, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v34, v13, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v14, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v34, v14, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v22, v4, s68
v_lshlrev_b32 v22, 0x2, v22                        // Bias address scaled by BPE
ds_read_b32 v24, v22 offset:0                      // load Bias
v_add_u32 v23, 1024, v22                           // add ScaleAlphaVec offset (3)
ds_read_b32 v25, v23 offset:0                      // load scaleAlpha
v_add_lshl_u32 v20, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v34, v20, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v21, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v21, v34, v21, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v29, v4, s68
v_lshlrev_b32 v29, 0x2, v29                        // Bias address scaled by BPE
ds_read_b32 v31, v29 offset:0                      // load Bias
v_add_u32 v30, 1024, v29                           // add ScaleAlphaVec offset (3)
ds_read_b32 v32, v30 offset:0                      // load scaleAlpha
v_add_lshl_u32 v27, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v34, v27, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v28, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v34, v28, s[72:73]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+12], acc60          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+19], acc61          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+26], acc62          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+33], acc63          // copy acc to vreg[63]

/* store after Acc, GSU: 2 */

/* store after Acc, GSU: 2 */

buffer_store_dword v12, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 12

buffer_store_dword v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 19

buffer_store_dword v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 26

buffer_store_dword v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 33
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//sourece store done, GSU:2

// check done start
// synchronizer offset cal
s_mul_i32 s13, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s11, s13, s[sgprWorkGroup2]
s_mul_i32 s12, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s12, s12, s[sgprWorkGroup0]
s_add_u32 s12, s12, s11
v_readfirstlane_b32 s11, v[vgprSerial]
s_mul_i32 s13, s13, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s11, s11, 0x6
s_mul_i32 s11, s13, s11                            // wave offset at batch
s_add_u32 s12, s11, s12
s_mul_i32 s13, s13, 4                              // cal a batch offset
s_mul_i32 s13, s13, 2                              // this batch offset
s_add_u32 s12, s12, s13
s_lshl_b32 s12, s12, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s12
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_and_b32 s11, s[sgprGSU], 0x3fff                  // Restore GSU
s_sub_u32 s11, s11, 0x1
s_atomic_dec s11, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s75, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s74, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s78, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s78, s78, 1                              // Free1
s_mul_hi_u32 s77, s78, s[sgprStrideC1J]            // Free1
s_mul_i32 s76, s78, s[sgprStrideC1J]               // Free1
s_add_u32 s74, s74, s76                            // Free1
s_addc_u32 s75, s75, s77                           // Free1
s_sub_u32 s78, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s78, s78, 1                              // Free2
s_mul_hi_u32 s77, s78, s[sgprStrideCK]             // Free2
s_mul_i32 s76, s78, s[sgprStrideCK]                // Free2
s_add_u32 s74, s74, s76                            // Free2
s_addc_u32 s75, s75, s77                           // Free2
s_lshl_b64 s[26:27], s[74:75], 2                   // scale by bpe

v_mov_b32 v34, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s11, 0x1
s_cbranch_scc1 label_NoBranch_FK2RT5SYT3O7M56C_0   // Only branch on scc0
// long branch sync
s_getpc_b64 s[44:45]                               // addr of next instr
s_add_i32 s46, label_Sync_EDN_Edge_2, 0x4          // target branch offset
s_add_u32 s44, s44, s46                            // add target branch offset
s_addc_u32 s45, s45, 0                             // add high and carry
s_setpc_b64 s[44:45]                               // branch to label_Sync_EDN_Edge_2
label_NoBranch_FK2RT5SYT3O7M56C_0:
// check done end

// buffer load start
buffer_load_dword v12, v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 12
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_156 // SyncAddbranchhere
buffer_load_dword v36, v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_156:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v12, v12, v36                            // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_156 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v35, v6, v34, s[32:33]               // protect if OOB
buffer_load_dword v36, v35, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_156     // Syncbranchhere

label_Synchronizer_read_add_end_1_156:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_156:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v34, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v19, v13, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 19
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_157 // SyncAddbranchhere
buffer_load_dword v36, v13, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_157:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v19, v19, v36                            // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_157 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v35, v13, v34, s[32:33]              // protect if OOB
buffer_load_dword v36, v35, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_157     // Syncbranchhere

label_Synchronizer_read_add_end_1_157:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_157:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v34, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v26, v20, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 26
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_158 // SyncAddbranchhere
buffer_load_dword v36, v20, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_158:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v26, v26, v36                            // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_158 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v35, v20, v34, s[32:33]              // protect if OOB
buffer_load_dword v36, v35, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_158     // Syncbranchhere

label_Synchronizer_read_add_end_1_158:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_158:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v34, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v33, v27, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 33
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_159 // SyncAddbranchhere
buffer_load_dword v36, v27, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_159:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v33, v33, v36                            // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_159 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v35, v27, v34, s[32:33]              // protect if OOB
buffer_load_dword v36, v35, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_159     // Syncbranchhere

label_Synchronizer_read_add_end_1_159:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_159:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 1, 7, 0), (0, 1, 7, 1), (0, 1, 7, 2), (0, 1, 7, 3)] */
v_mul_f32 v[vgprValuC+12], s[sgprAlpha], v[vgprValuC+12] // *= alpha
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+12], v11, v[vgprValuC+12]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+12], v10, v[vgprValuC+12]    // C += bias
v_cvt_f16_f32 v12, v[vgprValuC+12]                 // convert C to fp16
buffer_store_short v12, v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+19], v17, v[vgprValuC+19]    // C += bias
v_cvt_f16_f32 v19, v[vgprValuC+19]                 // convert C to fp16
buffer_store_short v19, v14, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+26], v24, v[vgprValuC+26]    // C += bias
v_cvt_f16_f32 v26, v[vgprValuC+26]                 // convert C to fp16
buffer_store_short v26, v21, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+33], v31, v[vgprValuC+33]    // C += bias
v_cvt_f16_f32 v33, v[vgprValuC+33]                 // convert C to fp16
buffer_store_short v33, v28, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
label_Sync_EDN_Edge_2:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//GW end
s_branch label_GW_End_1                            // jump to end
label_GW_Beta_1:
s_and_b32 s44, 127, s[sgprSizeI]                   // s44 = s[sgprSizeI] % 128
s_add_u32 s45, -0x1, s[sgprNumWorkGroups0]
s_cmp_ge_u32 s[sgprWorkGroup0], s45                // wg0 >= nwg0-1 ?
s_cselect_b32 s44, s44, 0                          // set rMT0
s_cmpk_gt_u32 s44, 0x0                             // rMT0 > 0
s_cbranch_scc1 label_GW_B1_E1_M                    // jump if edges required
s_and_b32 s44, 127, s[sgprSizeJ]                   // s44 = s[sgprSizeJ] % 128
s_add_u32 s45, -0x1, s[sgprNumWorkGroups1]
s_cmp_ge_u32 s[sgprWorkGroup1], s45                // wg1 >= nwg1-1
s_cselect_b32 s44, s44, 0                          // set rMT1
s_cmpk_gt_u32 s44, 0x0                             // rMT1 > 0
s_cbranch_scc1 label_GW_B1_E1_N                    // jump if edges required
label_GW_B1_E0:

/* edge=0, allocate 2 sgpr. perBatchTmpS=2 perBatchMaskS=0 perElementMaskS=0 elementsPerBatch=10 */
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,1,0,0:vw4); (0,0,1,0:vw4); (0,1,1,0:vw4); (0,0,2,0:vw4); (0,1,2,0:vw4); (0,0,3,0:vw4); (0,1,3,0:vw4); (0,0,4,0:vw4); (0,1,4,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v8, v2, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[12:13], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s12, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v9, v0, s12
v_lshlrev_b32 v9, 0x2, v9                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for LDS write
s_barrier                                          // LDS write barrier
ds_read_b128 v[16:19], v9 offset:0                 // load Bias
v_add_u32 v10, 1024, v9                            // add ScaleAlphaVec offset (1)
ds_read_b128 v[20:23], v10 offset:0                // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
buffer_load_dwordx2 v[28:29], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
ds_read_b128 v[32:35], v9 offset:256               // load Bias
ds_read_b128 v[36:39], v10 offset:256              // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[44:45], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
buffer_load_dwordx2 v[52:53], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[60:61], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
buffer_load_dwordx2 v[68:69], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[76:77], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
buffer_load_dwordx2 v[84:85], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[92:93], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
buffer_load_dwordx2 v[100:101], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_add_lshl_u32 v7, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc1           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+40], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+41], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+42], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+43], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+48], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+49], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+50], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+51], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+56], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+57], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+58], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+59], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+64], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+65], acc17          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+66], acc18          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+67], acc19          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+72], acc20          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+73], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+74], acc22          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+75], acc23          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+80], acc24          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+81], acc25          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+82], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+83], acc27          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+88], acc28          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+89], acc29          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+90], acc30          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+91], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+96], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+97], acc33          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+98], acc34          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+99], acc35          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+104], acc36         // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+105], acc37         // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+106], acc38         // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+107], acc39         // copy acc to vreg[39]

/* store after Acc, GSU: 2 */

/* store after Acc, GSU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 24

buffer_store_dwordx4 v[40:43], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256, sc0 sc1 // store D 40

s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[48:51], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 48

buffer_store_dwordx4 v[56:59], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256, sc0 sc1 // store D 56

s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[64:67], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 64

buffer_store_dwordx4 v[72:75], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256, sc0 sc1 // store D 72

s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[80:83], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 80

buffer_store_dwordx4 v[88:91], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256, sc0 sc1 // store D 88

s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[96:99], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 96

buffer_store_dwordx4 v[104:107], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256, sc0 sc1 // store D 104
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//sourece store done, GSU:2

// check done start
// synchronizer offset cal
s_mul_i32 s27, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s11, s27, s[sgprWorkGroup2]
s_mul_i32 s26, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s26, s26, s[sgprWorkGroup0]
s_add_u32 s26, s26, s11
v_readfirstlane_b32 s11, v[vgprSerial]
s_mul_i32 s27, s27, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s11, s11, 0x6
s_mul_i32 s11, s27, s11                            // wave offset at batch
s_add_u32 s26, s11, s26
s_lshl_b32 s26, s26, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s26
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_and_b32 s11, s[sgprGSU], 0x3fff                  // Restore GSU
s_sub_u32 s11, s11, 0x1
s_atomic_dec s11, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s69, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s68, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s72, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s72, s72, 1                              // Free1
s_mul_hi_u32 s71, s72, s[sgprStrideC1J]            // Free1
s_mul_i32 s70, s72, s[sgprStrideC1J]               // Free1
s_add_u32 s68, s68, s70                            // Free1
s_addc_u32 s69, s69, s71                           // Free1
s_sub_u32 s72, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s72, s72, 1                              // Free2
s_mul_hi_u32 s71, s72, s[sgprStrideCK]             // Free2
s_mul_i32 s70, s72, s[sgprStrideCK]                // Free2
s_add_u32 s68, s68, s70                            // Free2
s_addc_u32 s69, s69, s71                           // Free2
s_lshl_b64 s[32:33], s[68:69], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s11, 0x1
s_cbranch_scc1 label_NoBranch_2SBIW6DO1XBUK4Y9_0   // Only branch on scc0
// long branch sync
s_getpc_b64 s[44:45]                               // addr of next instr
s_add_i32 s46, label_Sync_EDN_Beta_0, 0x4          // target branch offset
s_add_u32 s44, s44, s46                            // add target branch offset
s_addc_u32 s45, s45, 0                             // add high and carry
s_setpc_b64 s[44:45]                               // branch to label_Sync_EDN_Beta_0
label_NoBranch_2SBIW6DO1XBUK4Y9_0:
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 24
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_80 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_80:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_80 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v108, v6, v11, s[38:39]              // protect if OOB
buffer_load_dwordx4 v[112:115], v108, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_80      // Syncbranchhere

label_Synchronizer_read_add_end_1_80:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_80:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[40:43], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU D 0 40
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_81 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_81:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_81 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v108, v6, v11, s[38:39]              // protect if OOB
buffer_load_dwordx4 v[112:115], v108, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_81      // Syncbranchhere

label_Synchronizer_read_add_end_1_81:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_81:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
s_lshl_b32 s38, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s44, s44, s38                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s45, s45, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s38
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[48:51], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 48
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_82 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_82:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_82 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v108, v6, v11, s[38:39]              // protect if OOB
buffer_load_dwordx4 v[112:115], v108, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_82      // Syncbranchhere

label_Synchronizer_read_add_end_1_82:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_82:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[56:59], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU D 0 56
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_83 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_83:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[56:57], v[56:57], v[112:113]        // buffer pk
v_pk_add_f32 v[58:59], v[58:59], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_83 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v108, v6, v11, s[38:39]              // protect if OOB
buffer_load_dwordx4 v[112:115], v108, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_83      // Syncbranchhere

label_Synchronizer_read_add_end_1_83:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_83:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
s_lshl_b32 s38, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s44, s44, s38                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s45, s45, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s38
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[64:67], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 64
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_84 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_84:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[64:65], v[64:65], v[112:113]        // buffer pk
v_pk_add_f32 v[66:67], v[66:67], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_84 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v108, v6, v11, s[38:39]              // protect if OOB
buffer_load_dwordx4 v[112:115], v108, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_84      // Syncbranchhere

label_Synchronizer_read_add_end_1_84:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_84:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[72:75], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU D 0 72
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_85 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_85:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[72:73], v[72:73], v[112:113]        // buffer pk
v_pk_add_f32 v[74:75], v[74:75], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_85 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v108, v6, v11, s[38:39]              // protect if OOB
buffer_load_dwordx4 v[112:115], v108, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_85      // Syncbranchhere

label_Synchronizer_read_add_end_1_85:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_85:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
s_lshl_b32 s38, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s44, s44, s38                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s45, s45, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s38
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[80:83], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 80
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_86 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_86:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[80:81], v[80:81], v[112:113]        // buffer pk
v_pk_add_f32 v[82:83], v[82:83], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_86 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v108, v6, v11, s[38:39]              // protect if OOB
buffer_load_dwordx4 v[112:115], v108, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_86      // Syncbranchhere

label_Synchronizer_read_add_end_1_86:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_86:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[88:91], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU D 0 88
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_87 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_87:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[88:89], v[88:89], v[112:113]        // buffer pk
v_pk_add_f32 v[90:91], v[90:91], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_87 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v108, v6, v11, s[38:39]              // protect if OOB
buffer_load_dwordx4 v[112:115], v108, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_87      // Syncbranchhere

label_Synchronizer_read_add_end_1_87:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_87:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
s_lshl_b32 s38, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s44, s44, s38                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s45, s45, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s38
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[96:99], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 96
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_88 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_88:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[96:97], v[96:97], v[112:113]        // buffer pk
v_pk_add_f32 v[98:99], v[98:99], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_88 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v108, v6, v11, s[38:39]              // protect if OOB
buffer_load_dwordx4 v[112:115], v108, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_88      // Syncbranchhere

label_Synchronizer_read_add_end_1_88:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_88:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[104:107], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU D 0 104
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_89 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_89:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[104:105], v[104:105], v[112:113]    // buffer pk
v_pk_add_f32 v[106:107], v[106:107], v[114:115]    // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_89 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v108, v6, v11, s[38:39]              // protect if OOB
buffer_load_dwordx4 v[112:115], v108, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_89      // Syncbranchhere

label_Synchronizer_read_add_end_1_89:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_89:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0), (0, 1, 1, 0), (0, 0, 2, 0), (0, 1, 2, 0), (0, 0, 3, 0), (0, 1, 3, 0), (0, 0, 4, 0), (0, 1, 4, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+97], s[sgprAlpha], v[vgprValuC+97] // *= alpha
v_mul_f32 v[vgprValuC+98], s[sgprAlpha], v[vgprValuC+98] // *= alpha
v_mul_f32 v[vgprValuC+99], s[sgprAlpha], v[vgprValuC+99] // *= alpha
v_mul_f32 v[vgprValuC+104], s[sgprAlpha], v[vgprValuC+104] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+107], s[sgprAlpha], v[vgprValuC+107] // *= alpha

/* apply mask, calc new C and issue writes */

v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= ScaleAlphaVecVMulPK(20)(0)
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= ScaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth

v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[36:37], v[vgprValuC+40:vgprValuC+40+1] // *= ScaleAlphaVecVMulPK(36)(0)
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[38:39], v[vgprValuC+42:vgprValuC+42+1] // *= ScaleAlphaVecVMulPK(36)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v28, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v28, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v29, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v29, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[32:33], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[34:35], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
buffer_store_dwordx2 v[40:41], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:128, sc0 sc1 // store TD not StoreRemapVectorWidth

v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= ScaleAlphaVecVMulPK(20)(0)
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= ScaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s12      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth

v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[36:37], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(36)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[38:39], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(36)(2)
v_fma_mix_f32 v[vgprValuC+56], s[sgprBeta], v52, v[vgprValuC+56] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+57], s[sgprBeta], v52, v[vgprValuC+57] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+58], s[sgprBeta], v53, v[vgprValuC+58] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+59], s[sgprBeta], v53, v[vgprValuC+59] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[32:33], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[34:35], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:128, sc0 sc1 // store TD not StoreRemapVectorWidth

v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[20:21], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(20)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[22:23], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+64], s[sgprBeta], v60, v[vgprValuC+64] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+65], s[sgprBeta], v60, v[vgprValuC+65] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+66], s[sgprBeta], v61, v[vgprValuC+66] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+67], s[sgprBeta], v61, v[vgprValuC+67] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[16:17], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[18:19], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s12      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[64:65], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth

v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[36:37], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(36)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[38:39], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(36)(2)
v_fma_mix_f32 v[vgprValuC+72], s[sgprBeta], v68, v[vgprValuC+72] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+73], s[sgprBeta], v68, v[vgprValuC+73] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+74], s[sgprBeta], v69, v[vgprValuC+74] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v69, v[vgprValuC+75] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[32:33], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[34:35], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:128, sc0 sc1 // store TD not StoreRemapVectorWidth

v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[20:21], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(20)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[22:23], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+80], s[sgprBeta], v76, v[vgprValuC+80] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+81], s[sgprBeta], v76, v[vgprValuC+81] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+82], s[sgprBeta], v77, v[vgprValuC+82] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+83], s[sgprBeta], v77, v[vgprValuC+83] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[16:17], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[18:19], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s12      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[80:81], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth

v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[36:37], v[vgprValuC+88:vgprValuC+88+1] // *= ScaleAlphaVecVMulPK(36)(0)
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[38:39], v[vgprValuC+90:vgprValuC+90+1] // *= ScaleAlphaVecVMulPK(36)(2)
v_fma_mix_f32 v[vgprValuC+88], s[sgprBeta], v84, v[vgprValuC+88] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+89], s[sgprBeta], v84, v[vgprValuC+89] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+90], s[sgprBeta], v85, v[vgprValuC+90] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+91], s[sgprBeta], v85, v[vgprValuC+91] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+88:vgprValuC+88+1], v[32:33], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[vgprValuC+90:vgprValuC+90+1], v[34:35], v[vgprValuC+90:vgprValuC+90+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+88], v[vgprValuC+88]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+89], v[vgprValuC+89]     // convert C to fp16
v_pack_b32_f16 v88, v[vgprValuC+88], v[vgprValuC+89] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+90], v[vgprValuC+90]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+91], v[vgprValuC+91]     // convert C to fp16
v_pack_b32_f16 v89, v[vgprValuC+90], v[vgprValuC+91] // Pack with neighbor
buffer_store_dwordx2 v[88:89], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:128, sc0 sc1 // store TD not StoreRemapVectorWidth

v_pk_mul_f32 v[vgprValuC+96:vgprValuC+96+1], v[20:21], v[vgprValuC+96:vgprValuC+96+1] // *= ScaleAlphaVecVMulPK(20)(0)
v_pk_mul_f32 v[vgprValuC+98:vgprValuC+98+1], v[22:23], v[vgprValuC+98:vgprValuC+98+1] // *= ScaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+96], s[sgprBeta], v92, v[vgprValuC+96] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+97], s[sgprBeta], v92, v[vgprValuC+97] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+98], s[sgprBeta], v93, v[vgprValuC+98] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+99], s[sgprBeta], v93, v[vgprValuC+99] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+96:vgprValuC+96+1], v[16:17], v[vgprValuC+96:vgprValuC+96+1] // C += bias
v_pk_add_f32 v[vgprValuC+98:vgprValuC+98+1], v[18:19], v[vgprValuC+98:vgprValuC+98+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+96], v[vgprValuC+96]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+97], v[vgprValuC+97]     // convert C to fp16
v_pack_b32_f16 v96, v[vgprValuC+96], v[vgprValuC+97] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+98], v[vgprValuC+98]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+99], v[vgprValuC+99]     // convert C to fp16
v_pack_b32_f16 v97, v[vgprValuC+98], v[vgprValuC+99] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s12      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[96:97], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth

v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[36:37], v[vgprValuC+104:vgprValuC+104+1] // *= ScaleAlphaVecVMulPK(36)(0)
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[38:39], v[vgprValuC+106:vgprValuC+106+1] // *= ScaleAlphaVecVMulPK(36)(2)
v_fma_mix_f32 v[vgprValuC+104], s[sgprBeta], v100, v[vgprValuC+104] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+105], s[sgprBeta], v100, v[vgprValuC+105] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+106], s[sgprBeta], v101, v[vgprValuC+106] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+107], s[sgprBeta], v101, v[vgprValuC+107] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+104:vgprValuC+104+1], v[32:33], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[vgprValuC+106:vgprValuC+106+1], v[34:35], v[vgprValuC+106:vgprValuC+106+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+104], v[vgprValuC+104]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+105], v[vgprValuC+105]   // convert C to fp16
v_pack_b32_f16 v104, v[vgprValuC+104], v[vgprValuC+105] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+106], v[vgprValuC+106]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+107], v[vgprValuC+107]   // convert C to fp16
v_pack_b32_f16 v105, v[vgprValuC+106], v[vgprValuC+107] // Pack with neighbor
buffer_store_dwordx2 v[104:105], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:128, sc0 sc1 // store TD not StoreRemapVectorWidth
label_Sync_EDN_Beta_0:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//GW end
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Beta Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,0,5,0:vw4); (0,1,5,0:vw4); (0,0,6,0:vw4); (0,1,6,0:vw4); (0,0,7,0:vw4); (0,1,7,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[12:13], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
ds_read_b128 v[16:19], v9 offset:0                 // load Bias
ds_read_b128 v[20:23], v10 offset:0                // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
buffer_load_dwordx2 v[28:29], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
ds_read_b128 v[32:35], v9 offset:256               // load Bias
ds_read_b128 v[36:39], v10 offset:256              // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[44:45], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
buffer_load_dwordx2 v[52:53], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[60:61], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
buffer_load_dwordx2 v[68:69], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
v_accvgpr_read_b32 v[vgprValuC+24], acc40          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+25], acc41          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+26], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+27], acc43          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+40], acc44          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+41], acc45          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+42], acc46          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+43], acc47          // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+48], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+49], acc49          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+50], acc50          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+51], acc51          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+56], acc52          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+57], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+58], acc54          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+59], acc55          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+64], acc56          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+65], acc57          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+66], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+67], acc59          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+72], acc60          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+73], acc61          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+74], acc62          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+75], acc63          // copy acc to vreg[63]

/* store after Acc, GSU: 2 */

/* store after Acc, GSU: 2 */

s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 24

buffer_store_dwordx4 v[40:43], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256, sc0 sc1 // store D 40

s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[48:51], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 48

buffer_store_dwordx4 v[56:59], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256, sc0 sc1 // store D 56

s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[64:67], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 64

buffer_store_dwordx4 v[72:75], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256, sc0 sc1 // store D 72
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//sourece store done, GSU:2

// check done start
// synchronizer offset cal
s_mul_i32 s27, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s11, s27, s[sgprWorkGroup2]
s_mul_i32 s26, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s26, s26, s[sgprWorkGroup0]
s_add_u32 s26, s26, s11
v_readfirstlane_b32 s11, v[vgprSerial]
s_mul_i32 s27, s27, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s11, s11, 0x6
s_mul_i32 s11, s27, s11                            // wave offset at batch
s_add_u32 s26, s11, s26
s_mul_i32 s27, s27, 4                              // cal a batch offset
s_mul_i32 s27, s27, 1                              // this batch offset
s_add_u32 s26, s26, s27
s_lshl_b32 s26, s26, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s26
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_and_b32 s11, s[sgprGSU], 0x3fff                  // Restore GSU
s_sub_u32 s11, s11, 0x1
s_atomic_dec s11, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s69, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s68, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s72, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s72, s72, 1                              // Free1
s_mul_hi_u32 s71, s72, s[sgprStrideC1J]            // Free1
s_mul_i32 s70, s72, s[sgprStrideC1J]               // Free1
s_add_u32 s68, s68, s70                            // Free1
s_addc_u32 s69, s69, s71                           // Free1
s_sub_u32 s72, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s72, s72, 1                              // Free2
s_mul_hi_u32 s71, s72, s[sgprStrideCK]             // Free2
s_mul_i32 s70, s72, s[sgprStrideCK]                // Free2
s_add_u32 s68, s68, s70                            // Free2
s_addc_u32 s69, s69, s71                           // Free2
s_lshl_b64 s[32:33], s[68:69], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s11, 0x1
s_cbranch_scc1 label_NoBranch_OJBEWL75LHEAPJLU_0   // Only branch on scc0
// long branch sync
s_getpc_b64 s[44:45]                               // addr of next instr
s_add_i32 s46, label_Sync_EDN_Beta_1, 0x4          // target branch offset
s_add_u32 s44, s44, s46                            // add target branch offset
s_addc_u32 s45, s45, 0                             // add high and carry
s_setpc_b64 s[44:45]                               // branch to label_Sync_EDN_Beta_1
label_NoBranch_OJBEWL75LHEAPJLU_0:
// check done end

// buffer load start
s_lshl_b32 s38, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s44, s44, s38                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s45, s45, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s38
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[24:27], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 24
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_90 // SyncAddbranchhere
buffer_load_dwordx4 v[80:83], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_90:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[80:81]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[82:83]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_90 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v76, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[80:83], v76, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_90      // Syncbranchhere

label_Synchronizer_read_add_end_1_90:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_90:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[40:43], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU D 0 40
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_91 // SyncAddbranchhere
buffer_load_dwordx4 v[80:83], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_91:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[80:81]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[82:83]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_91 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v76, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[80:83], v76, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_91      // Syncbranchhere

label_Synchronizer_read_add_end_1_91:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_91:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
s_lshl_b32 s38, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s44, s44, s38                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s45, s45, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s38
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[48:51], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 48
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_92 // SyncAddbranchhere
buffer_load_dwordx4 v[80:83], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_92:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[80:81]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[82:83]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_92 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v76, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[80:83], v76, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_92      // Syncbranchhere

label_Synchronizer_read_add_end_1_92:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_92:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[56:59], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU D 0 56
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_93 // SyncAddbranchhere
buffer_load_dwordx4 v[80:83], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_93:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[56:57], v[56:57], v[80:81]          // buffer pk
v_pk_add_f32 v[58:59], v[58:59], v[82:83]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_93 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v76, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[80:83], v76, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_93      // Syncbranchhere

label_Synchronizer_read_add_end_1_93:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_93:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
s_lshl_b32 s38, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s44, s44, s38                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s45, s45, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s38
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[64:67], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 64
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_94 // SyncAddbranchhere
buffer_load_dwordx4 v[80:83], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_94:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[64:65], v[64:65], v[80:81]          // buffer pk
v_pk_add_f32 v[66:67], v[66:67], v[82:83]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_94 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v76, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[80:83], v76, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_94      // Syncbranchhere

label_Synchronizer_read_add_end_1_94:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_94:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[72:75], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU D 0 72
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_95 // SyncAddbranchhere
buffer_load_dwordx4 v[80:83], v6, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_95:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[72:73], v[72:73], v[80:81]          // buffer pk
v_pk_add_f32 v[74:75], v[74:75], v[82:83]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_95 // SyncAddbranch
s_add_u32 s44, s44, s32
s_addc_u32 s45, s45, s33
v_cmp_ge_i32 s[38:39], 0, s[sgprGSUSync]
v_cndmask_b32 v76, v6, v11, s[38:39]               // protect if OOB
buffer_load_dwordx4 v[80:83], v76, s[44:47], 0 offen offset:256, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_95      // Syncbranchhere

label_Synchronizer_read_add_end_1_95:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_95:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 5, 0), (0, 1, 5, 0), (0, 0, 6, 0), (0, 1, 6, 0), (0, 0, 7, 0), (0, 1, 7, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha

/* apply mask, calc new C and issue writes */

v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= ScaleAlphaVecVMulPK(20)(0)
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= ScaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s12      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth

v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[36:37], v[vgprValuC+40:vgprValuC+40+1] // *= ScaleAlphaVecVMulPK(36)(0)
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[38:39], v[vgprValuC+42:vgprValuC+42+1] // *= ScaleAlphaVecVMulPK(36)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v28, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v28, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v29, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v29, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[32:33], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[34:35], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
buffer_store_dwordx2 v[40:41], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:128, sc0 sc1 // store TD not StoreRemapVectorWidth

v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= ScaleAlphaVecVMulPK(20)(0)
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= ScaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s12      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth

v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[36:37], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(36)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[38:39], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(36)(2)
v_fma_mix_f32 v[vgprValuC+56], s[sgprBeta], v52, v[vgprValuC+56] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+57], s[sgprBeta], v52, v[vgprValuC+57] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+58], s[sgprBeta], v53, v[vgprValuC+58] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+59], s[sgprBeta], v53, v[vgprValuC+59] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[32:33], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[34:35], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:128, sc0 sc1 // store TD not StoreRemapVectorWidth

v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[20:21], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(20)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[22:23], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+64], s[sgprBeta], v60, v[vgprValuC+64] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+65], s[sgprBeta], v60, v[vgprValuC+65] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+66], s[sgprBeta], v61, v[vgprValuC+66] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+67], s[sgprBeta], v61, v[vgprValuC+67] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[16:17], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[18:19], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s12      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[64:65], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth

v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[36:37], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(36)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[38:39], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(36)(2)
v_fma_mix_f32 v[vgprValuC+72], s[sgprBeta], v68, v[vgprValuC+72] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+73], s[sgprBeta], v68, v[vgprValuC+73] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+74], s[sgprBeta], v69, v[vgprValuC+74] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v69, v[vgprValuC+75] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[32:33], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[34:35], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:128, sc0 sc1 // store TD not StoreRemapVectorWidth
label_Sync_EDN_Beta_1:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//GW end
s_branch label_GW_End_1                            // jump to end
label_GW_B1_E1_N:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=8 */
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,1,0,0:vw4); (0,0,1,0:vw4); (0,1,1,0:vw4); (0,0,2,0:vw4); (0,1,2,0:vw4); (0,0,3,0:vw4); (0,1,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v110, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v110, v6, s[72:73]               // LDC clip if OOB. offset
buffer_load_dwordx2 v[12:13], v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s68
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for LDS write
s_barrier                                          // LDS write barrier
ds_read_b128 v[16:19], v8 offset:0                 // load Bias
v_add_u32 v9, 1024, v8                             // add ScaleAlphaVec offset (3)
ds_read_b128 v[20:23], v9 offset:0                 // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v110, v6, s[72:73]               // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v110, v7, s[72:73]               // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v10, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v110, v10, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[32:33], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s68
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
ds_read_b128 v[36:39], v28 offset:0                // load Bias
v_add_u32 v29, 1024, v28                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[40:43], v29 offset:0                // load scaleAlpha
v_add_lshl_u32 v10, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v110, v10, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v110, v11, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v30, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v110, v30, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[52:53], v30, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v48, v0, s68
v_lshlrev_b32 v48, 0x2, v48                        // Bias address scaled by BPE
v_add_u32 v49, 1024, v48                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v30, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v110, v30, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v31, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v110, v31, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v50, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v50, v110, v50, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[64:65], v50, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v60, v4, s68
v_lshlrev_b32 v60, 0x2, v60                        // Bias address scaled by BPE
v_add_u32 v61, 1024, v60                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v50, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v50, v110, v50, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v51, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v110, v51, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v62, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v110, v62, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[76:77], v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v0, s68
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_add_u32 v73, 1024, v72                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v62, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v110, v62, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v63, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v63, v110, v63, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v74, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v74, v110, v74, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[88:89], v74, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v84, v4, s68
v_lshlrev_b32 v84, 0x2, v84                        // Bias address scaled by BPE
v_add_u32 v85, 1024, v84                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v74, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v74, v110, v74, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v75, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v75, v110, v75, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v86, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v110, v86, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[100:101], v86, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v96, v0, s68
v_lshlrev_b32 v96, 0x2, v96                        // Bias address scaled by BPE
v_add_u32 v97, 1024, v96                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v86, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v110, v86, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v87, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v87, v110, v87, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v98, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v98, v110, v98, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[112:113], v98, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v108, v4, s68
v_lshlrev_b32 v108, 0x2, v108                      // Bias address scaled by BPE
v_add_u32 v109, 1024, v108                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v98, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v98, v110, v98, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v99, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v99, v110, v99, s[72:73]             // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc1           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+44], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+45], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+46], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+47], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+56], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+57], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+58], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+59], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+68], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+69], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+70], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+71], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+80], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+81], acc17          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+82], acc18          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+83], acc19          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+92], acc20          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+93], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+94], acc22          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+95], acc23          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+104], acc24         // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+105], acc25         // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+106], acc26         // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+107], acc27         // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+116], acc28         // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+117], acc29         // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+118], acc30         // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+119], acc31         // copy acc to vreg[31]

/* store after Acc, GSU: 2 */

/* store after Acc, GSU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 24

buffer_store_dwordx4 v[44:47], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 44

buffer_store_dwordx4 v[56:59], v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 56

buffer_store_dwordx4 v[68:71], v50, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 68

buffer_store_dwordx4 v[80:83], v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 80

buffer_store_dwordx4 v[92:95], v74, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 92

buffer_store_dwordx4 v[104:107], v86, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 104

buffer_store_dwordx4 v[116:119], v98, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 116
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//sourece store done, GSU:2

// check done start
// synchronizer offset cal
s_mul_i32 s13, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s11, s13, s[sgprWorkGroup2]
s_mul_i32 s12, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s12, s12, s[sgprWorkGroup0]
s_add_u32 s12, s12, s11
v_readfirstlane_b32 s11, v[vgprSerial]
s_mul_i32 s13, s13, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s11, s11, 0x6
s_mul_i32 s11, s13, s11                            // wave offset at batch
s_add_u32 s12, s11, s12
s_lshl_b32 s12, s12, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s12
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_and_b32 s11, s[sgprGSU], 0x3fff                  // Restore GSU
s_sub_u32 s11, s11, 0x1
s_atomic_dec s11, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s75, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s74, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s78, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s78, s78, 1                              // Free1
s_mul_hi_u32 s77, s78, s[sgprStrideC1J]            // Free1
s_mul_i32 s76, s78, s[sgprStrideC1J]               // Free1
s_add_u32 s74, s74, s76                            // Free1
s_addc_u32 s75, s75, s77                           // Free1
s_sub_u32 s78, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s78, s78, 1                              // Free2
s_mul_hi_u32 s77, s78, s[sgprStrideCK]             // Free2
s_mul_i32 s76, s78, s[sgprStrideCK]                // Free2
s_add_u32 s74, s74, s76                            // Free2
s_addc_u32 s75, s75, s77                           // Free2
s_lshl_b64 s[26:27], s[74:75], 2                   // scale by bpe

v_mov_b32 v110, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s11, 0x1
s_cbranch_scc1 label_NoBranch_8JS5LQ78S6MG5TUD_0   // Only branch on scc0
// long branch sync
s_getpc_b64 s[44:45]                               // addr of next instr
s_add_i32 s46, label_Sync_EDN_Beta_Edge_3, 0x4     // target branch offset
s_add_u32 s44, s44, s46                            // add target branch offset
s_addc_u32 s45, s45, 0                             // add high and carry
s_setpc_b64 s[44:45]                               // branch to label_Sync_EDN_Beta_Edge_3
label_NoBranch_8JS5LQ78S6MG5TUD_0:
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 24
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_64 // SyncAddbranchhere
buffer_load_dwordx4 v[120:123], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_64:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[120:121]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[122:123]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_64 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v111, v6, v110, s[32:33]             // protect if OOB
buffer_load_dwordx4 v[120:123], v111, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_64      // Syncbranchhere

label_Synchronizer_read_add_end_1_64:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_64:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v110, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[44:47], v10, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 44
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_65 // SyncAddbranchhere
buffer_load_dwordx4 v[120:123], v10, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_65:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[120:121]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[122:123]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_65 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v111, v10, v110, s[32:33]            // protect if OOB
buffer_load_dwordx4 v[120:123], v111, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_65      // Syncbranchhere

label_Synchronizer_read_add_end_1_65:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_65:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v110, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[56:59], v30, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 56
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_66 // SyncAddbranchhere
buffer_load_dwordx4 v[120:123], v30, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_66:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[56:57], v[56:57], v[120:121]        // buffer pk
v_pk_add_f32 v[58:59], v[58:59], v[122:123]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_66 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v111, v30, v110, s[32:33]            // protect if OOB
buffer_load_dwordx4 v[120:123], v111, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_66      // Syncbranchhere

label_Synchronizer_read_add_end_1_66:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_66:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v110, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[68:71], v50, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 68
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_67 // SyncAddbranchhere
buffer_load_dwordx4 v[120:123], v50, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_67:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[68:69], v[68:69], v[120:121]        // buffer pk
v_pk_add_f32 v[70:71], v[70:71], v[122:123]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_67 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v111, v50, v110, s[32:33]            // protect if OOB
buffer_load_dwordx4 v[120:123], v111, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_67      // Syncbranchhere

label_Synchronizer_read_add_end_1_67:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_67:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v110, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[80:83], v62, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 80
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_68 // SyncAddbranchhere
buffer_load_dwordx4 v[120:123], v62, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_68:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[80:81], v[80:81], v[120:121]        // buffer pk
v_pk_add_f32 v[82:83], v[82:83], v[122:123]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_68 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v111, v62, v110, s[32:33]            // protect if OOB
buffer_load_dwordx4 v[120:123], v111, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_68      // Syncbranchhere

label_Synchronizer_read_add_end_1_68:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_68:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v110, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[92:95], v74, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 92
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_69 // SyncAddbranchhere
buffer_load_dwordx4 v[120:123], v74, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_69:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[92:93], v[92:93], v[120:121]        // buffer pk
v_pk_add_f32 v[94:95], v[94:95], v[122:123]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_69 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v111, v74, v110, s[32:33]            // protect if OOB
buffer_load_dwordx4 v[120:123], v111, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_69      // Syncbranchhere

label_Synchronizer_read_add_end_1_69:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_69:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v110, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[104:107], v86, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 104
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_70 // SyncAddbranchhere
buffer_load_dwordx4 v[120:123], v86, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_70:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[104:105], v[104:105], v[120:121]    // buffer pk
v_pk_add_f32 v[106:107], v[106:107], v[122:123]    // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_70 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v111, v86, v110, s[32:33]            // protect if OOB
buffer_load_dwordx4 v[120:123], v111, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_70      // Syncbranchhere

label_Synchronizer_read_add_end_1_70:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_70:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v110, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[116:119], v98, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 116
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_71 // SyncAddbranchhere
buffer_load_dwordx4 v[120:123], v98, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_71:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[116:117], v[116:117], v[120:121]    // buffer pk
v_pk_add_f32 v[118:119], v[118:119], v[122:123]    // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_71 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v111, v98, v110, s[32:33]            // protect if OOB
buffer_load_dwordx4 v[120:123], v111, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_71      // Syncbranchhere

label_Synchronizer_read_add_end_1_71:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_71:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0), (0, 1, 1, 0), (0, 0, 2, 0), (0, 1, 2, 0), (0, 0, 3, 0), (0, 1, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+104], s[sgprAlpha], v[vgprValuC+104] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+107], s[sgprAlpha], v[vgprValuC+107] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+118], s[sgprAlpha], v[vgprValuC+118] // *= alpha
v_mul_f32 v[vgprValuC+119], s[sgprAlpha], v[vgprValuC+119] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= ScaleAlphaVecVMulPK(20)(0)
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= ScaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[40:41], v[vgprValuC+44:vgprValuC+44+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[42:43], v[vgprValuC+46:vgprValuC+46+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_fma_mix_f32 v[vgprValuC+44], s[sgprBeta], v32, v[vgprValuC+44] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+45], s[sgprBeta], v32, v[vgprValuC+45] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+46], s[sgprBeta], v33, v[vgprValuC+46] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v33, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[36:37], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[38:39], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[20:21], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(20)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[22:23], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+56], s[sgprBeta], v52, v[vgprValuC+56] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+57], s[sgprBeta], v52, v[vgprValuC+57] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+58], s[sgprBeta], v53, v[vgprValuC+58] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+59], s[sgprBeta], v53, v[vgprValuC+59] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[16:17], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[18:19], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v31, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[40:41], v[vgprValuC+68:vgprValuC+68+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[42:43], v[vgprValuC+70:vgprValuC+70+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_fma_mix_f32 v[vgprValuC+68], s[sgprBeta], v64, v[vgprValuC+68] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+69], s[sgprBeta], v64, v[vgprValuC+69] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+70], s[sgprBeta], v65, v[vgprValuC+70] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+71], s[sgprBeta], v65, v[vgprValuC+71] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+68:vgprValuC+68+1], v[36:37], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[vgprValuC+70:vgprValuC+70+1], v[38:39], v[vgprValuC+70:vgprValuC+70+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+68], v[vgprValuC+68]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+69], v[vgprValuC+69]     // convert C to fp16
v_pack_b32_f16 v68, v[vgprValuC+68], v[vgprValuC+69] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+70], v[vgprValuC+70]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+71], v[vgprValuC+71]     // convert C to fp16
v_pack_b32_f16 v69, v[vgprValuC+70], v[vgprValuC+71] // Pack with neighbor
buffer_store_dwordx2 v[68:69], v51, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[20:21], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(20)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[22:23], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+80], s[sgprBeta], v76, v[vgprValuC+80] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+81], s[sgprBeta], v76, v[vgprValuC+81] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+82], s[sgprBeta], v77, v[vgprValuC+82] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+83], s[sgprBeta], v77, v[vgprValuC+83] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[16:17], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[18:19], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v63, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[40:41], v[vgprValuC+92:vgprValuC+92+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[42:43], v[vgprValuC+94:vgprValuC+94+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_fma_mix_f32 v[vgprValuC+92], s[sgprBeta], v88, v[vgprValuC+92] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+93], s[sgprBeta], v88, v[vgprValuC+93] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+94], s[sgprBeta], v89, v[vgprValuC+94] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+95], s[sgprBeta], v89, v[vgprValuC+95] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+92:vgprValuC+92+1], v[36:37], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[vgprValuC+94:vgprValuC+94+1], v[38:39], v[vgprValuC+94:vgprValuC+94+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+92], v[vgprValuC+92]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+93], v[vgprValuC+93]     // convert C to fp16
v_pack_b32_f16 v92, v[vgprValuC+92], v[vgprValuC+93] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+94], v[vgprValuC+94]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+95], v[vgprValuC+95]     // convert C to fp16
v_pack_b32_f16 v93, v[vgprValuC+94], v[vgprValuC+95] // Pack with neighbor
buffer_store_dwordx2 v[92:93], v75, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[20:21], v[vgprValuC+104:vgprValuC+104+1] // *= ScaleAlphaVecVMulPK(20)(0)
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[22:23], v[vgprValuC+106:vgprValuC+106+1] // *= ScaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+104], s[sgprBeta], v100, v[vgprValuC+104] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+105], s[sgprBeta], v100, v[vgprValuC+105] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+106], s[sgprBeta], v101, v[vgprValuC+106] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+107], s[sgprBeta], v101, v[vgprValuC+107] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+104:vgprValuC+104+1], v[16:17], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[vgprValuC+106:vgprValuC+106+1], v[18:19], v[vgprValuC+106:vgprValuC+106+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+104], v[vgprValuC+104]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+105], v[vgprValuC+105]   // convert C to fp16
v_pack_b32_f16 v104, v[vgprValuC+104], v[vgprValuC+105] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+106], v[vgprValuC+106]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+107], v[vgprValuC+107]   // convert C to fp16
v_pack_b32_f16 v105, v[vgprValuC+106], v[vgprValuC+107] // Pack with neighbor
buffer_store_dwordx2 v[104:105], v87, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+116:vgprValuC+116+1], v[40:41], v[vgprValuC+116:vgprValuC+116+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+118:vgprValuC+118+1], v[42:43], v[vgprValuC+118:vgprValuC+118+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_fma_mix_f32 v[vgprValuC+116], s[sgprBeta], v112, v[vgprValuC+116] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v112, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+118], s[sgprBeta], v113, v[vgprValuC+118] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+119], s[sgprBeta], v113, v[vgprValuC+119] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+116:vgprValuC+116+1], v[36:37], v[vgprValuC+116:vgprValuC+116+1] // C += bias
v_pk_add_f32 v[vgprValuC+118:vgprValuC+118+1], v[38:39], v[vgprValuC+118:vgprValuC+118+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+116], v[vgprValuC+116]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+117], v[vgprValuC+117]   // convert C to fp16
v_pack_b32_f16 v116, v[vgprValuC+116], v[vgprValuC+117] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+118], v[vgprValuC+118]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+119], v[vgprValuC+119]   // convert C to fp16
v_pack_b32_f16 v117, v[vgprValuC+118], v[vgprValuC+119] // Pack with neighbor
buffer_store_dwordx2 v[116:117], v99, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
label_Sync_EDN_Beta_Edge_3:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//GW end
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,0,4,0:vw4); (0,1,4,0:vw4); (0,0,5,0:vw4); (0,1,5,0:vw4); (0,0,6,0:vw4); (0,1,6,0:vw4); (0,0,7,0:vw4); (0,1,7,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v110, BufferOOB
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v110, v6, s[72:73]               // LDC clip if OOB. offset
buffer_load_dwordx2 v[12:13], v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s68
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
ds_read_b128 v[16:19], v8 offset:0                 // load Bias
v_add_u32 v9, 1024, v8                             // add ScaleAlphaVec offset (3)
ds_read_b128 v[20:23], v9 offset:0                 // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v110, v6, s[72:73]               // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v110, v7, s[72:73]               // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v10, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v110, v10, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[32:33], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s68
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
ds_read_b128 v[36:39], v28 offset:0                // load Bias
v_add_u32 v29, 1024, v28                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[40:43], v29 offset:0                // load scaleAlpha
v_add_lshl_u32 v10, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v110, v10, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v110, v11, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v30, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v110, v30, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[52:53], v30, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v48, v0, s68
v_lshlrev_b32 v48, 0x2, v48                        // Bias address scaled by BPE
v_add_u32 v49, 1024, v48                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v30, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v110, v30, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v31, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v110, v31, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v50, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v50, v110, v50, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[64:65], v50, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v60, v4, s68
v_lshlrev_b32 v60, 0x2, v60                        // Bias address scaled by BPE
v_add_u32 v61, 1024, v60                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v50, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v50, v110, v50, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v51, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v110, v51, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v62, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v110, v62, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[76:77], v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v0, s68
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_add_u32 v73, 1024, v72                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v62, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v110, v62, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v63, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v63, v110, v63, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v74, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v74, v110, v74, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[88:89], v74, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v84, v4, s68
v_lshlrev_b32 v84, 0x2, v84                        // Bias address scaled by BPE
v_add_u32 v85, 1024, v84                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v74, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v74, v110, v74, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v75, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v75, v110, v75, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v86, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v110, v86, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[100:101], v86, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v96, v0, s68
v_lshlrev_b32 v96, 0x2, v96                        // Bias address scaled by BPE
v_add_u32 v97, 1024, v96                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v86, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v110, v86, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v87, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v87, v110, v87, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v98, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v98, v110, v98, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[112:113], v98, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v108, v4, s68
v_lshlrev_b32 v108, 0x2, v108                      // Bias address scaled by BPE
v_add_u32 v109, 1024, v108                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v98, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v98, v110, v98, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v99, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v99, v110, v99, s[72:73]             // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+25], acc33          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+26], acc34          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+27], acc35          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+44], acc36          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+45], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+46], acc38          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+47], acc39          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+56], acc40          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+57], acc41          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+58], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+59], acc43          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+68], acc44          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+69], acc45          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+70], acc46          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+71], acc47          // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+80], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+81], acc49          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+82], acc50          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+83], acc51          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+92], acc52          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+93], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+94], acc54          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+95], acc55          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+104], acc56         // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+105], acc57         // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+106], acc58         // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+107], acc59         // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+116], acc60         // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+117], acc61         // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+118], acc62         // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+119], acc63         // copy acc to vreg[63]

/* store after Acc, GSU: 2 */

/* store after Acc, GSU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 24

buffer_store_dwordx4 v[44:47], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 44

buffer_store_dwordx4 v[56:59], v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 56

buffer_store_dwordx4 v[68:71], v50, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 68

buffer_store_dwordx4 v[80:83], v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 80

buffer_store_dwordx4 v[92:95], v74, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 92

buffer_store_dwordx4 v[104:107], v86, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 104

buffer_store_dwordx4 v[116:119], v98, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 116
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//sourece store done, GSU:2

// check done start
// synchronizer offset cal
s_mul_i32 s13, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s11, s13, s[sgprWorkGroup2]
s_mul_i32 s12, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s12, s12, s[sgprWorkGroup0]
s_add_u32 s12, s12, s11
v_readfirstlane_b32 s11, v[vgprSerial]
s_mul_i32 s13, s13, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s11, s11, 0x6
s_mul_i32 s11, s13, s11                            // wave offset at batch
s_add_u32 s12, s11, s12
s_mul_i32 s13, s13, 4                              // cal a batch offset
s_mul_i32 s13, s13, 1                              // this batch offset
s_add_u32 s12, s12, s13
s_lshl_b32 s12, s12, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s12
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_and_b32 s11, s[sgprGSU], 0x3fff                  // Restore GSU
s_sub_u32 s11, s11, 0x1
s_atomic_dec s11, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s75, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s74, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s78, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s78, s78, 1                              // Free1
s_mul_hi_u32 s77, s78, s[sgprStrideC1J]            // Free1
s_mul_i32 s76, s78, s[sgprStrideC1J]               // Free1
s_add_u32 s74, s74, s76                            // Free1
s_addc_u32 s75, s75, s77                           // Free1
s_sub_u32 s78, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s78, s78, 1                              // Free2
s_mul_hi_u32 s77, s78, s[sgprStrideCK]             // Free2
s_mul_i32 s76, s78, s[sgprStrideCK]                // Free2
s_add_u32 s74, s74, s76                            // Free2
s_addc_u32 s75, s75, s77                           // Free2
s_lshl_b64 s[26:27], s[74:75], 2                   // scale by bpe

v_mov_b32 v110, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s11, 0x1
s_cbranch_scc1 label_NoBranch_6WYTQMTGINQXI8HQ_0   // Only branch on scc0
// long branch sync
s_getpc_b64 s[44:45]                               // addr of next instr
s_add_i32 s46, label_Sync_EDN_Beta_Edge_4, 0x4     // target branch offset
s_add_u32 s44, s44, s46                            // add target branch offset
s_addc_u32 s45, s45, 0                             // add high and carry
s_setpc_b64 s[44:45]                               // branch to label_Sync_EDN_Beta_Edge_4
label_NoBranch_6WYTQMTGINQXI8HQ_0:
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 24
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_72 // SyncAddbranchhere
buffer_load_dwordx4 v[120:123], v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_72:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[120:121]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[122:123]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_72 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v111, v6, v110, s[32:33]             // protect if OOB
buffer_load_dwordx4 v[120:123], v111, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_72      // Syncbranchhere

label_Synchronizer_read_add_end_1_72:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_72:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v110, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[44:47], v10, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 44
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_73 // SyncAddbranchhere
buffer_load_dwordx4 v[120:123], v10, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_73:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[120:121]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[122:123]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_73 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v111, v10, v110, s[32:33]            // protect if OOB
buffer_load_dwordx4 v[120:123], v111, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_73      // Syncbranchhere

label_Synchronizer_read_add_end_1_73:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_73:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v110, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[56:59], v30, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 56
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_74 // SyncAddbranchhere
buffer_load_dwordx4 v[120:123], v30, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_74:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[56:57], v[56:57], v[120:121]        // buffer pk
v_pk_add_f32 v[58:59], v[58:59], v[122:123]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_74 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v111, v30, v110, s[32:33]            // protect if OOB
buffer_load_dwordx4 v[120:123], v111, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_74      // Syncbranchhere

label_Synchronizer_read_add_end_1_74:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_74:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v110, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[68:71], v50, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 68
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_75 // SyncAddbranchhere
buffer_load_dwordx4 v[120:123], v50, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_75:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[68:69], v[68:69], v[120:121]        // buffer pk
v_pk_add_f32 v[70:71], v[70:71], v[122:123]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_75 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v111, v50, v110, s[32:33]            // protect if OOB
buffer_load_dwordx4 v[120:123], v111, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_75      // Syncbranchhere

label_Synchronizer_read_add_end_1_75:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_75:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v110, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[80:83], v62, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 80
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_76 // SyncAddbranchhere
buffer_load_dwordx4 v[120:123], v62, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_76:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[80:81], v[80:81], v[120:121]        // buffer pk
v_pk_add_f32 v[82:83], v[82:83], v[122:123]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_76 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v111, v62, v110, s[32:33]            // protect if OOB
buffer_load_dwordx4 v[120:123], v111, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_76      // Syncbranchhere

label_Synchronizer_read_add_end_1_76:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_76:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v110, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[92:95], v74, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 92
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_77 // SyncAddbranchhere
buffer_load_dwordx4 v[120:123], v74, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_77:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[92:93], v[92:93], v[120:121]        // buffer pk
v_pk_add_f32 v[94:95], v[94:95], v[122:123]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_77 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v111, v74, v110, s[32:33]            // protect if OOB
buffer_load_dwordx4 v[120:123], v111, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_77      // Syncbranchhere

label_Synchronizer_read_add_end_1_77:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_77:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v110, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[104:107], v86, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 104
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_78 // SyncAddbranchhere
buffer_load_dwordx4 v[120:123], v86, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_78:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[104:105], v[104:105], v[120:121]    // buffer pk
v_pk_add_f32 v[106:107], v[106:107], v[122:123]    // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_78 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v111, v86, v110, s[32:33]            // protect if OOB
buffer_load_dwordx4 v[120:123], v111, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_78      // Syncbranchhere

label_Synchronizer_read_add_end_1_78:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_78:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v110, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dwordx4 v[116:119], v98, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 116
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_79 // SyncAddbranchhere
buffer_load_dwordx4 v[120:123], v98, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16 4 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_79:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[116:117], v[116:117], v[120:121]    // buffer pk
v_pk_add_f32 v[118:119], v[118:119], v[122:123]    // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_79 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v111, v98, v110, s[32:33]            // protect if OOB
buffer_load_dwordx4 v[120:123], v111, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 16
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_79      // Syncbranchhere

label_Synchronizer_read_add_end_1_79:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_79:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 4, 0), (0, 1, 4, 0), (0, 0, 5, 0), (0, 1, 5, 0), (0, 0, 6, 0), (0, 1, 6, 0), (0, 0, 7, 0), (0, 1, 7, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+104], s[sgprAlpha], v[vgprValuC+104] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+107], s[sgprAlpha], v[vgprValuC+107] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+118], s[sgprAlpha], v[vgprValuC+118] // *= alpha
v_mul_f32 v[vgprValuC+119], s[sgprAlpha], v[vgprValuC+119] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= ScaleAlphaVecVMulPK(20)(0)
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= ScaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[40:41], v[vgprValuC+44:vgprValuC+44+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[42:43], v[vgprValuC+46:vgprValuC+46+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_fma_mix_f32 v[vgprValuC+44], s[sgprBeta], v32, v[vgprValuC+44] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+45], s[sgprBeta], v32, v[vgprValuC+45] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+46], s[sgprBeta], v33, v[vgprValuC+46] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v33, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[36:37], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[38:39], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[20:21], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(20)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[22:23], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+56], s[sgprBeta], v52, v[vgprValuC+56] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+57], s[sgprBeta], v52, v[vgprValuC+57] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+58], s[sgprBeta], v53, v[vgprValuC+58] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+59], s[sgprBeta], v53, v[vgprValuC+59] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[16:17], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[18:19], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v31, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[40:41], v[vgprValuC+68:vgprValuC+68+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[42:43], v[vgprValuC+70:vgprValuC+70+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_fma_mix_f32 v[vgprValuC+68], s[sgprBeta], v64, v[vgprValuC+68] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+69], s[sgprBeta], v64, v[vgprValuC+69] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+70], s[sgprBeta], v65, v[vgprValuC+70] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+71], s[sgprBeta], v65, v[vgprValuC+71] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+68:vgprValuC+68+1], v[36:37], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[vgprValuC+70:vgprValuC+70+1], v[38:39], v[vgprValuC+70:vgprValuC+70+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+68], v[vgprValuC+68]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+69], v[vgprValuC+69]     // convert C to fp16
v_pack_b32_f16 v68, v[vgprValuC+68], v[vgprValuC+69] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+70], v[vgprValuC+70]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+71], v[vgprValuC+71]     // convert C to fp16
v_pack_b32_f16 v69, v[vgprValuC+70], v[vgprValuC+71] // Pack with neighbor
buffer_store_dwordx2 v[68:69], v51, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[20:21], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(20)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[22:23], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+80], s[sgprBeta], v76, v[vgprValuC+80] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+81], s[sgprBeta], v76, v[vgprValuC+81] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+82], s[sgprBeta], v77, v[vgprValuC+82] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+83], s[sgprBeta], v77, v[vgprValuC+83] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[16:17], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[18:19], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v63, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[40:41], v[vgprValuC+92:vgprValuC+92+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[42:43], v[vgprValuC+94:vgprValuC+94+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_fma_mix_f32 v[vgprValuC+92], s[sgprBeta], v88, v[vgprValuC+92] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+93], s[sgprBeta], v88, v[vgprValuC+93] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+94], s[sgprBeta], v89, v[vgprValuC+94] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+95], s[sgprBeta], v89, v[vgprValuC+95] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+92:vgprValuC+92+1], v[36:37], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[vgprValuC+94:vgprValuC+94+1], v[38:39], v[vgprValuC+94:vgprValuC+94+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+92], v[vgprValuC+92]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+93], v[vgprValuC+93]     // convert C to fp16
v_pack_b32_f16 v92, v[vgprValuC+92], v[vgprValuC+93] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+94], v[vgprValuC+94]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+95], v[vgprValuC+95]     // convert C to fp16
v_pack_b32_f16 v93, v[vgprValuC+94], v[vgprValuC+95] // Pack with neighbor
buffer_store_dwordx2 v[92:93], v75, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[20:21], v[vgprValuC+104:vgprValuC+104+1] // *= ScaleAlphaVecVMulPK(20)(0)
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[22:23], v[vgprValuC+106:vgprValuC+106+1] // *= ScaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+104], s[sgprBeta], v100, v[vgprValuC+104] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+105], s[sgprBeta], v100, v[vgprValuC+105] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+106], s[sgprBeta], v101, v[vgprValuC+106] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+107], s[sgprBeta], v101, v[vgprValuC+107] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+104:vgprValuC+104+1], v[16:17], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[vgprValuC+106:vgprValuC+106+1], v[18:19], v[vgprValuC+106:vgprValuC+106+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+104], v[vgprValuC+104]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+105], v[vgprValuC+105]   // convert C to fp16
v_pack_b32_f16 v104, v[vgprValuC+104], v[vgprValuC+105] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+106], v[vgprValuC+106]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+107], v[vgprValuC+107]   // convert C to fp16
v_pack_b32_f16 v105, v[vgprValuC+106], v[vgprValuC+107] // Pack with neighbor
buffer_store_dwordx2 v[104:105], v87, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_pk_mul_f32 v[vgprValuC+116:vgprValuC+116+1], v[40:41], v[vgprValuC+116:vgprValuC+116+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+118:vgprValuC+118+1], v[42:43], v[vgprValuC+118:vgprValuC+118+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_fma_mix_f32 v[vgprValuC+116], s[sgprBeta], v112, v[vgprValuC+116] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v112, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+118], s[sgprBeta], v113, v[vgprValuC+118] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+119], s[sgprBeta], v113, v[vgprValuC+119] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+116:vgprValuC+116+1], v[36:37], v[vgprValuC+116:vgprValuC+116+1] // C += bias
v_pk_add_f32 v[vgprValuC+118:vgprValuC+118+1], v[38:39], v[vgprValuC+118:vgprValuC+118+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+116], v[vgprValuC+116]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+117], v[vgprValuC+117]   // convert C to fp16
v_pack_b32_f16 v116, v[vgprValuC+116], v[vgprValuC+117] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+118], v[vgprValuC+118]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+119], v[vgprValuC+119]   // convert C to fp16
v_pack_b32_f16 v117, v[vgprValuC+118], v[vgprValuC+119] // Pack with neighbor
buffer_store_dwordx2 v[116:117], v99, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
label_Sync_EDN_Beta_Edge_4:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//GW end
s_branch label_GW_End_1                            // jump to end
label_GW_B1_E1_M:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=26 */
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw1); (0,0,0,1:vw1); (0,0,0,2:vw1); (0,0,0,3:vw1); (0,1,0,0:vw1); (0,1,0,1:vw1); (0,1,0,2:vw1); (0,1,0,3:vw1); (0,0,1,0:vw1); (0,0,1,1:vw1); (0,0,1,2:vw1); (0,0,1,3:vw1); (0,1,1,0:vw1); (0,1,1,1:vw1); (0,1,1,2:vw1); (0,1,1,3:vw1); (0,0,2,0:vw1); (0,0,2,1:vw1); (0,0,2,2:vw1); (0,0,2,3:vw1); (0,1,2,0:vw1); (0,1,2,1:vw1); (0,1,2,2:vw1); (0,1,2,3:vw1); (0,0,3,0:vw1); (0,0,3,1:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v179, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v179, v6, s[72:73]               // LDC clip if OOB. offset
buffer_load_short_d16 v10, v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s68
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for LDS write
s_barrier                                          // LDS write barrier
ds_read_b32 v11, v8 offset:0                       // load Bias
v_add_u32 v9, 1024, v8                             // add ScaleAlphaVec offset (3)
ds_read_b32 v12, v9 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v179, v6, s[72:73]               // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v179, v7, s[72:73]               // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v14, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v179, v14, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v18, v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v16, v4, s68
v_lshlrev_b32 v16, 0x2, v16                        // Bias address scaled by BPE
ds_read_b32 v19, v16 offset:0                      // load Bias
v_add_u32 v17, 1024, v16                           // add ScaleAlphaVec offset (3)
ds_read_b32 v20, v17 offset:0                      // load scaleAlpha
v_add_lshl_u32 v14, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v179, v14, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v15, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v15, v179, v15, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v22, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v179, v22, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v26, v22, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v4, s68
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
ds_read_b32 v27, v24 offset:0                      // load Bias
v_add_u32 v25, 1024, v24                           // add ScaleAlphaVec offset (3)
ds_read_b32 v28, v25 offset:0                      // load scaleAlpha
v_add_lshl_u32 v22, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v179, v22, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v23, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v23, v179, v23, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v30, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v179, v30, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v34, v30, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v4, s68
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
ds_read_b32 v35, v32 offset:0                      // load Bias
v_add_u32 v33, 1024, v32                           // add ScaleAlphaVec offset (3)
ds_read_b32 v36, v33 offset:0                      // load scaleAlpha
v_add_lshl_u32 v30, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v179, v30, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v31, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v179, v31, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v38, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v179, v38, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v42, v38, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v4, s68
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
ds_read_b32 v43, v40 offset:0                      // load Bias
v_add_u32 v41, 1024, v40                           // add ScaleAlphaVec offset (3)
ds_read_b32 v44, v41 offset:0                      // load scaleAlpha
v_add_lshl_u32 v38, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v179, v38, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v39, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v39, v179, v39, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v46, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v179, v46, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v50, v46, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v48, v4, s68
v_lshlrev_b32 v48, 0x2, v48                        // Bias address scaled by BPE
ds_read_b32 v51, v48 offset:0                      // load Bias
v_add_u32 v49, 1024, v48                           // add ScaleAlphaVec offset (3)
ds_read_b32 v52, v49 offset:0                      // load scaleAlpha
v_add_lshl_u32 v46, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v179, v46, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v47, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v47, v179, v47, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v54, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v179, v54, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v58, v54, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v56, v4, s68
v_lshlrev_b32 v56, 0x2, v56                        // Bias address scaled by BPE
ds_read_b32 v59, v56 offset:0                      // load Bias
v_add_u32 v57, 1024, v56                           // add ScaleAlphaVec offset (3)
ds_read_b32 v60, v57 offset:0                      // load scaleAlpha
v_add_lshl_u32 v54, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v179, v54, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v55, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v179, v55, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v62, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v179, v62, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v66, v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v64, v4, s68
v_lshlrev_b32 v64, 0x2, v64                        // Bias address scaled by BPE
ds_read_b32 v67, v64 offset:0                      // load Bias
v_add_u32 v65, 1024, v64                           // add ScaleAlphaVec offset (3)
ds_read_b32 v68, v65 offset:0                      // load scaleAlpha
v_add_lshl_u32 v62, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v179, v62, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v63, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v63, v179, v63, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v70, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v70, v179, v70, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v74, v70, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v0, s68
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_add_u32 v73, 1024, v72                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v70, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v70, v179, v70, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v71, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v179, v71, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v179, v76, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v80, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v78, v4, s68
v_lshlrev_b32 v78, 0x2, v78                        // Bias address scaled by BPE
v_add_u32 v79, 1024, v78                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v76, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v179, v76, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v179, v77, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v82, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v82, v179, v82, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v86, v82, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v84, v4, s68
v_lshlrev_b32 v84, 0x2, v84                        // Bias address scaled by BPE
v_add_u32 v85, 1024, v84                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v82, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v82, v179, v82, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v83, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v179, v83, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v88, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v88, v179, v88, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v92, v88, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v90, v4, s68
v_lshlrev_b32 v90, 0x2, v90                        // Bias address scaled by BPE
v_add_u32 v91, 1024, v90                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v88, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v88, v179, v88, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v89, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v89, v179, v89, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v94, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v94, v179, v94, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v98, v94, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v96, v4, s68
v_lshlrev_b32 v96, 0x2, v96                        // Bias address scaled by BPE
v_add_u32 v97, 1024, v96                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v94, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v94, v179, v94, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v95, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v95, v179, v95, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v100, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v100, v179, v100, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v104, v100, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v102, v4, s68
v_lshlrev_b32 v102, 0x2, v102                      // Bias address scaled by BPE
v_add_u32 v103, 1024, v102                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v100, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v100, v179, v100, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v101, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v101, v179, v101, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v106, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v179, v106, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v110, v106, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v108, v4, s68
v_lshlrev_b32 v108, 0x2, v108                      // Bias address scaled by BPE
v_add_u32 v109, 1024, v108                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v106, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v179, v106, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v107, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v107, v179, v107, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v112, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v112, v179, v112, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v116, v112, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v114, v4, s68
v_lshlrev_b32 v114, 0x2, v114                      // Bias address scaled by BPE
v_add_u32 v115, 1024, v114                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v112, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v112, v179, v112, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v113, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v113, v179, v113, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v118, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v179, v118, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v122, v118, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v120, v0, s68
v_lshlrev_b32 v120, 0x2, v120                      // Bias address scaled by BPE
v_add_u32 v121, 1024, v120                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v179, v118, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v119, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v119, v179, v119, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v124, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v124, v179, v124, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v128, v124, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v126, v4, s68
v_lshlrev_b32 v126, 0x2, v126                      // Bias address scaled by BPE
v_add_u32 v127, 1024, v126                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v124, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v124, v179, v124, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v125, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v125, v179, v125, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v130, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v130, v179, v130, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v134, v130, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v132, v4, s68
v_lshlrev_b32 v132, 0x2, v132                      // Bias address scaled by BPE
v_add_u32 v133, 1024, v132                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v130, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v130, v179, v130, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v131, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v131, v179, v131, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v136, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v136, v179, v136, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v141, v136, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v138, v4, s68
v_lshlrev_b32 v138, 0x2, v138                      // Bias address scaled by BPE
v_add_u32 v140, 1024, v138                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v136, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v136, v179, v136, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v137, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v137, v179, v137, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v143, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v143, v179, v143, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v147, v143, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v145, v4, s68
v_lshlrev_b32 v145, 0x2, v145                      // Bias address scaled by BPE
v_add_u32 v146, 1024, v145                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v143, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v143, v179, v143, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v144, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v144, v179, v144, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v149, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v179, v149, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v153, v149, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v151, v4, s68
v_lshlrev_b32 v151, 0x2, v151                      // Bias address scaled by BPE
v_add_u32 v152, 1024, v151                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v149, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v179, v149, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v150, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v150, v179, v150, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v155, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v155, v179, v155, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v159, v155, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v157, v4, s68
v_lshlrev_b32 v157, 0x2, v157                      // Bias address scaled by BPE
v_add_u32 v158, 1024, v157                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v155, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v155, v179, v155, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v156, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v156, v179, v156, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v161, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v161, v179, v161, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v165, v161, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v163, v4, s68
v_lshlrev_b32 v163, 0x2, v163                      // Bias address scaled by BPE
v_add_u32 v164, 1024, v163                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v161, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v161, v179, v161, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v162, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v162, v179, v162, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v167, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v167, v179, v167, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v171, v167, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v169, v0, s68
v_lshlrev_b32 v169, 0x2, v169                      // Bias address scaled by BPE
v_add_u32 v170, 1024, v169                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v167, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v167, v179, v167, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v168, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v168, v179, v168, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v173, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v173, v179, v173, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v177, v173, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v175, v4, s68
v_lshlrev_b32 v175, 0x2, v175                      // Bias address scaled by BPE
v_add_u32 v176, 1024, v175                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v173, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v173, v179, v173, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v174, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v174, v179, v174, s[72:73]           // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+13], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc1           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+29], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+37], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+45], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+53], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+61], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+69], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+75], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+81], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+87], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+93], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+99], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+105], acc13         // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+111], acc14         // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+117], acc15         // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+123], acc16         // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+129], acc17         // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+135], acc18         // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+142], acc19         // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+148], acc20         // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+154], acc21         // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+160], acc22         // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+166], acc23         // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+172], acc24         // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+178], acc25         // copy acc to vreg[25]

/* store after Acc, GSU: 2 */

/* store after Acc, GSU: 2 */

buffer_store_dword v13, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 13

buffer_store_dword v21, v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 21

buffer_store_dword v29, v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 29

buffer_store_dword v37, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 37

buffer_store_dword v45, v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 45

buffer_store_dword v53, v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 53

buffer_store_dword v61, v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 61

buffer_store_dword v69, v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 69

buffer_store_dword v75, v70, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 75

buffer_store_dword v81, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 81

buffer_store_dword v87, v82, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 87

buffer_store_dword v93, v88, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 93

buffer_store_dword v99, v94, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 99

buffer_store_dword v105, v100, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 105

buffer_store_dword v111, v106, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 111

buffer_store_dword v117, v112, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 117

buffer_store_dword v123, v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 123

buffer_store_dword v129, v124, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 129

buffer_store_dword v135, v130, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 135

buffer_store_dword v142, v136, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 142

buffer_store_dword v148, v143, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 148

buffer_store_dword v154, v149, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 154

buffer_store_dword v160, v155, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 160

buffer_store_dword v166, v161, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 166

buffer_store_dword v172, v167, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 172

buffer_store_dword v178, v173, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 178
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//sourece store done, GSU:2

// check done start
// synchronizer offset cal
s_mul_i32 s13, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s11, s13, s[sgprWorkGroup2]
s_mul_i32 s12, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s12, s12, s[sgprWorkGroup0]
s_add_u32 s12, s12, s11
v_readfirstlane_b32 s11, v[vgprSerial]
s_mul_i32 s13, s13, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s11, s11, 0x6
s_mul_i32 s11, s13, s11                            // wave offset at batch
s_add_u32 s12, s11, s12
s_lshl_b32 s12, s12, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s12
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_and_b32 s11, s[sgprGSU], 0x3fff                  // Restore GSU
s_sub_u32 s11, s11, 0x1
s_atomic_dec s11, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s75, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s74, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s78, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s78, s78, 1                              // Free1
s_mul_hi_u32 s77, s78, s[sgprStrideC1J]            // Free1
s_mul_i32 s76, s78, s[sgprStrideC1J]               // Free1
s_add_u32 s74, s74, s76                            // Free1
s_addc_u32 s75, s75, s77                           // Free1
s_sub_u32 s78, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s78, s78, 1                              // Free2
s_mul_hi_u32 s77, s78, s[sgprStrideCK]             // Free2
s_mul_i32 s76, s78, s[sgprStrideCK]                // Free2
s_add_u32 s74, s74, s76                            // Free2
s_addc_u32 s75, s75, s77                           // Free2
s_lshl_b64 s[26:27], s[74:75], 2                   // scale by bpe

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s11, 0x1
s_cbranch_scc1 label_NoBranch_3771OV0I7Z7V62WI_0   // Only branch on scc0
// long branch sync
s_getpc_b64 s[44:45]                               // addr of next instr
s_add_i32 s46, label_Sync_EDN_Beta_Edge_0, 0x4     // target branch offset
s_add_u32 s44, s44, s46                            // add target branch offset
s_addc_u32 s45, s45, 0                             // add high and carry
s_setpc_b64 s[44:45]                               // branch to label_Sync_EDN_Beta_Edge_0
label_NoBranch_3771OV0I7Z7V62WI_0:
// check done end

// buffer load start
buffer_load_dword v13, v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 13
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1   // SyncAddbranchhere
buffer_load_dword v184, v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v13, v13, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip    // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v6, v179, s[32:33]             // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add         // Syncbranchhere

label_Synchronizer_read_add_end_1:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v21, v14, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 21
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_1 // SyncAddbranchhere
buffer_load_dword v184, v14, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_1:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v21, v21, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_1  // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v14, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_1       // Syncbranchhere

label_Synchronizer_read_add_end_1_1:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_1:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v29, v22, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 29
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_2 // SyncAddbranchhere
buffer_load_dword v184, v22, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_2:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v29, v29, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_2  // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v22, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_2       // Syncbranchhere

label_Synchronizer_read_add_end_1_2:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_2:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v37, v30, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 37
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_3 // SyncAddbranchhere
buffer_load_dword v184, v30, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_3:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v37, v37, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_3  // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v30, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_3       // Syncbranchhere

label_Synchronizer_read_add_end_1_3:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_3:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v45, v38, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 45
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_4 // SyncAddbranchhere
buffer_load_dword v184, v38, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_4:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v45, v45, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_4  // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v38, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_4       // Syncbranchhere

label_Synchronizer_read_add_end_1_4:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_4:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v53, v46, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 53
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_5 // SyncAddbranchhere
buffer_load_dword v184, v46, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_5:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v53, v53, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_5  // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v46, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_5       // Syncbranchhere

label_Synchronizer_read_add_end_1_5:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_5:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v61, v54, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 61
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_6 // SyncAddbranchhere
buffer_load_dword v184, v54, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_6:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v61, v61, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_6  // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v54, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_6       // Syncbranchhere

label_Synchronizer_read_add_end_1_6:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_6:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v69, v62, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 69
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_7 // SyncAddbranchhere
buffer_load_dword v184, v62, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_7:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v69, v69, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_7  // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v62, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_7       // Syncbranchhere

label_Synchronizer_read_add_end_1_7:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_7:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v75, v70, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 75
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_8 // SyncAddbranchhere
buffer_load_dword v184, v70, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_8:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v75, v75, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_8  // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v70, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_8       // Syncbranchhere

label_Synchronizer_read_add_end_1_8:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_8:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v81, v76, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 81
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_9 // SyncAddbranchhere
buffer_load_dword v184, v76, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_9:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v81, v81, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_9  // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v76, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_9       // Syncbranchhere

label_Synchronizer_read_add_end_1_9:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_9:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v87, v82, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 87
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_10 // SyncAddbranchhere
buffer_load_dword v184, v82, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_10:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v87, v87, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_10 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v82, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_10      // Syncbranchhere

label_Synchronizer_read_add_end_1_10:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_10:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v93, v88, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 93
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_11 // SyncAddbranchhere
buffer_load_dword v184, v88, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_11:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v93, v93, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_11 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v88, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_11      // Syncbranchhere

label_Synchronizer_read_add_end_1_11:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_11:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v99, v94, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 99
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_12 // SyncAddbranchhere
buffer_load_dword v184, v94, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_12:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v99, v99, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_12 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v94, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_12      // Syncbranchhere

label_Synchronizer_read_add_end_1_12:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_12:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v105, v100, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 105
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_13 // SyncAddbranchhere
buffer_load_dword v184, v100, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_13:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v105, v105, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_13 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v100, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_13      // Syncbranchhere

label_Synchronizer_read_add_end_1_13:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_13:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v111, v106, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 111
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_14 // SyncAddbranchhere
buffer_load_dword v184, v106, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_14:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v111, v111, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_14 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v106, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_14      // Syncbranchhere

label_Synchronizer_read_add_end_1_14:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_14:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v117, v112, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 117
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_15 // SyncAddbranchhere
buffer_load_dword v184, v112, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_15:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v117, v117, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_15 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v112, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_15      // Syncbranchhere

label_Synchronizer_read_add_end_1_15:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_15:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v123, v118, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 123
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_16 // SyncAddbranchhere
buffer_load_dword v184, v118, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_16:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v123, v123, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_16 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v118, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_16      // Syncbranchhere

label_Synchronizer_read_add_end_1_16:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_16:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v129, v124, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 129
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_17 // SyncAddbranchhere
buffer_load_dword v184, v124, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_17:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v129, v129, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_17 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v124, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_17      // Syncbranchhere

label_Synchronizer_read_add_end_1_17:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_17:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v135, v130, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 135
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_18 // SyncAddbranchhere
buffer_load_dword v184, v130, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_18:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v135, v135, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_18 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v130, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_18      // Syncbranchhere

label_Synchronizer_read_add_end_1_18:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_18:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v142, v136, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 142
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_19 // SyncAddbranchhere
buffer_load_dword v184, v136, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_19:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v142, v142, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_19 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v136, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_19      // Syncbranchhere

label_Synchronizer_read_add_end_1_19:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_19:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v148, v143, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 148
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_20 // SyncAddbranchhere
buffer_load_dword v184, v143, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_20:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v148, v148, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_20 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v143, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_20      // Syncbranchhere

label_Synchronizer_read_add_end_1_20:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_20:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v154, v149, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 154
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_21 // SyncAddbranchhere
buffer_load_dword v184, v149, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_21:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v154, v154, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_21 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v149, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_21      // Syncbranchhere

label_Synchronizer_read_add_end_1_21:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_21:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v160, v155, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 160
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_22 // SyncAddbranchhere
buffer_load_dword v184, v155, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_22:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v160, v160, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_22 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v155, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_22      // Syncbranchhere

label_Synchronizer_read_add_end_1_22:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_22:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v166, v161, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 166
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_23 // SyncAddbranchhere
buffer_load_dword v184, v161, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_23:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v166, v166, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_23 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v161, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_23      // Syncbranchhere

label_Synchronizer_read_add_end_1_23:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_23:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v172, v167, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 172
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_24 // SyncAddbranchhere
buffer_load_dword v184, v167, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_24:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v172, v172, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_24 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v167, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_24      // Syncbranchhere

label_Synchronizer_read_add_end_1_24:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_24:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v178, v173, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 178
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_25 // SyncAddbranchhere
buffer_load_dword v184, v173, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_25:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v178, v178, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_25 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v173, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_25      // Syncbranchhere

label_Synchronizer_read_add_end_1_25:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_25:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 0, 1), (0, 0, 0, 2), (0, 0, 0, 3), (0, 1, 0, 0), (0, 1, 0, 1), (0, 1, 0, 2), (0, 1, 0, 3), (0, 0, 1, 0), (0, 0, 1, 1), (0, 0, 1, 2), (0, 0, 1, 3), (0, 1, 1, 0), (0, 1, 1, 1), (0, 1, 1, 2), (0, 1, 1, 3), (0, 0, 2, 0), (0, 0, 2, 1), (0, 0, 2, 2), (0, 0, 2, 3), (0, 1, 2, 0), (0, 1, 2, 1), (0, 1, 2, 2), (0, 1, 2, 3), (0, 0, 3, 0), (0, 0, 3, 1)] */
v_mul_f32 v[vgprValuC+13], s[sgprAlpha], v[vgprValuC+13] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+99], s[sgprAlpha], v[vgprValuC+99] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+123], s[sgprAlpha], v[vgprValuC+123] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+135], s[sgprAlpha], v[vgprValuC+135] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+148], s[sgprAlpha], v[vgprValuC+148] // *= alpha
v_mul_f32 v[vgprValuC+154], s[sgprAlpha], v[vgprValuC+154] // *= alpha
v_mul_f32 v[vgprValuC+160], s[sgprAlpha], v[vgprValuC+160] // *= alpha
v_mul_f32 v[vgprValuC+166], s[sgprAlpha], v[vgprValuC+166] // *= alpha
v_mul_f32 v[vgprValuC+172], s[sgprAlpha], v[vgprValuC+172] // *= alpha
v_mul_f32 v[vgprValuC+178], s[sgprAlpha], v[vgprValuC+178] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+13], v12, v[vgprValuC+13]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+13], s[sgprBeta], v10, v[vgprValuC+13] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+13], v11, v[vgprValuC+13]    // C += bias
v_cvt_f16_f32 v13, v[vgprValuC+13]                 // convert C to fp16
buffer_store_short v13, v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+21], v20, v[vgprValuC+21]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+21], s[sgprBeta], v18, v[vgprValuC+21] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+21], v19, v[vgprValuC+21]    // C += bias
v_cvt_f16_f32 v21, v[vgprValuC+21]                 // convert C to fp16
buffer_store_short v21, v15, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+29], v28, v[vgprValuC+29]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+29], s[sgprBeta], v26, v[vgprValuC+29] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+29], v27, v[vgprValuC+29]    // C += bias
v_cvt_f16_f32 v29, v[vgprValuC+29]                 // convert C to fp16
buffer_store_short v29, v23, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+37], v36, v[vgprValuC+37]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v34, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+37], v35, v[vgprValuC+37]    // C += bias
v_cvt_f16_f32 v37, v[vgprValuC+37]                 // convert C to fp16
buffer_store_short v37, v31, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+45], v44, v[vgprValuC+45]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+45], s[sgprBeta], v42, v[vgprValuC+45] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+45], v43, v[vgprValuC+45]    // C += bias
v_cvt_f16_f32 v45, v[vgprValuC+45]                 // convert C to fp16
buffer_store_short v45, v39, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+53], v52, v[vgprValuC+53]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+53], s[sgprBeta], v50, v[vgprValuC+53] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+53], v51, v[vgprValuC+53]    // C += bias
v_cvt_f16_f32 v53, v[vgprValuC+53]                 // convert C to fp16
buffer_store_short v53, v47, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+61], v60, v[vgprValuC+61]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v58, v[vgprValuC+61] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+61], v59, v[vgprValuC+61]    // C += bias
v_cvt_f16_f32 v61, v[vgprValuC+61]                 // convert C to fp16
buffer_store_short v61, v55, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+69], v68, v[vgprValuC+69]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+69], s[sgprBeta], v66, v[vgprValuC+69] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+69], v67, v[vgprValuC+69]    // C += bias
v_cvt_f16_f32 v69, v[vgprValuC+69]                 // convert C to fp16
buffer_store_short v69, v63, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+75], v12, v[vgprValuC+75]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v74, v[vgprValuC+75] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+75], v11, v[vgprValuC+75]    // C += bias
v_cvt_f16_f32 v75, v[vgprValuC+75]                 // convert C to fp16
buffer_store_short v75, v71, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+81], v20, v[vgprValuC+81]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+81], s[sgprBeta], v80, v[vgprValuC+81] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+81], v19, v[vgprValuC+81]    // C += bias
v_cvt_f16_f32 v81, v[vgprValuC+81]                 // convert C to fp16
buffer_store_short v81, v77, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+87], v28, v[vgprValuC+87]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+87], s[sgprBeta], v86, v[vgprValuC+87] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+87], v27, v[vgprValuC+87]    // C += bias
v_cvt_f16_f32 v87, v[vgprValuC+87]                 // convert C to fp16
buffer_store_short v87, v83, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+93], v36, v[vgprValuC+93]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+93], s[sgprBeta], v92, v[vgprValuC+93] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+93], v35, v[vgprValuC+93]    // C += bias
v_cvt_f16_f32 v93, v[vgprValuC+93]                 // convert C to fp16
buffer_store_short v93, v89, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+99], v44, v[vgprValuC+99]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+99], s[sgprBeta], v98, v[vgprValuC+99] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+99], v43, v[vgprValuC+99]    // C += bias
v_cvt_f16_f32 v99, v[vgprValuC+99]                 // convert C to fp16
buffer_store_short v99, v95, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+105], v52, v[vgprValuC+105]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+105], s[sgprBeta], v104, v[vgprValuC+105] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+105], v51, v[vgprValuC+105]  // C += bias
v_cvt_f16_f32 v105, v[vgprValuC+105]               // convert C to fp16
buffer_store_short v105, v101, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+111], v60, v[vgprValuC+111]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+111], s[sgprBeta], v110, v[vgprValuC+111] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+111], v59, v[vgprValuC+111]  // C += bias
v_cvt_f16_f32 v111, v[vgprValuC+111]               // convert C to fp16
buffer_store_short v111, v107, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+117], v68, v[vgprValuC+117]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v116, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+117], v67, v[vgprValuC+117]  // C += bias
v_cvt_f16_f32 v117, v[vgprValuC+117]               // convert C to fp16
buffer_store_short v117, v113, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+123], v12, v[vgprValuC+123]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+123], s[sgprBeta], v122, v[vgprValuC+123] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+123], v11, v[vgprValuC+123]  // C += bias
v_cvt_f16_f32 v123, v[vgprValuC+123]               // convert C to fp16
buffer_store_short v123, v119, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+129], v20, v[vgprValuC+129]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+129], s[sgprBeta], v128, v[vgprValuC+129] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+129], v19, v[vgprValuC+129]  // C += bias
v_cvt_f16_f32 v129, v[vgprValuC+129]               // convert C to fp16
buffer_store_short v129, v125, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+135], v28, v[vgprValuC+135]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+135], s[sgprBeta], v134, v[vgprValuC+135] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+135], v27, v[vgprValuC+135]  // C += bias
v_cvt_f16_f32 v135, v[vgprValuC+135]               // convert C to fp16
buffer_store_short v135, v131, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+142], v36, v[vgprValuC+142]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+142], s[sgprBeta], v141, v[vgprValuC+142] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+142], v35, v[vgprValuC+142]  // C += bias
v_cvt_f16_f32 v142, v[vgprValuC+142]               // convert C to fp16
buffer_store_short v142, v137, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+148], v44, v[vgprValuC+148]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+148], s[sgprBeta], v147, v[vgprValuC+148] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+148], v43, v[vgprValuC+148]  // C += bias
v_cvt_f16_f32 v148, v[vgprValuC+148]               // convert C to fp16
buffer_store_short v148, v144, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+154], v52, v[vgprValuC+154]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+154], s[sgprBeta], v153, v[vgprValuC+154] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+154], v51, v[vgprValuC+154]  // C += bias
v_cvt_f16_f32 v154, v[vgprValuC+154]               // convert C to fp16
buffer_store_short v154, v150, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+160], v60, v[vgprValuC+160]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+160], s[sgprBeta], v159, v[vgprValuC+160] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+160], v59, v[vgprValuC+160]  // C += bias
v_cvt_f16_f32 v160, v[vgprValuC+160]               // convert C to fp16
buffer_store_short v160, v156, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+166], v68, v[vgprValuC+166]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+166], s[sgprBeta], v165, v[vgprValuC+166] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+166], v67, v[vgprValuC+166]  // C += bias
v_cvt_f16_f32 v166, v[vgprValuC+166]               // convert C to fp16
buffer_store_short v166, v162, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+172], v12, v[vgprValuC+172]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+172], s[sgprBeta], v171, v[vgprValuC+172] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+172], v11, v[vgprValuC+172]  // C += bias
v_cvt_f16_f32 v172, v[vgprValuC+172]               // convert C to fp16
buffer_store_short v172, v168, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+178], v20, v[vgprValuC+178]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+178], s[sgprBeta], v177, v[vgprValuC+178] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+178], v19, v[vgprValuC+178]  // C += bias
v_cvt_f16_f32 v178, v[vgprValuC+178]               // convert C to fp16
buffer_store_short v178, v174, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
label_Sync_EDN_Beta_Edge_0:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//GW end
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,0,3,2:vw1); (0,0,3,3:vw1); (0,1,3,0:vw1); (0,1,3,1:vw1); (0,1,3,2:vw1); (0,1,3,3:vw1); (0,0,4,0:vw1); (0,0,4,1:vw1); (0,0,4,2:vw1); (0,0,4,3:vw1); (0,1,4,0:vw1); (0,1,4,1:vw1); (0,1,4,2:vw1); (0,1,4,3:vw1); (0,0,5,0:vw1); (0,0,5,1:vw1); (0,0,5,2:vw1); (0,0,5,3:vw1); (0,1,5,0:vw1); (0,1,5,1:vw1); (0,1,5,2:vw1); (0,1,5,3:vw1); (0,0,6,0:vw1); (0,0,6,1:vw1); (0,0,6,2:vw1); (0,0,6,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v179, BufferOOB
/* (d1,vc1,d0,vc0)=(0,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v6, v2, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v179, v6, s[72:73]               // LDC clip if OOB. offset
buffer_load_short_d16 v10, v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v4, s68
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
ds_read_b32 v11, v8 offset:0                       // load Bias
v_add_u32 v9, 1024, v8                             // add ScaleAlphaVec offset (3)
ds_read_b32 v12, v9 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v4, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v179, v6, s[72:73]               // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v179, v7, s[72:73]               // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v14, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v179, v14, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v18, v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v16, v4, s68
v_lshlrev_b32 v16, 0x2, v16                        // Bias address scaled by BPE
ds_read_b32 v19, v16 offset:0                      // load Bias
v_add_u32 v17, 1024, v16                           // add ScaleAlphaVec offset (3)
ds_read_b32 v20, v17 offset:0                      // load scaleAlpha
v_add_lshl_u32 v14, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v179, v14, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v15, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v15, v179, v15, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v22, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v179, v22, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v26, v22, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v4, s68
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
ds_read_b32 v27, v24 offset:0                      // load Bias
v_add_u32 v25, 1024, v24                           // add ScaleAlphaVec offset (3)
ds_read_b32 v28, v25 offset:0                      // load scaleAlpha
v_add_lshl_u32 v22, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v179, v22, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v23, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v23, v179, v23, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v30, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v179, v30, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v34, v30, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v4, s68
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
ds_read_b32 v35, v32 offset:0                      // load Bias
v_add_u32 v33, 1024, v32                           // add ScaleAlphaVec offset (3)
ds_read_b32 v36, v33 offset:0                      // load scaleAlpha
v_add_lshl_u32 v30, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v179, v30, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v31, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v179, v31, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v38, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v179, v38, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v42, v38, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v4, s68
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
ds_read_b32 v43, v40 offset:0                      // load Bias
v_add_u32 v41, 1024, v40                           // add ScaleAlphaVec offset (3)
ds_read_b32 v44, v41 offset:0                      // load scaleAlpha
v_add_lshl_u32 v38, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v179, v38, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v39, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v39, v179, v39, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v46, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v179, v46, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v50, v46, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v48, v4, s68
v_lshlrev_b32 v48, 0x2, v48                        // Bias address scaled by BPE
ds_read_b32 v51, v48 offset:0                      // load Bias
v_add_u32 v49, 1024, v48                           // add ScaleAlphaVec offset (3)
ds_read_b32 v52, v49 offset:0                      // load scaleAlpha
v_add_lshl_u32 v46, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v179, v46, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v47, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v47, v179, v47, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v54, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v179, v54, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v58, v54, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v56, v0, s68
v_lshlrev_b32 v56, 0x2, v56                        // Bias address scaled by BPE
ds_read_b32 v59, v56 offset:0                      // load Bias
v_add_u32 v57, 1024, v56                           // add ScaleAlphaVec offset (3)
ds_read_b32 v60, v57 offset:0                      // load scaleAlpha
v_add_lshl_u32 v54, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v179, v54, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v55, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v179, v55, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v62, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v179, v62, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v66, v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v64, v4, s68
v_lshlrev_b32 v64, 0x2, v64                        // Bias address scaled by BPE
ds_read_b32 v67, v64 offset:0                      // load Bias
v_add_u32 v65, 1024, v64                           // add ScaleAlphaVec offset (3)
ds_read_b32 v68, v65 offset:0                      // load scaleAlpha
v_add_lshl_u32 v62, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v179, v62, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v63, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v63, v179, v63, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v70, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v70, v179, v70, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v74, v70, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v4, s68
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_add_u32 v73, 1024, v72                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v70, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v70, v179, v70, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v71, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v179, v71, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v179, v76, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v80, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v78, v4, s68
v_lshlrev_b32 v78, 0x2, v78                        // Bias address scaled by BPE
v_add_u32 v79, 1024, v78                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v76, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v179, v76, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v179, v77, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v82, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v82, v179, v82, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v86, v82, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v84, v4, s68
v_lshlrev_b32 v84, 0x2, v84                        // Bias address scaled by BPE
v_add_u32 v85, 1024, v84                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v82, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v82, v179, v82, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v83, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v179, v83, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v88, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v88, v179, v88, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v92, v88, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v90, v4, s68
v_lshlrev_b32 v90, 0x2, v90                        // Bias address scaled by BPE
v_add_u32 v91, 1024, v90                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v88, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v88, v179, v88, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v89, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v89, v179, v89, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v94, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v94, v179, v94, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v98, v94, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v96, v4, s68
v_lshlrev_b32 v96, 0x2, v96                        // Bias address scaled by BPE
v_add_u32 v97, 1024, v96                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v94, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v94, v179, v94, s[72:73]             // LDD clip if OOB. offset
v_add_lshl_u32 v95, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v95, v179, v95, s[72:73]             // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v100, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v100, v179, v100, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v104, v100, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v102, v4, s68
v_lshlrev_b32 v102, 0x2, v102                      // Bias address scaled by BPE
v_add_u32 v103, 1024, v102                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v100, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v100, v179, v100, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v101, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v101, v179, v101, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v106, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v179, v106, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v110, v106, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v108, v0, s68
v_lshlrev_b32 v108, 0x2, v108                      // Bias address scaled by BPE
v_add_u32 v109, 1024, v108                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v106, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v179, v106, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v107, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v107, v179, v107, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v112, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v112, v179, v112, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v116, v112, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v114, v4, s68
v_lshlrev_b32 v114, 0x2, v114                      // Bias address scaled by BPE
v_add_u32 v115, 1024, v114                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v112, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v112, v179, v112, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v113, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v113, v179, v113, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v118, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v179, v118, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v122, v118, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v120, v4, s68
v_lshlrev_b32 v120, 0x2, v120                      // Bias address scaled by BPE
v_add_u32 v121, 1024, v120                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v179, v118, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v119, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v119, v179, v119, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v124, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v124, v179, v124, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v128, v124, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v126, v4, s68
v_lshlrev_b32 v126, 0x2, v126                      // Bias address scaled by BPE
v_add_u32 v127, 1024, v126                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v124, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v124, v179, v124, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v125, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v125, v179, v125, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v130, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v130, v179, v130, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v134, v130, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v132, v4, s68
v_lshlrev_b32 v132, 0x2, v132                      // Bias address scaled by BPE
v_add_u32 v133, 1024, v132                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v130, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v130, v179, v130, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v131, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v131, v179, v131, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v136, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v136, v179, v136, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v141, v136, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v138, v4, s68
v_lshlrev_b32 v138, 0x2, v138                      // Bias address scaled by BPE
v_add_u32 v140, 1024, v138                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v136, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v136, v179, v136, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v137, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v137, v179, v137, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v143, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v143, v179, v143, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v147, v143, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v145, v4, s68
v_lshlrev_b32 v145, 0x2, v145                      // Bias address scaled by BPE
v_add_u32 v146, 1024, v145                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v143, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v143, v179, v143, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v144, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v144, v179, v144, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v149, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v179, v149, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v153, v149, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v151, v4, s68
v_lshlrev_b32 v151, 0x2, v151                      // Bias address scaled by BPE
v_add_u32 v152, 1024, v151                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v149, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v179, v149, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v150, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v150, v179, v150, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v155, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v155, v179, v155, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v159, v155, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v157, v0, s68
v_lshlrev_b32 v157, 0x2, v157                      // Bias address scaled by BPE
v_add_u32 v158, 1024, v157                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v155, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v155, v179, v155, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v156, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v156, v179, v156, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v161, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v161, v179, v161, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v165, v161, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v163, v4, s68
v_lshlrev_b32 v163, 0x2, v163                      // Bias address scaled by BPE
v_add_u32 v164, 1024, v163                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v161, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v161, v179, v161, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v162, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v162, v179, v162, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v167, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v167, v179, v167, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v171, v167, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v169, v4, s68
v_lshlrev_b32 v169, 0x2, v169                      // Bias address scaled by BPE
v_add_u32 v170, 1024, v169                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v167, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v167, v179, v167, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v168, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v168, v179, v168, s[72:73]           // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v173, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v173, v179, v173, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v177, v173, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v175, v4, s68
v_lshlrev_b32 v175, 0x2, v175                      // Bias address scaled by BPE
v_add_u32 v176, 1024, v175                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v173, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v173, v179, v173, s[72:73]           // LDD clip if OOB. offset
v_add_lshl_u32 v174, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v174, v179, v174, s[72:73]           // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+13], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+21], acc27          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+29], acc28          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+37], acc29          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+45], acc30          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+53], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+61], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+69], acc33          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+75], acc34          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+81], acc35          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+87], acc36          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+93], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+99], acc38          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+105], acc39         // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+111], acc40         // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+117], acc41         // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+123], acc42         // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+129], acc43         // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+135], acc44         // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+142], acc45         // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+148], acc46         // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+154], acc47         // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+160], acc48         // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+166], acc49         // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+172], acc50         // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+178], acc51         // copy acc to vreg[51]

/* store after Acc, GSU: 2 */

/* store after Acc, GSU: 2 */

buffer_store_dword v13, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 13

buffer_store_dword v21, v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 21

buffer_store_dword v29, v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 29

buffer_store_dword v37, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 37

buffer_store_dword v45, v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 45

buffer_store_dword v53, v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 53

buffer_store_dword v61, v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 61

buffer_store_dword v69, v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 69

buffer_store_dword v75, v70, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 75

buffer_store_dword v81, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 81

buffer_store_dword v87, v82, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 87

buffer_store_dword v93, v88, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 93

buffer_store_dword v99, v94, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 99

buffer_store_dword v105, v100, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 105

buffer_store_dword v111, v106, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 111

buffer_store_dword v117, v112, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 117

buffer_store_dword v123, v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 123

buffer_store_dword v129, v124, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 129

buffer_store_dword v135, v130, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 135

buffer_store_dword v142, v136, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 142

buffer_store_dword v148, v143, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 148

buffer_store_dword v154, v149, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 154

buffer_store_dword v160, v155, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 160

buffer_store_dword v166, v161, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 166

buffer_store_dword v172, v167, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 172

buffer_store_dword v178, v173, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 178
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//sourece store done, GSU:2

// check done start
// synchronizer offset cal
s_mul_i32 s13, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s11, s13, s[sgprWorkGroup2]
s_mul_i32 s12, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s12, s12, s[sgprWorkGroup0]
s_add_u32 s12, s12, s11
v_readfirstlane_b32 s11, v[vgprSerial]
s_mul_i32 s13, s13, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s11, s11, 0x6
s_mul_i32 s11, s13, s11                            // wave offset at batch
s_add_u32 s12, s11, s12
s_mul_i32 s13, s13, 4                              // cal a batch offset
s_mul_i32 s13, s13, 1                              // this batch offset
s_add_u32 s12, s12, s13
s_lshl_b32 s12, s12, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s12
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_and_b32 s11, s[sgprGSU], 0x3fff                  // Restore GSU
s_sub_u32 s11, s11, 0x1
s_atomic_dec s11, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s75, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s74, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s78, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s78, s78, 1                              // Free1
s_mul_hi_u32 s77, s78, s[sgprStrideC1J]            // Free1
s_mul_i32 s76, s78, s[sgprStrideC1J]               // Free1
s_add_u32 s74, s74, s76                            // Free1
s_addc_u32 s75, s75, s77                           // Free1
s_sub_u32 s78, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s78, s78, 1                              // Free2
s_mul_hi_u32 s77, s78, s[sgprStrideCK]             // Free2
s_mul_i32 s76, s78, s[sgprStrideCK]                // Free2
s_add_u32 s74, s74, s76                            // Free2
s_addc_u32 s75, s75, s77                           // Free2
s_lshl_b64 s[26:27], s[74:75], 2                   // scale by bpe

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s11, 0x1
s_cbranch_scc1 label_NoBranch_JH67858PX5L5EG5C_0   // Only branch on scc0
// long branch sync
s_getpc_b64 s[44:45]                               // addr of next instr
s_add_i32 s46, label_Sync_EDN_Beta_Edge_1, 0x4     // target branch offset
s_add_u32 s44, s44, s46                            // add target branch offset
s_addc_u32 s45, s45, 0                             // add high and carry
s_setpc_b64 s[44:45]                               // branch to label_Sync_EDN_Beta_Edge_1
label_NoBranch_JH67858PX5L5EG5C_0:
// check done end

// buffer load start
buffer_load_dword v13, v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 13
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_26 // SyncAddbranchhere
buffer_load_dword v184, v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_26:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v13, v13, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_26 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v6, v179, s[32:33]             // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_26      // Syncbranchhere

label_Synchronizer_read_add_end_1_26:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_26:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v21, v14, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 21
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_27 // SyncAddbranchhere
buffer_load_dword v184, v14, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_27:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v21, v21, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_27 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v14, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_27      // Syncbranchhere

label_Synchronizer_read_add_end_1_27:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_27:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v29, v22, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 29
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_28 // SyncAddbranchhere
buffer_load_dword v184, v22, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_28:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v29, v29, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_28 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v22, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_28      // Syncbranchhere

label_Synchronizer_read_add_end_1_28:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_28:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v37, v30, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 37
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_29 // SyncAddbranchhere
buffer_load_dword v184, v30, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_29:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v37, v37, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_29 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v30, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_29      // Syncbranchhere

label_Synchronizer_read_add_end_1_29:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_29:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v45, v38, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 45
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_30 // SyncAddbranchhere
buffer_load_dword v184, v38, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_30:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v45, v45, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_30 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v38, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_30      // Syncbranchhere

label_Synchronizer_read_add_end_1_30:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_30:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v53, v46, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 53
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_31 // SyncAddbranchhere
buffer_load_dword v184, v46, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_31:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v53, v53, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_31 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v46, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_31      // Syncbranchhere

label_Synchronizer_read_add_end_1_31:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_31:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v61, v54, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 61
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_32 // SyncAddbranchhere
buffer_load_dword v184, v54, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_32:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v61, v61, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_32 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v54, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_32      // Syncbranchhere

label_Synchronizer_read_add_end_1_32:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_32:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v69, v62, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 69
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_33 // SyncAddbranchhere
buffer_load_dword v184, v62, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_33:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v69, v69, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_33 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v62, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_33      // Syncbranchhere

label_Synchronizer_read_add_end_1_33:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_33:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v75, v70, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 75
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_34 // SyncAddbranchhere
buffer_load_dword v184, v70, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_34:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v75, v75, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_34 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v70, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_34      // Syncbranchhere

label_Synchronizer_read_add_end_1_34:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_34:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v81, v76, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 81
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_35 // SyncAddbranchhere
buffer_load_dword v184, v76, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_35:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v81, v81, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_35 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v76, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_35      // Syncbranchhere

label_Synchronizer_read_add_end_1_35:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_35:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v87, v82, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 87
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_36 // SyncAddbranchhere
buffer_load_dword v184, v82, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_36:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v87, v87, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_36 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v82, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_36      // Syncbranchhere

label_Synchronizer_read_add_end_1_36:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_36:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v93, v88, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 93
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_37 // SyncAddbranchhere
buffer_load_dword v184, v88, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_37:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v93, v93, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_37 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v88, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_37      // Syncbranchhere

label_Synchronizer_read_add_end_1_37:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_37:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v99, v94, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 99
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_38 // SyncAddbranchhere
buffer_load_dword v184, v94, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_38:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v99, v99, v184                           // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_38 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v94, v179, s[32:33]            // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_38      // Syncbranchhere

label_Synchronizer_read_add_end_1_38:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_38:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v105, v100, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 105
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_39 // SyncAddbranchhere
buffer_load_dword v184, v100, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_39:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v105, v105, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_39 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v100, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_39      // Syncbranchhere

label_Synchronizer_read_add_end_1_39:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_39:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v111, v106, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 111
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_40 // SyncAddbranchhere
buffer_load_dword v184, v106, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_40:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v111, v111, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_40 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v106, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_40      // Syncbranchhere

label_Synchronizer_read_add_end_1_40:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_40:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v117, v112, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 117
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_41 // SyncAddbranchhere
buffer_load_dword v184, v112, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_41:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v117, v117, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_41 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v112, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_41      // Syncbranchhere

label_Synchronizer_read_add_end_1_41:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_41:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v123, v118, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 123
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_42 // SyncAddbranchhere
buffer_load_dword v184, v118, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_42:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v123, v123, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_42 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v118, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_42      // Syncbranchhere

label_Synchronizer_read_add_end_1_42:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_42:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v129, v124, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 129
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_43 // SyncAddbranchhere
buffer_load_dword v184, v124, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_43:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v129, v129, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_43 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v124, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_43      // Syncbranchhere

label_Synchronizer_read_add_end_1_43:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_43:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v135, v130, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 135
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_44 // SyncAddbranchhere
buffer_load_dword v184, v130, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_44:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v135, v135, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_44 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v130, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_44      // Syncbranchhere

label_Synchronizer_read_add_end_1_44:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_44:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v142, v136, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 142
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_45 // SyncAddbranchhere
buffer_load_dword v184, v136, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_45:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v142, v142, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_45 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v136, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_45      // Syncbranchhere

label_Synchronizer_read_add_end_1_45:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_45:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v148, v143, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 148
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_46 // SyncAddbranchhere
buffer_load_dword v184, v143, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_46:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v148, v148, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_46 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v143, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_46      // Syncbranchhere

label_Synchronizer_read_add_end_1_46:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_46:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v154, v149, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 154
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_47 // SyncAddbranchhere
buffer_load_dword v184, v149, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_47:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v154, v154, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_47 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v149, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_47      // Syncbranchhere

label_Synchronizer_read_add_end_1_47:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_47:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v160, v155, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 160
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_48 // SyncAddbranchhere
buffer_load_dword v184, v155, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_48:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v160, v160, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_48 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v155, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_48      // Syncbranchhere

label_Synchronizer_read_add_end_1_48:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_48:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v166, v161, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 166
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_49 // SyncAddbranchhere
buffer_load_dword v184, v161, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_49:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v166, v166, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_49 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v161, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_49      // Syncbranchhere

label_Synchronizer_read_add_end_1_49:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_49:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v172, v167, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 172
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_50 // SyncAddbranchhere
buffer_load_dword v184, v167, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_50:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v172, v172, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_50 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v167, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_50      // Syncbranchhere

label_Synchronizer_read_add_end_1_50:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_50:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v179, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v178, v173, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 178
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_51 // SyncAddbranchhere
buffer_load_dword v184, v173, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_51:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v178, v178, v184                         // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_51 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v180, v173, v179, s[32:33]           // protect if OOB
buffer_load_dword v184, v180, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_51      // Syncbranchhere

label_Synchronizer_read_add_end_1_51:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_51:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 3, 2), (0, 0, 3, 3), (0, 1, 3, 0), (0, 1, 3, 1), (0, 1, 3, 2), (0, 1, 3, 3), (0, 0, 4, 0), (0, 0, 4, 1), (0, 0, 4, 2), (0, 0, 4, 3), (0, 1, 4, 0), (0, 1, 4, 1), (0, 1, 4, 2), (0, 1, 4, 3), (0, 0, 5, 0), (0, 0, 5, 1), (0, 0, 5, 2), (0, 0, 5, 3), (0, 1, 5, 0), (0, 1, 5, 1), (0, 1, 5, 2), (0, 1, 5, 3), (0, 0, 6, 0), (0, 0, 6, 1), (0, 0, 6, 2), (0, 0, 6, 3)] */
v_mul_f32 v[vgprValuC+13], s[sgprAlpha], v[vgprValuC+13] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+99], s[sgprAlpha], v[vgprValuC+99] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+123], s[sgprAlpha], v[vgprValuC+123] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+135], s[sgprAlpha], v[vgprValuC+135] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+148], s[sgprAlpha], v[vgprValuC+148] // *= alpha
v_mul_f32 v[vgprValuC+154], s[sgprAlpha], v[vgprValuC+154] // *= alpha
v_mul_f32 v[vgprValuC+160], s[sgprAlpha], v[vgprValuC+160] // *= alpha
v_mul_f32 v[vgprValuC+166], s[sgprAlpha], v[vgprValuC+166] // *= alpha
v_mul_f32 v[vgprValuC+172], s[sgprAlpha], v[vgprValuC+172] // *= alpha
v_mul_f32 v[vgprValuC+178], s[sgprAlpha], v[vgprValuC+178] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+13], v12, v[vgprValuC+13]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+13], s[sgprBeta], v10, v[vgprValuC+13] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+13], v11, v[vgprValuC+13]    // C += bias
v_cvt_f16_f32 v13, v[vgprValuC+13]                 // convert C to fp16
buffer_store_short v13, v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+21], v20, v[vgprValuC+21]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+21], s[sgprBeta], v18, v[vgprValuC+21] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+21], v19, v[vgprValuC+21]    // C += bias
v_cvt_f16_f32 v21, v[vgprValuC+21]                 // convert C to fp16
buffer_store_short v21, v15, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+29], v28, v[vgprValuC+29]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+29], s[sgprBeta], v26, v[vgprValuC+29] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+29], v27, v[vgprValuC+29]    // C += bias
v_cvt_f16_f32 v29, v[vgprValuC+29]                 // convert C to fp16
buffer_store_short v29, v23, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+37], v36, v[vgprValuC+37]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v34, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+37], v35, v[vgprValuC+37]    // C += bias
v_cvt_f16_f32 v37, v[vgprValuC+37]                 // convert C to fp16
buffer_store_short v37, v31, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+45], v44, v[vgprValuC+45]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+45], s[sgprBeta], v42, v[vgprValuC+45] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+45], v43, v[vgprValuC+45]    // C += bias
v_cvt_f16_f32 v45, v[vgprValuC+45]                 // convert C to fp16
buffer_store_short v45, v39, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+53], v52, v[vgprValuC+53]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+53], s[sgprBeta], v50, v[vgprValuC+53] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+53], v51, v[vgprValuC+53]    // C += bias
v_cvt_f16_f32 v53, v[vgprValuC+53]                 // convert C to fp16
buffer_store_short v53, v47, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+61], v60, v[vgprValuC+61]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v58, v[vgprValuC+61] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+61], v59, v[vgprValuC+61]    // C += bias
v_cvt_f16_f32 v61, v[vgprValuC+61]                 // convert C to fp16
buffer_store_short v61, v55, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+69], v68, v[vgprValuC+69]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+69], s[sgprBeta], v66, v[vgprValuC+69] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+69], v67, v[vgprValuC+69]    // C += bias
v_cvt_f16_f32 v69, v[vgprValuC+69]                 // convert C to fp16
buffer_store_short v69, v63, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+75], v12, v[vgprValuC+75]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v74, v[vgprValuC+75] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+75], v11, v[vgprValuC+75]    // C += bias
v_cvt_f16_f32 v75, v[vgprValuC+75]                 // convert C to fp16
buffer_store_short v75, v71, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+81], v20, v[vgprValuC+81]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+81], s[sgprBeta], v80, v[vgprValuC+81] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+81], v19, v[vgprValuC+81]    // C += bias
v_cvt_f16_f32 v81, v[vgprValuC+81]                 // convert C to fp16
buffer_store_short v81, v77, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+87], v28, v[vgprValuC+87]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+87], s[sgprBeta], v86, v[vgprValuC+87] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+87], v27, v[vgprValuC+87]    // C += bias
v_cvt_f16_f32 v87, v[vgprValuC+87]                 // convert C to fp16
buffer_store_short v87, v83, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+93], v36, v[vgprValuC+93]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+93], s[sgprBeta], v92, v[vgprValuC+93] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+93], v35, v[vgprValuC+93]    // C += bias
v_cvt_f16_f32 v93, v[vgprValuC+93]                 // convert C to fp16
buffer_store_short v93, v89, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+99], v44, v[vgprValuC+99]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+99], s[sgprBeta], v98, v[vgprValuC+99] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+99], v43, v[vgprValuC+99]    // C += bias
v_cvt_f16_f32 v99, v[vgprValuC+99]                 // convert C to fp16
buffer_store_short v99, v95, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+105], v52, v[vgprValuC+105]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+105], s[sgprBeta], v104, v[vgprValuC+105] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+105], v51, v[vgprValuC+105]  // C += bias
v_cvt_f16_f32 v105, v[vgprValuC+105]               // convert C to fp16
buffer_store_short v105, v101, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+111], v60, v[vgprValuC+111]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+111], s[sgprBeta], v110, v[vgprValuC+111] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+111], v59, v[vgprValuC+111]  // C += bias
v_cvt_f16_f32 v111, v[vgprValuC+111]               // convert C to fp16
buffer_store_short v111, v107, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+117], v68, v[vgprValuC+117]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v116, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+117], v67, v[vgprValuC+117]  // C += bias
v_cvt_f16_f32 v117, v[vgprValuC+117]               // convert C to fp16
buffer_store_short v117, v113, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+123], v12, v[vgprValuC+123]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+123], s[sgprBeta], v122, v[vgprValuC+123] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+123], v11, v[vgprValuC+123]  // C += bias
v_cvt_f16_f32 v123, v[vgprValuC+123]               // convert C to fp16
buffer_store_short v123, v119, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+129], v20, v[vgprValuC+129]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+129], s[sgprBeta], v128, v[vgprValuC+129] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+129], v19, v[vgprValuC+129]  // C += bias
v_cvt_f16_f32 v129, v[vgprValuC+129]               // convert C to fp16
buffer_store_short v129, v125, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+135], v28, v[vgprValuC+135]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+135], s[sgprBeta], v134, v[vgprValuC+135] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+135], v27, v[vgprValuC+135]  // C += bias
v_cvt_f16_f32 v135, v[vgprValuC+135]               // convert C to fp16
buffer_store_short v135, v131, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+142], v36, v[vgprValuC+142]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+142], s[sgprBeta], v141, v[vgprValuC+142] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+142], v35, v[vgprValuC+142]  // C += bias
v_cvt_f16_f32 v142, v[vgprValuC+142]               // convert C to fp16
buffer_store_short v142, v137, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+148], v44, v[vgprValuC+148]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+148], s[sgprBeta], v147, v[vgprValuC+148] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+148], v43, v[vgprValuC+148]  // C += bias
v_cvt_f16_f32 v148, v[vgprValuC+148]               // convert C to fp16
buffer_store_short v148, v144, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+154], v52, v[vgprValuC+154]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+154], s[sgprBeta], v153, v[vgprValuC+154] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+154], v51, v[vgprValuC+154]  // C += bias
v_cvt_f16_f32 v154, v[vgprValuC+154]               // convert C to fp16
buffer_store_short v154, v150, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+160], v60, v[vgprValuC+160]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+160], s[sgprBeta], v159, v[vgprValuC+160] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+160], v59, v[vgprValuC+160]  // C += bias
v_cvt_f16_f32 v160, v[vgprValuC+160]               // convert C to fp16
buffer_store_short v160, v156, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+166], v68, v[vgprValuC+166]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+166], s[sgprBeta], v165, v[vgprValuC+166] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+166], v67, v[vgprValuC+166]  // C += bias
v_cvt_f16_f32 v166, v[vgprValuC+166]               // convert C to fp16
buffer_store_short v166, v162, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+172], v12, v[vgprValuC+172]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+172], s[sgprBeta], v171, v[vgprValuC+172] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+172], v11, v[vgprValuC+172]  // C += bias
v_cvt_f16_f32 v172, v[vgprValuC+172]               // convert C to fp16
buffer_store_short v172, v168, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+178], v20, v[vgprValuC+178]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+178], s[sgprBeta], v177, v[vgprValuC+178] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+178], v19, v[vgprValuC+178]  // C += bias
v_cvt_f16_f32 v178, v[vgprValuC+178]               // convert C to fp16
buffer_store_short v178, v174, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
label_Sync_EDN_Beta_Edge_1:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//GW end
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #2 (d1,d0,vc1,vc0) = */
/*    (0,1,6,0:vw1); (0,1,6,1:vw1); (0,1,6,2:vw1); (0,1,6,3:vw1); (0,0,7,0:vw1); (0,0,7,1:vw1); (0,0,7,2:vw1); (0,0,7,3:vw1); (0,1,7,0:vw1); (0,1,7,1:vw1); (0,1,7,2:vw1); (0,1,7,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v94, BufferOOB
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v6, v2, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v94, v6, s[72:73]                // LDC clip if OOB. offset
buffer_load_short_d16 v10, v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v4, s68
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
ds_read_b32 v11, v8 offset:0                       // load Bias
v_add_u32 v9, 1024, v8                             // add ScaleAlphaVec offset (3)
ds_read_b32 v12, v9 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v4, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v94, v6, s[72:73]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v94, v7, s[72:73]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v14, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v94, v14, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16_hi v18, v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v16, v4, s68
v_lshlrev_b32 v16, 0x2, v16                        // Bias address scaled by BPE
ds_read_b32 v19, v16 offset:0                      // load Bias
v_add_u32 v17, 1024, v16                           // add ScaleAlphaVec offset (3)
ds_read_b32 v20, v17 offset:0                      // load scaleAlpha
v_add_lshl_u32 v14, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v94, v14, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v15, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v15, v94, v15, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v22, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v94, v22, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16 v26, v22, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v4, s68
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
ds_read_b32 v27, v24 offset:0                      // load Bias
v_add_u32 v25, 1024, v24                           // add ScaleAlphaVec offset (3)
ds_read_b32 v28, v25 offset:0                      // load scaleAlpha
v_add_lshl_u32 v22, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v94, v22, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v23, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v23, v94, v23, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v30, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v94, v30, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16_hi v34, v30, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v4, s68
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
ds_read_b32 v35, v32 offset:0                      // load Bias
v_add_u32 v33, 1024, v32                           // add ScaleAlphaVec offset (3)
ds_read_b32 v36, v33 offset:0                      // load scaleAlpha
v_add_lshl_u32 v30, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v94, v30, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v31, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v94, v31, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v38, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v94, v38, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16 v42, v38, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s68
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
ds_read_b32 v43, v40 offset:0                      // load Bias
v_add_u32 v41, 1024, v40                           // add ScaleAlphaVec offset (3)
ds_read_b32 v44, v41 offset:0                      // load scaleAlpha
v_add_lshl_u32 v38, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v94, v38, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v39, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v39, v94, v39, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v46, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v94, v46, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16_hi v50, v46, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v48, v4, s68
v_lshlrev_b32 v48, 0x2, v48                        // Bias address scaled by BPE
ds_read_b32 v51, v48 offset:0                      // load Bias
v_add_u32 v49, 1024, v48                           // add ScaleAlphaVec offset (3)
ds_read_b32 v52, v49 offset:0                      // load scaleAlpha
v_add_lshl_u32 v46, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v94, v46, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v47, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v47, v94, v47, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v54, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v94, v54, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16 v58, v54, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v56, v4, s68
v_lshlrev_b32 v56, 0x2, v56                        // Bias address scaled by BPE
ds_read_b32 v59, v56 offset:0                      // load Bias
v_add_u32 v57, 1024, v56                           // add ScaleAlphaVec offset (3)
ds_read_b32 v60, v57 offset:0                      // load scaleAlpha
v_add_lshl_u32 v54, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v94, v54, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v55, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v94, v55, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v62, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v94, v62, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16_hi v66, v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v64, v4, s68
v_lshlrev_b32 v64, 0x2, v64                        // Bias address scaled by BPE
ds_read_b32 v67, v64 offset:0                      // load Bias
v_add_u32 v65, 1024, v64                           // add ScaleAlphaVec offset (3)
ds_read_b32 v68, v65 offset:0                      // load scaleAlpha
v_add_lshl_u32 v62, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v94, v62, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v63, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v63, v94, v63, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v70, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v70, v94, v70, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16 v74, v70, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v4, s68
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_add_u32 v73, 1024, v72                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v70, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v70, v94, v70, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v71, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v94, v71, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v94, v76, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16_hi v80, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v78, v4, s68
v_lshlrev_b32 v78, 0x2, v78                        // Bias address scaled by BPE
v_add_u32 v79, 1024, v78                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v76, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v94, v76, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v94, v77, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v82, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v82, v94, v82, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16 v86, v82, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v84, v4, s68
v_lshlrev_b32 v84, 0x2, v84                        // Bias address scaled by BPE
v_add_u32 v85, 1024, v84                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v82, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v82, v94, v82, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v83, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v94, v83, s[72:73]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1

/* edge Protect */
v_add_lshl_u32 v88, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v88, v94, v88, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16_hi v92, v88, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v90, v4, s68
v_lshlrev_b32 v90, 0x2, v90                        // Bias address scaled by BPE
v_add_u32 v91, 1024, v90                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v88, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v88, v94, v88, s[72:73]              // LDD clip if OOB. offset
v_add_lshl_u32 v89, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v89, v94, v89, s[72:73]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+13], acc52          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+21], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+29], acc54          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+37], acc55          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+45], acc56          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+53], acc57          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+61], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+69], acc59          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+75], acc60          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+81], acc61          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+87], acc62          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+93], acc63          // copy acc to vreg[63]

/* store after Acc, GSU: 2 */

/* store after Acc, GSU: 2 */

buffer_store_dword v13, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 13

buffer_store_dword v21, v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 21

buffer_store_dword v29, v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 29

buffer_store_dword v37, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 37

buffer_store_dword v45, v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 45

buffer_store_dword v53, v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 53

buffer_store_dword v61, v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 61

buffer_store_dword v69, v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 69

buffer_store_dword v75, v70, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 75

buffer_store_dword v81, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 81

buffer_store_dword v87, v82, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 87

buffer_store_dword v93, v88, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 93
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//sourece store done, GSU:2

// check done start
// synchronizer offset cal
s_mul_i32 s13, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s11, s13, s[sgprWorkGroup2]
s_mul_i32 s12, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s12, s12, s[sgprWorkGroup0]
s_add_u32 s12, s12, s11
v_readfirstlane_b32 s11, v[vgprSerial]
s_mul_i32 s13, s13, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s11, s11, 0x6
s_mul_i32 s11, s13, s11                            // wave offset at batch
s_add_u32 s12, s11, s12
s_mul_i32 s13, s13, 4                              // cal a batch offset
s_mul_i32 s13, s13, 2                              // this batch offset
s_add_u32 s12, s12, s13
s_lshl_b32 s12, s12, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s12
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_and_b32 s11, s[sgprGSU], 0x3fff                  // Restore GSU
s_sub_u32 s11, s11, 0x1
s_atomic_dec s11, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s75, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s74, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s78, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s78, s78, 1                              // Free1
s_mul_hi_u32 s77, s78, s[sgprStrideC1J]            // Free1
s_mul_i32 s76, s78, s[sgprStrideC1J]               // Free1
s_add_u32 s74, s74, s76                            // Free1
s_addc_u32 s75, s75, s77                           // Free1
s_sub_u32 s78, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s78, s78, 1                              // Free2
s_mul_hi_u32 s77, s78, s[sgprStrideCK]             // Free2
s_mul_i32 s76, s78, s[sgprStrideCK]                // Free2
s_add_u32 s74, s74, s76                            // Free2
s_addc_u32 s75, s75, s77                           // Free2
s_lshl_b64 s[26:27], s[74:75], 2                   // scale by bpe

v_mov_b32 v94, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s11, 0x1
s_cbranch_scc1 label_NoBranch_Q190VQB7029OUKKC_0   // Only branch on scc0
// long branch sync
s_getpc_b64 s[44:45]                               // addr of next instr
s_add_i32 s46, label_Sync_EDN_Beta_Edge_2, 0x4     // target branch offset
s_add_u32 s44, s44, s46                            // add target branch offset
s_addc_u32 s45, s45, 0                             // add high and carry
s_setpc_b64 s[44:45]                               // branch to label_Sync_EDN_Beta_Edge_2
label_NoBranch_Q190VQB7029OUKKC_0:
// check done end

// buffer load start
buffer_load_dword v13, v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 13
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_52 // SyncAddbranchhere
buffer_load_dword v96, v6, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_52:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v13, v13, v96                            // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_52 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v95, v6, v94, s[32:33]               // protect if OOB
buffer_load_dword v96, v95, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_52      // Syncbranchhere

label_Synchronizer_read_add_end_1_52:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_52:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v94, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v21, v14, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 21
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_53 // SyncAddbranchhere
buffer_load_dword v96, v14, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_53:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v21, v21, v96                            // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_53 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v95, v14, v94, s[32:33]              // protect if OOB
buffer_load_dword v96, v95, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_53      // Syncbranchhere

label_Synchronizer_read_add_end_1_53:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_53:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v94, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v29, v22, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 29
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_54 // SyncAddbranchhere
buffer_load_dword v96, v22, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_54:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v29, v29, v96                            // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_54 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v95, v22, v94, s[32:33]              // protect if OOB
buffer_load_dword v96, v95, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_54      // Syncbranchhere

label_Synchronizer_read_add_end_1_54:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_54:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v94, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v37, v30, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 37
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_55 // SyncAddbranchhere
buffer_load_dword v96, v30, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_55:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v37, v37, v96                            // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_55 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v95, v30, v94, s[32:33]              // protect if OOB
buffer_load_dword v96, v95, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_55      // Syncbranchhere

label_Synchronizer_read_add_end_1_55:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_55:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v94, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v45, v38, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 45
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_56 // SyncAddbranchhere
buffer_load_dword v96, v38, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_56:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v45, v45, v96                            // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_56 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v95, v38, v94, s[32:33]              // protect if OOB
buffer_load_dword v96, v95, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_56      // Syncbranchhere

label_Synchronizer_read_add_end_1_56:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_56:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v94, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v53, v46, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 53
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_57 // SyncAddbranchhere
buffer_load_dword v96, v46, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_57:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v53, v53, v96                            // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_57 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v95, v46, v94, s[32:33]              // protect if OOB
buffer_load_dword v96, v95, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_57      // Syncbranchhere

label_Synchronizer_read_add_end_1_57:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_57:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v94, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v61, v54, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 61
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_58 // SyncAddbranchhere
buffer_load_dword v96, v54, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_58:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v61, v61, v96                            // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_58 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v95, v54, v94, s[32:33]              // protect if OOB
buffer_load_dword v96, v95, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_58      // Syncbranchhere

label_Synchronizer_read_add_end_1_58:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_58:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v94, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v69, v62, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 69
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_59 // SyncAddbranchhere
buffer_load_dword v96, v62, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_59:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v69, v69, v96                            // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_59 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v95, v62, v94, s[32:33]              // protect if OOB
buffer_load_dword v96, v95, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_59      // Syncbranchhere

label_Synchronizer_read_add_end_1_59:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_59:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v94, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v75, v70, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 75
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_60 // SyncAddbranchhere
buffer_load_dword v96, v70, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_60:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v75, v75, v96                            // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_60 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v95, v70, v94, s[32:33]              // protect if OOB
buffer_load_dword v96, v95, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_60      // Syncbranchhere

label_Synchronizer_read_add_end_1_60:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_60:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v94, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v81, v76, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 81
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_61 // SyncAddbranchhere
buffer_load_dword v96, v76, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_61:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v81, v81, v96                            // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_61 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v95, v76, v94, s[32:33]              // protect if OOB
buffer_load_dword v96, v95, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_61      // Syncbranchhere

label_Synchronizer_read_add_end_1_61:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_61:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v94, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v87, v82, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 87
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_62 // SyncAddbranchhere
buffer_load_dword v96, v82, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_62:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v87, v87, v96                            // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_62 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v95, v82, v94, s[32:33]              // protect if OOB
buffer_load_dword v96, v95, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_62      // Syncbranchhere

label_Synchronizer_read_add_end_1_62:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_62:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v94, BufferOOB
s_mov_b32 s44, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s45, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s46, s[sgprSrdD+2]
s_mov_b32 s47, s[sgprSrdD+3]
buffer_load_dword v93, v88, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU D 0 93
s_and_b32 s[sgprGSUSync], s[sgprGSU], 0x3fff       // Restore GSU
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_63 // SyncAddbranchhere
buffer_load_dword v96, v88, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4 1 0
// buffer load end

// buffer add start
label_Synchronizer_read_add_63:  /// Synchronizer read add

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_add_f32 v93, v93, v96                            // buffer add
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_skip_63 // SyncAddbranch
s_add_u32 s44, s44, s26
s_addc_u32 s45, s45, s27
v_cmp_ge_i32 s[32:33], 0, s[sgprGSUSync]
v_cndmask_b32 v95, v88, v94, s[32:33]              // protect if OOB
buffer_load_dword v96, v95, s[44:47], 0 offen offset:0, sc0 sc1 // load GSU DD 4
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], 0x0
s_cbranch_scc1 label_Synchronizer_read_add_63      // Syncbranchhere

label_Synchronizer_read_add_end_1_63:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_63:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 1, 6, 0), (0, 1, 6, 1), (0, 1, 6, 2), (0, 1, 6, 3), (0, 0, 7, 0), (0, 0, 7, 1), (0, 0, 7, 2), (0, 0, 7, 3), (0, 1, 7, 0), (0, 1, 7, 1), (0, 1, 7, 2), (0, 1, 7, 3)] */
v_mul_f32 v[vgprValuC+13], s[sgprAlpha], v[vgprValuC+13] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+13], v12, v[vgprValuC+13]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+13], s[sgprBeta], v10, v[vgprValuC+13] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+13], v11, v[vgprValuC+13]    // C += bias
v_cvt_f16_f32 v13, v[vgprValuC+13]                 // convert C to fp16
buffer_store_short v13, v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+21], v20, v[vgprValuC+21]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+21], s[sgprBeta], v18, v[vgprValuC+21] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+21], v19, v[vgprValuC+21]    // C += bias
v_cvt_f16_f32 v21, v[vgprValuC+21]                 // convert C to fp16
buffer_store_short v21, v15, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+29], v28, v[vgprValuC+29]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+29], s[sgprBeta], v26, v[vgprValuC+29] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+29], v27, v[vgprValuC+29]    // C += bias
v_cvt_f16_f32 v29, v[vgprValuC+29]                 // convert C to fp16
buffer_store_short v29, v23, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+37], v36, v[vgprValuC+37]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v34, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+37], v35, v[vgprValuC+37]    // C += bias
v_cvt_f16_f32 v37, v[vgprValuC+37]                 // convert C to fp16
buffer_store_short v37, v31, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+45], v44, v[vgprValuC+45]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+45], s[sgprBeta], v42, v[vgprValuC+45] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+45], v43, v[vgprValuC+45]    // C += bias
v_cvt_f16_f32 v45, v[vgprValuC+45]                 // convert C to fp16
buffer_store_short v45, v39, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+53], v52, v[vgprValuC+53]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+53], s[sgprBeta], v50, v[vgprValuC+53] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+53], v51, v[vgprValuC+53]    // C += bias
v_cvt_f16_f32 v53, v[vgprValuC+53]                 // convert C to fp16
buffer_store_short v53, v47, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+61], v60, v[vgprValuC+61]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v58, v[vgprValuC+61] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+61], v59, v[vgprValuC+61]    // C += bias
v_cvt_f16_f32 v61, v[vgprValuC+61]                 // convert C to fp16
buffer_store_short v61, v55, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+69], v68, v[vgprValuC+69]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+69], s[sgprBeta], v66, v[vgprValuC+69] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+69], v67, v[vgprValuC+69]    // C += bias
v_cvt_f16_f32 v69, v[vgprValuC+69]                 // convert C to fp16
buffer_store_short v69, v63, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+75], v12, v[vgprValuC+75]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v74, v[vgprValuC+75] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+75], v11, v[vgprValuC+75]    // C += bias
v_cvt_f16_f32 v75, v[vgprValuC+75]                 // convert C to fp16
buffer_store_short v75, v71, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+81], v20, v[vgprValuC+81]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+81], s[sgprBeta], v80, v[vgprValuC+81] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+81], v19, v[vgprValuC+81]    // C += bias
v_cvt_f16_f32 v81, v[vgprValuC+81]                 // convert C to fp16
buffer_store_short v81, v77, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+87], v28, v[vgprValuC+87]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+87], s[sgprBeta], v86, v[vgprValuC+87] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+87], v27, v[vgprValuC+87]    // C += bias
v_cvt_f16_f32 v87, v[vgprValuC+87]                 // convert C to fp16
buffer_store_short v87, v83, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
v_mul_f32 v[vgprValuC+93], v36, v[vgprValuC+93]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+93], s[sgprBeta], v92, v[vgprValuC+93] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+93], v35, v[vgprValuC+93]    // C += bias
v_cvt_f16_f32 v93, v[vgprValuC+93]                 // convert C to fp16
buffer_store_short v93, v89, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
label_Sync_EDN_Beta_Edge_2:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//GW end
s_branch label_GW_End_1                            // jump to end
label_GW_End_1:
s_getpc_b64 s[44:45]                               // addr of next instr
s_add_i32 s46, label_KernelEnd, 0x4                // target branch offset
s_add_u32 s44, s44, s46                            // add target branch offset
s_addc_u32 s45, s45, 0                             // add high and carry
s_setpc_b64 s[44:45]                               // branch to label_KernelEnd
label_GSU_5:
.set sgprAddressScaleAlphaVec, 32
.set sgprSrdScaleAlphaVec, 44
s_mov_b32 s[sgprSrdScaleAlphaVec+0], s[sgprAddressScaleAlphaVec+0] // init SRD base address (lower)
s_mov_b32 s[sgprSrdScaleAlphaVec+1], s[sgprAddressScaleAlphaVec+1] // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdScaleAlphaVec+3], Srd127_96     // Set bits 127_96 in post-loop SRD
s_cmp_eq_u64 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], 0 // s[AddressScaleAlphaVec] == 0 ?
s_cbranch_scc0 label_ScaleAlphaVec_2AddrValid      // branch if s[AddressScaleAlphaVec] != 0
s_mov_b32 s[sgprSrdScaleAlphaVec+2], 0
s_branch label_ScaleAlphaVec_2AddrValid_End
label_ScaleAlphaVec_2AddrValid:
s_mov_b32 s[sgprSrdScaleAlphaVec+2], s[sgprSizeI]
label_ScaleAlphaVec_2AddrValid_End:

s_mul_i32 s[sgprSrdScaleAlphaVec+2], 0x4, s[sgprSrdScaleAlphaVec+2] // ScaleAlphaVec scaled by BPE
s_add_u32 s11, s[sgprWorkGroup2], 0x1
s_mul_i32 s11, s[sgprBiasStride], s11              // stride * (wg+1)
s_cmp_eq_u32 s11, 0x0                              // bias stride = 0?
s_cselect_b32 s11, s[sgprSizeI], s11
s_mov_b32 s[sgprSrdBias+0], s[sgprAddressBias+0]   // init SRD base address (lower)
s_mov_b32 s[sgprSrdBias+1], s[sgprAddressBias+1]   // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdBias+3], Srd127_96              // Set bits 127_96 in post-loop SRD
s_cmp_eq_u64 s[sgprAddressBias:sgprAddressBias+1], 0 // s[AddressBias] == 0 ?
s_cbranch_scc0 label_Bias_2AddrValid               // branch if s[AddressBias] != 0
s_mov_b32 s[sgprSrdBias+2], 0
s_branch label_Bias_2AddrValid_End
label_Bias_2AddrValid:
s_mov_b32 s[sgprSrdBias+2], s11
label_Bias_2AddrValid_End:

label_Load_Biasf32_0_2:
s_cmpk_lg_u32 s[sgprBiasType], 0                   // BiasType != 0
s_cbranch_scc1 label_Load_Biasf16_0_2              // Branch if true

/******************************************/
/* Read vector to LDS                     */
/******************************************/
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_add_u32 v8, s68, v[vgprSerial]                   // coord 0 = wgp0 * MT0 + thread offset
s_mul_i32 s[sgprSrdBias+2], 0x4, s[sgprSrdBias+2]  // scaled by BPE
s_mul_i32 s68, s[sgprBiasStride], s[sgprWorkGroup2] // Stride * WG
v_add_u32 v6, s68, v8                              // coord 0 = wgp0 * MT0 + thread offset + Stride * WG
v_lshlrev_b32 v6, 0x2, v6                          // Global bias address scaled by BPE
v_lshlrev_b32 v7, 0x2, v8                          // Global scaleAlpha address scaled by BPE
s_mul_i32 s68, 128, s[sgprWorkGroup1]              // wgp1 * MT1
v_add_u32 v8, s68, v[vgprSerial]                   // coord 1 = wgp1 * MT1 + thread offset
buffer_load_dword v4, v6, s[sgprSrdBias:sgprSrdBias+3], 0 offen offset:0 // Load Bias
buffer_load_dword v5, v7, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // Load ScaleAlphaVec
v_lshlrev_b32 v8, 0x2, v[vgprSerial]               // Local address scaled by BPE
s_barrier                                          // wait for all global loads.
s_waitcnt vmcnt(1)                                 // wait for global load
ds_write_b32 v8, v4 offset:0                       // store bias
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
s_waitcnt vmcnt(0)                                 // wait for global load
v_cndmask_b32 v5, 1.0, v5, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
ds_write_b32 v8, v5 offset:1024                    // store scaleAlpha
s_branch label_Load_Bias_End_2                     // Branch to load bias end
label_Load_Biasf16_0_2:
s_cmpk_lg_u32 s[sgprBiasType], 4                   // BiasType != 4
s_cbranch_scc1 label_Load_Bias_End_2               // Branch if true

/******************************************/
/* Read vector to LDS                     */
/******************************************/
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_add_u32 v8, s68, v[vgprSerial]                   // coord 0 = wgp0 * MT0 + thread offset
s_mul_i32 s[sgprSrdBias+2], 0x2, s[sgprSrdBias+2]  // scaled by BPE
s_mul_i32 s68, s[sgprBiasStride], s[sgprWorkGroup2] // Stride * WG
v_add_u32 v6, s68, v8                              // coord 0 = wgp0 * MT0 + thread offset + Stride * WG
v_lshlrev_b32 v6, 0x1, v6                          // Global bias address scaled by BPE
v_lshlrev_b32 v7, 0x2, v8                          // Global scaleAlpha address scaled by BPE
s_mul_i32 s68, 128, s[sgprWorkGroup1]              // wgp1 * MT1
v_add_u32 v8, s68, v[vgprSerial]                   // coord 1 = wgp1 * MT1 + thread offset
buffer_load_short_d16 v4, v6, s[sgprSrdBias:sgprSrdBias+3], 0 offen offset:0 // Load Bias
buffer_load_dword v5, v7, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // Load ScaleAlphaVec
v_lshlrev_b32 v8, 0x2, v[vgprSerial]               // Local address scaled by BPE
s_barrier                                          // wait for all global loads.
s_waitcnt vmcnt(1)                                 // wait for global load
v_cvt_f32_f16 v4, v4                               // convert to FP32
ds_write_b32 v8, v4 offset:0                       // store bias
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
s_waitcnt vmcnt(0)                                 // wait for global load
v_cndmask_b32 v5, 1.0, v5, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
ds_write_b32 v8, v5 offset:1024                    // store scaleAlpha
s_branch label_Load_Bias_End_2                     // Branch to load bias end
label_Load_Bias_End_2:
.set sgprAddressScaleAlphaVec, UNDEF
.set sgprSrdScaleAlphaVec, UNDEF
s_cmpk_eq_u32 s[sgprBeta], 0x0                     // Beta == 0
s_cbranch_scc0 label_GW_Beta_2                     // Branch if Beta is not zero

s_and_b32 s44, 127, s[sgprSizeI]                   // s44 = s[sgprSizeI] % 128
s_add_u32 s45, -0x1, s[sgprNumWorkGroups0]
s_cmp_ge_u32 s[sgprWorkGroup0], s45                // wg0 >= nwg0-1 ?
s_cselect_b32 s44, s44, 0                          // set rMT0
s_cmpk_gt_u32 s44, 0x0                             // rMT0 > 0
s_cbranch_scc1 label_GW_B0_E1_M_1                  // jump if edges required
s_and_b32 s44, 127, s[sgprSizeJ]                   // s44 = s[sgprSizeJ] % 128
s_add_u32 s45, -0x1, s[sgprNumWorkGroups1]
s_cmp_ge_u32 s[sgprWorkGroup1], s45                // wg1 >= nwg1-1
s_cselect_b32 s44, s44, 0                          // set rMT1
s_cmpk_gt_u32 s44, 0x0                             // rMT1 > 0
s_cbranch_scc1 label_GW_B0_E1_N_1                  // jump if edges required
label_GW_B0_E0_2:

/* edge=0, allocate 2 sgpr. perBatchTmpS=2 perBatchMaskS=0 perElementMaskS=0 elementsPerBatch=14 */
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,1,0,0:vw4); (0,0,1,0:vw4); (0,1,1,0:vw4); (0,0,2,0:vw4); (0,1,2,0:vw4); (0,0,3,0:vw4); (0,1,3,0:vw4); (0,0,4,0:vw4); (0,1,4,0:vw4); (0,0,5,0:vw4); (0,1,5,0:vw4); (0,0,6,0:vw4); (0,1,6,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s12, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s12
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for LDS write
s_barrier                                          // LDS write barrier
ds_read_b128 v[12:15], v8 offset:0                 // load Bias
v_add_u32 v9, 1024, v8                             // add ScaleAlphaVec offset (1)
ds_read_b128 v[16:19], v9 offset:0                 // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
ds_read_b128 v[24:27], v8 offset:256               // load Bias
ds_read_b128 v[28:31], v9 offset:256               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
v_add_lshl_u32 v6, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc1           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+40], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+41], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+42], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+43], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+44], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+45], acc17          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+46], acc18          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+47], acc19          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+48], acc20          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+49], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+50], acc22          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+51], acc23          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+52], acc24          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+53], acc25          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+54], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+55], acc27          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+56], acc28          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+57], acc29          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+58], acc30          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+59], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+60], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+61], acc33          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+62], acc34          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+63], acc35          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+64], acc36          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+65], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+66], acc38          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+67], acc39          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+68], acc40          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+69], acc41          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+70], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+71], acc43          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+72], acc44          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+73], acc45          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+74], acc46          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+75], acc47          // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+76], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+77], acc49          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+78], acc50          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+79], acc51          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+80], acc52          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+81], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+82], acc54          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+83], acc55          // copy acc to vreg[55]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0), (0, 1, 1, 0), (0, 0, 2, 0), (0, 1, 2, 0), (0, 0, 3, 0), (0, 1, 3, 0), (0, 0, 4, 0), (0, 1, 4, 0), (0, 0, 5, 0), (0, 1, 5, 0), (0, 0, 6, 0), (0, 1, 6, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(2)                               // lgkmcnt(2) = 4 - 1 (bias) - 1 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D

s_waitcnt lgkmcnt(0)                               // lgkmcnt(0) = 4 - 2 (bias) - 2 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[28:29], v[vgprValuC+40:vgprValuC+40+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[30:31], v[vgprValuC+42:vgprValuC+42+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[24:25], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[26:27], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
buffer_store_dwordx2 v[40:41], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[28:29], v[vgprValuC+48:vgprValuC+48+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[30:31], v[vgprValuC+50:vgprValuC+50+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[24:25], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[26:27], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[16:17], v[vgprValuC+52:vgprValuC+52+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[18:19], v[vgprValuC+54:vgprValuC+54+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+52:vgprValuC+52+1], v[12:13], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[vgprValuC+54:vgprValuC+54+1], v[14:15], v[vgprValuC+54:vgprValuC+54+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+52], v[vgprValuC+52]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+53], v[vgprValuC+53]     // convert C to fp16
v_pack_b32_f16 v52, v[vgprValuC+52], v[vgprValuC+53] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+54], v[vgprValuC+54]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+55], v[vgprValuC+55]     // convert C to fp16
v_pack_b32_f16 v53, v[vgprValuC+54], v[vgprValuC+55] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[52:53], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[28:29], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[30:31], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[24:25], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[26:27], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[12:13], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[14:15], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[60:61], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[28:29], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[30:31], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[16:17], v[vgprValuC+68:vgprValuC+68+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[18:19], v[vgprValuC+70:vgprValuC+70+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+68:vgprValuC+68+1], v[12:13], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[vgprValuC+70:vgprValuC+70+1], v[14:15], v[vgprValuC+70:vgprValuC+70+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+68], v[vgprValuC+68]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+69], v[vgprValuC+69]     // convert C to fp16
v_pack_b32_f16 v68, v[vgprValuC+68], v[vgprValuC+69] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+70], v[vgprValuC+70]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+71], v[vgprValuC+71]     // convert C to fp16
v_pack_b32_f16 v69, v[vgprValuC+70], v[vgprValuC+71] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[68:69], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[28:29], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[30:31], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[24:25], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[26:27], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[16:17], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[18:19], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[12:13], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[14:15], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[76:77], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[28:29], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[30:31], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,0,7,0:vw4); (0,1,7,0:vw4)        */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
ds_read_b128 v[12:15], v8 offset:0                 // load Bias
ds_read_b128 v[16:19], v9 offset:0                 // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
ds_read_b128 v[24:27], v8 offset:256               // load Bias
ds_read_b128 v[28:31], v9 offset:256               // load scaleAlpha
v_accvgpr_read_b32 v[vgprValuC+20], acc56          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+21], acc57          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+22], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+23], acc59          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+32], acc60          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+33], acc61          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+34], acc62          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+35], acc63          // copy acc to vreg[63]

/* rC *= alpha batchElements=[(0, 0, 7, 0), (0, 1, 7, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(2)                               // lgkmcnt(2) = 4 - 1 (bias) - 1 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D

s_waitcnt lgkmcnt(0)                               // lgkmcnt(0) = 4 - 2 (bias) - 2 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_2                            // jump to end
label_GW_B0_E1_N_1:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=10 */
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,1,0,0:vw4); (0,0,1,0:vw4); (0,1,1,0:vw4); (0,0,2,0:vw4); (0,1,2,0:vw4); (0,0,3,0:vw4); (0,1,3,0:vw4); (0,0,4,0:vw4); (0,1,4,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v92, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v0, s68
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for LDS write
s_barrier                                          // LDS write barrier
ds_read_b128 v[12:15], v7 offset:0                 // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b128 v[16:19], v8 offset:0                 // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v92, v6, s[72:73]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v4, s68
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
ds_read_b128 v[24:27], v10 offset:0                // load Bias
v_add_u32 v11, 1024, v10                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[28:31], v11 offset:0                // load scaleAlpha
v_add_lshl_u32 v9, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v92, v9, s[72:73]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v37, v0, s68
v_lshlrev_b32 v37, 0x2, v37                        // Bias address scaled by BPE
v_add_u32 v38, 1024, v37                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v36, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v92, v36, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v44, v4, s68
v_lshlrev_b32 v44, 0x2, v44                        // Bias address scaled by BPE
v_add_u32 v45, 1024, v44                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v39, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v39, v92, v39, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v47, v0, s68
v_lshlrev_b32 v47, 0x2, v47                        // Bias address scaled by BPE
v_add_u32 v52, 1024, v47                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v46, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v92, v46, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v4, s68
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_add_u32 v55, 1024, v54                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v53, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v53, v92, v53, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v65, v0, s68
v_lshlrev_b32 v65, 0x2, v65                        // Bias address scaled by BPE
v_add_u32 v66, 1024, v65                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v64, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v64, v92, v64, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v4, s68
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_add_u32 v73, 1024, v72                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v67, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v67, v92, v67, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v75, v0, s68
v_lshlrev_b32 v75, 0x2, v75                        // Bias address scaled by BPE
v_add_u32 v80, 1024, v75                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v74, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v74, v92, v74, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v4, s68
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_add_u32 v83, 1024, v82                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v81, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v92, v81, s[72:73]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc1           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+48], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+49], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+50], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+51], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+56], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+57], acc17          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+58], acc18          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+59], acc19          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+60], acc20          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+61], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+62], acc22          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+63], acc23          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+68], acc24          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+69], acc25          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+70], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+71], acc27          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+76], acc28          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+77], acc29          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+78], acc30          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+79], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+84], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+85], acc33          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+86], acc34          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+87], acc35          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+88], acc36          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+89], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+90], acc38          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+91], acc39          // copy acc to vreg[39]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0), (0, 1, 1, 0), (0, 0, 2, 0), (0, 1, 2, 0), (0, 0, 3, 0), (0, 1, 3, 0), (0, 0, 4, 0), (0, 1, 4, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[12:13], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[14:15], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
buffer_store_dwordx2 v[40:41], v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[28:29], v[vgprValuC+48:vgprValuC+48+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[30:31], v[vgprValuC+50:vgprValuC+50+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[24:25], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[26:27], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v39, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[16:17], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[18:19], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[12:13], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[14:15], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[28:29], v[vgprValuC+60:vgprValuC+60+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[30:31], v[vgprValuC+62:vgprValuC+62+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[24:25], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[26:27], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
buffer_store_dwordx2 v[60:61], v53, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[16:17], v[vgprValuC+68:vgprValuC+68+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[18:19], v[vgprValuC+70:vgprValuC+70+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+68:vgprValuC+68+1], v[12:13], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[vgprValuC+70:vgprValuC+70+1], v[14:15], v[vgprValuC+70:vgprValuC+70+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+68], v[vgprValuC+68]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+69], v[vgprValuC+69]     // convert C to fp16
v_pack_b32_f16 v68, v[vgprValuC+68], v[vgprValuC+69] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+70], v[vgprValuC+70]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+71], v[vgprValuC+71]     // convert C to fp16
v_pack_b32_f16 v69, v[vgprValuC+70], v[vgprValuC+71] // Pack with neighbor
buffer_store_dwordx2 v[68:69], v64, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[28:29], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[30:31], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[24:25], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[26:27], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
buffer_store_dwordx2 v[76:77], v67, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[16:17], v[vgprValuC+84:vgprValuC+84+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[18:19], v[vgprValuC+86:vgprValuC+86+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+84:vgprValuC+84+1], v[12:13], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[vgprValuC+86:vgprValuC+86+1], v[14:15], v[vgprValuC+86:vgprValuC+86+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+84], v[vgprValuC+84]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+85], v[vgprValuC+85]     // convert C to fp16
v_pack_b32_f16 v84, v[vgprValuC+84], v[vgprValuC+85] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+86], v[vgprValuC+86]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+87], v[vgprValuC+87]     // convert C to fp16
v_pack_b32_f16 v85, v[vgprValuC+86], v[vgprValuC+87] // Pack with neighbor
buffer_store_dwordx2 v[84:85], v74, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[28:29], v[vgprValuC+88:vgprValuC+88+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[30:31], v[vgprValuC+90:vgprValuC+90+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+88:vgprValuC+88+1], v[24:25], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[vgprValuC+90:vgprValuC+90+1], v[26:27], v[vgprValuC+90:vgprValuC+90+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+88], v[vgprValuC+88]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+89], v[vgprValuC+89]     // convert C to fp16
v_pack_b32_f16 v88, v[vgprValuC+88], v[vgprValuC+89] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+90], v[vgprValuC+90]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+91], v[vgprValuC+91]     // convert C to fp16
v_pack_b32_f16 v89, v[vgprValuC+90], v[vgprValuC+91] // Pack with neighbor
buffer_store_dwordx2 v[88:89], v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,0,5,0:vw4); (0,1,5,0:vw4); (0,0,6,0:vw4); (0,1,6,0:vw4); (0,0,7,0:vw4); (0,1,7,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v64, BufferOOB
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v0, s68
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b128 v[12:15], v7 offset:0                 // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b128 v[16:19], v8 offset:0                 // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v64, v6, s[72:73]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v4, s68
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
ds_read_b128 v[24:27], v10 offset:0                // load Bias
v_add_u32 v11, 1024, v10                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[28:31], v11 offset:0                // load scaleAlpha
v_add_lshl_u32 v9, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v64, v9, s[72:73]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v37, v0, s68
v_lshlrev_b32 v37, 0x2, v37                        // Bias address scaled by BPE
v_add_u32 v38, 1024, v37                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v36, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v64, v36, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v44, v4, s68
v_lshlrev_b32 v44, 0x2, v44                        // Bias address scaled by BPE
v_add_u32 v45, 1024, v44                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v39, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v39, v64, v39, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v47, v0, s68
v_lshlrev_b32 v47, 0x2, v47                        // Bias address scaled by BPE
v_add_u32 v52, 1024, v47                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v46, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v64, v46, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v4, s68
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_add_u32 v55, 1024, v54                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v53, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v53, v64, v53, s[72:73]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc40          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+21], acc41          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+22], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+23], acc43          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+32], acc44          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+33], acc45          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+34], acc46          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+35], acc47          // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+40], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+41], acc49          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+42], acc50          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+43], acc51          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+48], acc52          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+49], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+50], acc54          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+51], acc55          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+56], acc56          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+57], acc57          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+58], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+59], acc59          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+60], acc60          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+61], acc61          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+62], acc62          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+63], acc63          // copy acc to vreg[63]

/* rC *= alpha batchElements=[(0, 0, 5, 0), (0, 1, 5, 0), (0, 0, 6, 0), (0, 1, 6, 0), (0, 0, 7, 0), (0, 1, 7, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[12:13], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[14:15], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
buffer_store_dwordx2 v[40:41], v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[28:29], v[vgprValuC+48:vgprValuC+48+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[30:31], v[vgprValuC+50:vgprValuC+50+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[24:25], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[26:27], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v39, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[16:17], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[18:19], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[12:13], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[14:15], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[28:29], v[vgprValuC+60:vgprValuC+60+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[30:31], v[vgprValuC+62:vgprValuC+62+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[24:25], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[26:27], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
buffer_store_dwordx2 v[60:61], v53, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_2                            // jump to end
label_GW_B0_E1_M_1:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=30 */
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw1); (0,0,0,1:vw1); (0,0,0,2:vw1); (0,0,0,3:vw1); (0,1,0,0:vw1); (0,1,0,1:vw1); (0,1,0,2:vw1); (0,1,0,3:vw1); (0,0,1,0:vw1); (0,0,1,1:vw1); (0,0,1,2:vw1); (0,0,1,3:vw1); (0,1,1,0:vw1); (0,1,1,1:vw1); (0,1,1,2:vw1); (0,1,1,3:vw1); (0,0,2,0:vw1); (0,0,2,1:vw1); (0,0,2,2:vw1); (0,0,2,3:vw1); (0,1,2,0:vw1); (0,1,2,1:vw1); (0,1,2,2:vw1); (0,1,2,3:vw1); (0,0,3,0:vw1); (0,0,3,1:vw1); (0,0,3,2:vw1); (0,0,3,3:vw1); (0,1,3,0:vw1); (0,1,3,1:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v143, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v0, s68
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for LDS write
s_barrier                                          // LDS write barrier
ds_read_b32 v9, v7 offset:0                        // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v10, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v143, v6, s[72:73]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v13, v4, s68
v_lshlrev_b32 v13, 0x2, v13                        // Bias address scaled by BPE
ds_read_b32 v15, v13 offset:0                      // load Bias
v_add_u32 v14, 1024, v13                           // add ScaleAlphaVec offset (3)
ds_read_b32 v16, v14 offset:0                      // load scaleAlpha
v_add_lshl_u32 v12, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v143, v12, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v19, v4, s68
v_lshlrev_b32 v19, 0x2, v19                        // Bias address scaled by BPE
ds_read_b32 v21, v19 offset:0                      // load Bias
v_add_u32 v20, 1024, v19                           // add ScaleAlphaVec offset (3)
ds_read_b32 v22, v20 offset:0                      // load scaleAlpha
v_add_lshl_u32 v18, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v143, v18, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v25, v4, s68
v_lshlrev_b32 v25, 0x2, v25                        // Bias address scaled by BPE
ds_read_b32 v27, v25 offset:0                      // load Bias
v_add_u32 v26, 1024, v25                           // add ScaleAlphaVec offset (3)
ds_read_b32 v28, v26 offset:0                      // load scaleAlpha
v_add_lshl_u32 v24, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v143, v24, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v31, v4, s68
v_lshlrev_b32 v31, 0x2, v31                        // Bias address scaled by BPE
ds_read_b32 v33, v31 offset:0                      // load Bias
v_add_u32 v32, 1024, v31                           // add ScaleAlphaVec offset (3)
ds_read_b32 v34, v32 offset:0                      // load scaleAlpha
v_add_lshl_u32 v30, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v143, v30, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v37, v4, s68
v_lshlrev_b32 v37, 0x2, v37                        // Bias address scaled by BPE
ds_read_b32 v39, v37 offset:0                      // load Bias
v_add_u32 v38, 1024, v37                           // add ScaleAlphaVec offset (3)
ds_read_b32 v40, v38 offset:0                      // load scaleAlpha
v_add_lshl_u32 v36, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v143, v36, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v43, v4, s68
v_lshlrev_b32 v43, 0x2, v43                        // Bias address scaled by BPE
ds_read_b32 v45, v43 offset:0                      // load Bias
v_add_u32 v44, 1024, v43                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v44 offset:0                      // load scaleAlpha
v_add_lshl_u32 v42, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v143, v42, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v4, s68
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
ds_read_b32 v51, v49 offset:0                      // load Bias
v_add_u32 v50, 1024, v49                           // add ScaleAlphaVec offset (3)
ds_read_b32 v52, v50 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v143, v48, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v55, v0, s68
v_lshlrev_b32 v55, 0x2, v55                        // Bias address scaled by BPE
v_add_u32 v56, 1024, v55                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v54, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v143, v54, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v59, v4, s68
v_lshlrev_b32 v59, 0x2, v59                        // Bias address scaled by BPE
v_add_u32 v60, 1024, v59                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v58, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v58, v143, v58, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v4, s68
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
v_add_u32 v64, 1024, v63                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v62, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v143, v62, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s68
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
v_add_u32 v68, 1024, v67                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v143, v66, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v71, v4, s68
v_lshlrev_b32 v71, 0x2, v71                        // Bias address scaled by BPE
v_add_u32 v72, 1024, v71                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v70, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v70, v143, v70, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v75, v4, s68
v_lshlrev_b32 v75, 0x2, v75                        // Bias address scaled by BPE
v_add_u32 v76, 1024, v75                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v74, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v74, v143, v74, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v79, v4, s68
v_lshlrev_b32 v79, 0x2, v79                        // Bias address scaled by BPE
v_add_u32 v80, 1024, v79                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v78, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v78, v143, v78, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v83, v4, s68
v_lshlrev_b32 v83, 0x2, v83                        // Bias address scaled by BPE
v_add_u32 v84, 1024, v83                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v82, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v82, v143, v82, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v87, v0, s68
v_lshlrev_b32 v87, 0x2, v87                        // Bias address scaled by BPE
v_add_u32 v88, 1024, v87                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v86, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v143, v86, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v4, s68
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
v_add_u32 v92, 1024, v91                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v90, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v143, v90, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v95, v4, s68
v_lshlrev_b32 v95, 0x2, v95                        // Bias address scaled by BPE
v_add_u32 v96, 1024, v95                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v94, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v94, v143, v94, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v99, v4, s68
v_lshlrev_b32 v99, 0x2, v99                        // Bias address scaled by BPE
v_add_u32 v100, 1024, v99                          // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v98, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v98, v143, v98, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v103, v4, s68
v_lshlrev_b32 v103, 0x2, v103                      // Bias address scaled by BPE
v_add_u32 v104, 1024, v103                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v102, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v102, v143, v102, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v107, v4, s68
v_lshlrev_b32 v107, 0x2, v107                      // Bias address scaled by BPE
v_add_u32 v108, 1024, v107                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v106, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v143, v106, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v111, v4, s68
v_lshlrev_b32 v111, 0x2, v111                      // Bias address scaled by BPE
v_add_u32 v112, 1024, v111                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v110, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v110, v143, v110, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v115, v4, s68
v_lshlrev_b32 v115, 0x2, v115                      // Bias address scaled by BPE
v_add_u32 v116, 1024, v115                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v114, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v114, v143, v114, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v0, s68
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v120, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v143, v118, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v123, v4, s68
v_lshlrev_b32 v123, 0x2, v123                      // Bias address scaled by BPE
v_add_u32 v124, 1024, v123                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v122, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v122, v143, v122, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v127, v4, s68
v_lshlrev_b32 v127, 0x2, v127                      // Bias address scaled by BPE
v_add_u32 v128, 1024, v127                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v126, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v126, v143, v126, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v131, v4, s68
v_lshlrev_b32 v131, 0x2, v131                      // Bias address scaled by BPE
v_add_u32 v132, 1024, v131                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v130, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v130, v143, v130, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v135, v4, s68
v_lshlrev_b32 v135, 0x2, v135                      // Bias address scaled by BPE
v_add_u32 v136, 1024, v135                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v134, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v134, v143, v134, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v140, v4, s68
v_lshlrev_b32 v140, 0x2, v140                      // Bias address scaled by BPE
v_add_u32 v141, 1024, v140                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v138, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v143, v138, s[72:73]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+17], acc1           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+23], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+29], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+35], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+41], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+47], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+53], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+57], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+61], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+65], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+69], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+73], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+77], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+81], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+85], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+89], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+93], acc17          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+97], acc18          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+101], acc19         // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+105], acc20         // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+109], acc21         // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+113], acc22         // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+117], acc23         // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+121], acc24         // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+125], acc25         // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+129], acc26         // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+133], acc27         // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+137], acc28         // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+142], acc29         // copy acc to vreg[29]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 0, 1), (0, 0, 0, 2), (0, 0, 0, 3), (0, 1, 0, 0), (0, 1, 0, 1), (0, 1, 0, 2), (0, 1, 0, 3), (0, 0, 1, 0), (0, 0, 1, 1), (0, 0, 1, 2), (0, 0, 1, 3), (0, 1, 1, 0), (0, 1, 1, 1), (0, 1, 1, 2), (0, 1, 1, 3), (0, 0, 2, 0), (0, 0, 2, 1), (0, 0, 2, 2), (0, 0, 2, 3), (0, 1, 2, 0), (0, 1, 2, 1), (0, 1, 2, 2), (0, 1, 2, 3), (0, 0, 3, 0), (0, 0, 3, 1), (0, 0, 3, 2), (0, 0, 3, 3), (0, 1, 3, 0), (0, 1, 3, 1)] */
v_mul_f32 v[vgprValuC+11], s[sgprAlpha], v[vgprValuC+11] // *= alpha
v_mul_f32 v[vgprValuC+17], s[sgprAlpha], v[vgprValuC+17] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+97], s[sgprAlpha], v[vgprValuC+97] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+121], s[sgprAlpha], v[vgprValuC+121] // *= alpha
v_mul_f32 v[vgprValuC+125], s[sgprAlpha], v[vgprValuC+125] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+133], s[sgprAlpha], v[vgprValuC+133] // *= alpha
v_mul_f32 v[vgprValuC+137], s[sgprAlpha], v[vgprValuC+137] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+11], v10, v[vgprValuC+11]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+11], v9, v[vgprValuC+11]     // C += bias
v_cvt_f16_f32 v11, v[vgprValuC+11]                 // convert C to fp16
buffer_store_short v11, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+17], v16, v[vgprValuC+17]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+17], v15, v[vgprValuC+17]    // C += bias
v_cvt_f16_f32 v17, v[vgprValuC+17]                 // convert C to fp16
buffer_store_short v17, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+23], v22, v[vgprValuC+23]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+23], v21, v[vgprValuC+23]    // C += bias
v_cvt_f16_f32 v23, v[vgprValuC+23]                 // convert C to fp16
buffer_store_short v23, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+29], v28, v[vgprValuC+29]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+29], v27, v[vgprValuC+29]    // C += bias
v_cvt_f16_f32 v29, v[vgprValuC+29]                 // convert C to fp16
buffer_store_short v29, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+35], v34, v[vgprValuC+35]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+35], v33, v[vgprValuC+35]    // C += bias
v_cvt_f16_f32 v35, v[vgprValuC+35]                 // convert C to fp16
buffer_store_short v35, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+41], v40, v[vgprValuC+41]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+41], v39, v[vgprValuC+41]    // C += bias
v_cvt_f16_f32 v41, v[vgprValuC+41]                 // convert C to fp16
buffer_store_short v41, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+53], v52, v[vgprValuC+53]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+53], v51, v[vgprValuC+53]    // C += bias
v_cvt_f16_f32 v53, v[vgprValuC+53]                 // convert C to fp16
buffer_store_short v53, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+57], v10, v[vgprValuC+57]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+57], v9, v[vgprValuC+57]     // C += bias
v_cvt_f16_f32 v57, v[vgprValuC+57]                 // convert C to fp16
buffer_store_short v57, v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+61], v16, v[vgprValuC+61]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+61], v15, v[vgprValuC+61]    // C += bias
v_cvt_f16_f32 v61, v[vgprValuC+61]                 // convert C to fp16
buffer_store_short v61, v58, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+65], v22, v[vgprValuC+65]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+65], v21, v[vgprValuC+65]    // C += bias
v_cvt_f16_f32 v65, v[vgprValuC+65]                 // convert C to fp16
buffer_store_short v65, v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+69], v28, v[vgprValuC+69]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+69], v27, v[vgprValuC+69]    // C += bias
v_cvt_f16_f32 v69, v[vgprValuC+69]                 // convert C to fp16
buffer_store_short v69, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+73], v34, v[vgprValuC+73]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+73], v33, v[vgprValuC+73]    // C += bias
v_cvt_f16_f32 v73, v[vgprValuC+73]                 // convert C to fp16
buffer_store_short v73, v70, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+77], v40, v[vgprValuC+77]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+77], v39, v[vgprValuC+77]    // C += bias
v_cvt_f16_f32 v77, v[vgprValuC+77]                 // convert C to fp16
buffer_store_short v77, v74, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+81], v46, v[vgprValuC+81]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+81], v45, v[vgprValuC+81]    // C += bias
v_cvt_f16_f32 v81, v[vgprValuC+81]                 // convert C to fp16
buffer_store_short v81, v78, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+85], v52, v[vgprValuC+85]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+85], v51, v[vgprValuC+85]    // C += bias
v_cvt_f16_f32 v85, v[vgprValuC+85]                 // convert C to fp16
buffer_store_short v85, v82, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+89], v10, v[vgprValuC+89]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+89], v9, v[vgprValuC+89]     // C += bias
v_cvt_f16_f32 v89, v[vgprValuC+89]                 // convert C to fp16
buffer_store_short v89, v86, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+93], v16, v[vgprValuC+93]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+93], v15, v[vgprValuC+93]    // C += bias
v_cvt_f16_f32 v93, v[vgprValuC+93]                 // convert C to fp16
buffer_store_short v93, v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+97], v22, v[vgprValuC+97]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+97], v21, v[vgprValuC+97]    // C += bias
v_cvt_f16_f32 v97, v[vgprValuC+97]                 // convert C to fp16
buffer_store_short v97, v94, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+101], v28, v[vgprValuC+101]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+101], v27, v[vgprValuC+101]  // C += bias
v_cvt_f16_f32 v101, v[vgprValuC+101]               // convert C to fp16
buffer_store_short v101, v98, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+105], v34, v[vgprValuC+105]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+105], v33, v[vgprValuC+105]  // C += bias
v_cvt_f16_f32 v105, v[vgprValuC+105]               // convert C to fp16
buffer_store_short v105, v102, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+109], v40, v[vgprValuC+109]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+109], v39, v[vgprValuC+109]  // C += bias
v_cvt_f16_f32 v109, v[vgprValuC+109]               // convert C to fp16
buffer_store_short v109, v106, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+113], v46, v[vgprValuC+113]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+113], v45, v[vgprValuC+113]  // C += bias
v_cvt_f16_f32 v113, v[vgprValuC+113]               // convert C to fp16
buffer_store_short v113, v110, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+117], v52, v[vgprValuC+117]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+117], v51, v[vgprValuC+117]  // C += bias
v_cvt_f16_f32 v117, v[vgprValuC+117]               // convert C to fp16
buffer_store_short v117, v114, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+121], v10, v[vgprValuC+121]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+121], v9, v[vgprValuC+121]   // C += bias
v_cvt_f16_f32 v121, v[vgprValuC+121]               // convert C to fp16
buffer_store_short v121, v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+125], v16, v[vgprValuC+125]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+125], v15, v[vgprValuC+125]  // C += bias
v_cvt_f16_f32 v125, v[vgprValuC+125]               // convert C to fp16
buffer_store_short v125, v122, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+129], v22, v[vgprValuC+129]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+129], v21, v[vgprValuC+129]  // C += bias
v_cvt_f16_f32 v129, v[vgprValuC+129]               // convert C to fp16
buffer_store_short v129, v126, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+133], v28, v[vgprValuC+133]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+133], v27, v[vgprValuC+133]  // C += bias
v_cvt_f16_f32 v133, v[vgprValuC+133]               // convert C to fp16
buffer_store_short v133, v130, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+137], v34, v[vgprValuC+137]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+137], v33, v[vgprValuC+137]  // C += bias
v_cvt_f16_f32 v137, v[vgprValuC+137]               // convert C to fp16
buffer_store_short v137, v134, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+142], v40, v[vgprValuC+142]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+142], v39, v[vgprValuC+142]  // C += bias
v_cvt_f16_f32 v142, v[vgprValuC+142]               // convert C to fp16
buffer_store_short v142, v138, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,1,3,2:vw1); (0,1,3,3:vw1); (0,0,4,0:vw1); (0,0,4,1:vw1); (0,0,4,2:vw1); (0,0,4,3:vw1); (0,1,4,0:vw1); (0,1,4,1:vw1); (0,1,4,2:vw1); (0,1,4,3:vw1); (0,0,5,0:vw1); (0,0,5,1:vw1); (0,0,5,2:vw1); (0,0,5,3:vw1); (0,1,5,0:vw1); (0,1,5,1:vw1); (0,1,5,2:vw1); (0,1,5,3:vw1); (0,0,6,0:vw1); (0,0,6,1:vw1); (0,0,6,2:vw1); (0,0,6,3:vw1); (0,1,6,0:vw1); (0,1,6,1:vw1); (0,1,6,2:vw1); (0,1,6,3:vw1); (0,0,7,0:vw1); (0,0,7,1:vw1); (0,0,7,2:vw1); (0,0,7,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v143, BufferOOB
/* (d1,vc1,d0,vc0)=(0,3,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v4, s68
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b32 v9, v7 offset:0                        // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v10, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v143, v6, s[72:73]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v13, v4, s68
v_lshlrev_b32 v13, 0x2, v13                        // Bias address scaled by BPE
ds_read_b32 v15, v13 offset:0                      // load Bias
v_add_u32 v14, 1024, v13                           // add ScaleAlphaVec offset (3)
ds_read_b32 v16, v14 offset:0                      // load scaleAlpha
v_add_lshl_u32 v12, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v143, v12, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v19, v0, s68
v_lshlrev_b32 v19, 0x2, v19                        // Bias address scaled by BPE
ds_read_b32 v21, v19 offset:0                      // load Bias
v_add_u32 v20, 1024, v19                           // add ScaleAlphaVec offset (3)
ds_read_b32 v22, v20 offset:0                      // load scaleAlpha
v_add_lshl_u32 v18, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v143, v18, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v25, v4, s68
v_lshlrev_b32 v25, 0x2, v25                        // Bias address scaled by BPE
ds_read_b32 v27, v25 offset:0                      // load Bias
v_add_u32 v26, 1024, v25                           // add ScaleAlphaVec offset (3)
ds_read_b32 v28, v26 offset:0                      // load scaleAlpha
v_add_lshl_u32 v24, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v143, v24, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v31, v4, s68
v_lshlrev_b32 v31, 0x2, v31                        // Bias address scaled by BPE
ds_read_b32 v33, v31 offset:0                      // load Bias
v_add_u32 v32, 1024, v31                           // add ScaleAlphaVec offset (3)
ds_read_b32 v34, v32 offset:0                      // load scaleAlpha
v_add_lshl_u32 v30, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v143, v30, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v37, v4, s68
v_lshlrev_b32 v37, 0x2, v37                        // Bias address scaled by BPE
ds_read_b32 v39, v37 offset:0                      // load Bias
v_add_u32 v38, 1024, v37                           // add ScaleAlphaVec offset (3)
ds_read_b32 v40, v38 offset:0                      // load scaleAlpha
v_add_lshl_u32 v36, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v143, v36, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v43, v4, s68
v_lshlrev_b32 v43, 0x2, v43                        // Bias address scaled by BPE
ds_read_b32 v45, v43 offset:0                      // load Bias
v_add_u32 v44, 1024, v43                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v44 offset:0                      // load scaleAlpha
v_add_lshl_u32 v42, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v143, v42, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v4, s68
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
ds_read_b32 v51, v49 offset:0                      // load Bias
v_add_u32 v50, 1024, v49                           // add ScaleAlphaVec offset (3)
ds_read_b32 v52, v50 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v143, v48, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v55, v4, s68
v_lshlrev_b32 v55, 0x2, v55                        // Bias address scaled by BPE
v_add_u32 v56, 1024, v55                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v54, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v143, v54, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v59, v4, s68
v_lshlrev_b32 v59, 0x2, v59                        // Bias address scaled by BPE
v_add_u32 v60, 1024, v59                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v58, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v58, v143, v58, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v0, s68
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
v_add_u32 v64, 1024, v63                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v62, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v143, v62, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s68
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
v_add_u32 v68, 1024, v67                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v143, v66, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v71, v4, s68
v_lshlrev_b32 v71, 0x2, v71                        // Bias address scaled by BPE
v_add_u32 v72, 1024, v71                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v70, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v70, v143, v70, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v75, v4, s68
v_lshlrev_b32 v75, 0x2, v75                        // Bias address scaled by BPE
v_add_u32 v76, 1024, v75                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v74, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v74, v143, v74, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v79, v4, s68
v_lshlrev_b32 v79, 0x2, v79                        // Bias address scaled by BPE
v_add_u32 v80, 1024, v79                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v78, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v78, v143, v78, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v83, v4, s68
v_lshlrev_b32 v83, 0x2, v83                        // Bias address scaled by BPE
v_add_u32 v84, 1024, v83                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v82, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v82, v143, v82, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v87, v4, s68
v_lshlrev_b32 v87, 0x2, v87                        // Bias address scaled by BPE
v_add_u32 v88, 1024, v87                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v86, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v143, v86, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v4, s68
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
v_add_u32 v92, 1024, v91                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v90, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v143, v90, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v95, v0, s68
v_lshlrev_b32 v95, 0x2, v95                        // Bias address scaled by BPE
v_add_u32 v96, 1024, v95                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v94, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v94, v143, v94, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v99, v4, s68
v_lshlrev_b32 v99, 0x2, v99                        // Bias address scaled by BPE
v_add_u32 v100, 1024, v99                          // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v98, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v98, v143, v98, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v103, v4, s68
v_lshlrev_b32 v103, 0x2, v103                      // Bias address scaled by BPE
v_add_u32 v104, 1024, v103                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v102, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v102, v143, v102, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v107, v4, s68
v_lshlrev_b32 v107, 0x2, v107                      // Bias address scaled by BPE
v_add_u32 v108, 1024, v107                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v106, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v143, v106, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v111, v4, s68
v_lshlrev_b32 v111, 0x2, v111                      // Bias address scaled by BPE
v_add_u32 v112, 1024, v111                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v110, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v110, v143, v110, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v115, v4, s68
v_lshlrev_b32 v115, 0x2, v115                      // Bias address scaled by BPE
v_add_u32 v116, 1024, v115                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v114, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v114, v143, v114, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v4, s68
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v120, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v143, v118, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v123, v4, s68
v_lshlrev_b32 v123, 0x2, v123                      // Bias address scaled by BPE
v_add_u32 v124, 1024, v123                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v122, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v122, v143, v122, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v127, v0, s68
v_lshlrev_b32 v127, 0x2, v127                      // Bias address scaled by BPE
v_add_u32 v128, 1024, v127                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v126, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v126, v143, v126, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v131, v4, s68
v_lshlrev_b32 v131, 0x2, v131                      // Bias address scaled by BPE
v_add_u32 v132, 1024, v131                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v130, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v130, v143, v130, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v135, v4, s68
v_lshlrev_b32 v135, 0x2, v135                      // Bias address scaled by BPE
v_add_u32 v136, 1024, v135                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v134, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v134, v143, v134, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v140, v4, s68
v_lshlrev_b32 v140, 0x2, v140                      // Bias address scaled by BPE
v_add_u32 v141, 1024, v140                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v138, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v143, v138, s[72:73]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc30          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+17], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+23], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+29], acc33          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+35], acc34          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+41], acc35          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+47], acc36          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+53], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+57], acc38          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+61], acc39          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+65], acc40          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+69], acc41          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+73], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+77], acc43          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+81], acc44          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+85], acc45          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+89], acc46          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+93], acc47          // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+97], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+101], acc49         // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+105], acc50         // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+109], acc51         // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+113], acc52         // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+117], acc53         // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+121], acc54         // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+125], acc55         // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+129], acc56         // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+133], acc57         // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+137], acc58         // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+142], acc59         // copy acc to vreg[59]

/* rC *= alpha batchElements=[(0, 1, 3, 2), (0, 1, 3, 3), (0, 0, 4, 0), (0, 0, 4, 1), (0, 0, 4, 2), (0, 0, 4, 3), (0, 1, 4, 0), (0, 1, 4, 1), (0, 1, 4, 2), (0, 1, 4, 3), (0, 0, 5, 0), (0, 0, 5, 1), (0, 0, 5, 2), (0, 0, 5, 3), (0, 1, 5, 0), (0, 1, 5, 1), (0, 1, 5, 2), (0, 1, 5, 3), (0, 0, 6, 0), (0, 0, 6, 1), (0, 0, 6, 2), (0, 0, 6, 3), (0, 1, 6, 0), (0, 1, 6, 1), (0, 1, 6, 2), (0, 1, 6, 3), (0, 0, 7, 0), (0, 0, 7, 1), (0, 0, 7, 2), (0, 0, 7, 3)] */
v_mul_f32 v[vgprValuC+11], s[sgprAlpha], v[vgprValuC+11] // *= alpha
v_mul_f32 v[vgprValuC+17], s[sgprAlpha], v[vgprValuC+17] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+97], s[sgprAlpha], v[vgprValuC+97] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+121], s[sgprAlpha], v[vgprValuC+121] // *= alpha
v_mul_f32 v[vgprValuC+125], s[sgprAlpha], v[vgprValuC+125] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+133], s[sgprAlpha], v[vgprValuC+133] // *= alpha
v_mul_f32 v[vgprValuC+137], s[sgprAlpha], v[vgprValuC+137] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+11], v10, v[vgprValuC+11]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+11], v9, v[vgprValuC+11]     // C += bias
v_cvt_f16_f32 v11, v[vgprValuC+11]                 // convert C to fp16
buffer_store_short v11, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+17], v16, v[vgprValuC+17]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+17], v15, v[vgprValuC+17]    // C += bias
v_cvt_f16_f32 v17, v[vgprValuC+17]                 // convert C to fp16
buffer_store_short v17, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+23], v22, v[vgprValuC+23]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+23], v21, v[vgprValuC+23]    // C += bias
v_cvt_f16_f32 v23, v[vgprValuC+23]                 // convert C to fp16
buffer_store_short v23, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+29], v28, v[vgprValuC+29]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+29], v27, v[vgprValuC+29]    // C += bias
v_cvt_f16_f32 v29, v[vgprValuC+29]                 // convert C to fp16
buffer_store_short v29, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+35], v34, v[vgprValuC+35]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+35], v33, v[vgprValuC+35]    // C += bias
v_cvt_f16_f32 v35, v[vgprValuC+35]                 // convert C to fp16
buffer_store_short v35, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+41], v40, v[vgprValuC+41]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+41], v39, v[vgprValuC+41]    // C += bias
v_cvt_f16_f32 v41, v[vgprValuC+41]                 // convert C to fp16
buffer_store_short v41, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+53], v52, v[vgprValuC+53]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+53], v51, v[vgprValuC+53]    // C += bias
v_cvt_f16_f32 v53, v[vgprValuC+53]                 // convert C to fp16
buffer_store_short v53, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+57], v10, v[vgprValuC+57]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+57], v9, v[vgprValuC+57]     // C += bias
v_cvt_f16_f32 v57, v[vgprValuC+57]                 // convert C to fp16
buffer_store_short v57, v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+61], v16, v[vgprValuC+61]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+61], v15, v[vgprValuC+61]    // C += bias
v_cvt_f16_f32 v61, v[vgprValuC+61]                 // convert C to fp16
buffer_store_short v61, v58, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+65], v22, v[vgprValuC+65]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+65], v21, v[vgprValuC+65]    // C += bias
v_cvt_f16_f32 v65, v[vgprValuC+65]                 // convert C to fp16
buffer_store_short v65, v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+69], v28, v[vgprValuC+69]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+69], v27, v[vgprValuC+69]    // C += bias
v_cvt_f16_f32 v69, v[vgprValuC+69]                 // convert C to fp16
buffer_store_short v69, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+73], v34, v[vgprValuC+73]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+73], v33, v[vgprValuC+73]    // C += bias
v_cvt_f16_f32 v73, v[vgprValuC+73]                 // convert C to fp16
buffer_store_short v73, v70, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+77], v40, v[vgprValuC+77]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+77], v39, v[vgprValuC+77]    // C += bias
v_cvt_f16_f32 v77, v[vgprValuC+77]                 // convert C to fp16
buffer_store_short v77, v74, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+81], v46, v[vgprValuC+81]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+81], v45, v[vgprValuC+81]    // C += bias
v_cvt_f16_f32 v81, v[vgprValuC+81]                 // convert C to fp16
buffer_store_short v81, v78, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+85], v52, v[vgprValuC+85]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+85], v51, v[vgprValuC+85]    // C += bias
v_cvt_f16_f32 v85, v[vgprValuC+85]                 // convert C to fp16
buffer_store_short v85, v82, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+89], v10, v[vgprValuC+89]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+89], v9, v[vgprValuC+89]     // C += bias
v_cvt_f16_f32 v89, v[vgprValuC+89]                 // convert C to fp16
buffer_store_short v89, v86, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+93], v16, v[vgprValuC+93]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+93], v15, v[vgprValuC+93]    // C += bias
v_cvt_f16_f32 v93, v[vgprValuC+93]                 // convert C to fp16
buffer_store_short v93, v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+97], v22, v[vgprValuC+97]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+97], v21, v[vgprValuC+97]    // C += bias
v_cvt_f16_f32 v97, v[vgprValuC+97]                 // convert C to fp16
buffer_store_short v97, v94, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+101], v28, v[vgprValuC+101]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+101], v27, v[vgprValuC+101]  // C += bias
v_cvt_f16_f32 v101, v[vgprValuC+101]               // convert C to fp16
buffer_store_short v101, v98, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+105], v34, v[vgprValuC+105]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+105], v33, v[vgprValuC+105]  // C += bias
v_cvt_f16_f32 v105, v[vgprValuC+105]               // convert C to fp16
buffer_store_short v105, v102, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+109], v40, v[vgprValuC+109]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+109], v39, v[vgprValuC+109]  // C += bias
v_cvt_f16_f32 v109, v[vgprValuC+109]               // convert C to fp16
buffer_store_short v109, v106, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+113], v46, v[vgprValuC+113]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+113], v45, v[vgprValuC+113]  // C += bias
v_cvt_f16_f32 v113, v[vgprValuC+113]               // convert C to fp16
buffer_store_short v113, v110, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+117], v52, v[vgprValuC+117]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+117], v51, v[vgprValuC+117]  // C += bias
v_cvt_f16_f32 v117, v[vgprValuC+117]               // convert C to fp16
buffer_store_short v117, v114, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+121], v10, v[vgprValuC+121]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+121], v9, v[vgprValuC+121]   // C += bias
v_cvt_f16_f32 v121, v[vgprValuC+121]               // convert C to fp16
buffer_store_short v121, v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+125], v16, v[vgprValuC+125]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+125], v15, v[vgprValuC+125]  // C += bias
v_cvt_f16_f32 v125, v[vgprValuC+125]               // convert C to fp16
buffer_store_short v125, v122, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+129], v22, v[vgprValuC+129]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+129], v21, v[vgprValuC+129]  // C += bias
v_cvt_f16_f32 v129, v[vgprValuC+129]               // convert C to fp16
buffer_store_short v129, v126, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+133], v28, v[vgprValuC+133]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+133], v27, v[vgprValuC+133]  // C += bias
v_cvt_f16_f32 v133, v[vgprValuC+133]               // convert C to fp16
buffer_store_short v133, v130, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+137], v34, v[vgprValuC+137]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+137], v33, v[vgprValuC+137]  // C += bias
v_cvt_f16_f32 v137, v[vgprValuC+137]               // convert C to fp16
buffer_store_short v137, v134, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+142], v40, v[vgprValuC+142]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+142], v39, v[vgprValuC+142]  // C += bias
v_cvt_f16_f32 v142, v[vgprValuC+142]               // convert C to fp16
buffer_store_short v142, v138, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #2 (d1,d0,vc1,vc0) = */
/*    (0,1,7,0:vw1); (0,1,7,1:vw1); (0,1,7,2:vw1); (0,1,7,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v30, BufferOOB
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v4, s68
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b32 v9, v7 offset:0                        // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v10, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v30, v6, s[72:73]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v13, v4, s68
v_lshlrev_b32 v13, 0x2, v13                        // Bias address scaled by BPE
ds_read_b32 v15, v13 offset:0                      // load Bias
v_add_u32 v14, 1024, v13                           // add ScaleAlphaVec offset (3)
ds_read_b32 v16, v14 offset:0                      // load scaleAlpha
v_add_lshl_u32 v12, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v30, v12, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v19, v4, s68
v_lshlrev_b32 v19, 0x2, v19                        // Bias address scaled by BPE
ds_read_b32 v21, v19 offset:0                      // load Bias
v_add_u32 v20, 1024, v19                           // add ScaleAlphaVec offset (3)
ds_read_b32 v22, v20 offset:0                      // load scaleAlpha
v_add_lshl_u32 v18, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v30, v18, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v25, v4, s68
v_lshlrev_b32 v25, 0x2, v25                        // Bias address scaled by BPE
ds_read_b32 v27, v25 offset:0                      // load Bias
v_add_u32 v26, 1024, v25                           // add ScaleAlphaVec offset (3)
ds_read_b32 v28, v26 offset:0                      // load scaleAlpha
v_add_lshl_u32 v24, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v30, v24, s[72:73]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc60          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+17], acc61          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+23], acc62          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+29], acc63          // copy acc to vreg[63]

/* rC *= alpha batchElements=[(0, 1, 7, 0), (0, 1, 7, 1), (0, 1, 7, 2), (0, 1, 7, 3)] */
v_mul_f32 v[vgprValuC+11], s[sgprAlpha], v[vgprValuC+11] // *= alpha
v_mul_f32 v[vgprValuC+17], s[sgprAlpha], v[vgprValuC+17] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+11], v10, v[vgprValuC+11]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+11], v9, v[vgprValuC+11]     // C += bias
v_cvt_f16_f32 v11, v[vgprValuC+11]                 // convert C to fp16
buffer_store_short v11, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+17], v16, v[vgprValuC+17]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+17], v15, v[vgprValuC+17]    // C += bias
v_cvt_f16_f32 v17, v[vgprValuC+17]                 // convert C to fp16
buffer_store_short v17, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+23], v22, v[vgprValuC+23]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+23], v21, v[vgprValuC+23]    // C += bias
v_cvt_f16_f32 v23, v[vgprValuC+23]                 // convert C to fp16
buffer_store_short v23, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+29], v28, v[vgprValuC+29]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+29], v27, v[vgprValuC+29]    // C += bias
v_cvt_f16_f32 v29, v[vgprValuC+29]                 // convert C to fp16
buffer_store_short v29, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_2                            // jump to end
label_GW_Beta_2:
s_and_b32 s44, 127, s[sgprSizeI]                   // s44 = s[sgprSizeI] % 128
s_add_u32 s45, -0x1, s[sgprNumWorkGroups0]
s_cmp_ge_u32 s[sgprWorkGroup0], s45                // wg0 >= nwg0-1 ?
s_cselect_b32 s44, s44, 0                          // set rMT0
s_cmpk_gt_u32 s44, 0x0                             // rMT0 > 0
s_cbranch_scc1 label_GW_B1_E1_M_1                  // jump if edges required
s_and_b32 s44, 127, s[sgprSizeJ]                   // s44 = s[sgprSizeJ] % 128
s_add_u32 s45, -0x1, s[sgprNumWorkGroups1]
s_cmp_ge_u32 s[sgprWorkGroup1], s45                // wg1 >= nwg1-1
s_cselect_b32 s44, s44, 0                          // set rMT1
s_cmpk_gt_u32 s44, 0x0                             // rMT1 > 0
s_cbranch_scc1 label_GW_B1_E1_N_1                  // jump if edges required
label_GW_B1_E0_1:

/* edge=0, allocate 2 sgpr. perBatchTmpS=2 perBatchMaskS=0 perElementMaskS=0 elementsPerBatch=12 */
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,1,0,0:vw4); (0,0,1,0:vw4); (0,1,1,0:vw4); (0,0,2,0:vw4); (0,1,2,0:vw4); (0,0,3,0:vw4); (0,1,3,0:vw4); (0,0,4,0:vw4); (0,1,4,0:vw4); (0,0,5,0:vw4); (0,1,5,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v7, v2, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[10:11], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s12, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s12
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for LDS write
s_barrier                                          // LDS write barrier
ds_read_b128 v[12:15], v8 offset:0                 // load Bias
v_add_u32 v9, 1024, v8                             // add ScaleAlphaVec offset (1)
ds_read_b128 v[16:19], v9 offset:0                 // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
buffer_load_dwordx2 v[24:25], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
ds_read_b128 v[28:31], v8 offset:256               // load Bias
ds_read_b128 v[32:35], v9 offset:256               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[26:27], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
buffer_load_dwordx2 v[44:45], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[46:47], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
buffer_load_dwordx2 v[56:57], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[58:59], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
buffer_load_dwordx2 v[68:69], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[70:71], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
buffer_load_dwordx2 v[80:81], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[82:83], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
buffer_load_dwordx2 v[92:93], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
v_add_lshl_u32 v6, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc1           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+36], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+37], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+38], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+39], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+48], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+49], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+50], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+51], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+52], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+53], acc17          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+54], acc18          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+55], acc19          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+60], acc20          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+61], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+62], acc22          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+63], acc23          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+64], acc24          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+65], acc25          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+66], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+67], acc27          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+72], acc28          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+73], acc29          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+74], acc30          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+75], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+76], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+77], acc33          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+78], acc34          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+79], acc35          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+84], acc36          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+85], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+86], acc38          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+87], acc39          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+88], acc40          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+89], acc41          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+90], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+91], acc43          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+96], acc44          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+97], acc45          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+98], acc46          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+99], acc47          // copy acc to vreg[47]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0), (0, 1, 1, 0), (0, 0, 2, 0), (0, 1, 2, 0), (0, 0, 3, 0), (0, 1, 3, 0), (0, 0, 4, 0), (0, 1, 4, 0), (0, 0, 5, 0), (0, 1, 5, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+97], s[sgprAlpha], v[vgprValuC+97] // *= alpha
v_mul_f32 v[vgprValuC+98], s[sgprAlpha], v[vgprValuC+98] // *= alpha
v_mul_f32 v[vgprValuC+99], s[sgprAlpha], v[vgprValuC+99] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(2), vmcnt(11)                    // vmcnt(11) = 12 - 1 (beta) lgkmcnt(2) = 4 - 1 (bias) - 1 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+20], s[sgprBeta], v10, v[vgprValuC+20] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+21], s[sgprBeta], v10, v[vgprValuC+21] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+22], s[sgprBeta], v11, v[vgprValuC+22] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+23], s[sgprBeta], v11, v[vgprValuC+23] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D

s_waitcnt lgkmcnt(0), vmcnt(11)                    // vmcnt(10) = 12 - 2 (beta) lgkmcnt(0) = 4 - 2 (bias) - 2 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[32:33], v[vgprValuC+36:vgprValuC+36+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[34:35], v[vgprValuC+38:vgprValuC+38+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v24, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v24, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v25, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v25, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[28:29], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[30:31], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D

s_waitcnt vmcnt(11)                                // vmcnt(9) = 12 - 3 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v26, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v26, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v27, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v27, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[12:13], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[14:15], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D

s_waitcnt vmcnt(11)                                // vmcnt(8) = 12 - 4 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[32:33], v[vgprValuC+48:vgprValuC+48+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[34:35], v[vgprValuC+50:vgprValuC+50+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[28:29], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[30:31], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D

s_waitcnt vmcnt(11)                                // vmcnt(7) = 12 - 5 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[16:17], v[vgprValuC+52:vgprValuC+52+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[18:19], v[vgprValuC+54:vgprValuC+54+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+52], s[sgprBeta], v46, v[vgprValuC+52] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+53], s[sgprBeta], v46, v[vgprValuC+53] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v47, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+55], s[sgprBeta], v47, v[vgprValuC+55] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+52:vgprValuC+52+1], v[12:13], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[vgprValuC+54:vgprValuC+54+1], v[14:15], v[vgprValuC+54:vgprValuC+54+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+52], v[vgprValuC+52]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+53], v[vgprValuC+53]     // convert C to fp16
v_pack_b32_f16 v52, v[vgprValuC+52], v[vgprValuC+53] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+54], v[vgprValuC+54]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+55], v[vgprValuC+55]     // convert C to fp16
v_pack_b32_f16 v53, v[vgprValuC+54], v[vgprValuC+55] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[52:53], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D

s_waitcnt vmcnt(11)                                // vmcnt(6) = 12 - 6 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[32:33], v[vgprValuC+60:vgprValuC+60+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[34:35], v[vgprValuC+62:vgprValuC+62+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+60], s[sgprBeta], v56, v[vgprValuC+60] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v56, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+62], s[sgprBeta], v57, v[vgprValuC+62] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+63], s[sgprBeta], v57, v[vgprValuC+63] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[28:29], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[30:31], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
buffer_store_dwordx2 v[60:61], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D

s_waitcnt vmcnt(11)                                // vmcnt(5) = 12 - 7 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[16:17], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[18:19], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+64], s[sgprBeta], v58, v[vgprValuC+64] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+65], s[sgprBeta], v58, v[vgprValuC+65] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+66], s[sgprBeta], v59, v[vgprValuC+66] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+67], s[sgprBeta], v59, v[vgprValuC+67] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[12:13], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[14:15], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[64:65], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D

s_waitcnt vmcnt(11)                                // vmcnt(4) = 12 - 8 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[32:33], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[34:35], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+72], s[sgprBeta], v68, v[vgprValuC+72] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+73], s[sgprBeta], v68, v[vgprValuC+73] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+74], s[sgprBeta], v69, v[vgprValuC+74] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v69, v[vgprValuC+75] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[28:29], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[30:31], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D

s_waitcnt vmcnt(11)                                // vmcnt(3) = 12 - 9 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[16:17], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[18:19], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+76], s[sgprBeta], v70, v[vgprValuC+76] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+77], s[sgprBeta], v70, v[vgprValuC+77] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+78], s[sgprBeta], v71, v[vgprValuC+78] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+79], s[sgprBeta], v71, v[vgprValuC+79] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[12:13], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[14:15], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[76:77], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D

s_waitcnt vmcnt(11)                                // vmcnt(2) = 12 - 10 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[32:33], v[vgprValuC+84:vgprValuC+84+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[34:35], v[vgprValuC+86:vgprValuC+86+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+84], s[sgprBeta], v80, v[vgprValuC+84] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+85], s[sgprBeta], v80, v[vgprValuC+85] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+86], s[sgprBeta], v81, v[vgprValuC+86] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+87], s[sgprBeta], v81, v[vgprValuC+87] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+84:vgprValuC+84+1], v[28:29], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[vgprValuC+86:vgprValuC+86+1], v[30:31], v[vgprValuC+86:vgprValuC+86+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+84], v[vgprValuC+84]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+85], v[vgprValuC+85]     // convert C to fp16
v_pack_b32_f16 v84, v[vgprValuC+84], v[vgprValuC+85] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+86], v[vgprValuC+86]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+87], v[vgprValuC+87]     // convert C to fp16
v_pack_b32_f16 v85, v[vgprValuC+86], v[vgprValuC+87] // Pack with neighbor
buffer_store_dwordx2 v[84:85], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D

s_waitcnt vmcnt(11)                                // vmcnt(1) = 12 - 11 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[16:17], v[vgprValuC+88:vgprValuC+88+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[18:19], v[vgprValuC+90:vgprValuC+90+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+88], s[sgprBeta], v82, v[vgprValuC+88] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+89], s[sgprBeta], v82, v[vgprValuC+89] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+90], s[sgprBeta], v83, v[vgprValuC+90] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+91], s[sgprBeta], v83, v[vgprValuC+91] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+88:vgprValuC+88+1], v[12:13], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[vgprValuC+90:vgprValuC+90+1], v[14:15], v[vgprValuC+90:vgprValuC+90+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+88], v[vgprValuC+88]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+89], v[vgprValuC+89]     // convert C to fp16
v_pack_b32_f16 v88, v[vgprValuC+88], v[vgprValuC+89] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+90], v[vgprValuC+90]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+91], v[vgprValuC+91]     // convert C to fp16
v_pack_b32_f16 v89, v[vgprValuC+90], v[vgprValuC+91] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[88:89], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D

s_waitcnt vmcnt(11)                                // vmcnt(0) = 12 - 12 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+96:vgprValuC+96+1], v[32:33], v[vgprValuC+96:vgprValuC+96+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+98:vgprValuC+98+1], v[34:35], v[vgprValuC+98:vgprValuC+98+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+96], s[sgprBeta], v92, v[vgprValuC+96] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+97], s[sgprBeta], v92, v[vgprValuC+97] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+98], s[sgprBeta], v93, v[vgprValuC+98] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+99], s[sgprBeta], v93, v[vgprValuC+99] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+96:vgprValuC+96+1], v[28:29], v[vgprValuC+96:vgprValuC+96+1] // C += bias
v_pk_add_f32 v[vgprValuC+98:vgprValuC+98+1], v[30:31], v[vgprValuC+98:vgprValuC+98+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+96], v[vgprValuC+96]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+97], v[vgprValuC+97]     // convert C to fp16
v_pack_b32_f16 v96, v[vgprValuC+96], v[vgprValuC+97] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+98], v[vgprValuC+98]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+99], v[vgprValuC+99]     // convert C to fp16
v_pack_b32_f16 v97, v[vgprValuC+98], v[vgprValuC+99] // Pack with neighbor
buffer_store_dwordx2 v[96:97], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Beta Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,0,6,0:vw4); (0,1,6,0:vw4); (0,0,7,0:vw4); (0,1,7,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[10:11], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
ds_read_b128 v[12:15], v8 offset:0                 // load Bias
ds_read_b128 v[16:19], v9 offset:0                 // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
buffer_load_dwordx2 v[24:25], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
ds_read_b128 v[28:31], v8 offset:256               // load Bias
ds_read_b128 v[32:35], v9 offset:256               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[26:27], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
buffer_load_dwordx2 v[44:45], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
v_accvgpr_read_b32 v[vgprValuC+20], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+21], acc49          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+22], acc50          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+23], acc51          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+36], acc52          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+37], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+38], acc54          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+39], acc55          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+40], acc56          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+41], acc57          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+42], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+43], acc59          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+48], acc60          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+49], acc61          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+50], acc62          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+51], acc63          // copy acc to vreg[63]

/* rC *= alpha batchElements=[(0, 0, 6, 0), (0, 1, 6, 0), (0, 0, 7, 0), (0, 1, 7, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(2), vmcnt(3)                     // vmcnt(3) = 4 - 1 (beta) lgkmcnt(2) = 4 - 1 (bias) - 1 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+20], s[sgprBeta], v10, v[vgprValuC+20] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+21], s[sgprBeta], v10, v[vgprValuC+21] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+22], s[sgprBeta], v11, v[vgprValuC+22] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+23], s[sgprBeta], v11, v[vgprValuC+23] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D

s_waitcnt lgkmcnt(0), vmcnt(3)                     // vmcnt(2) = 4 - 2 (beta) lgkmcnt(0) = 4 - 2 (bias) - 2 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[32:33], v[vgprValuC+36:vgprValuC+36+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[34:35], v[vgprValuC+38:vgprValuC+38+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v24, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v24, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v25, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v25, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[28:29], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[30:31], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D

s_waitcnt vmcnt(3)                                 // vmcnt(1) = 4 - 3 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v26, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v26, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v27, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v27, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[12:13], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[14:15], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D

s_waitcnt vmcnt(3)                                 // vmcnt(0) = 4 - 4 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[32:33], v[vgprValuC+48:vgprValuC+48+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[34:35], v[vgprValuC+50:vgprValuC+50+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[28:29], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[30:31], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128, sc0 sc1 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_2                            // jump to end
label_GW_B1_E1_N_1:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=10 */
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,1,0,0:vw4); (0,0,1,0:vw4); (0,1,1,0:vw4); (0,0,2,0:vw4); (0,1,2,0:vw4); (0,0,3,0:vw4); (0,1,3,0:vw4); (0,0,4,0:vw4); (0,1,4,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v112, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v112, v6, s[72:73]               // LDC clip if OOB. offset
buffer_load_dwordx2 v[10:11], v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v0, s68
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for LDS write
s_barrier                                          // LDS write barrier
ds_read_b128 v[12:15], v7 offset:0                 // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b128 v[16:19], v8 offset:0                 // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v112, v6, s[72:73]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v9, v2, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v112, v9, s[72:73]               // LDC clip if OOB. offset
buffer_load_dwordx2 v[26:27], v9, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v4, s68
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
ds_read_b128 v[28:31], v24 offset:0                // load Bias
v_add_u32 v25, 1024, v24                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[32:35], v25 offset:0                // load scaleAlpha
v_add_lshl_u32 v9, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v112, v9, s[72:73]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v40, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v112, v40, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[44:45], v40, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v41, v0, s68
v_lshlrev_b32 v41, 0x2, v41                        // Bias address scaled by BPE
v_add_u32 v42, 1024, v41                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v40, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v112, v40, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v43, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v112, v43, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[52:53], v43, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s68
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
v_add_u32 v47, 1024, v46                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v43, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v112, v43, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v54, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v112, v54, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[62:63], v54, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v55, v0, s68
v_lshlrev_b32 v55, 0x2, v55                        // Bias address scaled by BPE
v_add_u32 v60, 1024, v55                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v54, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v112, v54, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v61, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v112, v61, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[70:71], v61, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v68, v4, s68
v_lshlrev_b32 v68, 0x2, v68                        // Bias address scaled by BPE
v_add_u32 v69, 1024, v68                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v61, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v112, v61, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v76, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v112, v76, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[80:81], v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v0, s68
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
v_add_u32 v78, 1024, v77                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v76, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v112, v76, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v79, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v79, v112, v79, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[88:89], v79, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v4, s68
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_add_u32 v83, 1024, v82                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v79, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v79, v112, v79, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v90, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v112, v90, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[98:99], v90, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v0, s68
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
v_add_u32 v96, 1024, v91                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v90, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v112, v90, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v97, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v112, v97, s[72:73]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[106:107], v97, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v104, v4, s68
v_lshlrev_b32 v104, 0x2, v104                      // Bias address scaled by BPE
v_add_u32 v105, 1024, v104                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v97, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v112, v97, s[72:73]             // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc1           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+36], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+37], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+38], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+39], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+48], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+49], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+50], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+51], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+56], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+57], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+58], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+59], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+64], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+65], acc17          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+66], acc18          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+67], acc19          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+72], acc20          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+73], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+74], acc22          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+75], acc23          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+84], acc24          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+85], acc25          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+86], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+87], acc27          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+92], acc28          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+93], acc29          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+94], acc30          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+95], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+100], acc32         // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+101], acc33         // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+102], acc34         // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+103], acc35         // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+108], acc36         // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+109], acc37         // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+110], acc38         // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+111], acc39         // copy acc to vreg[39]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0), (0, 1, 1, 0), (0, 0, 2, 0), (0, 1, 2, 0), (0, 0, 3, 0), (0, 1, 3, 0), (0, 0, 4, 0), (0, 1, 4, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+108], s[sgprAlpha], v[vgprValuC+108] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+20], s[sgprBeta], v10, v[vgprValuC+20] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+21], s[sgprBeta], v10, v[vgprValuC+21] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+22], s[sgprBeta], v11, v[vgprValuC+22] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+23], s[sgprBeta], v11, v[vgprValuC+23] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[32:33], v[vgprValuC+36:vgprValuC+36+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[34:35], v[vgprValuC+38:vgprValuC+38+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v26, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v26, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v27, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v27, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[28:29], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[30:31], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[12:13], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[14:15], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[32:33], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[34:35], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+56], s[sgprBeta], v52, v[vgprValuC+56] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+57], s[sgprBeta], v52, v[vgprValuC+57] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+58], s[sgprBeta], v53, v[vgprValuC+58] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+59], s[sgprBeta], v53, v[vgprValuC+59] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[28:29], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[30:31], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v43, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[16:17], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[18:19], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+64], s[sgprBeta], v62, v[vgprValuC+64] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+65], s[sgprBeta], v62, v[vgprValuC+65] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+66], s[sgprBeta], v63, v[vgprValuC+66] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+67], s[sgprBeta], v63, v[vgprValuC+67] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[12:13], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[14:15], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[32:33], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[34:35], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+72], s[sgprBeta], v70, v[vgprValuC+72] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+73], s[sgprBeta], v70, v[vgprValuC+73] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+74], s[sgprBeta], v71, v[vgprValuC+74] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v71, v[vgprValuC+75] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[28:29], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[30:31], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[16:17], v[vgprValuC+84:vgprValuC+84+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[18:19], v[vgprValuC+86:vgprValuC+86+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+84], s[sgprBeta], v80, v[vgprValuC+84] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+85], s[sgprBeta], v80, v[vgprValuC+85] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+86], s[sgprBeta], v81, v[vgprValuC+86] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+87], s[sgprBeta], v81, v[vgprValuC+87] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+84:vgprValuC+84+1], v[12:13], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[vgprValuC+86:vgprValuC+86+1], v[14:15], v[vgprValuC+86:vgprValuC+86+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+84], v[vgprValuC+84]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+85], v[vgprValuC+85]     // convert C to fp16
v_pack_b32_f16 v84, v[vgprValuC+84], v[vgprValuC+85] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+86], v[vgprValuC+86]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+87], v[vgprValuC+87]     // convert C to fp16
v_pack_b32_f16 v85, v[vgprValuC+86], v[vgprValuC+87] // Pack with neighbor
buffer_store_dwordx2 v[84:85], v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[32:33], v[vgprValuC+92:vgprValuC+92+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[34:35], v[vgprValuC+94:vgprValuC+94+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+92], s[sgprBeta], v88, v[vgprValuC+92] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+93], s[sgprBeta], v88, v[vgprValuC+93] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+94], s[sgprBeta], v89, v[vgprValuC+94] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+95], s[sgprBeta], v89, v[vgprValuC+95] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+92:vgprValuC+92+1], v[28:29], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[vgprValuC+94:vgprValuC+94+1], v[30:31], v[vgprValuC+94:vgprValuC+94+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+92], v[vgprValuC+92]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+93], v[vgprValuC+93]     // convert C to fp16
v_pack_b32_f16 v92, v[vgprValuC+92], v[vgprValuC+93] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+94], v[vgprValuC+94]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+95], v[vgprValuC+95]     // convert C to fp16
v_pack_b32_f16 v93, v[vgprValuC+94], v[vgprValuC+95] // Pack with neighbor
buffer_store_dwordx2 v[92:93], v79, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[16:17], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[18:19], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+100], s[sgprBeta], v98, v[vgprValuC+100] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+101], s[sgprBeta], v98, v[vgprValuC+101] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+102], s[sgprBeta], v99, v[vgprValuC+102] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+103], s[sgprBeta], v99, v[vgprValuC+103] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[12:13], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[14:15], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
buffer_store_dwordx2 v[100:101], v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[32:33], v[vgprValuC+108:vgprValuC+108+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[34:35], v[vgprValuC+110:vgprValuC+110+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+108], s[sgprBeta], v106, v[vgprValuC+108] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+109], s[sgprBeta], v106, v[vgprValuC+109] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+110], s[sgprBeta], v107, v[vgprValuC+110] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+111], s[sgprBeta], v107, v[vgprValuC+111] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+108:vgprValuC+108+1], v[28:29], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[vgprValuC+110:vgprValuC+110+1], v[30:31], v[vgprValuC+110:vgprValuC+110+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+108], v[vgprValuC+108]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+109], v[vgprValuC+109]   // convert C to fp16
v_pack_b32_f16 v108, v[vgprValuC+108], v[vgprValuC+109] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+110], v[vgprValuC+110]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+111], v[vgprValuC+111]   // convert C to fp16
v_pack_b32_f16 v109, v[vgprValuC+110], v[vgprValuC+111] // Pack with neighbor
buffer_store_dwordx2 v[108:109], v97, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,0,5,0:vw4); (0,1,5,0:vw4); (0,0,6,0:vw4); (0,1,6,0:vw4); (0,0,7,0:vw4); (0,1,7,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v76, BufferOOB
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v76, v6, s[72:73]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[10:11], v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v0, s68
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b128 v[12:15], v7 offset:0                 // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b128 v[16:19], v8 offset:0                 // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v76, v6, s[72:73]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v9, v2, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v76, v9, s[72:73]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[26:27], v9, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v4, s68
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
ds_read_b128 v[28:31], v24 offset:0                // load Bias
v_add_u32 v25, 1024, v24                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[32:35], v25 offset:0                // load scaleAlpha
v_add_lshl_u32 v9, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v76, v9, s[72:73]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v40, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v76, v40, s[72:73]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[44:45], v40, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v41, v0, s68
v_lshlrev_b32 v41, 0x2, v41                        // Bias address scaled by BPE
v_add_u32 v42, 1024, v41                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v40, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v76, v40, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v43, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v76, v43, s[72:73]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[52:53], v43, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s68
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
v_add_u32 v47, 1024, v46                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v43, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v76, v43, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v54, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v76, v54, s[72:73]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[62:63], v54, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v55, v0, s68
v_lshlrev_b32 v55, 0x2, v55                        // Bias address scaled by BPE
v_add_u32 v60, 1024, v55                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v54, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v76, v54, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v61, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v76, v61, s[72:73]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[70:71], v61, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v68, v4, s68
v_lshlrev_b32 v68, 0x2, v68                        // Bias address scaled by BPE
v_add_u32 v69, 1024, v68                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v61, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v76, v61, s[72:73]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc40          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+21], acc41          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+22], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+23], acc43          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+36], acc44          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+37], acc45          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+38], acc46          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+39], acc47          // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+48], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+49], acc49          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+50], acc50          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+51], acc51          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+56], acc52          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+57], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+58], acc54          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+59], acc55          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+64], acc56          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+65], acc57          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+66], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+67], acc59          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+72], acc60          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+73], acc61          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+74], acc62          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+75], acc63          // copy acc to vreg[63]

/* rC *= alpha batchElements=[(0, 0, 5, 0), (0, 1, 5, 0), (0, 0, 6, 0), (0, 1, 6, 0), (0, 0, 7, 0), (0, 1, 7, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+20], s[sgprBeta], v10, v[vgprValuC+20] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+21], s[sgprBeta], v10, v[vgprValuC+21] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+22], s[sgprBeta], v11, v[vgprValuC+22] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+23], s[sgprBeta], v11, v[vgprValuC+23] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[32:33], v[vgprValuC+36:vgprValuC+36+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[34:35], v[vgprValuC+38:vgprValuC+38+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v26, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v26, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v27, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v27, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[28:29], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[30:31], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[12:13], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[14:15], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[32:33], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[34:35], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+56], s[sgprBeta], v52, v[vgprValuC+56] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+57], s[sgprBeta], v52, v[vgprValuC+57] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+58], s[sgprBeta], v53, v[vgprValuC+58] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+59], s[sgprBeta], v53, v[vgprValuC+59] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[28:29], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[30:31], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v43, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[16:17], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[18:19], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+64], s[sgprBeta], v62, v[vgprValuC+64] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+65], s[sgprBeta], v62, v[vgprValuC+65] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+66], s[sgprBeta], v63, v[vgprValuC+66] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+67], s[sgprBeta], v63, v[vgprValuC+67] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[12:13], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[14:15], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[32:33], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[34:35], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+72], s[sgprBeta], v70, v[vgprValuC+72] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+73], s[sgprBeta], v70, v[vgprValuC+73] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+74], s[sgprBeta], v71, v[vgprValuC+74] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v71, v[vgprValuC+75] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[28:29], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[30:31], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_2                            // jump to end
label_GW_B1_E1_M_1:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=26 */
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw1); (0,0,0,1:vw1); (0,0,0,2:vw1); (0,0,0,3:vw1); (0,1,0,0:vw1); (0,1,0,1:vw1); (0,1,0,2:vw1); (0,1,0,3:vw1); (0,0,1,0:vw1); (0,0,1,1:vw1); (0,0,1,2:vw1); (0,0,1,3:vw1); (0,1,1,0:vw1); (0,1,1,1:vw1); (0,1,1,2:vw1); (0,1,1,3:vw1); (0,0,2,0:vw1); (0,0,2,1:vw1); (0,0,2,2:vw1); (0,0,2,3:vw1); (0,1,2,0:vw1); (0,1,2,1:vw1); (0,1,2,2:vw1); (0,1,2,3:vw1); (0,0,3,0:vw1); (0,0,3,1:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v153, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v153, v6, s[72:73]               // LDC clip if OOB. offset
buffer_load_short_d16 v9, v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v0, s68
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for LDS write
s_barrier                                          // LDS write barrier
ds_read_b32 v10, v7 offset:0                       // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v11, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v153, v6, s[72:73]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v13, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v153, v13, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v4, s68
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
ds_read_b32 v17, v14 offset:0                      // load Bias
v_add_u32 v15, 1024, v14                           // add ScaleAlphaVec offset (3)
ds_read_b32 v18, v15 offset:0                      // load scaleAlpha
v_add_lshl_u32 v13, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v153, v13, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v153, v20, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s68
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
ds_read_b32 v24, v21 offset:0                      // load Bias
v_add_u32 v22, 1024, v21                           // add ScaleAlphaVec offset (3)
ds_read_b32 v25, v22 offset:0                      // load scaleAlpha
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v153, v20, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v153, v27, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s68
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
ds_read_b32 v31, v28 offset:0                      // load Bias
v_add_u32 v29, 1024, v28                           // add ScaleAlphaVec offset (3)
ds_read_b32 v32, v29 offset:0                      // load scaleAlpha
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v153, v27, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v153, v34, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s68
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
ds_read_b32 v38, v35 offset:0                      // load Bias
v_add_u32 v36, 1024, v35                           // add ScaleAlphaVec offset (3)
ds_read_b32 v39, v36 offset:0                      // load scaleAlpha
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v153, v34, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v41, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v153, v41, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s68
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
ds_read_b32 v45, v42 offset:0                      // load Bias
v_add_u32 v43, 1024, v42                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v43 offset:0                      // load scaleAlpha
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v153, v41, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v48, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v153, v48, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v51, v48, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v4, s68
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
ds_read_b32 v52, v49 offset:0                      // load Bias
v_add_u32 v50, 1024, v49                           // add ScaleAlphaVec offset (3)
ds_read_b32 v53, v50 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v153, v48, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v55, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v153, v55, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v58, v55, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v56, v4, s68
v_lshlrev_b32 v56, 0x2, v56                        // Bias address scaled by BPE
ds_read_b32 v59, v56 offset:0                      // load Bias
v_add_u32 v57, 1024, v56                           // add ScaleAlphaVec offset (3)
ds_read_b32 v60, v57 offset:0                      // load scaleAlpha
v_add_lshl_u32 v55, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v153, v55, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v62, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v153, v62, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v65, v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v0, s68
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
v_add_u32 v64, 1024, v63                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v62, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v153, v62, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v67, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v67, v153, v67, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v70, v67, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v68, v4, s68
v_lshlrev_b32 v68, 0x2, v68                        // Bias address scaled by BPE
v_add_u32 v69, 1024, v68                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v67, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v67, v153, v67, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v72, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v72, v153, v72, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v75, v72, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v73, v4, s68
v_lshlrev_b32 v73, 0x2, v73                        // Bias address scaled by BPE
v_add_u32 v74, 1024, v73                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v72, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v72, v153, v72, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v77, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v153, v77, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v80, v77, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v78, v4, s68
v_lshlrev_b32 v78, 0x2, v78                        // Bias address scaled by BPE
v_add_u32 v79, 1024, v78                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v153, v77, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v82, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v82, v153, v82, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v85, v82, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v83, v4, s68
v_lshlrev_b32 v83, 0x2, v83                        // Bias address scaled by BPE
v_add_u32 v84, 1024, v83                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v82, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v82, v153, v82, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v87, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v87, v153, v87, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v90, v87, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v88, v4, s68
v_lshlrev_b32 v88, 0x2, v88                        // Bias address scaled by BPE
v_add_u32 v89, 1024, v88                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v87, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v87, v153, v87, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v92, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v92, v153, v92, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v95, v92, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v93, v4, s68
v_lshlrev_b32 v93, 0x2, v93                        // Bias address scaled by BPE
v_add_u32 v94, 1024, v93                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v92, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v92, v153, v92, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v97, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v153, v97, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v100, v97, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v98, v4, s68
v_lshlrev_b32 v98, 0x2, v98                        // Bias address scaled by BPE
v_add_u32 v99, 1024, v98                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v97, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v153, v97, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v102, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v102, v153, v102, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v105, v102, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v103, v0, s68
v_lshlrev_b32 v103, 0x2, v103                      // Bias address scaled by BPE
v_add_u32 v104, 1024, v103                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v102, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v102, v153, v102, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v107, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v107, v153, v107, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v110, v107, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v108, v4, s68
v_lshlrev_b32 v108, 0x2, v108                      // Bias address scaled by BPE
v_add_u32 v109, 1024, v108                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v107, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v107, v153, v107, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v112, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v112, v153, v112, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v115, v112, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v113, v4, s68
v_lshlrev_b32 v113, 0x2, v113                      // Bias address scaled by BPE
v_add_u32 v114, 1024, v113                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v112, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v112, v153, v112, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v117, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v117, v153, v117, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v120, v117, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v118, v4, s68
v_lshlrev_b32 v118, 0x2, v118                      // Bias address scaled by BPE
v_add_u32 v119, 1024, v118                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v117, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v117, v153, v117, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v122, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v122, v153, v122, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v125, v122, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v123, v4, s68
v_lshlrev_b32 v123, 0x2, v123                      // Bias address scaled by BPE
v_add_u32 v124, 1024, v123                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v122, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v122, v153, v122, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v127, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v127, v153, v127, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v130, v127, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v128, v4, s68
v_lshlrev_b32 v128, 0x2, v128                      // Bias address scaled by BPE
v_add_u32 v129, 1024, v128                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v127, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v127, v153, v127, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v132, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v132, v153, v132, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v135, v132, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v133, v4, s68
v_lshlrev_b32 v133, 0x2, v133                      // Bias address scaled by BPE
v_add_u32 v134, 1024, v133                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v132, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v132, v153, v132, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v137, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v137, v153, v137, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v141, v137, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v138, v4, s68
v_lshlrev_b32 v138, 0x2, v138                      // Bias address scaled by BPE
v_add_u32 v140, 1024, v138                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v137, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v137, v153, v137, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v143, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v143, v153, v143, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v146, v143, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v144, v0, s68
v_lshlrev_b32 v144, 0x2, v144                      // Bias address scaled by BPE
v_add_u32 v145, 1024, v144                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v143, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v143, v153, v143, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v148, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v148, v153, v148, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v151, v148, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v149, v4, s68
v_lshlrev_b32 v149, 0x2, v149                      // Bias address scaled by BPE
v_add_u32 v150, 1024, v149                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v148, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v148, v153, v148, s[72:73]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+12], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+19], acc1           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+33], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+40], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+47], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+54], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+61], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+66], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+71], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+76], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+81], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+86], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+91], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+96], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+101], acc15         // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+106], acc16         // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+111], acc17         // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+116], acc18         // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+121], acc19         // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+126], acc20         // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+131], acc21         // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+136], acc22         // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+142], acc23         // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+147], acc24         // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+152], acc25         // copy acc to vreg[25]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 0, 1), (0, 0, 0, 2), (0, 0, 0, 3), (0, 1, 0, 0), (0, 1, 0, 1), (0, 1, 0, 2), (0, 1, 0, 3), (0, 0, 1, 0), (0, 0, 1, 1), (0, 0, 1, 2), (0, 0, 1, 3), (0, 1, 1, 0), (0, 1, 1, 1), (0, 1, 1, 2), (0, 1, 1, 3), (0, 0, 2, 0), (0, 0, 2, 1), (0, 0, 2, 2), (0, 0, 2, 3), (0, 1, 2, 0), (0, 1, 2, 1), (0, 1, 2, 2), (0, 1, 2, 3), (0, 0, 3, 0), (0, 0, 3, 1)] */
v_mul_f32 v[vgprValuC+12], s[sgprAlpha], v[vgprValuC+12] // *= alpha
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+121], s[sgprAlpha], v[vgprValuC+121] // *= alpha
v_mul_f32 v[vgprValuC+126], s[sgprAlpha], v[vgprValuC+126] // *= alpha
v_mul_f32 v[vgprValuC+131], s[sgprAlpha], v[vgprValuC+131] // *= alpha
v_mul_f32 v[vgprValuC+136], s[sgprAlpha], v[vgprValuC+136] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+147], s[sgprAlpha], v[vgprValuC+147] // *= alpha
v_mul_f32 v[vgprValuC+152], s[sgprAlpha], v[vgprValuC+152] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+12], v11, v[vgprValuC+12]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+12], s[sgprBeta], v9, v[vgprValuC+12] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+12], v10, v[vgprValuC+12]    // C += bias
v_cvt_f16_f32 v12, v[vgprValuC+12]                 // convert C to fp16
buffer_store_short v12, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+19], s[sgprBeta], v16, v[vgprValuC+19] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+19], v17, v[vgprValuC+19]    // C += bias
v_cvt_f16_f32 v19, v[vgprValuC+19]                 // convert C to fp16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v23, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+26], v24, v[vgprValuC+26]    // C += bias
v_cvt_f16_f32 v26, v[vgprValuC+26]                 // convert C to fp16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v30, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+33], v31, v[vgprValuC+33]    // C += bias
v_cvt_f16_f32 v33, v[vgprValuC+33]                 // convert C to fp16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v37, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+40], v38, v[vgprValuC+40]    // C += bias
v_cvt_f16_f32 v40, v[vgprValuC+40]                 // convert C to fp16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v44, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+54], v53, v[vgprValuC+54]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v51, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+54], v52, v[vgprValuC+54]    // C += bias
v_cvt_f16_f32 v54, v[vgprValuC+54]                 // convert C to fp16
buffer_store_short v54, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+61], v60, v[vgprValuC+61]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v58, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+61], v59, v[vgprValuC+61]    // C += bias
v_cvt_f16_f32 v61, v[vgprValuC+61]                 // convert C to fp16
buffer_store_short v61, v55, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+66], v11, v[vgprValuC+66]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+66], s[sgprBeta], v65, v[vgprValuC+66] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+66], v10, v[vgprValuC+66]    // C += bias
v_cvt_f16_f32 v66, v[vgprValuC+66]                 // convert C to fp16
buffer_store_short v66, v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+71], v18, v[vgprValuC+71]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+71], s[sgprBeta], v70, v[vgprValuC+71] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+71], v17, v[vgprValuC+71]    // C += bias
v_cvt_f16_f32 v71, v[vgprValuC+71]                 // convert C to fp16
buffer_store_short v71, v67, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+76], v25, v[vgprValuC+76]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+76], s[sgprBeta], v75, v[vgprValuC+76] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+76], v24, v[vgprValuC+76]    // C += bias
v_cvt_f16_f32 v76, v[vgprValuC+76]                 // convert C to fp16
buffer_store_short v76, v72, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+81], v32, v[vgprValuC+81]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+81], s[sgprBeta], v80, v[vgprValuC+81] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+81], v31, v[vgprValuC+81]    // C += bias
v_cvt_f16_f32 v81, v[vgprValuC+81]                 // convert C to fp16
buffer_store_short v81, v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+86], v39, v[vgprValuC+86]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+86], s[sgprBeta], v85, v[vgprValuC+86] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+86], v38, v[vgprValuC+86]    // C += bias
v_cvt_f16_f32 v86, v[vgprValuC+86]                 // convert C to fp16
buffer_store_short v86, v82, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+91], v46, v[vgprValuC+91]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+91], s[sgprBeta], v90, v[vgprValuC+91] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+91], v45, v[vgprValuC+91]    // C += bias
v_cvt_f16_f32 v91, v[vgprValuC+91]                 // convert C to fp16
buffer_store_short v91, v87, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+96], v53, v[vgprValuC+96]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+96], s[sgprBeta], v95, v[vgprValuC+96] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+96], v52, v[vgprValuC+96]    // C += bias
v_cvt_f16_f32 v96, v[vgprValuC+96]                 // convert C to fp16
buffer_store_short v96, v92, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+101], v60, v[vgprValuC+101]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+101], s[sgprBeta], v100, v[vgprValuC+101] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+101], v59, v[vgprValuC+101]  // C += bias
v_cvt_f16_f32 v101, v[vgprValuC+101]               // convert C to fp16
buffer_store_short v101, v97, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+106], v11, v[vgprValuC+106]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+106], s[sgprBeta], v105, v[vgprValuC+106] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+106], v10, v[vgprValuC+106]  // C += bias
v_cvt_f16_f32 v106, v[vgprValuC+106]               // convert C to fp16
buffer_store_short v106, v102, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+111], v18, v[vgprValuC+111]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+111], s[sgprBeta], v110, v[vgprValuC+111] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+111], v17, v[vgprValuC+111]  // C += bias
v_cvt_f16_f32 v111, v[vgprValuC+111]               // convert C to fp16
buffer_store_short v111, v107, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+116], v25, v[vgprValuC+116]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+116], s[sgprBeta], v115, v[vgprValuC+116] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+116], v24, v[vgprValuC+116]  // C += bias
v_cvt_f16_f32 v116, v[vgprValuC+116]               // convert C to fp16
buffer_store_short v116, v112, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+121], v32, v[vgprValuC+121]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+121], s[sgprBeta], v120, v[vgprValuC+121] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+121], v31, v[vgprValuC+121]  // C += bias
v_cvt_f16_f32 v121, v[vgprValuC+121]               // convert C to fp16
buffer_store_short v121, v117, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+126], v39, v[vgprValuC+126]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+126], s[sgprBeta], v125, v[vgprValuC+126] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+126], v38, v[vgprValuC+126]  // C += bias
v_cvt_f16_f32 v126, v[vgprValuC+126]               // convert C to fp16
buffer_store_short v126, v122, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+131], v46, v[vgprValuC+131]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+131], s[sgprBeta], v130, v[vgprValuC+131] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+131], v45, v[vgprValuC+131]  // C += bias
v_cvt_f16_f32 v131, v[vgprValuC+131]               // convert C to fp16
buffer_store_short v131, v127, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+136], v53, v[vgprValuC+136]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+136], s[sgprBeta], v135, v[vgprValuC+136] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+136], v52, v[vgprValuC+136]  // C += bias
v_cvt_f16_f32 v136, v[vgprValuC+136]               // convert C to fp16
buffer_store_short v136, v132, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+142], v60, v[vgprValuC+142]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+142], s[sgprBeta], v141, v[vgprValuC+142] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+142], v59, v[vgprValuC+142]  // C += bias
v_cvt_f16_f32 v142, v[vgprValuC+142]               // convert C to fp16
buffer_store_short v142, v137, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+147], v11, v[vgprValuC+147]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+147], s[sgprBeta], v146, v[vgprValuC+147] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+147], v10, v[vgprValuC+147]  // C += bias
v_cvt_f16_f32 v147, v[vgprValuC+147]               // convert C to fp16
buffer_store_short v147, v143, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+152], v18, v[vgprValuC+152]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+152], s[sgprBeta], v151, v[vgprValuC+152] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+152], v17, v[vgprValuC+152]  // C += bias
v_cvt_f16_f32 v152, v[vgprValuC+152]               // convert C to fp16
buffer_store_short v152, v148, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,0,3,2:vw1); (0,0,3,3:vw1); (0,1,3,0:vw1); (0,1,3,1:vw1); (0,1,3,2:vw1); (0,1,3,3:vw1); (0,0,4,0:vw1); (0,0,4,1:vw1); (0,0,4,2:vw1); (0,0,4,3:vw1); (0,1,4,0:vw1); (0,1,4,1:vw1); (0,1,4,2:vw1); (0,1,4,3:vw1); (0,0,5,0:vw1); (0,0,5,1:vw1); (0,0,5,2:vw1); (0,0,5,3:vw1); (0,1,5,0:vw1); (0,1,5,1:vw1); (0,1,5,2:vw1); (0,1,5,3:vw1); (0,0,6,0:vw1); (0,0,6,1:vw1); (0,0,6,2:vw1); (0,0,6,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v153, BufferOOB
/* (d1,vc1,d0,vc0)=(0,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v6, v2, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v153, v6, s[72:73]               // LDC clip if OOB. offset
buffer_load_short_d16 v9, v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v4, s68
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b32 v10, v7 offset:0                       // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v11, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v153, v6, s[72:73]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v13, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v153, v13, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v4, s68
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
ds_read_b32 v17, v14 offset:0                      // load Bias
v_add_u32 v15, 1024, v14                           // add ScaleAlphaVec offset (3)
ds_read_b32 v18, v15 offset:0                      // load scaleAlpha
v_add_lshl_u32 v13, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v153, v13, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v153, v20, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s68
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
ds_read_b32 v24, v21 offset:0                      // load Bias
v_add_u32 v22, 1024, v21                           // add ScaleAlphaVec offset (3)
ds_read_b32 v25, v22 offset:0                      // load scaleAlpha
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v153, v20, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v153, v27, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s68
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
ds_read_b32 v31, v28 offset:0                      // load Bias
v_add_u32 v29, 1024, v28                           // add ScaleAlphaVec offset (3)
ds_read_b32 v32, v29 offset:0                      // load scaleAlpha
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v153, v27, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v153, v34, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s68
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
ds_read_b32 v38, v35 offset:0                      // load Bias
v_add_u32 v36, 1024, v35                           // add ScaleAlphaVec offset (3)
ds_read_b32 v39, v36 offset:0                      // load scaleAlpha
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v153, v34, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v41, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v153, v41, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s68
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
ds_read_b32 v45, v42 offset:0                      // load Bias
v_add_u32 v43, 1024, v42                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v43 offset:0                      // load scaleAlpha
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v153, v41, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v48, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v153, v48, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v51, v48, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v0, s68
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
ds_read_b32 v52, v49 offset:0                      // load Bias
v_add_u32 v50, 1024, v49                           // add ScaleAlphaVec offset (3)
ds_read_b32 v53, v50 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v153, v48, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v55, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v153, v55, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v58, v55, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v56, v4, s68
v_lshlrev_b32 v56, 0x2, v56                        // Bias address scaled by BPE
ds_read_b32 v59, v56 offset:0                      // load Bias
v_add_u32 v57, 1024, v56                           // add ScaleAlphaVec offset (3)
ds_read_b32 v60, v57 offset:0                      // load scaleAlpha
v_add_lshl_u32 v55, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v153, v55, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v62, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v153, v62, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v65, v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v4, s68
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
v_add_u32 v64, 1024, v63                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v62, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v153, v62, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v67, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v67, v153, v67, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v70, v67, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v68, v4, s68
v_lshlrev_b32 v68, 0x2, v68                        // Bias address scaled by BPE
v_add_u32 v69, 1024, v68                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v67, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v67, v153, v67, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v72, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v72, v153, v72, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v75, v72, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v73, v4, s68
v_lshlrev_b32 v73, 0x2, v73                        // Bias address scaled by BPE
v_add_u32 v74, 1024, v73                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v72, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v72, v153, v72, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v77, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v153, v77, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v80, v77, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v78, v4, s68
v_lshlrev_b32 v78, 0x2, v78                        // Bias address scaled by BPE
v_add_u32 v79, 1024, v78                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v153, v77, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v82, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v82, v153, v82, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v85, v82, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v83, v4, s68
v_lshlrev_b32 v83, 0x2, v83                        // Bias address scaled by BPE
v_add_u32 v84, 1024, v83                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v82, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v82, v153, v82, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v87, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v87, v153, v87, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v90, v87, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v88, v4, s68
v_lshlrev_b32 v88, 0x2, v88                        // Bias address scaled by BPE
v_add_u32 v89, 1024, v88                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v87, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v87, v153, v87, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v92, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v92, v153, v92, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16 v95, v92, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v93, v0, s68
v_lshlrev_b32 v93, 0x2, v93                        // Bias address scaled by BPE
v_add_u32 v94, 1024, v93                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v92, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v92, v153, v92, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v97, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v153, v97, s[72:73]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v100, v97, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v98, v4, s68
v_lshlrev_b32 v98, 0x2, v98                        // Bias address scaled by BPE
v_add_u32 v99, 1024, v98                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v97, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v153, v97, s[72:73]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v102, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v102, v153, v102, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v105, v102, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v103, v4, s68
v_lshlrev_b32 v103, 0x2, v103                      // Bias address scaled by BPE
v_add_u32 v104, 1024, v103                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v102, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v102, v153, v102, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v107, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v107, v153, v107, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v110, v107, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v108, v4, s68
v_lshlrev_b32 v108, 0x2, v108                      // Bias address scaled by BPE
v_add_u32 v109, 1024, v108                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v107, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v107, v153, v107, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v112, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v112, v153, v112, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v115, v112, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v113, v4, s68
v_lshlrev_b32 v113, 0x2, v113                      // Bias address scaled by BPE
v_add_u32 v114, 1024, v113                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v112, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v112, v153, v112, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v117, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v117, v153, v117, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v120, v117, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v118, v4, s68
v_lshlrev_b32 v118, 0x2, v118                      // Bias address scaled by BPE
v_add_u32 v119, 1024, v118                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v117, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v117, v153, v117, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v122, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v122, v153, v122, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v125, v122, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v123, v4, s68
v_lshlrev_b32 v123, 0x2, v123                      // Bias address scaled by BPE
v_add_u32 v124, 1024, v123                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v122, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v122, v153, v122, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v127, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v127, v153, v127, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v130, v127, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v128, v4, s68
v_lshlrev_b32 v128, 0x2, v128                      // Bias address scaled by BPE
v_add_u32 v129, 1024, v128                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v127, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v127, v153, v127, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v132, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v132, v153, v132, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v135, v132, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v133, v0, s68
v_lshlrev_b32 v133, 0x2, v133                      // Bias address scaled by BPE
v_add_u32 v134, 1024, v133                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v132, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v132, v153, v132, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v137, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v137, v153, v137, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v141, v137, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v138, v4, s68
v_lshlrev_b32 v138, 0x2, v138                      // Bias address scaled by BPE
v_add_u32 v140, 1024, v138                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v137, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v137, v153, v137, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v143, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v143, v153, v143, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16 v146, v143, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v144, v4, s68
v_lshlrev_b32 v144, 0x2, v144                      // Bias address scaled by BPE
v_add_u32 v145, 1024, v144                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v143, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v143, v153, v143, s[72:73]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v148, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v148, v153, v148, s[72:73]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v151, v148, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v149, v4, s68
v_lshlrev_b32 v149, 0x2, v149                      // Bias address scaled by BPE
v_add_u32 v150, 1024, v149                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v148, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v148, v153, v148, s[72:73]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+12], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+19], acc27          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+26], acc28          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+33], acc29          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+40], acc30          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+47], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+54], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+61], acc33          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+66], acc34          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+71], acc35          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+76], acc36          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+81], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+86], acc38          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+91], acc39          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+96], acc40          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+101], acc41         // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+106], acc42         // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+111], acc43         // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+116], acc44         // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+121], acc45         // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+126], acc46         // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+131], acc47         // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+136], acc48         // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+142], acc49         // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+147], acc50         // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+152], acc51         // copy acc to vreg[51]

/* rC *= alpha batchElements=[(0, 0, 3, 2), (0, 0, 3, 3), (0, 1, 3, 0), (0, 1, 3, 1), (0, 1, 3, 2), (0, 1, 3, 3), (0, 0, 4, 0), (0, 0, 4, 1), (0, 0, 4, 2), (0, 0, 4, 3), (0, 1, 4, 0), (0, 1, 4, 1), (0, 1, 4, 2), (0, 1, 4, 3), (0, 0, 5, 0), (0, 0, 5, 1), (0, 0, 5, 2), (0, 0, 5, 3), (0, 1, 5, 0), (0, 1, 5, 1), (0, 1, 5, 2), (0, 1, 5, 3), (0, 0, 6, 0), (0, 0, 6, 1), (0, 0, 6, 2), (0, 0, 6, 3)] */
v_mul_f32 v[vgprValuC+12], s[sgprAlpha], v[vgprValuC+12] // *= alpha
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+121], s[sgprAlpha], v[vgprValuC+121] // *= alpha
v_mul_f32 v[vgprValuC+126], s[sgprAlpha], v[vgprValuC+126] // *= alpha
v_mul_f32 v[vgprValuC+131], s[sgprAlpha], v[vgprValuC+131] // *= alpha
v_mul_f32 v[vgprValuC+136], s[sgprAlpha], v[vgprValuC+136] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+147], s[sgprAlpha], v[vgprValuC+147] // *= alpha
v_mul_f32 v[vgprValuC+152], s[sgprAlpha], v[vgprValuC+152] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+12], v11, v[vgprValuC+12]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+12], s[sgprBeta], v9, v[vgprValuC+12] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+12], v10, v[vgprValuC+12]    // C += bias
v_cvt_f16_f32 v12, v[vgprValuC+12]                 // convert C to fp16
buffer_store_short v12, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+19], s[sgprBeta], v16, v[vgprValuC+19] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+19], v17, v[vgprValuC+19]    // C += bias
v_cvt_f16_f32 v19, v[vgprValuC+19]                 // convert C to fp16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v23, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+26], v24, v[vgprValuC+26]    // C += bias
v_cvt_f16_f32 v26, v[vgprValuC+26]                 // convert C to fp16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v30, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+33], v31, v[vgprValuC+33]    // C += bias
v_cvt_f16_f32 v33, v[vgprValuC+33]                 // convert C to fp16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v37, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+40], v38, v[vgprValuC+40]    // C += bias
v_cvt_f16_f32 v40, v[vgprValuC+40]                 // convert C to fp16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v44, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+54], v53, v[vgprValuC+54]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v51, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+54], v52, v[vgprValuC+54]    // C += bias
v_cvt_f16_f32 v54, v[vgprValuC+54]                 // convert C to fp16
buffer_store_short v54, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+61], v60, v[vgprValuC+61]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v58, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+61], v59, v[vgprValuC+61]    // C += bias
v_cvt_f16_f32 v61, v[vgprValuC+61]                 // convert C to fp16
buffer_store_short v61, v55, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+66], v11, v[vgprValuC+66]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+66], s[sgprBeta], v65, v[vgprValuC+66] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+66], v10, v[vgprValuC+66]    // C += bias
v_cvt_f16_f32 v66, v[vgprValuC+66]                 // convert C to fp16
buffer_store_short v66, v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+71], v18, v[vgprValuC+71]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+71], s[sgprBeta], v70, v[vgprValuC+71] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+71], v17, v[vgprValuC+71]    // C += bias
v_cvt_f16_f32 v71, v[vgprValuC+71]                 // convert C to fp16
buffer_store_short v71, v67, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+76], v25, v[vgprValuC+76]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+76], s[sgprBeta], v75, v[vgprValuC+76] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+76], v24, v[vgprValuC+76]    // C += bias
v_cvt_f16_f32 v76, v[vgprValuC+76]                 // convert C to fp16
buffer_store_short v76, v72, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+81], v32, v[vgprValuC+81]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+81], s[sgprBeta], v80, v[vgprValuC+81] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+81], v31, v[vgprValuC+81]    // C += bias
v_cvt_f16_f32 v81, v[vgprValuC+81]                 // convert C to fp16
buffer_store_short v81, v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+86], v39, v[vgprValuC+86]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+86], s[sgprBeta], v85, v[vgprValuC+86] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+86], v38, v[vgprValuC+86]    // C += bias
v_cvt_f16_f32 v86, v[vgprValuC+86]                 // convert C to fp16
buffer_store_short v86, v82, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+91], v46, v[vgprValuC+91]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+91], s[sgprBeta], v90, v[vgprValuC+91] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+91], v45, v[vgprValuC+91]    // C += bias
v_cvt_f16_f32 v91, v[vgprValuC+91]                 // convert C to fp16
buffer_store_short v91, v87, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+96], v53, v[vgprValuC+96]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+96], s[sgprBeta], v95, v[vgprValuC+96] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+96], v52, v[vgprValuC+96]    // C += bias
v_cvt_f16_f32 v96, v[vgprValuC+96]                 // convert C to fp16
buffer_store_short v96, v92, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+101], v60, v[vgprValuC+101]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+101], s[sgprBeta], v100, v[vgprValuC+101] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+101], v59, v[vgprValuC+101]  // C += bias
v_cvt_f16_f32 v101, v[vgprValuC+101]               // convert C to fp16
buffer_store_short v101, v97, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+106], v11, v[vgprValuC+106]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+106], s[sgprBeta], v105, v[vgprValuC+106] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+106], v10, v[vgprValuC+106]  // C += bias
v_cvt_f16_f32 v106, v[vgprValuC+106]               // convert C to fp16
buffer_store_short v106, v102, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+111], v18, v[vgprValuC+111]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+111], s[sgprBeta], v110, v[vgprValuC+111] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+111], v17, v[vgprValuC+111]  // C += bias
v_cvt_f16_f32 v111, v[vgprValuC+111]               // convert C to fp16
buffer_store_short v111, v107, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+116], v25, v[vgprValuC+116]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+116], s[sgprBeta], v115, v[vgprValuC+116] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+116], v24, v[vgprValuC+116]  // C += bias
v_cvt_f16_f32 v116, v[vgprValuC+116]               // convert C to fp16
buffer_store_short v116, v112, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+121], v32, v[vgprValuC+121]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+121], s[sgprBeta], v120, v[vgprValuC+121] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+121], v31, v[vgprValuC+121]  // C += bias
v_cvt_f16_f32 v121, v[vgprValuC+121]               // convert C to fp16
buffer_store_short v121, v117, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+126], v39, v[vgprValuC+126]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+126], s[sgprBeta], v125, v[vgprValuC+126] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+126], v38, v[vgprValuC+126]  // C += bias
v_cvt_f16_f32 v126, v[vgprValuC+126]               // convert C to fp16
buffer_store_short v126, v122, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+131], v46, v[vgprValuC+131]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+131], s[sgprBeta], v130, v[vgprValuC+131] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+131], v45, v[vgprValuC+131]  // C += bias
v_cvt_f16_f32 v131, v[vgprValuC+131]               // convert C to fp16
buffer_store_short v131, v127, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+136], v53, v[vgprValuC+136]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+136], s[sgprBeta], v135, v[vgprValuC+136] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+136], v52, v[vgprValuC+136]  // C += bias
v_cvt_f16_f32 v136, v[vgprValuC+136]               // convert C to fp16
buffer_store_short v136, v132, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+142], v60, v[vgprValuC+142]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+142], s[sgprBeta], v141, v[vgprValuC+142] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+142], v59, v[vgprValuC+142]  // C += bias
v_cvt_f16_f32 v142, v[vgprValuC+142]               // convert C to fp16
buffer_store_short v142, v137, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+147], v11, v[vgprValuC+147]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+147], s[sgprBeta], v146, v[vgprValuC+147] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+147], v10, v[vgprValuC+147]  // C += bias
v_cvt_f16_f32 v147, v[vgprValuC+147]               // convert C to fp16
buffer_store_short v147, v143, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+152], v18, v[vgprValuC+152]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+152], s[sgprBeta], v151, v[vgprValuC+152] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+152], v17, v[vgprValuC+152]  // C += bias
v_cvt_f16_f32 v152, v[vgprValuC+152]               // convert C to fp16
buffer_store_short v152, v148, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #2 (d1,d0,vc1,vc0) = */
/*    (0,1,6,0:vw1); (0,1,6,1:vw1); (0,1,6,2:vw1); (0,1,6,3:vw1); (0,0,7,0:vw1); (0,0,7,1:vw1); (0,0,7,2:vw1); (0,0,7,3:vw1); (0,1,7,0:vw1); (0,1,7,1:vw1); (0,1,7,2:vw1); (0,1,7,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v82, BufferOOB
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v6, v2, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v82, v6, s[72:73]                // LDC clip if OOB. offset
buffer_load_short_d16 v9, v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v4, s68
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b32 v10, v7 offset:0                       // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v11, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v82, v6, s[72:73]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v13, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v82, v13, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16_hi v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v4, s68
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
ds_read_b32 v17, v14 offset:0                      // load Bias
v_add_u32 v15, 1024, v14                           // add ScaleAlphaVec offset (3)
ds_read_b32 v18, v15 offset:0                      // load scaleAlpha
v_add_lshl_u32 v13, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v82, v13, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v82, v20, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s68
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
ds_read_b32 v24, v21 offset:0                      // load Bias
v_add_u32 v22, 1024, v21                           // add ScaleAlphaVec offset (3)
ds_read_b32 v25, v22 offset:0                      // load scaleAlpha
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v82, v20, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v82, v27, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16_hi v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s68
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
ds_read_b32 v31, v28 offset:0                      // load Bias
v_add_u32 v29, 1024, v28                           // add ScaleAlphaVec offset (3)
ds_read_b32 v32, v29 offset:0                      // load scaleAlpha
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v82, v27, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[68:69], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v34, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v82, v34, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v0, s68
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
ds_read_b32 v38, v35 offset:0                      // load Bias
v_add_u32 v36, 1024, v35                           // add ScaleAlphaVec offset (3)
ds_read_b32 v39, v36 offset:0                      // load scaleAlpha
v_add_lshl_u32 v34, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v82, v34, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v41, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v82, v41, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16_hi v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s68
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
ds_read_b32 v45, v42 offset:0                      // load Bias
v_add_u32 v43, 1024, v42                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v43 offset:0                      // load scaleAlpha
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v82, v41, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v48, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v82, v48, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16 v51, v48, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v4, s68
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
ds_read_b32 v52, v49 offset:0                      // load Bias
v_add_u32 v50, 1024, v49                           // add ScaleAlphaVec offset (3)
ds_read_b32 v53, v50 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v82, v48, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v55, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v82, v55, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16_hi v58, v55, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v56, v4, s68
v_lshlrev_b32 v56, 0x2, v56                        // Bias address scaled by BPE
ds_read_b32 v59, v56 offset:0                      // load Bias
v_add_u32 v57, 1024, v56                           // add ScaleAlphaVec offset (3)
ds_read_b32 v60, v57 offset:0                      // load scaleAlpha
v_add_lshl_u32 v55, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v82, v55, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v62, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v82, v62, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16 v65, v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v4, s68
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
v_add_u32 v64, 1024, v63                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v62, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v82, v62, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,1) */
s_mov_b32 s68, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v67, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v67, v82, v67, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16_hi v70, v67, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v68, v4, s68
v_lshlrev_b32 v68, 0x2, v68                        // Bias address scaled by BPE
v_add_u32 v69, 1024, v68                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v67, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v67, v82, v67, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,2) */
s_mov_b32 s68, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v72, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v72, v82, v72, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16 v75, v72, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v73, v4, s68
v_lshlrev_b32 v73, 0x2, v73                        // Bias address scaled by BPE
v_add_u32 v74, 1024, v73                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v72, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v72, v82, v72, s[72:73]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,3) */
s_mov_b32 s68, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s68                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[68:69], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[72:73], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[72:73], s[68:69], s[72:73]             // in0 && in1
v_add_lshl_u32 v77, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v82, v77, s[72:73]              // LDC clip if OOB. offset
buffer_load_short_d16_hi v80, v77, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s68, 128, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v78, v4, s68
v_lshlrev_b32 v78, 0x2, v78                        // Bias address scaled by BPE
v_add_u32 v79, 1024, v78                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v82, v77, s[72:73]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+12], acc52          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+19], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+26], acc54          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+33], acc55          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+40], acc56          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+47], acc57          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+54], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+61], acc59          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+66], acc60          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+71], acc61          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+76], acc62          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+81], acc63          // copy acc to vreg[63]

/* rC *= alpha batchElements=[(0, 1, 6, 0), (0, 1, 6, 1), (0, 1, 6, 2), (0, 1, 6, 3), (0, 0, 7, 0), (0, 0, 7, 1), (0, 0, 7, 2), (0, 0, 7, 3), (0, 1, 7, 0), (0, 1, 7, 1), (0, 1, 7, 2), (0, 1, 7, 3)] */
v_mul_f32 v[vgprValuC+12], s[sgprAlpha], v[vgprValuC+12] // *= alpha
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+12], v11, v[vgprValuC+12]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+12], s[sgprBeta], v9, v[vgprValuC+12] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+12], v10, v[vgprValuC+12]    // C += bias
v_cvt_f16_f32 v12, v[vgprValuC+12]                 // convert C to fp16
buffer_store_short v12, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+19], s[sgprBeta], v16, v[vgprValuC+19] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+19], v17, v[vgprValuC+19]    // C += bias
v_cvt_f16_f32 v19, v[vgprValuC+19]                 // convert C to fp16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v23, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+26], v24, v[vgprValuC+26]    // C += bias
v_cvt_f16_f32 v26, v[vgprValuC+26]                 // convert C to fp16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v30, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+33], v31, v[vgprValuC+33]    // C += bias
v_cvt_f16_f32 v33, v[vgprValuC+33]                 // convert C to fp16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v37, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+40], v38, v[vgprValuC+40]    // C += bias
v_cvt_f16_f32 v40, v[vgprValuC+40]                 // convert C to fp16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v44, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+54], v53, v[vgprValuC+54]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v51, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+54], v52, v[vgprValuC+54]    // C += bias
v_cvt_f16_f32 v54, v[vgprValuC+54]                 // convert C to fp16
buffer_store_short v54, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+61], v60, v[vgprValuC+61]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v58, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+61], v59, v[vgprValuC+61]    // C += bias
v_cvt_f16_f32 v61, v[vgprValuC+61]                 // convert C to fp16
buffer_store_short v61, v55, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+66], v11, v[vgprValuC+66]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+66], s[sgprBeta], v65, v[vgprValuC+66] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+66], v10, v[vgprValuC+66]    // C += bias
v_cvt_f16_f32 v66, v[vgprValuC+66]                 // convert C to fp16
buffer_store_short v66, v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+71], v18, v[vgprValuC+71]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+71], s[sgprBeta], v70, v[vgprValuC+71] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+71], v17, v[vgprValuC+71]    // C += bias
v_cvt_f16_f32 v71, v[vgprValuC+71]                 // convert C to fp16
buffer_store_short v71, v67, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+76], v25, v[vgprValuC+76]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+76], s[sgprBeta], v75, v[vgprValuC+76] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+76], v24, v[vgprValuC+76]    // C += bias
v_cvt_f16_f32 v76, v[vgprValuC+76]                 // convert C to fp16
buffer_store_short v76, v72, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
v_mul_f32 v[vgprValuC+81], v32, v[vgprValuC+81]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+81], s[sgprBeta], v80, v[vgprValuC+81] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+81], v31, v[vgprValuC+81]    // C += bias
v_cvt_f16_f32 v81, v[vgprValuC+81]                 // convert C to fp16
buffer_store_short v81, v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_2                            // jump to end
label_GW_End_2:
label_KernelEnd:
s_endpgm                                           // Kernel End
label_ASM_End:  /// The end of the kernel
