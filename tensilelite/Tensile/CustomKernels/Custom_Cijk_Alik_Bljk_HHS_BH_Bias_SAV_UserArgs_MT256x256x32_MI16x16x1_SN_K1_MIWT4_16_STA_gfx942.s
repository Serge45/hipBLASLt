
/******************************************/
/* Begin Kernel                           */
/******************************************/
.amdgcn_target "amdgcn-amd-amdhsa--gfx942"
.text
.protected Custom_Cijk_Alik_Bljk_HHS_BH_Bias_SAV_UserArgs_MT256x256x32_MI16x16x1_SN_K1_MIWT4_16_STA_gfx942
.globl Custom_Cijk_Alik_Bljk_HHS_BH_Bias_SAV_UserArgs_MT256x256x32_MI16x16x1_SN_K1_MIWT4_16_STA_gfx942
.p2align 8
.type Custom_Cijk_Alik_Bljk_HHS_BH_Bias_SAV_UserArgs_MT256x256x32_MI16x16x1_SN_K1_MIWT4_16_STA_gfx942,@function
.section .rodata,#alloc
.p2align 6
.amdhsa_kernel Custom_Cijk_Alik_Bljk_HHS_BH_Bias_SAV_UserArgs_MT256x256x32_MI16x16x1_SN_K1_MIWT4_16_STA_gfx942
  .amdhsa_user_sgpr_kernarg_segment_ptr 1
  .amdhsa_accum_offset 256 // accvgpr offset
  .amdhsa_next_free_vgpr 512 // vgprs
  .amdhsa_next_free_sgpr 73 // sgprs
  .amdhsa_group_segment_fixed_size 50176 // lds bytes
  .amdhsa_private_segment_fixed_size 0
  .amdhsa_system_sgpr_workgroup_id_x 1
  .amdhsa_system_sgpr_workgroup_id_y 1
  .amdhsa_system_sgpr_workgroup_id_z 1
  .amdhsa_system_vgpr_workitem_id 0
  .amdhsa_float_denorm_mode_32 3
  .amdhsa_float_denorm_mode_16_64 3
  .amdhsa_user_sgpr_count 13
  .amdhsa_user_sgpr_kernarg_preload_length 11
  .amdhsa_user_sgpr_kernarg_preload_offset 0
.end_amdhsa_kernel
.text
/* Num VGPR   =256 */
/* Num AccVGPR=256 */
/* Num SGPR   =73 */

/******************************************/
/* Optimizations and Config:              */
/******************************************/
/* ThreadTile= 16 x 16 */
/* SubGroup= 16 x 16 */
/* VectorWidthA=1 */
/* VectorWidthB=8 */
/* GlobalReadVectorWidthA=8, GlobalReadVectorWidthB=8 */
/* DirectToLdsA=False */
/* DirectToLdsB=False */
/* UseSgprForGRO=0 */
.amdgpu_metadata
---
custom.config:
  ProblemType:
    OperationType: GEMM
    DataType: h
    DestDataType: h
    ComputeDataType: s
    HighPrecisionAccumulate: True
    TransposeA: True
    TransposeB: False
    UseBeta: True
    Batched: True
    Activation: False
    SwizzleTensorA: True
    SupportUserArgs: True
    UseBias: 1
    UseScaleAlphaVec: 1
  MatrixInstruction: [16, 16, 16, 1, 1, 4, 16, 4, 1]
  1LDSBuffer: 0
  ScheduleIterAlg: 3
  DepthU: 32
  StaggerU: 0
  WorkGroupMapping: 8
  WaveSeparateGlobalReadA: 1
  WaveSeparateGlobalReadB: 1
  GlobalReadVectorWidthA: 8
  GlobalReadVectorWidthB: 8
  AssertFree0ElementMultiple: 16
  AssertFree1ElementMultiple: 16
  AssertSummationElementMultiple: 32
  GlobalSplitU: 1
  GlobalSplitUAlgorithm: "MultipleBuffer"
  NoReject: 1
  InternalSupportParams:
   KernArgsVersion: 2
   SupportUserGSU: True
   SupportCustomWGM: True
   SupportCustomStaggerU: True
   UseUniversalArgs: True
amdhsa.version:
  - 1
  - 1
amdhsa.kernels:
  - .name: Custom_Cijk_Alik_Bljk_HHS_BH_Bias_SAV_UserArgs_MT256x256x32_MI16x16x1_SN_K1_MIWT4_16_STA_gfx942
    .symbol: 'Custom_Cijk_Alik_Bljk_HHS_BH_Bias_SAV_UserArgs_MT256x256x32_MI16x16x1_SN_K1_MIWT4_16_STA_gfx942.kd'
    .language:                   OpenCL C
    .language_version:
      - 2
      - 0
    .args:
      - .name:            Gemm info
        .size:            4
        .offset:          0
        .value_kind:      by_value
        .value_type:      u32
      - .name:            kernel info0
        .size:            4
        .offset:          4
        .value_kind:      by_value
        .value_type:      u32
      - .name:            kernel info1
        .size:            4
        .offset:          8
        .value_kind:      by_value
        .value_type:      u32
      - .name:            numWG
        .size:            4
        .offset:          12
        .value_kind:      by_value
        .value_type:      u32
      - .name:            SizesFree0
        .size:            4
        .offset:          16
        .value_kind:      by_value
        .value_type:      u32
      - .name:            SizesFree1
        .size:            4
        .offset:          20
        .value_kind:      by_value
        .value_type:      u32
      - .name:            SizesFree2
        .size:            4
        .offset:          24
        .value_kind:      by_value
        .value_type:      u32
      - .name:            SizesSum0
        .size:            4
        .offset:          28
        .value_kind:      by_value
        .value_type:      u32
      - .name:            D
        .size:            8
        .offset:          32
        .value_kind:      global_buffer
        .value_type:      f16
        .address_space:   generic
      - .name:            C
        .size:            8
        .offset:          40
        .value_kind:      global_buffer
        .value_type:      f16
        .address_space:   generic
      - .name:            A
        .size:            8
        .offset:          48
        .value_kind:      global_buffer
        .value_type:      f16
        .address_space:   generic
      - .name:            B
        .size:            8
        .offset:          56
        .value_kind:      global_buffer
        .value_type:      f16
        .address_space:   generic
      - .name:            strideD0
        .size:            4
        .offset:          64
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideD1
        .size:            4
        .offset:          68
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideC0
        .size:            4
        .offset:          72
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideC1
        .size:            4
        .offset:          76
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideA0
        .size:            4
        .offset:          80
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideA1
        .size:            4
        .offset:          84
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideB0
        .size:            4
        .offset:          88
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideB1
        .size:            4
        .offset:          92
        .value_kind:      by_value
        .value_type:      u32
      - .name:            alpha
        .size:            4
        .offset:          96
        .value_kind:      by_value
        .value_type:      f32
      - .name:            beta
        .size:            4
        .offset:          100
        .value_kind:      by_value
        .value_type:      f32
      - .name:            AddressScaleAlphaVec
        .size:            8
        .offset:          104
        .value_kind:      global_buffer
        .value_type:      f32
        .address_space:   generic
      - .name:            bias
        .size:            8
        .offset:          112
        .value_kind:      global_buffer
        .value_type:      void
        .address_space:   generic
      - .name:            biasType
        .size:            4
        .offset:          120
        .value_kind:      by_value
        .value_type:      u32
      - .name:            StrideBias
        .size:            4
        .offset:          124
        .value_kind:      by_value
        .value_type:      u32
    .group_segment_fixed_size:   50176
    .kernarg_segment_align:      8
    .kernarg_segment_size:       128
    .max_flat_workgroup_size:    256
    .private_segment_fixed_size: 0
    .sgpr_count:                 72
    .sgpr_spill_count:           0
    .vgpr_count:                 256
    .vgpr_spill_count:           0
    .wavefront_size:             64
...
.end_amdgpu_metadata
Custom_Cijk_Alik_Bljk_HHS_BH_Bias_SAV_UserArgs_MT256x256x32_MI16x16x1_SN_K1_MIWT4_16_STA_gfx942:
label_ASM_Start:  /// Main body of the asm kernel

/* Magic div and mod functions */
.macro V_MAGIC_DIV dstIdx:req dividend:req magicNumber:req magicShift:req magicA:req
    v_mul_hi_u32 v[\dstIdx+1] \dividend \magicNumber
    v_mul_lo_u32 v[\dstIdx+0] \dividend \magicA
    v_add_u32 v[\dstIdx+0] v[\dstIdx+0] v[\dstIdx+1]
    v_lshrrev_b32 v[\dstIdx+0] \magicShift v[\dstIdx+0]
.endm

/******************************************/
/* VGPR Assignments                       */
/******************************************/
/* ValuC range: [0-0), serializedStore enabled */
.set vgprValuC, 0
/* ValuA/B   Xn=PLR buffer idx,  In=InnerUnroll idx */
.set vgprValuA_X0_I0, 0
.set vgprValuA_X1_I0, 8
.set vgprValuB_X0_I0, 16
.set vgprValuB_X1_I0, 48
.set vgprLocalWriteAddrB, 80
.set vgprGlobalReadOffsetA, 81
.set vgprGlobalReadOffsetB, 85
.set vgprG2LA, 90
.set vgprG2LA2, 106
.set vgprG2LB, 122
.set vgprLocalReadAddrB, 138
.set vgprSerial, 139

/******************************************/
/* SGPR Assignments                       */
/******************************************/
.set sgprKernArgAddress, 0
.set sgprWorkGroup0, 2
.set sgprWorkGroup1, 3
.set sgprWorkGroup2, 4
.set sgprArgType, 5
.set sgprGSUSumIdx, 6
.set sgprGSULog2BpeC, 8
.set sgprGSULog2BpeD, 9
.set sgprStaggerU, 10
.set sgprWGM, 11
.set sgprLoopCounterL, 12
.set sgprOrigLoopCounter, 13
.set sgprSrdD, 16
.set sgprSrdC, 20
.set sgprNumWorkGroups0, 14
.set sgprNumWorkGroups1, 15
.set sgprSizesFree, 24
.set sgprSizesSum, 27
.set sgprAddressD, 28
.set sgprAddressC, 30
.set sgprAddressA, 32
.set sgprAddressB, 34
.set sgprStridesD, 36
.set sgprStridesC, 38
.set sgprStridesA, 40
.set sgprStridesB, 42
.set sgprAlpha, 44
.set sgprBeta, 45
.set sgprGSU, 46

/* Size Assignments */
.set sgprSizeI, sgprSizesFree+0
.set sgprSizeJ, sgprSizesFree+1
.set sgprSizeK, sgprSizesFree+2
.set sgprSizeL, sgprSizesSum+0

/* Stride Assignments */
.set constStrideD0I, 1
.set sgprStrideD1J, sgprStridesD+0
.set sgprStrideDK, sgprStridesD+1
.set constStrideC0I, 1
.set sgprStrideC1J, sgprStridesC+0
.set sgprStrideCK, sgprStridesC+1
.set constStrideAL, 1
.set sgprStrideA0I, sgprStridesA+0
.set sgprStrideAK, sgprStridesA+1
.set constStrideBL, 1
.set sgprStrideB1J, sgprStridesB+0
.set sgprStrideBK, sgprStridesB+1

.set MT0, 256
.set MT1, 256
.set DepthU, 32
.set BpeA, 2
.set BpeALog2, 1
.set BpeB, 2
.set BpeBLog2, 1
.set BpeAGR, 2
.set BpeAGRLog2, 1
.set BpeBGR, 2
.set BpeBGRLog2, 1
/* Number of elements to shift-left SRD */
.set SrdShiftLeftA, 8
.set SrdShiftLeftB, 8
/* 2GB limit - set offsets to -1 to exceed this and clamp */
.set BufferLimit, 0xffffffff
.set BufferOOB, 0x80000000

/******************************************/
/* Bits 127:96 of SRD.                    */
/* hex: 0x00020000                        */
/* dst_sel_x (3b): 0                      */
/* dst_sel_y (3b): 0                      */
/* dst_sel_z (3b): 0                      */
/* dst_sel_w (3b): 0                      */
/* num_format (3b): 0                     */
/* data_format (4b): 4                    */
/* user_vm_enable (1b): 0                 */
/* user_vm_mode (1b): 0                   */
/* index_stride (2b): 0                   */
/* add_tid_enable (1b): 0                 */
/* _unusedA (3b): 0                       */
/* nv (1b): 0                             */
/* _unusedB (2b): 0                       */
/* type (2b): 0                           */
/******************************************/
.set Srd127_96, 0x00020000

/* Global Offset A */
.macro GLOBAL_OFFSET_A vgprAddr:req vgprOffsetL:req vgprOffset0I:req vgprTmp:req
    v_mul_lo_u32 v[\vgprTmp+0] s[sgprStrideA0I] v[\vgprOffset0I] // mul d1 lower
    v_add_co_u32 v[\vgprAddr+0] vcc v[\vgprOffsetL] v[\vgprTmp+0] // accumulate K lower
    v_add_u32 v[\vgprAddr+0] 0x8 v[\vgprAddr+0]      // add prepad for pointer shift
    v_lshlrev_b32 v[\vgprAddr+0] 0x1 v[\vgprAddr+0]  // offset *= bytes/element
.endm

/* Global Offset A */
.macro GLOBAL_SWIZZLED_OFFSET_A vgprAddr:req vgprOffsetL:req vgprOffset0I:req vgprTmp:req
    v_mul_lo_u32 v[\vgprTmp+0] s72 v[\vgprOffset0I] // mul d1 lower
    v_add_co_u32 v[\vgprAddr+0] vcc v[\vgprOffsetL] v[\vgprTmp+0] // accumulate K lower
    v_add_u32 v[\vgprAddr+0] 0x8 v[\vgprAddr+0]      // add prepad for pointer shift
    v_lshlrev_b32 v[\vgprAddr+0] 0x1 v[\vgprAddr+0]  // offset *= bytes/element
.endm

/* Global Offset B */
.macro GLOBAL_OFFSET_B vgprAddr:req vgprOffsetL:req vgprOffset1J:req vgprTmp:req
    v_mul_lo_u32 v[\vgprTmp+0] s[sgprStrideB1J] v[\vgprOffset1J] // mul d1 lower
    v_add_co_u32 v[\vgprAddr+0] vcc v[\vgprOffsetL] v[\vgprTmp+0] // accumulate K lower
    v_add_u32 v[\vgprAddr+0] 0x8 v[\vgprAddr+0]      // add prepad for pointer shift
    v_lshlrev_b32 v[\vgprAddr+0] 0x1 v[\vgprAddr+0]  // offset *= bytes/element
.endm

/* Dynamic Scalar Divide: vQuotient=vDividend/vDivisor; vRemainder=vDividend%vDivisor; */
.macro DYNAMIC_VECTOR_DIVIDE vQuotient vRemainder vDividend vDivisor vTmp0 vTmp1 sTmp
    v_cvt_f32_u32 v[\vQuotient] v[\vDivisor]
    v_rcp_f32 v[\vQuotient] v[\vQuotient]
    v_mul_f32 v[\vQuotient] 0x4f800000 v[\vQuotient]
    v_cvt_u32_f32 v[\vQuotient] v[\vQuotient]
    v_mul_lo_u32 v[\vRemainder] v[\vDivisor] v[\vQuotient]
    v_mul_hi_u32 v[\vTmp0] v[\vDivisor] v[\vQuotient]
    v_sub_co_u32 v[\vTmp1] vcc 0x0 v[\vRemainder]
    v_cmp_ne_i32 s[\sTmp:\sTmp+1] 0x0 v[\vTmp0]
    v_cndmask_b32 v[\vRemainder] v[\vTmp1] v[\vRemainder] s[\sTmp:\sTmp+1]
    v_mul_hi_u32 v[\vRemainder] v[\vRemainder] v[\vQuotient]
    v_sub_co_u32 v[\vTmp0] vcc v[\vQuotient] v[\vRemainder]
    v_add_co_u32 v[\vQuotient] vcc v[\vQuotient] v[\vRemainder]
    v_cndmask_b32 v[\vQuotient] v[\vQuotient] v[\vTmp0] s[\sTmp:\sTmp+1]
    v_mul_hi_u32 v[\vQuotient] v[\vQuotient] v[\vDividend]
    v_mul_lo_u32 v[\vRemainder] v[\vQuotient] v[\vDivisor]
    v_sub_co_u32 v[\vTmp0] vcc v[\vDividend] v[\vRemainder]
    v_cmp_ge_u32 s[\sTmp:\sTmp+1] v[\vDividend] v[\vRemainder]
    v_add_co_u32 v[\vRemainder] vcc 0x1 v[\vQuotient]
    v_add_co_u32 v[\vTmp1] vcc -1 v[\vQuotient]
    v_cmp_le_u32 vcc v[\vDivisor] v[\vTmp0]
    s_and_b64 vcc s[\sTmp:\sTmp+1] vcc
    v_cndmask_b32 v[\vQuotient] v[\vQuotient] v[\vRemainder] vcc
    v_cndmask_b32 v[\vQuotient] v[\vTmp1] v[\vQuotient] s[\sTmp:\sTmp+1]
    v_cmp_ne_i32 vcc 0x0 v[\vDivisor]
    v_cndmask_b32 v[\vQuotient] -1 v[\vQuotient] vcc // final result
    v_mul_lo_u32 v[\vRemainder] v[\vQuotient] v[\vDivisor]
    v_sub_co_u32 v[\vRemainder] vcc v[\vDividend] v[\vRemainder] // final result
.endm

/******************************************/
/* Allocate Resources                     */
/******************************************/

/* Load num of Gemms */
s_load_dword s47, s[sgprKernArgAddress:sgprKernArgAddress+1], 0x0

/* Load packed kernel args (StaggerU/GSU) */
s_load_dword s49, s[sgprKernArgAddress:sgprKernArgAddress+1], 0x4

/* Load WGM data */
s_load_dword s[sgprWGM], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x8

/* Load num of WGs */
s_load_dword s50, s[sgprKernArgAddress:sgprKernArgAddress+1], 0xc
s_waitcnt lgkmcnt(0)
s_lshr_b32 s48, s47, 0x1e                          // Get arg type
s_and_b32 s47, 0x3fffffff, s47                     // Get nums of gemm
s_cmp_eq_u32 s48, 0                                // Is kernel args
s_cbranch_scc0 label_HBMArgs
s_add_u32 s[sgprKernArgAddress], s[sgprKernArgAddress], 0x10 // Shift common args
s_addc_u32 s[sgprKernArgAddress+1], s[sgprKernArgAddress+1], 0x0

/* Load Kernel Args */
s_load_dwordx16 s[24:39], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x0
s_load_dwordx4 s[40:43], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x40
s_load_dwordx2 s[44:45], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x50
s_waitcnt lgkmcnt(0)
s_branch label_LoadArgsEnd
label_HBMArgs:

/* Load address of kernel arguments */
s_load_dwordx2 s[sgprKernArgAddress:sgprKernArgAddress+1], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x10
s_waitcnt lgkmcnt(0)                               // wait for args to load
label_LoadArgsEnd:
s_branch label_common_kernel_entry

/* pad 37 snops to satisfy 0x100 code size for Preload Backward Compatibility Prologue */
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
label_Preload_Offset_Start:
s_and_b32 s47, 0x3fffffff, s2                      // Get nums of gemm
s_lshr_b32 s48, s2, 0x1e                           // Get arg type
s_mov_b32 s49, s3                                  // Preload internal args
s_cmp_eq_u32 s48, 0                                // Is kernel args
s_cbranch_scc0 label_Preload_HBMArgs
s_add_u32 s[sgprKernArgAddress], s[sgprKernArgAddress], 0x10 // Shift common args
s_addc_u32 s[sgprKernArgAddress+1], s[sgprKernArgAddress+1], 0x0

/* Load Kernel Args */
s_load_dword s31, s[sgprKernArgAddress:sgprKernArgAddress+1], 0x1c
s_load_dwordx8 s[32:39], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x20
s_load_dwordx4 s[40:43], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x40
s_load_dwordx2 s[44:45], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x50
s_mov_b32 s24, s6                                  // move preload data to correct sgpr
s_mov_b32 s25, s7                                  // move preload data to correct sgpr
s_mov_b32 s26, s8                                  // move preload data to correct sgpr
s_mov_b32 s27, s9                                  // move preload data to correct sgpr
s_mov_b32 s28, s10                                 // move preload data to correct sgpr
s_mov_b32 s29, s11                                 // move preload data to correct sgpr
s_mov_b32 s30, s12                                 // move preload data to correct sgpr
s_branch label_Preload_LoadArgsEnd
label_Preload_HBMArgs:
s_mov_b64 s[sgprKernArgAddress:sgprKernArgAddress+1], s[6:7] // Load address of kernel arguments
label_Preload_LoadArgsEnd:
s_mov_b32 s[sgprWGM], s4                           // Preload internal args2
s_mov_b32 s50, s5                                  // Load num of WGs
label_common_kernel_entry:  /// for both preload/non-preload common code
s_mov_b32 s[sgprWorkGroup0+0], s13                 // restore workgroup id
s_mov_b32 s[sgprWorkGroup0+1], s14                 // restore workgroup id
s_mov_b32 s[sgprWorkGroup0+2], s15                 // restore workgroup id
s_and_b32 s[sgprStaggerU], s49, 0xffff0000         // Restore StaggerU related vars
s_lshr_b32 s[sgprStaggerU], s[sgprStaggerU], 0x10
s_and_b32 s[sgprGSU], s49, 0xffff                  // Restore GSUConfig and GSU
s_mov_b32 s[sgprArgType], s48
s_mov_b32 m0, 0xc400                               // LDS clamp at 50176 bytes
v_mov_b32 v[vgprSerial], v0                        // thread serial id

/* remap workgroup to XCCs */
s_lshr_b32 s56, s[sgprWGM], 0x10                   // Get WGMXCC
s_and_b32 s56, 0x3f, s56                           // Get WGMXCC
s_lshr_b32 s57, s[sgprWGM], 0x16                   // Get CU_Count
/* remap WGs if WGMXCC > 1 */
s_cmp_gt_u32 s56, 1
s_cbranch_scc0 label_skip_WGMXCC
/* only remap WGs in the range */
v_cvt_f32_u32 v6, s56                              // s53 = s50 / s56
v_rcp_iflag_f32 v6, v6                             // s53 = s50 / s56
v_cvt_f32_u32 v7, s50                              // s53 = s50 / s56
v_mul_f32 v6, v6, v7                               // s53 = s50 / s56
v_cvt_u32_f32 v6, v6                               // s53 = s50 / s56
v_mul_u32_u24 v7, v6, s56                          // s53 = s50 / s56
v_sub_u32 v7, s50, v7                              // s53 = s50 / s56
v_cmpx_eq_u32 exec, v7, s56                        // s53 = s50 / s56
v_add_u32 v6, 1, v6                                // s53 = s50 / s56
s_mov_b64 exec, -1                                 // s53 = s50 / s56
v_readfirstlane_b32 s53, v6                        // quotient
s_mul_i32 s53, s53, s56
s_cmp_ge_u32 s[sgprWorkGroup0], s53
s_cbranch_scc1 label_skip_WGMXCC
s_cmp_eq_u32 s57, 0                                // CU_Count == 0 ?
s_cbranch_scc0 label_XCCG_nonzero
v_cvt_f32_u32 v6, s56                              // s53 = s[sgprWorkGroup0] / s56
v_rcp_iflag_f32 v6, v6                             // s53 = s[sgprWorkGroup0] / s56
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // s53 = s[sgprWorkGroup0] / s56
v_mul_f32 v6, v6, v7                               // s53 = s[sgprWorkGroup0] / s56
v_cvt_u32_f32 v6, v6                               // s53 = s[sgprWorkGroup0] / s56
v_mul_u32_u24 v7, v6, s56                          // s53 = s[sgprWorkGroup0] / s56
v_sub_u32 v7, s[sgprWorkGroup0], v7                // s53 = s[sgprWorkGroup0] / s56
v_cmpx_eq_u32 exec, v7, s56                        // s53 = s[sgprWorkGroup0] / s56
v_add_u32 v6, 1, v6                                // s53 = s[sgprWorkGroup0] / s56
v_mov_b32 v7, 0                                    // s54 = s[sgprWorkGroup0] % s56
s_mov_b64 exec, -1                                 // s53 = s[sgprWorkGroup0] / s56
v_readfirstlane_b32 s53, v6                        // quotient
v_readfirstlane_b32 s54, v7                        // remainder
v_cvt_f32_u32 v6, s56                              // s55 = s50 / s56
v_rcp_iflag_f32 v6, v6                             // s55 = s50 / s56
v_cvt_f32_u32 v7, s50                              // s55 = s50 / s56
v_mul_f32 v6, v6, v7                               // s55 = s50 / s56
v_cvt_u32_f32 v6, v6                               // s55 = s50 / s56
v_mul_u32_u24 v7, v6, s56                          // s55 = s50 / s56
v_sub_u32 v7, s50, v7                              // s55 = s50 / s56
v_cmpx_eq_u32 exec, v7, s56                        // s55 = s50 / s56
v_add_u32 v6, 1, v6                                // s55 = s50 / s56
s_mov_b64 exec, -1                                 // s55 = s50 / s56
v_readfirstlane_b32 s55, v6                        // quotient
s_mul_i32 s54, s54, s55
s_add_u32 s[sgprWorkGroup0], s53, s54
s_branch label_skip_WGMXCC
label_XCCG_nonzero:
/* temp0 = (wg//CU_Count)*CU_Count */
v_cvt_f32_u32 v6, s57                              // wg//CU_Count
v_rcp_iflag_f32 v6, v6                             // wg//CU_Count
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // wg//CU_Count
v_mul_f32 v6, v6, v7                               // wg//CU_Count
v_cvt_u32_f32 v6, v6                               // wg//CU_Count
v_mul_u32_u24 v7, v6, s57                          // wg//CU_Count
v_sub_u32 v7, s[sgprWorkGroup0], v7                // wg//CU_Count
v_cmpx_eq_u32 exec, v7, s57                        // wg//CU_Count
v_add_u32 v6, 1, v6                                // wg//CU_Count
v_mov_b32 v7, 0                                    // wg//CU_Count
s_mov_b64 exec, -1                                 // wg//CU_Count
v_readfirstlane_b32 s53, v6                        // quotient
v_readfirstlane_b32 s54, v7                        // remainder
s_mul_i32 s53, s53, s57
/* temp1 = (wg%CU_Count)//WGMXCC */
v_cvt_f32_u32 v6, s56                              // s54 = s54 / s56
v_rcp_iflag_f32 v6, v6                             // s54 = s54 / s56
v_cvt_f32_u32 v7, s54                              // s54 = s54 / s56
v_mul_f32 v6, v6, v7                               // s54 = s54 / s56
v_cvt_u32_f32 v6, v6                               // s54 = s54 / s56
v_mul_u32_u24 v7, v6, s56                          // s54 = s54 / s56
v_sub_u32 v7, s54, v7                              // s54 = s54 / s56
v_cmpx_eq_u32 exec, v7, s56                        // s54 = s54 / s56
v_add_u32 v6, 1, v6                                // s54 = s54 / s56
s_mov_b64 exec, -1                                 // s54 = s54 / s56
v_readfirstlane_b32 s54, v6                        // quotient
/* temp0 = temp0 + temp1 */
s_add_u32 s53, s53, s54
/* temp1 = (wg%WGMXCC) * ((WGs - (WGs//CU_Count) * CU_Count) if (wg > (WGs//CU_Count) * CU_Count) else CU_Count)//WGMXCC */
v_cvt_f32_u32 v6, s57                              // WGs//CU_Count
v_rcp_iflag_f32 v6, v6                             // WGs//CU_Count
v_cvt_f32_u32 v7, s50                              // WGs//CU_Count
v_mul_f32 v6, v6, v7                               // WGs//CU_Count
v_cvt_u32_f32 v6, v6                               // WGs//CU_Count
v_mul_u32_u24 v7, v6, s57                          // WGs//CU_Count
v_sub_u32 v7, s50, v7                              // WGs//CU_Count
v_cmpx_eq_u32 exec, v7, s57                        // WGs//CU_Count
v_add_u32 v6, 1, v6                                // WGs//CU_Count
s_mov_b64 exec, -1                                 // WGs//CU_Count
v_readfirstlane_b32 s54, v6                        // quotient
s_mul_i32 s54, s54, s57
s_sub_u32 s55, s50, s54
s_cmp_gt_u32 s[sgprWorkGroup0], s54
s_cselect_b32 s54, s55, s57
v_cvt_f32_u32 v6, s56                              // s54 = s54 / s56
v_rcp_iflag_f32 v6, v6                             // s54 = s54 / s56
v_cvt_f32_u32 v7, s54                              // s54 = s54 / s56
v_mul_f32 v6, v6, v7                               // s54 = s54 / s56
v_cvt_u32_f32 v6, v6                               // s54 = s54 / s56
v_mul_u32_u24 v7, v6, s56                          // s54 = s54 / s56
v_sub_u32 v7, s54, v7                              // s54 = s54 / s56
v_cmpx_eq_u32 exec, v7, s56                        // s54 = s54 / s56
v_add_u32 v6, 1, v6                                // s54 = s54 / s56
s_mov_b64 exec, -1                                 // s54 = s54 / s56
v_readfirstlane_b32 s54, v6                        // quotient
v_cvt_f32_u32 v6, s56                              // s52 = s[sgprWorkGroup0] / s56
v_rcp_iflag_f32 v6, v6                             // s52 = s[sgprWorkGroup0] / s56
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // s52 = s[sgprWorkGroup0] / s56
v_mul_f32 v6, v6, v7                               // s52 = s[sgprWorkGroup0] / s56
v_cvt_u32_f32 v6, v6                               // s52 = s[sgprWorkGroup0] / s56
v_mul_u32_u24 v7, v6, s56                          // s52 = s[sgprWorkGroup0] / s56
v_sub_u32 v7, s[sgprWorkGroup0], v7                // s52 = s[sgprWorkGroup0] / s56
v_cmpx_eq_u32 exec, v7, s56                        // s52 = s[sgprWorkGroup0] / s56
v_add_u32 v6, 1, v6                                // s52 = s[sgprWorkGroup0] / s56
v_mov_b32 v7, 0                                    // s55 = s[sgprWorkGroup0] % s56
s_mov_b64 exec, -1                                 // s52 = s[sgprWorkGroup0] / s56
v_readfirstlane_b32 s52, v6                        // quotient
v_readfirstlane_b32 s55, v7                        // remainder
s_mul_i32 s54, s54, s55
/* WorkGroup0 = temp0 + temp1 */
s_add_u32 s[sgprWorkGroup0], s53, s54
label_skip_WGMXCC:  /// skip WGMXCC if no enough WGs to remap
s_cmp_eq_u32 s48, 0
s_cbranch_scc0 label_MultiGemm
/* init: add vgpr [0...80) to pool */
/* init: add vgpr [0...0) to pool */
/* init: add agpr [0...256) to pool */

/******************************************/
/* Local Read Addresses                   */
/******************************************/

/* local read addresses: tile assignments a/b */
/* lr1J */
v_and_b32 v1, 63, v[vgprSerial]                    // 0. thread id in wave: wtid = tid % wavelength(64)
v_and_b32 v0, 15, v1                               // 1. N offset: nIdx = wtid % MI_N(16)
v_lshlrev_b32 v0, 0x5, v0                          // 1. N offset: nOffset = nIdx * nStride(32)
/* Skip. 2. block offset: bnOffset = 0 when num1DBlocks = 1 */
v_lshlrev_b32 v0, 0x3, v0                          // 4. apply VectorWidth: bnOffset = bnOffset * vw(8)
v_lshrrev_b32 v1, 4, v1                            // 5. K offset: kIdx = wtid / (MIN(16) * MIBB(1))
v_lshlrev_b32 v1, 0x3, v1                          // 5. K offset: lrKOffset = kIdx * mStride(8)
v_add_u32 v0, v1, v0                               // 6. offset in wave: lrOffset = bnOffset + lrKOffset

/* local read addresses: final offsets a */

/* local read addresses: final offsets b */
v_lshrrev_b32 v1, 6, v[vgprSerial]                 // v1 = v[vgprSerial] / 64
v_lshrrev_b32 v1, 2, v1                            // LSU offset: Get LSU wave_id
s_mov_b32 s49, 32                                  // LSU offset: stride = lsuStride(32) when umlds==True
v_mul_lo_u32 v1, s49, v1                           // LSU offset: lsuoffset = wave_id*lsuStride*(MT1+PAD)
v_add_lshl_u32 v[vgprLocalReadAddrB], v1, v0, 0x1  // Final Offset: offset = (lro1+lsuoffset)*bpeDS
v_lshrrev_b32 v2, 9, v[vgprLocalReadAddrB]         // Final Offset: padding 32 per block 512
v_lshlrev_b32 v2, 0x5, v2                          // Final Offset: padding 32 per block 512
v_add_u32 v[vgprLocalReadAddrB], v2, v[vgprLocalReadAddrB] // Final Offset: add padding 32 per block 512

/* local read addresses: declare addresses a */
/* N/A */

/* local read addresses: declare addresses b */

/******************************************/
/* Local Write Addresses                  */
/******************************************/
/* LVCA = 4 */
/* v1 = A-unroll = serial/LVCA */
/* TileAssignment for DirectToVgprA */
v_and_b32 v1, 63, v[vgprSerial]                    // 0. thread id in wave: wtid = tid % wavelength(64)
v_lshrrev_b32 v0, 6, v[vgprSerial]                 // 7. wave offset in N dimen: wtid = tid / dividedForWaveId(64)
v_lshrrev_b32 v4, 5, s[sgprSizesSum] // numKr = DimK / 32
v_mul_u32_u24 v0, v4, v0 // wtid * numKr
                                                   // 1. N offset: nOffset = nIdx * nStride(1) (multiplier is 1, do nothing)
/* Skip. 2. block offset: bnOffset = 0 when num1DBlocks = 1 */
                                                   // 4. apply VectorWidth: bnOffset = bnOffset * vw(1) (multiplier is 1, do nothing)
//v_lshrrev_b32 v1, 4, v1                            // 5. K offset: kIdx = wtid / (MIN(16) * MIBB(1))
//v_lshrrev_b32 v4, 6, v[vgprSerial]                 // 7. wave offset in N dimen: wtid = tid / dividedForWaveId(64)
//v_and_b32 v4, 3, v4                                // 7. wave offset in M dimen: wtid0 = wtid / num1DWaves(4)
//v_lshlrev_b32 v4, 0x4, v4                          // 7. wave offset in M dimen: wOffset = wtid0 * W0Stride(16)
//v_add_u32 v0, v4, v0                               // 7. final local read offset: flrOffset = lrOffset + WOffset
/* unroll *= glvw */
v_lshlrev_b32 v1, 0x3, v1                          // v1 = v1 * 8
v_mov_b32 v4, v1                                   // copy for GlobalSplitU
/* LVCB = 4 */
/* v3 = B-unroll = serial%LVCB */
v_lshrrev_b32 v2, 2, v[vgprSerial]                 // v2 = v[vgprSerial] / 4
v_and_b32 v3, 3, v[vgprSerial]                     // v3 = v[vgprSerial] % 4
/* unroll *= glvw */
v_lshlrev_b32 v3, 0x3, v3                          // v3 = v3 * 8
v_mov_b32 v5, v3                                   // copy for GlobalSplitU
/* lwaUnrollAssignmentA = v4 */
/* lwaUnrollAssignmentB = v5 */

/* local write addresses: first offset a */

/* local write addresses: first offset b */
v_mul_u32_u24 v[vgprLocalWriteAddrB], 0x20, v2     // lwBL**(DepthU_Compute + PAD)
v_add_lshl_u32 v[vgprLocalWriteAddrB], v5, v[vgprLocalWriteAddrB], 0x1 // lwFOB = (lwBB + lwBL*(DepthU+PAD))*bpeDS
v_lshrrev_b32 v6, 9, v[vgprLocalWriteAddrB]        // padding 32 per block 512
v_lshlrev_b32 v6, 0x5, v6                          // padding 32 per block 512
v_add_u32 v[vgprLocalWriteAddrB], v6, v[vgprLocalWriteAddrB] // add padding 32 per block 512
v_mov_b32 v8, MT0                                  // set MT0 into sgpr
v_mov_b32 v7, s[sgprSizesFree+0]                   // set Free0 size
v_cvt_f32_u32 v6, v8                               // v6 = ceil(v7 / v8)
v_rcp_iflag_f32 v6, v6                             // v6 = ceil(v7 / v8)
v_cvt_f32_u32 v9, v7                               // v6 = ceil(v7 / v8)
v_mul_f32 v6, v6, v9                               // v6 = ceil(v7 / v8)
v_cvt_u32_f32 v6, v6                               // v6 = ceil(v7 / v8)
v_mul_u32_u24 v9, v6, v8                           // v6 = ceil(v7 / v8)
v_sub_u32 v9, v7, v9                               // v6 = ceil(v7 / v8)
v_cmp_ne_u32 vcc, v9, 0                            // v6 = ceil(v7 / v8)
v_addc_co_u32 v6, vcc, v6, 0, vcc                  // ceil
v_mov_b32 v8, MT1                                  // set MT1 into sgpr
v_mov_b32 v7, s[sgprSizesFree+1]                   // set Free1 size
v_readfirstlane_b32 s[sgprNumWorkGroups0], v6      // set back to numWorkGroup0
v_cvt_f32_u32 v6, v8                               // v6 = ceil(v7 / v8)
v_rcp_iflag_f32 v6, v6                             // v6 = ceil(v7 / v8)
v_cvt_f32_u32 v9, v7                               // v6 = ceil(v7 / v8)
v_mul_f32 v6, v6, v9                               // v6 = ceil(v7 / v8)
v_cvt_u32_f32 v6, v6                               // v6 = ceil(v7 / v8)
v_mul_u32_u24 v9, v6, v8                           // v6 = ceil(v7 / v8)
v_sub_u32 v9, v7, v9                               // v6 = ceil(v7 / v8)
v_cmp_ne_u32 vcc, v9, 0                            // v6 = ceil(v7 / v8)
v_addc_co_u32 v6, vcc, v6, 0, vcc                  // ceil
s_nop 0                                            // 1 wait states
v_readfirstlane_b32 s[sgprNumWorkGroups1], v6      // set back to numWorkGroup1
s_waitcnt lgkmcnt(0)                               // wait for 44/0 bytes of kern args

/* remap wg from 1D(idxWG012) to 3D(wg2,wg1,wg0) */
/* wg2 = idxWG012 * smallMagicNumber(1/(numWG0*numWG1)) */
s_mul_i32 s48, s[sgprNumWorkGroups0], s[sgprNumWorkGroups1]
s_and_b32 s49, s[sgprGSU], 0x3fff                  // Restore GSU
s_mul_i32 s48, s48, s49
v_cvt_f32_u32 v6, s48                              // s48 = s[sgprWorkGroup0] / s48
v_rcp_iflag_f32 v6, v6                             // s48 = s[sgprWorkGroup0] / s48
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // s48 = s[sgprWorkGroup0] / s48
v_mul_f32 v6, v6, v7                               // s48 = s[sgprWorkGroup0] / s48
v_cvt_u32_f32 v6, v6                               // s48 = s[sgprWorkGroup0] / s48
v_mul_u32_u24 v7, v6, s48                          // s48 = s[sgprWorkGroup0] / s48
v_sub_u32 v7, s[sgprWorkGroup0], v7                // s48 = s[sgprWorkGroup0] / s48
v_cmpx_eq_u32 exec, v7, s48                        // s48 = s[sgprWorkGroup0] / s48
v_add_u32 v6, 1, v6                                // s48 = s[sgprWorkGroup0] / s48
s_mov_b64 exec, -1                                 // s48 = s[sgprWorkGroup0] / s48
v_readfirstlane_b32 s48, v6                        // quotient
s_mov_b32 s[sgprWorkGroup2], s48
/* idxWG01 = idxWG012 - wg2 * numWG0 * numWG1 */
s_mul_i32 s48, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s48, s48, s[sgprWorkGroup2]
s_mul_i32 s48, s48, s49
s_sub_u32 s[sgprWorkGroup0], s[sgprWorkGroup0], s48
/* wg1 = idxWG01 * smallMagicNumber(1/numWG0) */
v_cvt_f32_u32 v6, s[sgprNumWorkGroups0]            // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_rcp_iflag_f32 v6, v6                             // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_mul_f32 v6, v6, v7                               // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_cvt_u32_f32 v6, v6                               // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_mul_u32_u24 v7, v6, s[sgprNumWorkGroups0]        // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_sub_u32 v7, s[sgprWorkGroup0], v7                // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_cmpx_eq_u32 exec, v7, s[sgprNumWorkGroups0]      // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_add_u32 v6, 1, v6                                // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
s_mov_b64 exec, -1                                 // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_readfirstlane_b32 s48, v6                        // quotient
s_mov_b32 s[sgprWorkGroup1], s48
/* wg0 = idxWG01 - wg1 * numWG0 */
s_mul_i32 s48, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_sub_u32 s[sgprWorkGroup0], s[sgprWorkGroup0], s48
s_branch label_MultiGemmEnd
label_MultiGemm:

/* Check if custom structure pointer is null */
s_cmp_eq_u32 s[sgprArgType], 2                     // ArgType == 2 ?
s_cbranch_scc1 label_IsExternalValid               // branch if ArgType == 2
s_mov_b32 s15, 112
s_mul_i32 s54, s47, 4
s_mov_b64 s[48:49], s[sgprKernArgAddress:sgprKernArgAddress+1]
s_branch label_IsExternalValidEnd
label_IsExternalValid:
s_mov_b32 s15, 196
s_mov_b32 s54, 0x0
s_mov_b64 s[48:49], s[sgprKernArgAddress:sgprKernArgAddress+1]
label_IsExternalValidEnd:

/* Grouped Gemm:: prefetch 1 arg load */
s_mov_b32 s14, 1
s_mov_b32 s55, 0
s_load_dwordx4 s[24:27], s[48:49], s54
s_cmpk_eq_u32 s47, 1                               // if gemm_count is 1?
s_cbranch_scc1 label_wgTable_noLoadLoop

/* Grouped Gemm:: accumulate numTiles for each gemm */
/* Grouped Gemm:: loop start */
label_Loop_GemmCount:
s_waitcnt lgkmcnt(0)
s_lshr_b32 s52, s24, 8                             // s52 = s24 / 256
s_and_b32 s50, 255, s24                            // s50 = s24 % 256
s_addc_u32 s52, s52, 0x0
s_lshr_b32 s53, s25, 8                             // s53 = s25 / 256
s_and_b32 s50, 255, s25                            // s50 = s25 % 256
s_addc_u32 s53, s53, 0x0
s_mul_i32 s52, s52, s53
s_mul_i32 s52, s52, s26
s_and_b32 s53, s[sgprGSU], 0x3fff                  // Restore GSU
s_mul_i32 s52, s52, s53
s_add_u32 s55, s55, s52
s_cmp_lt_u32 s[sgprWorkGroup0], s55
s_cbranch_scc1 label_FOUND
s_add_u32 s54, s54, s15
s_load_dwordx4 s[24:27], s[48:49], s54
s_add_u32 s14, s14, 1
s_cmp_lt_u32 s14, s47
s_cbranch_scc1 label_Loop_GemmCount

/* Grouped Gemm:: noLoadLoop */
label_wgTable_noLoadLoop:
s_waitcnt lgkmcnt(0)
s_lshr_b32 s52, s24, 8                             // s52 = s24 / 256
s_and_b32 s50, 255, s24                            // s50 = s24 % 256
s_addc_u32 s52, s52, 0x0
s_lshr_b32 s53, s25, 8                             // s53 = s25 / 256
s_and_b32 s50, 255, s25                            // s50 = s25 % 256
s_addc_u32 s53, s53, 0x0
s_mul_i32 s52, s52, s53
s_mul_i32 s52, s52, s26
s_and_b32 s48, s[sgprGSU], 0x3fff                  // Restore GSU
s_mul_i32 s52, s52, s48
s_add_u32 s55, s55, s52

/* Grouped Gemm:: gemmIndex found */
label_FOUND:
s_sub_u32 s49, s14, 1
s_sub_u32 s48, s55, s52
s_sub_u32 s[sgprWorkGroup0], s[sgprWorkGroup0], s48
/* Check if custom structure pointer is null */
s_cmp_eq_u32 s[sgprArgType], 2                     // ArgType == 2 ?
s_cbranch_scc1 label_LoadExternalStruct            // branch if ArgType == 2

/* Grouped Gemm: offset argument address to gemm */
/* Grouped Gemm: offset address from wg_table_start to args_start */
s_lshl2_add_u32 s[sgprKernArgAddress], s47, s[sgprKernArgAddress]
s_addc_u32 s[sgprKernArgAddress+1], s[sgprKernArgAddress+1], 0x0
/* Grouped Gemm: offset address from args_start to gemm_start */
s_mul_i32 s49, s49, 112
s_add_u32 s[sgprKernArgAddress], s[sgprKernArgAddress], s49
s_addc_u32 s[sgprKernArgAddress+1], s[sgprKernArgAddress+1], 0x0

/* Load Kernel Args */
s_load_dwordx16 s[28:43], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x10
s_load_dwordx2 s[44:45], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x50
s_branch label_LoadExternalStructEnd
label_LoadExternalStruct:
/* Grouped Gemm: offset address from args_start to gemm_start */
s_mul_i32 s49, s49, 196
s_add_u32 s[sgprKernArgAddress], s[sgprKernArgAddress], s49
s_addc_u32 s[sgprKernArgAddress+1], s[sgprKernArgAddress+1], 0x0
s_load_dwordx16 s[28:43], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x10
s_load_dword s44, s[sgprKernArgAddress:sgprKernArgAddress+1], 0x50
// Read Beta
s_load_dword s45, s[sgprKernArgAddress:sgprKernArgAddress+1], 0x60
label_LoadExternalStructEnd:
/* init: add vgpr [0...80) to pool */
/* init: add vgpr [0...0) to pool */
/* init: add agpr [0...256) to pool */

/******************************************/
/* Local Read Addresses                   */
/******************************************/

/* local read addresses: tile assignments a/b */
/* lr1J */
v_and_b32 v1, 63, v[vgprSerial]                    // 0. thread id in wave: wtid = tid % wavelength(64)
v_and_b32 v0, 15, v1                               // 1. N offset: nIdx = wtid % MI_N(16)
v_lshlrev_b32 v0, 0x5, v0                          // 1. N offset: nOffset = nIdx * nStride(32)
/* Skip. 2. block offset: bnOffset = 0 when num1DBlocks = 1 */
v_lshlrev_b32 v0, 0x3, v0                          // 4. apply VectorWidth: bnOffset = bnOffset * vw(8)
v_lshrrev_b32 v1, 4, v1                            // 5. K offset: kIdx = wtid / (MIN(16) * MIBB(1))
v_lshlrev_b32 v1, 0x3, v1                          // 5. K offset: lrKOffset = kIdx * mStride(8)
v_add_u32 v0, v1, v0                               // 6. offset in wave: lrOffset = bnOffset + lrKOffset

/* local read addresses: final offsets a */

/* local read addresses: final offsets b */
v_lshrrev_b32 v1, 6, v[vgprSerial]                 // v1 = v[vgprSerial] / 64
v_lshrrev_b32 v1, 2, v1                            // LSU offset: Get LSU wave_id
s_mov_b32 s49, 32                                  // LSU offset: stride = lsuStride(32) when umlds==True
v_mul_lo_u32 v1, s49, v1                           // LSU offset: lsuoffset = wave_id*lsuStride*(MT1+PAD)
v_add_lshl_u32 v[vgprLocalReadAddrB], v1, v0, 0x1  // Final Offset: offset = (lro1+lsuoffset)*bpeDS
v_lshrrev_b32 v2, 9, v[vgprLocalReadAddrB]         // Final Offset: padding 32 per block 512
v_lshlrev_b32 v2, 0x5, v2                          // Final Offset: padding 32 per block 512
v_add_u32 v[vgprLocalReadAddrB], v2, v[vgprLocalReadAddrB] // Final Offset: add padding 32 per block 512

/* local read addresses: declare addresses a */
/* N/A */

/* local read addresses: declare addresses b */

/******************************************/
/* Local Write Addresses                  */
/******************************************/
/* LVCA = 4 */
/* v1 = A-unroll = serial/LVCA */
/* TileAssignment for DirectToVgprA */
v_and_b32 v1, 63, v[vgprSerial]                    // 0. thread id in wave: wtid = tid % wavelength(64)
v_lshrrev_b32 v0, 6, v[vgprSerial]                 // 7. wave offset in N dimen: wtid = tid / dividedForWaveId(64)
v_lshrrev_b32 v4, 5, s[sgprSizesSum] // numKr = DimK / 32
v_mul_u32_u24 v0, v4, v0 // wtid * numKr
                                                   // 1. N offset: nOffset = nIdx * nStride(1) (multiplier is 1, do nothing)
/* Skip. 2. block offset: bnOffset = 0 when num1DBlocks = 1 */
                                                   // 4. apply VectorWidth: bnOffset = bnOffset * vw(1) (multiplier is 1, do nothing)
//v_lshrrev_b32 v1, 4, v1                            // 5. K offset: kIdx = wtid / (MIN(16) * MIBB(1))
//v_lshrrev_b32 v4, 6, v[vgprSerial]                 // 7. wave offset in N dimen: wtid = tid / dividedForWaveId(64)
//v_and_b32 v4, 3, v4                                // 7. wave offset in M dimen: wtid0 = wtid / num1DWaves(4)
//v_lshlrev_b32 v4, 0x4, v4                          // 7. wave offset in M dimen: wOffset = wtid0 * W0Stride(16)
//v_add_u32 v0, v4, v0                               // 7. final local read offset: flrOffset = lrOffset + WOffset
/* unroll *= glvw */
v_lshlrev_b32 v1, 0x3, v1                          // v1 = v1 * 8
v_mov_b32 v4, v1                                   // copy for GlobalSplitU
/* LVCB = 4 */
/* v3 = B-unroll = serial%LVCB */
v_lshrrev_b32 v2, 2, v[vgprSerial]                 // v2 = v[vgprSerial] / 4
v_and_b32 v3, 3, v[vgprSerial]                     // v3 = v[vgprSerial] % 4
/* unroll *= glvw */
v_lshlrev_b32 v3, 0x3, v3                          // v3 = v3 * 8
v_mov_b32 v5, v3                                   // copy for GlobalSplitU
/* lwaUnrollAssignmentA = v4 */
/* lwaUnrollAssignmentB = v5 */

/* local write addresses: first offset a */

/* local write addresses: first offset b */
v_mul_u32_u24 v[vgprLocalWriteAddrB], 0x20, v2     // lwBL**(DepthU_Compute + PAD)
v_add_lshl_u32 v[vgprLocalWriteAddrB], v5, v[vgprLocalWriteAddrB], 0x1 // lwFOB = (lwBB + lwBL*(DepthU+PAD))*bpeDS
v_lshrrev_b32 v6, 9, v[vgprLocalWriteAddrB]        // padding 32 per block 512
v_lshlrev_b32 v6, 0x5, v6                          // padding 32 per block 512
v_add_u32 v[vgprLocalWriteAddrB], v6, v[vgprLocalWriteAddrB] // add padding 32 per block 512
v_mov_b32 v8, MT0                                  // set MT0 into sgpr
v_mov_b32 v7, s[sgprSizesFree+0]                   // set Free0 size
v_cvt_f32_u32 v6, v8                               // v6 = ceil(v7 / v8)
v_rcp_iflag_f32 v6, v6                             // v6 = ceil(v7 / v8)
v_cvt_f32_u32 v9, v7                               // v6 = ceil(v7 / v8)
v_mul_f32 v6, v6, v9                               // v6 = ceil(v7 / v8)
v_cvt_u32_f32 v6, v6                               // v6 = ceil(v7 / v8)
v_mul_u32_u24 v9, v6, v8                           // v6 = ceil(v7 / v8)
v_sub_u32 v9, v7, v9                               // v6 = ceil(v7 / v8)
v_cmp_ne_u32 vcc, v9, 0                            // v6 = ceil(v7 / v8)
v_addc_co_u32 v6, vcc, v6, 0, vcc                  // ceil
v_mov_b32 v8, MT1                                  // set MT1 into sgpr
v_mov_b32 v7, s[sgprSizesFree+1]                   // set Free1 size
v_readfirstlane_b32 s[sgprNumWorkGroups0], v6      // set back to numWorkGroup0
v_cvt_f32_u32 v6, v8                               // v6 = ceil(v7 / v8)
v_rcp_iflag_f32 v6, v6                             // v6 = ceil(v7 / v8)
v_cvt_f32_u32 v9, v7                               // v6 = ceil(v7 / v8)
v_mul_f32 v6, v6, v9                               // v6 = ceil(v7 / v8)
v_cvt_u32_f32 v6, v6                               // v6 = ceil(v7 / v8)
v_mul_u32_u24 v9, v6, v8                           // v6 = ceil(v7 / v8)
v_sub_u32 v9, v7, v9                               // v6 = ceil(v7 / v8)
v_cmp_ne_u32 vcc, v9, 0                            // v6 = ceil(v7 / v8)
v_addc_co_u32 v6, vcc, v6, 0, vcc                  // ceil
s_nop 0                                            // 1 wait states
v_readfirstlane_b32 s[sgprNumWorkGroups1], v6      // set back to numWorkGroup1
s_waitcnt lgkmcnt(0)                               // wait for 44/0 bytes of kern args

/* Early stop if N(SizeFreeJ) == 0 */
s_cmp_eq_u32 s[sgprSizeJ], 0x0
s_cbranch_scc0 label_NoEarlyStop_N0
label_EarlyStop_if_N_is_0:
s_endpgm
label_NoEarlyStop_N0:

/* remap wg from 1D(idxWG012) to 3D(wg2,wg1,wg0) */
/* wg2 = idxWG012 * smallMagicNumber(1/(numWG0*numWG1)) */
s_mul_i32 s48, s[sgprNumWorkGroups0], s[sgprNumWorkGroups1]
s_and_b32 s49, s[sgprGSU], 0x3fff                  // Restore GSU
s_mul_i32 s48, s48, s49
v_cvt_f32_u32 v6, s48                              // s48 = s[sgprWorkGroup0] / s48
v_rcp_iflag_f32 v6, v6                             // s48 = s[sgprWorkGroup0] / s48
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // s48 = s[sgprWorkGroup0] / s48
v_mul_f32 v6, v6, v7                               // s48 = s[sgprWorkGroup0] / s48
v_cvt_u32_f32 v6, v6                               // s48 = s[sgprWorkGroup0] / s48
v_mul_u32_u24 v7, v6, s48                          // s48 = s[sgprWorkGroup0] / s48
v_sub_u32 v7, s[sgprWorkGroup0], v7                // s48 = s[sgprWorkGroup0] / s48
v_cmpx_eq_u32 exec, v7, s48                        // s48 = s[sgprWorkGroup0] / s48
v_add_u32 v6, 1, v6                                // s48 = s[sgprWorkGroup0] / s48
s_mov_b64 exec, -1                                 // s48 = s[sgprWorkGroup0] / s48
v_readfirstlane_b32 s48, v6                        // quotient
s_mov_b32 s[sgprWorkGroup2], s48
/* idxWG01 = idxWG012 - wg2 * numWG0 * numWG1 */
s_mul_i32 s48, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s48, s48, s[sgprWorkGroup2]
s_mul_i32 s48, s48, s49
s_sub_u32 s[sgprWorkGroup0], s[sgprWorkGroup0], s48
/* wg1 = idxWG01 * smallMagicNumber(1/numWG0) */
v_cvt_f32_u32 v6, s[sgprNumWorkGroups0]            // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_rcp_iflag_f32 v6, v6                             // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_mul_f32 v6, v6, v7                               // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_cvt_u32_f32 v6, v6                               // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_mul_u32_u24 v7, v6, s[sgprNumWorkGroups0]        // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_sub_u32 v7, s[sgprWorkGroup0], v7                // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_cmpx_eq_u32 exec, v7, s[sgprNumWorkGroups0]      // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_add_u32 v6, 1, v6                                // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
s_mov_b64 exec, -1                                 // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_readfirstlane_b32 s48, v6                        // quotient
s_mov_b32 s[sgprWorkGroup1], s48
/* wg0 = idxWG01 - wg1 * numWG0 */
s_mul_i32 s48, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_sub_u32 s[sgprWorkGroup0], s[sgprWorkGroup0], s48

/* Early stop if wg exceed */
s_cmp_ge_u32 s[sgprWorkGroup2], s[sgprSizesFree+2]
s_cbranch_scc0 label_NoEarlyStop_wgExceed
label_EarlyStop_if_wg_exceed:
s_endpgm
label_NoEarlyStop_wgExceed:

label_MultiGemmEnd:
.set sgprSrdA, 48
.set sgprSrdB, 52
.set sgprShadowLimitA, 56
.set sgprShadowLimitB, 58
.set sgprStaggerUIter, 47
.set sgprStaggerUIterDTV, 60
.set sgprWrapUA, 61
.set sgprWrapUB, 63
.set sgprGlobalReadIncsA, 65
.set sgprGlobalReadIncsB, 66
s_sub_u32 s[sgprAddressA+0], s[sgprAddressA+0], 16 // pre-pad to make room for possible pointer shift
s_subb_u32 s[sgprAddressA+1], s[sgprAddressA+1], 0 // pre-pad to make room for possible pointer shift
s_sub_u32 s[sgprAddressB+0], s[sgprAddressB+0], 16 // pre-pad to make room for possible pointer shift
s_subb_u32 s[sgprAddressB+1], s[sgprAddressB+1], 0 // pre-pad to make room for possible pointer shift

/* Short circuit condition if Alpha == 0, then sumDims=0 */
v_cmp_eq_f32 vcc, s[sgprAlpha], 0.0                // s[Alpha] == 0.0f ?
s_cbranch_vccz label_AlphaNonZero                  // branch if s[Alpha] != 0
s_mov_b32 s[sgprSizesSum+0], 0x0                   // Set summation dim=0 if Alpha == 0
label_AlphaNonZero:

/******************************************/
/* Begin setupNewTile                     */
/******************************************/

/* global read addresses: work-group */
/* graWorkGroup mapping */
s_and_b32 s67, s[sgprGSU], 0x3fff                  // Restore GSU
s_cmp_eq_u32 s67, 1                                // GSU == 1 ?
s_cbranch_scc1 label_GSU                           // branch if GSU == 1
// GSU-not-WGMapRR :nwg1 = (size1J + MT1J - 1) / MT1J;
s_and_b32 s67, s[sgprGSU], 0x4000                  // SCC = (GSUWGMRR == 1) ?
s_cbranch_scc1 label_GSUWGMRR                      // branch if GSUWGMRR == 1
s_and_b32 s67, s[sgprGSU], 0x3fff                  // Restore GSU
v_cvt_f32_u32 v6, s67                              // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s67
v_rcp_iflag_f32 v6, v6                             // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s67
v_cvt_f32_u32 v7, s[sgprWorkGroup1]                // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s67
v_mul_f32 v6, v6, v7                               // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s67
v_cvt_u32_f32 v6, v6                               // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s67
v_mul_u32_u24 v7, v6, s67                          // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s67
v_sub_u32 v7, s[sgprWorkGroup1], v7                // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s67
v_cmpx_eq_u32 exec, v7, s67                        // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s67
v_add_u32 v6, 1, v6                                // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s67
v_mov_b32 v7, 0                                    // s[sgprGSUSumIdx] = s[sgprWorkGroup1] % s67
s_mov_b64 exec, -1                                 // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s67
v_readfirstlane_b32 s[sgprWorkGroup1], v6          // quotient
v_readfirstlane_b32 s[sgprGSUSumIdx], v7           // remainder
s_branch label_GSUWGMRR_End
label_GSUWGMRR:
v_cvt_f32_u32 v6, s[sgprNumWorkGroups1]            // s[sgprGSUSumIdx] = s[sgprWorkGroup1] / s[sgprNumWorkGroups1]
v_rcp_iflag_f32 v6, v6                             // s[sgprGSUSumIdx] = s[sgprWorkGroup1] / s[sgprNumWorkGroups1]
v_cvt_f32_u32 v7, s[sgprWorkGroup1]                // s[sgprGSUSumIdx] = s[sgprWorkGroup1] / s[sgprNumWorkGroups1]
v_mul_f32 v6, v6, v7                               // s[sgprGSUSumIdx] = s[sgprWorkGroup1] / s[sgprNumWorkGroups1]
v_cvt_u32_f32 v6, v6                               // s[sgprGSUSumIdx] = s[sgprWorkGroup1] / s[sgprNumWorkGroups1]
v_mul_u32_u24 v7, v6, s[sgprNumWorkGroups1]        // s[sgprGSUSumIdx] = s[sgprWorkGroup1] / s[sgprNumWorkGroups1]
v_sub_u32 v7, s[sgprWorkGroup1], v7                // s[sgprGSUSumIdx] = s[sgprWorkGroup1] / s[sgprNumWorkGroups1]
v_cmpx_eq_u32 exec, v7, s[sgprNumWorkGroups1]      // s[sgprGSUSumIdx] = s[sgprWorkGroup1] / s[sgprNumWorkGroups1]
v_add_u32 v6, 1, v6                                // s[sgprGSUSumIdx] = s[sgprWorkGroup1] / s[sgprNumWorkGroups1]
v_mov_b32 v7, 0                                    // s[sgprWorkGroup1] = s[sgprWorkGroup1] % s[sgprNumWorkGroups1]
s_mov_b64 exec, -1                                 // s[sgprGSUSumIdx] = s[sgprWorkGroup1] / s[sgprNumWorkGroups1]
v_readfirstlane_b32 s[sgprGSUSumIdx], v6           // quotient
v_readfirstlane_b32 s[sgprWorkGroup1], v7          // remainder
label_GSUWGMRR_End:
s_mov_b32 s[sgprGSULog2BpeC], 1
s_mov_b32 s[sgprGSULog2BpeD], 2
s_branch label_GSU_End
label_GSU:
s_mov_b64 s[sgprGSUSumIdx:sgprGSUSumIdx+1], 0      // Set GSUSumIdx to 0
s_mov_b32 s[sgprGSULog2BpeC], 1
s_mov_b32 s[sgprGSULog2BpeD], 1
label_GSU_End:
s_sext_i32_i16 s[sgprWGM], s[sgprWGM]              // Restore WGM
s_cmp_gt_i32 s[sgprWGM], 1                         // WGM > 1 ?
s_cbranch_scc1 label_WGMPositive                   // branch if WGM > 1
s_cmp_ge_i32 s[sgprWGM], 0                         // WGM >= 0 ?
s_cbranch_scc1 label_WGM                           // branch if WGM >= 0
s_abs_i32 s[sgprWGM], s[sgprWGM]                   // abs(WGM)
v_cvt_f32_u32 v6, s[sgprWGM]                       // WGM
v_rcp_iflag_f32 v6, v6                             // WGM
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // WGM
v_mul_f32 v6, v6, v7                               // WGM
v_cvt_u32_f32 v6, v6                               // WGM
v_mul_u32_u24 v7, v6, s[sgprWGM]                   // WGM
v_sub_u32 v7, s[sgprWorkGroup0], v7                // WGM
v_cmpx_eq_u32 exec, v7, s[sgprWGM]                 // WGM
v_add_u32 v6, 1, v6                                // WGM
s_mov_b64 exec, -1                                 // WGM
v_readfirstlane_b32 s67, v6                        // quotient
s_mul_i32 s70, s67, s[sgprWGM]                     // quotient * non-magic divisor
s_sub_u32 s70, s[sgprWorkGroup0], s70              // WorkGroup0=remainder
s_mul_i32 s70, s70, s[sgprNumWorkGroups1]          // (wg1 % WGM)*NumWorkGroups1
s_add_u32 s70, s70, s[sgprWorkGroup1]              // wgSerial = wg0 + (wg1 % WGM)*NumWorkGroups1
v_cvt_f32_u32 v6, s[sgprWGM]                       // WGM
v_rcp_iflag_f32 v6, v6                             // WGM
v_cvt_f32_u32 v7, s[sgprNumWorkGroups0]            // WGM
v_mul_f32 v6, v6, v7                               // WGM
v_cvt_u32_f32 v6, v6                               // WGM
v_mul_u32_u24 v7, v6, s[sgprWGM]                   // WGM
v_sub_u32 v7, s[sgprNumWorkGroups0], v7            // WGM
v_cmpx_eq_u32 exec, v7, s[sgprWGM]                 // WGM
v_add_u32 v6, 1, v6                                // WGM
s_mov_b64 exec, -1                                 // WGM
v_readfirstlane_b32 s68, v6                        // quotient
s_mul_i32 s69, s[sgprWGM], s68                     // quotient * non-magic divisor
s_sub_u32 s69, s[sgprNumWorkGroups0], s69          // NumWorkGroups0=remainder
s_cmp_eq_u32 s69, 0                                // remainder == 0 ?
s_cmov_b32 s69, s[sgprWGM]                         // remainder = WGM if remainder == 0
s_cmp_ge_u32 s67, s68                              // blockId >= numFullBlocks ?
s_cselect_b32 s68, s69, s[sgprWGM]
v_cvt_f32_u32 v6, s68                              // s[sgprWorkGroup1] = s70 / s68
v_rcp_iflag_f32 v6, v6                             // s[sgprWorkGroup1] = s70 / s68
v_cvt_f32_u32 v7, s70                              // s[sgprWorkGroup1] = s70 / s68
v_mul_f32 v6, v6, v7                               // s[sgprWorkGroup1] = s70 / s68
v_cvt_u32_f32 v6, v6                               // s[sgprWorkGroup1] = s70 / s68
v_mul_u32_u24 v7, v6, s68                          // s[sgprWorkGroup1] = s70 / s68
v_sub_u32 v7, s70, v7                              // s[sgprWorkGroup1] = s70 / s68
v_cmpx_eq_u32 exec, v7, s68                        // s[sgprWorkGroup1] = s70 / s68
v_add_u32 v6, 1, v6                                // s[sgprWorkGroup1] = s70 / s68
v_mov_b32 v7, 0                                    // s[sgprWorkGroup0] = s70 % s68
s_mov_b64 exec, -1                                 // s[sgprWorkGroup1] = s70 / s68
v_readfirstlane_b32 s[sgprWorkGroup1], v6          // quotient
v_readfirstlane_b32 s[sgprWorkGroup0], v7          // remainder
s_mul_i32 s[sgprWorkGroup0], s[sgprWorkGroup1], s68 // quotient * non-magic divisor
s_sub_u32 s[sgprWorkGroup0], s70, s[sgprWorkGroup0] // WorkGroup0=remainder
s_mul_i32 s67, s67, s[sgprWGM]                     // blockId * WGM
s_add_u32 s[sgprWorkGroup0], s[sgprWorkGroup0], s67 // wg1 += blockId * WGM
s_branch label_WGM
label_WGMPositive:
v_cvt_f32_u32 v6, s[sgprWGM]                       // WGM
v_rcp_iflag_f32 v6, v6                             // WGM
v_cvt_f32_u32 v7, s[sgprWorkGroup1]                // WGM
v_mul_f32 v6, v6, v7                               // WGM
v_cvt_u32_f32 v6, v6                               // WGM
v_mul_u32_u24 v7, v6, s[sgprWGM]                   // WGM
v_sub_u32 v7, s[sgprWorkGroup1], v7                // WGM
v_cmpx_eq_u32 exec, v7, s[sgprWGM]                 // WGM
v_add_u32 v6, 1, v6                                // WGM
s_mov_b64 exec, -1                                 // WGM
v_readfirstlane_b32 s67, v6                        // quotient
s_mul_i32 s70, s67, s[sgprWGM]                     // quotient * non-magic divisor
s_sub_u32 s70, s[sgprWorkGroup1], s70              // WorkGroup1=remainder
s_mul_i32 s70, s70, s[sgprNumWorkGroups0]          // (wg1 % WGM)*NumWorkGroups0
s_add_u32 s70, s70, s[sgprWorkGroup0]              // wgSerial = wg0 + (wg1 % WGM)*NumWorkGroups0
v_cvt_f32_u32 v6, s[sgprWGM]                       // WGM
v_rcp_iflag_f32 v6, v6                             // WGM
v_cvt_f32_u32 v7, s[sgprNumWorkGroups1]            // WGM
v_mul_f32 v6, v6, v7                               // WGM
v_cvt_u32_f32 v6, v6                               // WGM
v_mul_u32_u24 v7, v6, s[sgprWGM]                   // WGM
v_sub_u32 v7, s[sgprNumWorkGroups1], v7            // WGM
v_cmpx_eq_u32 exec, v7, s[sgprWGM]                 // WGM
v_add_u32 v6, 1, v6                                // WGM
s_mov_b64 exec, -1                                 // WGM
v_readfirstlane_b32 s68, v6                        // quotient
s_mul_i32 s69, s[sgprWGM], s68                     // quotient * non-magic divisor
s_sub_u32 s69, s[sgprNumWorkGroups1], s69          // NumWorkGroups1=remainder
s_cmp_eq_u32 s69, 0                                // remainder == 0 ?
s_cmov_b32 s69, s[sgprWGM]                         // remainder = WGM if remainder == 0
s_cmp_ge_u32 s67, s68                              // blockId >= numFullBlocks ?
s_cselect_b32 s68, s69, s[sgprWGM]
v_cvt_f32_u32 v6, s68                              // s[sgprWorkGroup0] = s70 / s68
v_rcp_iflag_f32 v6, v6                             // s[sgprWorkGroup0] = s70 / s68
v_cvt_f32_u32 v7, s70                              // s[sgprWorkGroup0] = s70 / s68
v_mul_f32 v6, v6, v7                               // s[sgprWorkGroup0] = s70 / s68
v_cvt_u32_f32 v6, v6                               // s[sgprWorkGroup0] = s70 / s68
v_mul_u32_u24 v7, v6, s68                          // s[sgprWorkGroup0] = s70 / s68
v_sub_u32 v7, s70, v7                              // s[sgprWorkGroup0] = s70 / s68
v_cmpx_eq_u32 exec, v7, s68                        // s[sgprWorkGroup0] = s70 / s68
v_add_u32 v6, 1, v6                                // s[sgprWorkGroup0] = s70 / s68
v_mov_b32 v7, 0                                    // s[sgprWorkGroup1] = s70 % s68
s_mov_b64 exec, -1                                 // s[sgprWorkGroup0] = s70 / s68
v_readfirstlane_b32 s[sgprWorkGroup0], v6          // quotient
v_readfirstlane_b32 s[sgprWorkGroup1], v7          // remainder
s_mul_i32 s[sgprWorkGroup1], s[sgprWorkGroup0], s68 // quotient * non-magic divisor
s_sub_u32 s[sgprWorkGroup1], s70, s[sgprWorkGroup1] // WorkGroup1=remainder
s_mul_i32 s67, s67, s[sgprWGM]                     // blockId * WGM
s_add_u32 s[sgprWorkGroup1], s[sgprWorkGroup1], s67 // wg1 += blockId * WGM
label_WGM:

/* global read addresses: tile offset assignment a */
/* graTileAssignmentA = v0 */

/* global read addresses: tile offset assignment b */
/* graTileAssignmentB = v2 */

/* global read addresses: unroll assignment a */
/* v1 */

/* global read addresses: unroll assignment b */
/* v3 */

/* global read addresses: other free assignments */
/* s[sgprWorkGroup2] */

/* global read addresses: tile offsets a */
v_mov_b32 v6, v0                                   // groA0I_0
s_lshr_b32 s72, s[sgprSizesSum], 5                // # of Kr
s_lshl_b32 s72, s72, 2
v_add_co_u32 v7, vcc, s72, v6                       // groA0I_1 += LSPA
v_add_co_u32 v8, vcc, s72, v7                       // groA0I_2 += LSPA
v_add_co_u32 v9, vcc, s72, v8                       // groA0I_3 += LSPA
//v_add_co_u32 v7, vcc, 64, v6                       // groA0I_1 += LSPA
//v_add_co_u32 v8, vcc, 64, v7                       // groA0I_2 += LSPA
//v_add_co_u32 v9, vcc, 64, v8                       // groA0I_3 += LSPA

/* global read addresses: tile offsets b */
v_mov_b32 v10, v2                                  // groB1J_0
v_add_co_u32 v11, vcc, 64, v10                     // groB1J_1 += LSPB
v_add_co_u32 v12, vcc, 64, v11                     // groB1J_2 += LSPB
v_add_co_u32 v13, vcc, 64, v12                     // groB1J_3 += LSPB

/* global read addresses: unroll offsets a */
v_mov_b32 v14, v1                                  // groAL_0

/* global read addresses: unroll offsets b */
v_mov_b32 v15, v3                                  // groBL_0

/* global read addresses: final offsets a */
//GLOBAL_OFFSET_A vgprGlobalReadOffsetA+0, 14,  6, 16 // gROA_0_0_0_0
//GLOBAL_OFFSET_A vgprGlobalReadOffsetA+1, 14,  7, 16 // gROA_0_0_1_0
//GLOBAL_OFFSET_A vgprGlobalReadOffsetA+2, 14,  8, 16 // gROA_0_0_2_0
//GLOBAL_OFFSET_A vgprGlobalReadOffsetA+3, 14,  9, 16 // gROA_0_0_3_0
s_mov_b32 s72, 16*32
GLOBAL_SWIZZLED_OFFSET_A vgprGlobalReadOffsetA+0, 14,  6, 16 // gROA_0_0_0_0
GLOBAL_SWIZZLED_OFFSET_A vgprGlobalReadOffsetA+1, 14,  7, 16 // gROA_0_0_1_0
GLOBAL_SWIZZLED_OFFSET_A vgprGlobalReadOffsetA+2, 14,  8, 16 // gROA_0_0_2_0
GLOBAL_SWIZZLED_OFFSET_A vgprGlobalReadOffsetA+3, 14,  9, 16 // gROA_0_0_3_0

/* global read addresses: final offsets b */
GLOBAL_OFFSET_B vgprGlobalReadOffsetB+0, 15, 10, 6 // gROB_0_0_0_0
GLOBAL_OFFSET_B vgprGlobalReadOffsetB+1, 15, 11, 6 // gROB_0_0_1_0
GLOBAL_OFFSET_B vgprGlobalReadOffsetB+2, 15, 12, 6 // gROB_0_0_2_0
GLOBAL_OFFSET_B vgprGlobalReadOffsetB+3, 15, 13, 6 // gROB_0_0_3_0

/* global read addresses: addresses a */
/* max read offset = size[n] * stride[n-1] */
s_mul_hi_u32 s71, s[sgprWorkGroup0], 256           // WorkGroup[01] * MT
s_mul_i32 s70, s[sgprWorkGroup0], 256              // WorkGroup[01] * MT
s_mul_hi_u32 s71, s70, s[sgprStrideA0I]            // tlu=0, scaled tile-offset by stride
s_mul_i32 s70, s70, s[sgprStrideA0I]               // tlu=0, scaled tile-offset by stride
s_and_b32 s68, s[sgprGSU], 0x8000                  // SCC = (GSUC == 1) ?
s_cbranch_scc1 label_GSUC_A                        // branch if GSUC == 1
s_mul_hi_u32 s69, 32, s[sgprGSUSumIdx]             // gsuOffset = DepthU*GSUSumIdx
s_mul_i32 s68, 32, s[sgprGSUSumIdx]                // gsuOffset = DepthU*GSUSumIdx
s_branch label_GSUC_A_End
label_GSUC_A:
s_lshr_b32 s[sgprLoopCounterL], s[sgprSizesSum], 5 // s[LoopCounterL] = s[sgprSizesSum] / 32
s_and_b32 s[sgprGSUSumIdx+1], s[sgprGSU], 0x3fff   // Restore GSU
v_cvt_f32_u32 v0, s[sgprGSUSumIdx+1]               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_rcp_iflag_f32 v0, v0                             // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_cvt_f32_u32 v1, s[sgprLoopCounterL]              // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_mul_f32 v0, v0, v1                               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_cvt_u32_f32 v0, v0                               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_mul_u32_u24 v1, v0, s[sgprGSUSumIdx+1]           // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_sub_u32 v1, s[sgprLoopCounterL], v1              // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_cmpx_eq_u32 exec, v1, s[sgprGSUSumIdx+1]         // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_add_u32 v0, 1, v0                                // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_mov_b32 v1, 0                                    // s[sgprGSUSumIdx+1] = s[sgprLoopCounterL] % s[sgprGSUSumIdx+1]
s_mov_b64 exec, -1                                 // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_readfirstlane_b32 s[sgprLoopCounterL], v0        // quotient
v_readfirstlane_b32 s[sgprGSUSumIdx+1], v1         // remainder
s_mul_i32 s69, s[sgprLoopCounterL], s[sgprGSUSumIdx] // quotient*GSUSumIdx
s_add_u32 s68, 1, s[sgprLoopCounterL]              // quotient+1
s_add_u32 s69, s69, s[sgprGSUSumIdx+1]             // quotient*GSUSumIdx+remainder
s_mul_i32 s68, s68, s[sgprGSUSumIdx]               // (quotient+1)*GSUSumIdx
s_cmp_lt_u32 s[sgprGSUSumIdx], s[sgprGSUSumIdx+1]  // gsuSumIdx < numIterPerWgRemainder
s_cselect_b32 s68, s68, s69                        // (quotient+1)*GSUSumIdx if needed
s_mul_hi_u32 s69, s68, 32                          // gsuOffset = DepthU*accumulatedNumOfLoopCounterL
s_mul_i32 s68, s68, 32                             // gsuOffset = DepthU*accumulatedNumOfLoopCounterL
label_GSUC_A_End:
s_add_u32 s70, s70, s68                            // accum GsuOffset term to tilestart
s_addc_u32 s71, s71, s69                           // accum GsuOffset term to tilestart
s_mov_b32 s[sgprShadowLimitA+0], 1                 // Init tensor size
s_mov_b32 s[sgprShadowLimitA+1], 0                 // init tensor size
s_sub_u32 s68, s[sgprSizeL], 1                     // (size-1)
s_mul_hi_u32 s69, constStrideAL, s68               // stride x (size-1)
s_mul_i32 s68, constStrideAL, s68                  // stride x (size-1)
s_add_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s68 // sum tensor size
s_addc_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s69 // sum tensor size
s_sub_u32 s68, s[sgprSizeI], 1                     // (size-1)
s_mul_hi_u32 s69, s[sgprStrideA0I], s68            // stride x (size-1)
s_mul_i32 s68, s[sgprStrideA0I], s68               // stride x (size-1)
s_add_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s68 // sum tensor size
s_addc_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s69 // sum tensor size
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s70 // sub tileStart
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s71 // sub tileStart
s_lshl_b64 s[sgprShadowLimitA:sgprShadowLimitA+1], s[sgprShadowLimitA:sgprShadowLimitA+1], 0x1 // Set limit to use bytes
s_add_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], 16 // extend limit for pre-pad
s_addc_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], 0 // extend limit for pre-pad
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
s_mul_hi_u32 s69, s[sgprStrideAK], s[sgprWorkGroup2] // Stride*WG
s_mul_i32 s68, s[sgprStrideAK], s[sgprWorkGroup2]  // Stride*WG
s_add_u32 s70, s70, s68                            // accum wg term to tilestart
s_addc_u32 s71, s71, s69                           // accum wg term to tilestart
s_lshl_b64 s[70:71], s[70:71], 0x1                 // tileStart *= BPE
s_add_u32 s[sgprSrdA+0], s[sgprAddressA+0], s70    // SRD base = Address+ tileStart0
s_addc_u32 s[sgprSrdA+1], s[sgprAddressA+1], s71   // SRD base = Address+ tileStart1
s_mov_b32 s[sgprSrdA+3], Srd127_96                 // Set bits 127_96 in SRD

/* global read addresses: addresses b */
/* max read offset = size[n] * stride[n-1] */
s_mul_hi_u32 s71, s[sgprWorkGroup1], 256           // WorkGroup[01] * MT
s_mul_i32 s70, s[sgprWorkGroup1], 256              // WorkGroup[01] * MT
s_mul_hi_u32 s71, s70, s[sgprStrideB1J]            // tlu=0, scaled tile-offset by stride
s_mul_i32 s70, s70, s[sgprStrideB1J]               // tlu=0, scaled tile-offset by stride
s_and_b32 s68, s[sgprGSU], 0x8000                  // SCC = (GSUC == 1) ?
s_cbranch_scc1 label_GSUC_B                        // branch if GSUC == 1
s_mul_hi_u32 s69, 32*16, s[sgprGSUSumIdx]             // gsuOffset = DepthU*GSUSumIdx
s_mul_i32 s68, 32*16, s[sgprGSUSumIdx]                // gsuOffset = DepthU*GSUSumIdx
s_branch label_GSUC_B_End
label_GSUC_B:
s_lshr_b32 s[sgprLoopCounterL], s[sgprSizesSum], 5 // s[LoopCounterL] = s[sgprSizesSum] / 32
s_and_b32 s[sgprGSUSumIdx+1], s[sgprGSU], 0x3fff   // Restore GSU
v_cvt_f32_u32 v0, s[sgprGSUSumIdx+1]               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_rcp_iflag_f32 v0, v0                             // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_cvt_f32_u32 v1, s[sgprLoopCounterL]              // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_mul_f32 v0, v0, v1                               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_cvt_u32_f32 v0, v0                               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_mul_u32_u24 v1, v0, s[sgprGSUSumIdx+1]           // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_sub_u32 v1, s[sgprLoopCounterL], v1              // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_cmpx_eq_u32 exec, v1, s[sgprGSUSumIdx+1]         // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_add_u32 v0, 1, v0                                // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_mov_b32 v1, 0                                    // s[sgprGSUSumIdx+1] = s[sgprLoopCounterL] % s[sgprGSUSumIdx+1]
s_mov_b64 exec, -1                                 // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_readfirstlane_b32 s[sgprLoopCounterL], v0        // quotient
v_readfirstlane_b32 s[sgprGSUSumIdx+1], v1         // remainder
s_mul_i32 s69, s[sgprLoopCounterL], s[sgprGSUSumIdx] // quotient*GSUSumIdx
s_add_u32 s68, 1, s[sgprLoopCounterL]              // quotient+1
s_add_u32 s69, s69, s[sgprGSUSumIdx+1]             // quotient*GSUSumIdx+remainder
s_mul_i32 s68, s68, s[sgprGSUSumIdx]               // (quotient+1)*GSUSumIdx
s_cmp_lt_u32 s[sgprGSUSumIdx], s[sgprGSUSumIdx+1]  // gsuSumIdx < numIterPerWgRemainder
s_cselect_b32 s68, s68, s69                        // (quotient+1)*GSUSumIdx if needed
s_mul_hi_u32 s69, s68, 32                          // gsuOffset = DepthU*accumulatedNumOfLoopCounterL
s_mul_i32 s68, s68, 32                             // gsuOffset = DepthU*accumulatedNumOfLoopCounterL
label_GSUC_B_End:
s_add_u32 s70, s70, s68                            // accum GsuOffset term to tilestart
s_addc_u32 s71, s71, s69                           // accum GsuOffset term to tilestart
s_mov_b32 s[sgprShadowLimitB+0], 1                 // Init tensor size
s_mov_b32 s[sgprShadowLimitB+1], 0                 // init tensor size
s_sub_u32 s68, s[sgprSizeL], 1                     // (size-1)
s_mul_hi_u32 s69, constStrideBL, s68               // stride x (size-1)
s_mul_i32 s68, constStrideBL, s68                  // stride x (size-1)
s_add_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s68 // sum tensor size
s_addc_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s69 // sum tensor size
s_sub_u32 s68, s[sgprSizeJ], 1                     // (size-1)
s_mul_hi_u32 s69, s[sgprStrideB1J], s68            // stride x (size-1)
s_mul_i32 s68, s[sgprStrideB1J], s68               // stride x (size-1)
s_add_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s68 // sum tensor size
s_addc_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s69 // sum tensor size
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s70 // sub tileStart
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s71 // sub tileStart
s_lshl_b64 s[sgprShadowLimitB:sgprShadowLimitB+1], s[sgprShadowLimitB:sgprShadowLimitB+1], 0x1 // Set limit to use bytes
s_add_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], 16 // extend limit for pre-pad
s_addc_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], 0 // extend limit for pre-pad
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
s_mul_hi_u32 s69, s[sgprStrideBK], s[sgprWorkGroup2] // Stride*WG
s_mul_i32 s68, s[sgprStrideBK], s[sgprWorkGroup2]  // Stride*WG
s_add_u32 s70, s70, s68                            // accum wg term to tilestart
s_addc_u32 s71, s71, s69                           // accum wg term to tilestart
s_lshl_b64 s[70:71], s[70:71], 0x1                 // tileStart *= BPE
s_add_u32 s[sgprSrdB+0], s[sgprAddressB+0], s70    // SRD base = Address+ tileStart0
s_addc_u32 s[sgprSrdB+1], s[sgprAddressB+1], s71   // SRD base = Address+ tileStart1
s_mov_b32 s[sgprSrdB+3], Srd127_96                 // Set bits 127_96 in SRD

/* global read addresses: increments a */
s_and_b32 s69, s[sgprGSU], 0x3fff                  // Restore GSU
s_mul_i32 s69, s69, DepthU*BpeAGR*16                  // GSU*DepthU*Bpe
s_and_b32 s68, s[sgprGSU], 0x8000                  // SCC = (GSUC == 1) ?
s_cselect_b32 s[sgprGlobalReadIncsA+0], DepthU*BpeAGR, s69 // incrA (unrollIdx)

/* global read addresses: increments b */
s_and_b32 s69, s[sgprGSU], 0x3fff                  // Restore GSU
s_mul_i32 s69, s69, DepthU*BpeBGR                  // GSU*DepthU*Bpe
s_and_b32 s68, s[sgprGSU], 0x8000                  // SCC = (GSUC == 1) ?
s_cselect_b32 s[sgprGlobalReadIncsB+0], DepthU*BpeBGR, s69 // incrB (unrollIdx)
/* declare loop num iterations */
s_lshr_b32 s[sgprLoopCounterL], s[sgprSizesSum+0], 5 // s[sgprLoopCounterL] = s[sgprSizesSum+0] / 32
s_and_b32 s68, s[sgprGSU], 0x3fff                  // Restore GSU
s_cmp_eq_u32 s68, 1                                // GSU == 1 ?
s_cbranch_scc1 label_GSU_1                         // branch if GSU == 1
s_and_b32 s[sgprGSUSumIdx+1], s[sgprGSU], 0x3fff   // Restore GSU
v_cvt_f32_u32 v0, s[sgprGSUSumIdx+1]               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_rcp_iflag_f32 v0, v0                             // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_cvt_f32_u32 v1, s[sgprLoopCounterL]              // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_mul_f32 v0, v0, v1                               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_cvt_u32_f32 v0, v0                               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_mul_u32_u24 v1, v0, s[sgprGSUSumIdx+1]           // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_sub_u32 v1, s[sgprLoopCounterL], v1              // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_cmpx_eq_u32 exec, v1, s[sgprGSUSumIdx+1]         // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_add_u32 v0, 1, v0                                // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_mov_b32 v1, 0                                    // s[sgprGSUSumIdx+1] = s[sgprLoopCounterL] % s[sgprGSUSumIdx+1]
s_mov_b64 exec, -1                                 // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSUSumIdx+1]
v_readfirstlane_b32 s[sgprLoopCounterL], v0        // quotient
v_readfirstlane_b32 s[sgprGSUSumIdx+1], v1         // remainder
s_add_u32 s68, 1, s[sgprLoopCounterL]              // tmp<-numIterMyWg+1
s_cmp_lt_u32 s[sgprGSUSumIdx], s[sgprGSUSumIdx+1]  // gsuSumIdx < numIterPerWgRemainder
s_cmov_b32 s[sgprLoopCounterL], s68                // numIterMyWg++ if needed
label_GSU_1:
s_mov_b32 s[sgprOrigLoopCounter], s[sgprLoopCounterL] // copy loop counter
s_and_b32 s70, s[sgprStaggerU], 0x1f00
s_lshr_b32 s70, s70, 0x8
s_and_b32 s71, s[sgprStaggerU], 0xe000
s_and_b32 s[sgprStaggerU], s[sgprStaggerU], 0xff
s_mov_b32 s68, s[sgprStaggerU]                     // init staggerU
label_beginStaggerUIter:
s_lshl_b32 s69, s68, s70                           // shift by StaggerUStride
s_cmp_ge_u32 s[sgprOrigLoopCounter], s69           // loopCount >= current shift Count
s_cbranch_scc1 label_endStaggerUIter               // jump to end
s_lshr_b32 s68, s68, 1                             // step down to smaller stagger
s_branch label_beginStaggerUIter                   // jump to begin
label_endStaggerUIter:
s_sub_u32 s69, s68, 1                              // staggerU mask
s_cmp_ge_u32 s68, 1                                // if current staggerU >= 1
s_cselect_b32 s[sgprStaggerUIter], s69, 0          // set Mask
s_cmp_eq_u32 s71, 0x0
s_cbranch_scc1 label_StaggerUMapping_1
s_mov_b32 s68, s[sgprWorkGroup0]
s_branch label_staggerInputEnd
label_StaggerUMapping_1:
s_cmp_eq_u32 s71, 0x2000
s_cbranch_scc1 label_StaggerUMapping_2
s_mov_b32 s68, s[sgprWorkGroup1]
s_branch label_staggerInputEnd
label_StaggerUMapping_2:
s_cmp_eq_u32 s71, 0x4000
s_cbranch_scc1 label_StaggerUMapping_3
s_mov_b32 s68, -0x1
s_branch label_staggerInputEnd
label_StaggerUMapping_3:
s_cmp_eq_u32 s71, 0x6000
s_cbranch_scc1 label_StaggerUMapping_4
s_mul_i32 s69, s[sgprNumWorkGroups0], s[sgprWorkGroup1]
s_add_u32 s68, s68, s69
s_add_u32 s68, s68, s[sgprWorkGroup0]
s_branch label_staggerInputEnd
label_StaggerUMapping_4:
s_cmp_eq_u32 s71, 0x8000
s_cbranch_scc1 label_staggerInputEnd
s_mov_b32 s68, -0x1
s_branch label_staggerInputEnd
label_staggerInputEnd:
s_and_b32 s[sgprStaggerUIter], s[sgprStaggerUIter], s68 // Compute actual stagger start for this tile
s_lshl_b32 s[sgprStaggerUIter], s[sgprStaggerUIter], s70 // shift by StaggerUStride

/* SRDs += (StaggerUIter) * GlobalReadIncsA+0 */
s_mul_hi_i32 s69, s[sgprStaggerUIter], s[sgprGlobalReadIncsA+0] //  stagger byte offset
s_mul_i32 s68, s[sgprStaggerUIter], s[sgprGlobalReadIncsA+0] //  stagger byte offset
s_mul_hi_i32 s[sgprWrapUA+1], s[sgprLoopCounterL], s[sgprGlobalReadIncsA+0] // Number of bytes accessed by the unroll loop
s_mul_i32 s[sgprWrapUA+0], s[sgprLoopCounterL], s[sgprGlobalReadIncsA+0] // Number of bytes accessed by the unroll loop
s_sub_u32 s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0], s[sgprWrapUA+0] // remove one iteration
s_subb_u32 s[sgprWrapUA+1], 0, s[sgprWrapUA+1]     // remove one iteration
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s68        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s69       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s68 // limit -= inc)
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s69 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32

/* SRDs += (StaggerUIter) * GlobalReadIncsB+0 */
s_mul_hi_i32 s69, s[sgprStaggerUIter], s[sgprGlobalReadIncsB+0] //  stagger byte offset
s_mul_i32 s68, s[sgprStaggerUIter], s[sgprGlobalReadIncsB+0] //  stagger byte offset
s_mul_hi_i32 s[sgprWrapUB+1], s[sgprLoopCounterL], s[sgprGlobalReadIncsB+0] // Number of bytes accessed by the unroll loop
s_mul_i32 s[sgprWrapUB+0], s[sgprLoopCounterL], s[sgprGlobalReadIncsB+0] // Number of bytes accessed by the unroll loop
s_sub_u32 s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0], s[sgprWrapUB+0] // remove one iteration
s_subb_u32 s[sgprWrapUB+1], 0, s[sgprWrapUB+1]     // remove one iteration
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s68        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s69       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s68 // limit -= inc)
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s69 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
s_add_u32 s[sgprStaggerUIterDTV], s[sgprStaggerUIter], 1 // Subtract (PGR-1); StaggerUIter now contains target iteration to wrap
s_add_u32 s[sgprStaggerUIter], s[sgprStaggerUIter], 2 // Subtract (PGR-1); StaggerUIter now contains target iteration to wrap
/* local read addresses: init pointers a */
/* local read addresses: init pointers b */

/* localReadInitPointers */

/* prefetch: global -> local */
s_cmp_eq_u32 s[sgprLoopCounterL], 0                // at last iteration?
s_cbranch_scc1 label_ShadowInitStart               // skip to ShadowInitStart iter b/c numIter==0
buffer_load_dwordx4 v[vgprG2LB+0:vgprG2LB+0+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_0_0
buffer_load_dwordx4 v[vgprG2LB+4:vgprG2LB+4+3], v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_1_0
buffer_load_dwordx4 v[vgprG2LB+8:vgprG2LB+8+3], v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_2_0
buffer_load_dwordx4 v[vgprG2LB+12:vgprG2LB+12+3], v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_3_0
buffer_load_dwordx4 v[vgprG2LA+0:vgprG2LA+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_0_0
buffer_load_dwordx4 v[vgprG2LA+4:vgprG2LA+4+3], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_1_0
buffer_load_dwordx4 v[vgprG2LA+8:vgprG2LA+8+3], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_2_0
buffer_load_dwordx4 v[vgprG2LA+12:vgprG2LA+12+3], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_3_0

/* global read inc B loopL */
s_add_u32 s70, s[sgprLoopCounterL], 1              // remove pf(1)
s_cmp_eq_u32 s[sgprStaggerUIter], s70              // Is this wrapIter? (pf)
s_cselect_b32 s68, s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0] // incLower <- ?
s_cselect_b32 s69, s[sgprWrapUB+1], 0              // incUpper <- ?
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s68        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s69       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s68 // limit -= inc)
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s69 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32

/******************************************/
/* End setupNewTile                       */
/******************************************/
label_ShadowInitStart:
s_mov_b32 s[sgprSrdD+0], s[sgprAddressD+0]         // init SRD base address (lower)
s_mov_b32 s[sgprSrdD+1], s[sgprAddressD+1]         // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdD+2], 0x80000000
s_mov_b32 s[sgprSrdD+3], Srd127_96                 // Set bits 127_96 in post-loop SRD

s_mov_b32 s[sgprSrdC+0], s[sgprAddressC+0]         // init SRD base address (lower)
s_mov_b32 s[sgprSrdC+1], s[sgprAddressC+1]         // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdC+2], 0x80000000
s_mov_b32 s[sgprSrdC+3], Srd127_96                 // Set bits 127_96 in post-loop SRD


s_mul_i32 s70, MT1, s[sgprWorkGroup1]              // <- wg1*MT1
s_mul_hi_u32 s69, s70, s[sgprStrideC1J]            // ScaleC s70 by Stride
s_mul_i32 s68, s70, s[sgprStrideC1J]               // ScaleC s70 by Stride
s_lshl_b64 s[68:69], s[68:69], s[sgprGSULog2BpeC]  // scale by bpe
s_add_u32 s[sgprSrdC+0], s[sgprAddressC+0], s68    // add lo to SRD
s_addc_u32 s[sgprSrdC+1], s[sgprAddressC+1], s69   // add hi to SRD
s_mul_hi_u32 s69, s70, s[sgprStrideD1J]            // ScaleD s70 by Stride
s_mul_i32 s68, s70, s[sgprStrideD1J]               // ScaleD s70 by Stride
s_lshl_b64 s[68:69], s[68:69], s[sgprGSULog2BpeD]  // scale by bpe
s_add_u32 s[sgprSrdD+0], s[sgprAddressD+0], s68    // add lo to SRD
s_addc_u32 s[sgprSrdD+1], s[sgprAddressD+1], s69   // add hi to SRD

s_mul_hi_u32 s69, s[sgprWorkGroup2], s[sgprStrideCK] // ScaleC s[sgprWorkGroup2] by Stride
s_mul_i32 s68, s[sgprWorkGroup2], s[sgprStrideCK]  // ScaleC s[sgprWorkGroup2] by Stride
s_lshl_b64 s[68:69], s[68:69], s[sgprGSULog2BpeC]  // scale by bpe
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s68        // add lo to SRD
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], s69       // add hi to SRD
s_mul_hi_u32 s69, s[sgprWorkGroup2], s[sgprStrideDK] // ScaleD s[sgprWorkGroup2] by Stride
s_mul_i32 s68, s[sgprWorkGroup2], s[sgprStrideDK]  // ScaleD s[sgprWorkGroup2] by Stride
s_lshl_b64 s[68:69], s[68:69], s[sgprGSULog2BpeD]  // scale by bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s68        // add lo to SRD
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], s69       // add hi to SRD

s_and_b32 s67, s[sgprGSU], 0x3fff                  // Restore GSU
s_cmp_eq_u32 s67, 1                                // GSU == 1 ?
s_cbranch_scc1 label_GSU_2                         // branch if GSU == 1
// GSU Output Buffer offset: Free0 + (Free1-1)*StrideC1J + (Free2-1)*StrideCK * GSUIdx * bpe%s
s_mul_hi_u32 s69, s[sgprSizesFree+0], s[sgprGSUSumIdx] // Free0
s_mul_i32 s68, s[sgprSizesFree+0], s[sgprGSUSumIdx] // Free0
s_sub_u32 s67, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s67, s67, s[sgprGSUSumIdx]               // Free1
s_mul_hi_u32 s70, s67, s[sgprStrideC1J]            // Free1
s_mul_i32 s67, s67, s[sgprStrideC1J]               // Free1
s_add_u32 s68, s68, s67                            // Free1
s_addc_u32 s69, s69, s70                           // Free1
s_sub_u32 s67, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s67, s67, s[sgprGSUSumIdx]               // Free2
s_mul_hi_u32 s70, s67, s[sgprStrideCK]             // Free2
s_mul_i32 s67, s67, s[sgprStrideCK]                // Free2
s_add_u32 s68, s68, s67                            // Free2
s_addc_u32 s69, s69, s70                           // Free2
s_lshl_b64 s[68:69], s[68:69], 2                   // scale by bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s68        // add lo GSU offset to SRD
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], s69       // add hi GSU offset to SRD
label_GSU_2:
.set sgprGSULog2BpeC, UNDEF
.set sgprAddressC, UNDEF
.set sgprAddressD, UNDEF

/* initC: remove ValuC vgpr buffer [0...0) from pool */

/* initC: remove acc vgpr buffer [0...256) from pool */

/* initC: remove ValuA/B vgpr buffer [0...80) from pool */
v_accvgpr_write acc0, 0x0                          // initC
v_accvgpr_write acc1, 0x0                          // initC
v_accvgpr_write acc2, 0x0                          // initC
v_accvgpr_write acc3, 0x0                          // initC
v_accvgpr_write acc4, 0x0                          // initC
v_accvgpr_write acc5, 0x0                          // initC
v_accvgpr_write acc6, 0x0                          // initC
v_accvgpr_write acc7, 0x0                          // initC
v_accvgpr_write acc8, 0x0                          // initC
v_accvgpr_write acc9, 0x0                          // initC
v_accvgpr_write acc10, 0x0                         // initC
v_accvgpr_write acc11, 0x0                         // initC
v_accvgpr_write acc12, 0x0                         // initC
v_accvgpr_write acc13, 0x0                         // initC
v_accvgpr_write acc14, 0x0                         // initC
v_accvgpr_write acc15, 0x0                         // initC
v_accvgpr_write acc16, 0x0                         // initC
v_accvgpr_write acc17, 0x0                         // initC
v_accvgpr_write acc18, 0x0                         // initC
v_accvgpr_write acc19, 0x0                         // initC
v_accvgpr_write acc20, 0x0                         // initC
v_accvgpr_write acc21, 0x0                         // initC
v_accvgpr_write acc22, 0x0                         // initC
v_accvgpr_write acc23, 0x0                         // initC
v_accvgpr_write acc24, 0x0                         // initC
v_accvgpr_write acc25, 0x0                         // initC
v_accvgpr_write acc26, 0x0                         // initC
v_accvgpr_write acc27, 0x0                         // initC
v_accvgpr_write acc28, 0x0                         // initC
v_accvgpr_write acc29, 0x0                         // initC
v_accvgpr_write acc30, 0x0                         // initC
v_accvgpr_write acc31, 0x0                         // initC
v_accvgpr_write acc32, 0x0                         // initC
v_accvgpr_write acc33, 0x0                         // initC
v_accvgpr_write acc34, 0x0                         // initC
v_accvgpr_write acc35, 0x0                         // initC
v_accvgpr_write acc36, 0x0                         // initC
v_accvgpr_write acc37, 0x0                         // initC
v_accvgpr_write acc38, 0x0                         // initC
v_accvgpr_write acc39, 0x0                         // initC
v_accvgpr_write acc40, 0x0                         // initC
v_accvgpr_write acc41, 0x0                         // initC
v_accvgpr_write acc42, 0x0                         // initC
v_accvgpr_write acc43, 0x0                         // initC
v_accvgpr_write acc44, 0x0                         // initC
v_accvgpr_write acc45, 0x0                         // initC
v_accvgpr_write acc46, 0x0                         // initC
v_accvgpr_write acc47, 0x0                         // initC
v_accvgpr_write acc48, 0x0                         // initC
v_accvgpr_write acc49, 0x0                         // initC
v_accvgpr_write acc50, 0x0                         // initC
v_accvgpr_write acc51, 0x0                         // initC
v_accvgpr_write acc52, 0x0                         // initC
v_accvgpr_write acc53, 0x0                         // initC
v_accvgpr_write acc54, 0x0                         // initC
v_accvgpr_write acc55, 0x0                         // initC
v_accvgpr_write acc56, 0x0                         // initC
v_accvgpr_write acc57, 0x0                         // initC
v_accvgpr_write acc58, 0x0                         // initC
v_accvgpr_write acc59, 0x0                         // initC
v_accvgpr_write acc60, 0x0                         // initC
v_accvgpr_write acc61, 0x0                         // initC
v_accvgpr_write acc62, 0x0                         // initC
v_accvgpr_write acc63, 0x0                         // initC
v_accvgpr_write acc64, 0x0                         // initC
v_accvgpr_write acc65, 0x0                         // initC
v_accvgpr_write acc66, 0x0                         // initC
v_accvgpr_write acc67, 0x0                         // initC
v_accvgpr_write acc68, 0x0                         // initC
v_accvgpr_write acc69, 0x0                         // initC
v_accvgpr_write acc70, 0x0                         // initC
v_accvgpr_write acc71, 0x0                         // initC
v_accvgpr_write acc72, 0x0                         // initC
v_accvgpr_write acc73, 0x0                         // initC
v_accvgpr_write acc74, 0x0                         // initC
v_accvgpr_write acc75, 0x0                         // initC
v_accvgpr_write acc76, 0x0                         // initC
v_accvgpr_write acc77, 0x0                         // initC
v_accvgpr_write acc78, 0x0                         // initC
v_accvgpr_write acc79, 0x0                         // initC
v_accvgpr_write acc80, 0x0                         // initC
v_accvgpr_write acc81, 0x0                         // initC
v_accvgpr_write acc82, 0x0                         // initC
v_accvgpr_write acc83, 0x0                         // initC
v_accvgpr_write acc84, 0x0                         // initC
v_accvgpr_write acc85, 0x0                         // initC
v_accvgpr_write acc86, 0x0                         // initC
v_accvgpr_write acc87, 0x0                         // initC
v_accvgpr_write acc88, 0x0                         // initC
v_accvgpr_write acc89, 0x0                         // initC
v_accvgpr_write acc90, 0x0                         // initC
v_accvgpr_write acc91, 0x0                         // initC
v_accvgpr_write acc92, 0x0                         // initC
v_accvgpr_write acc93, 0x0                         // initC
v_accvgpr_write acc94, 0x0                         // initC
v_accvgpr_write acc95, 0x0                         // initC
v_accvgpr_write acc96, 0x0                         // initC
v_accvgpr_write acc97, 0x0                         // initC
v_accvgpr_write acc98, 0x0                         // initC
v_accvgpr_write acc99, 0x0                         // initC
v_accvgpr_write acc100, 0x0                        // initC
v_accvgpr_write acc101, 0x0                        // initC
v_accvgpr_write acc102, 0x0                        // initC
v_accvgpr_write acc103, 0x0                        // initC
v_accvgpr_write acc104, 0x0                        // initC
v_accvgpr_write acc105, 0x0                        // initC
v_accvgpr_write acc106, 0x0                        // initC
v_accvgpr_write acc107, 0x0                        // initC
v_accvgpr_write acc108, 0x0                        // initC
v_accvgpr_write acc109, 0x0                        // initC
v_accvgpr_write acc110, 0x0                        // initC
v_accvgpr_write acc111, 0x0                        // initC
v_accvgpr_write acc112, 0x0                        // initC
v_accvgpr_write acc113, 0x0                        // initC
v_accvgpr_write acc114, 0x0                        // initC
v_accvgpr_write acc115, 0x0                        // initC
v_accvgpr_write acc116, 0x0                        // initC
v_accvgpr_write acc117, 0x0                        // initC
v_accvgpr_write acc118, 0x0                        // initC
v_accvgpr_write acc119, 0x0                        // initC
v_accvgpr_write acc120, 0x0                        // initC
v_accvgpr_write acc121, 0x0                        // initC
v_accvgpr_write acc122, 0x0                        // initC
v_accvgpr_write acc123, 0x0                        // initC
v_accvgpr_write acc124, 0x0                        // initC
v_accvgpr_write acc125, 0x0                        // initC
v_accvgpr_write acc126, 0x0                        // initC
v_accvgpr_write acc127, 0x0                        // initC
v_accvgpr_write acc128, 0x0                        // initC
v_accvgpr_write acc129, 0x0                        // initC
v_accvgpr_write acc130, 0x0                        // initC
v_accvgpr_write acc131, 0x0                        // initC
v_accvgpr_write acc132, 0x0                        // initC
v_accvgpr_write acc133, 0x0                        // initC
v_accvgpr_write acc134, 0x0                        // initC
v_accvgpr_write acc135, 0x0                        // initC
v_accvgpr_write acc136, 0x0                        // initC
v_accvgpr_write acc137, 0x0                        // initC
v_accvgpr_write acc138, 0x0                        // initC
v_accvgpr_write acc139, 0x0                        // initC
v_accvgpr_write acc140, 0x0                        // initC
v_accvgpr_write acc141, 0x0                        // initC
v_accvgpr_write acc142, 0x0                        // initC
v_accvgpr_write acc143, 0x0                        // initC
v_accvgpr_write acc144, 0x0                        // initC
v_accvgpr_write acc145, 0x0                        // initC
v_accvgpr_write acc146, 0x0                        // initC
v_accvgpr_write acc147, 0x0                        // initC
v_accvgpr_write acc148, 0x0                        // initC
v_accvgpr_write acc149, 0x0                        // initC
v_accvgpr_write acc150, 0x0                        // initC
v_accvgpr_write acc151, 0x0                        // initC
v_accvgpr_write acc152, 0x0                        // initC
v_accvgpr_write acc153, 0x0                        // initC
v_accvgpr_write acc154, 0x0                        // initC
v_accvgpr_write acc155, 0x0                        // initC
v_accvgpr_write acc156, 0x0                        // initC
v_accvgpr_write acc157, 0x0                        // initC
v_accvgpr_write acc158, 0x0                        // initC
v_accvgpr_write acc159, 0x0                        // initC
v_accvgpr_write acc160, 0x0                        // initC
v_accvgpr_write acc161, 0x0                        // initC
v_accvgpr_write acc162, 0x0                        // initC
v_accvgpr_write acc163, 0x0                        // initC
v_accvgpr_write acc164, 0x0                        // initC
v_accvgpr_write acc165, 0x0                        // initC
v_accvgpr_write acc166, 0x0                        // initC
v_accvgpr_write acc167, 0x0                        // initC
v_accvgpr_write acc168, 0x0                        // initC
v_accvgpr_write acc169, 0x0                        // initC
v_accvgpr_write acc170, 0x0                        // initC
v_accvgpr_write acc171, 0x0                        // initC
v_accvgpr_write acc172, 0x0                        // initC
v_accvgpr_write acc173, 0x0                        // initC
v_accvgpr_write acc174, 0x0                        // initC
v_accvgpr_write acc175, 0x0                        // initC
v_accvgpr_write acc176, 0x0                        // initC
v_accvgpr_write acc177, 0x0                        // initC
v_accvgpr_write acc178, 0x0                        // initC
v_accvgpr_write acc179, 0x0                        // initC
v_accvgpr_write acc180, 0x0                        // initC
v_accvgpr_write acc181, 0x0                        // initC
v_accvgpr_write acc182, 0x0                        // initC
v_accvgpr_write acc183, 0x0                        // initC
v_accvgpr_write acc184, 0x0                        // initC
v_accvgpr_write acc185, 0x0                        // initC
v_accvgpr_write acc186, 0x0                        // initC
v_accvgpr_write acc187, 0x0                        // initC
v_accvgpr_write acc188, 0x0                        // initC
v_accvgpr_write acc189, 0x0                        // initC
v_accvgpr_write acc190, 0x0                        // initC
v_accvgpr_write acc191, 0x0                        // initC
v_accvgpr_write acc192, 0x0                        // initC
v_accvgpr_write acc193, 0x0                        // initC
v_accvgpr_write acc194, 0x0                        // initC
v_accvgpr_write acc195, 0x0                        // initC
v_accvgpr_write acc196, 0x0                        // initC
v_accvgpr_write acc197, 0x0                        // initC
v_accvgpr_write acc198, 0x0                        // initC
v_accvgpr_write acc199, 0x0                        // initC
v_accvgpr_write acc200, 0x0                        // initC
v_accvgpr_write acc201, 0x0                        // initC
v_accvgpr_write acc202, 0x0                        // initC
v_accvgpr_write acc203, 0x0                        // initC
v_accvgpr_write acc204, 0x0                        // initC
v_accvgpr_write acc205, 0x0                        // initC
v_accvgpr_write acc206, 0x0                        // initC
v_accvgpr_write acc207, 0x0                        // initC
v_accvgpr_write acc208, 0x0                        // initC
v_accvgpr_write acc209, 0x0                        // initC
v_accvgpr_write acc210, 0x0                        // initC
v_accvgpr_write acc211, 0x0                        // initC
v_accvgpr_write acc212, 0x0                        // initC
v_accvgpr_write acc213, 0x0                        // initC
v_accvgpr_write acc214, 0x0                        // initC
v_accvgpr_write acc215, 0x0                        // initC
v_accvgpr_write acc216, 0x0                        // initC
v_accvgpr_write acc217, 0x0                        // initC
v_accvgpr_write acc218, 0x0                        // initC
v_accvgpr_write acc219, 0x0                        // initC
v_accvgpr_write acc220, 0x0                        // initC
v_accvgpr_write acc221, 0x0                        // initC
v_accvgpr_write acc222, 0x0                        // initC
v_accvgpr_write acc223, 0x0                        // initC
v_accvgpr_write acc224, 0x0                        // initC
v_accvgpr_write acc225, 0x0                        // initC
v_accvgpr_write acc226, 0x0                        // initC
v_accvgpr_write acc227, 0x0                        // initC
v_accvgpr_write acc228, 0x0                        // initC
v_accvgpr_write acc229, 0x0                        // initC
v_accvgpr_write acc230, 0x0                        // initC
v_accvgpr_write acc231, 0x0                        // initC
v_accvgpr_write acc232, 0x0                        // initC
v_accvgpr_write acc233, 0x0                        // initC
v_accvgpr_write acc234, 0x0                        // initC
v_accvgpr_write acc235, 0x0                        // initC
v_accvgpr_write acc236, 0x0                        // initC
v_accvgpr_write acc237, 0x0                        // initC
v_accvgpr_write acc238, 0x0                        // initC
v_accvgpr_write acc239, 0x0                        // initC
v_accvgpr_write acc240, 0x0                        // initC
v_accvgpr_write acc241, 0x0                        // initC
v_accvgpr_write acc242, 0x0                        // initC
v_accvgpr_write acc243, 0x0                        // initC
v_accvgpr_write acc244, 0x0                        // initC
v_accvgpr_write acc245, 0x0                        // initC
v_accvgpr_write acc246, 0x0                        // initC
v_accvgpr_write acc247, 0x0                        // initC
v_accvgpr_write acc248, 0x0                        // initC
v_accvgpr_write acc249, 0x0                        // initC
v_accvgpr_write acc250, 0x0                        // initC
v_accvgpr_write acc251, 0x0                        // initC
v_accvgpr_write acc252, 0x0                        // initC
v_accvgpr_write acc253, 0x0                        // initC
v_accvgpr_write acc254, 0x0                        // initC
v_accvgpr_write acc255, 0x0                        // initC
s_cmp_eq_u32 s[sgprLoopCounterL], 0                // at last iteration?

/* after InitC, skip to end of prefetch last iter if numIter==0 */
s_cbranch_scc0 label_NoBranch_AR5WU8DTY906QQMR_0   // Only branch on scc1
s_getpc_b64 s[28:29]                               // addr of next instr
s_add_i32 s30, label_PrefetchGlobalLastIterEnd, 0x4 // target branch offset
s_add_u32 s28, s28, s30                            // add target branch offset
s_addc_u32 s29, s29, 0                             // add high and carry
s_setpc_b64 s[28:29]                               // branch to label_PrefetchGlobalLastIterEnd
label_NoBranch_AR5WU8DTY906QQMR_0:
s_waitcnt vmcnt(4)                                 // 8wait for global read

/* local write a */

/* local write b */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+3] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+4:vgprG2LB+4+3] offset:4352 // lwoB_0_0_1_0 = (0*LSCB)*(MT1J+PAD) + (1*LSPB) = 4352
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+8:vgprG2LB+8+3] offset:8704 // lwoB_0_0_2_0 = (0*LSCB)*(MT1J+PAD) + (2*LSPB) = 8704
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+12:vgprG2LB+12+3] offset:13056 // lwoB_0_0_3_0 = (0*LSCB)*(MT1J+PAD) + (3*LSPB) = 13056

/* local write swap a */

/* local write swap b */
v_xor_b32 v[vgprLocalWriteAddrB+0], 0x8000, v[vgprLocalWriteAddrB+0] // swap Red Blk
s_cmp_eq_u32 s[sgprLoopCounterL], 0x1              // PGR=2 but only 1 loop
s_cbranch_scc1 label_skipPGR2_0                    // PGR=2 but only 1 loop
buffer_load_dwordx4 v[vgprG2LB+0:vgprG2LB+0+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_0_0
buffer_load_dwordx4 v[vgprG2LB+4:vgprG2LB+4+3], v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_1_0
buffer_load_dwordx4 v[vgprG2LB+8:vgprG2LB+8+3], v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_2_0
buffer_load_dwordx4 v[vgprG2LB+12:vgprG2LB+12+3], v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_3_0
label_skipPGR2_0:

/******************************************/
/* Unrolled Loop(s) - Begin               */
/******************************************/
label_openLoopL:
s_cmp_eq_u32 s[sgprLoopCounterL], 0x1              // LoopCounterL < EndCounter
s_cbranch_scc1 label_toPGR1_0                      // PGR=2 but only 1 loop, toPGR1
s_cmp_le_u32 s[sgprLoopCounterL], 0x2              // LoopCounterL < EndCounter
s_cbranch_scc1 label_LoopEndL                      // do not enter LoopL
label_LoopBeginL:

/******************************************/
/* Unrolled Loop 1/2 - Begin              */
/******************************************/
s_waitcnt lgkmcnt(0)                               // 1wait for local write
// Skip force waitcnt0
s_barrier                                          // 4sync for global read

/* Begin Each Unroll: Check VGPR.checkin for INT8 LW */

/* iter 0 */
s_waitcnt vmcnt(4)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:18, lwStartMfmaIndex:26, lwEndMfmaIndex:127  */
/*  numMfmaForLR:30, syncPlrMfmaIndex:0  */
/*  mfmaIndex:0  */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+3], v[vgprLocalReadAddrB] offset:128 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+3], v[vgprLocalReadAddrB] offset:192 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+3], v[vgprLocalReadAddrB] offset:256 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+3], v[vgprLocalReadAddrB] offset:320 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+3], v[vgprLocalReadAddrB] offset:384 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+28:vgprValuB_X0_I0+28+3], v[vgprLocalReadAddrB] offset:448 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+32:vgprValuB_X0_I0+32+3], v[vgprLocalReadAddrB] offset:8704 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+36:vgprValuB_X0_I0+36+3], v[vgprLocalReadAddrB] offset:8768 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+40:vgprValuB_X0_I0+40+3], v[vgprLocalReadAddrB] offset:8832 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+44:vgprValuB_X0_I0+44+3], v[vgprLocalReadAddrB] offset:8896 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+48:vgprValuB_X0_I0+48+3], v[vgprLocalReadAddrB] offset:8960 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+52:vgprValuB_X0_I0+52+3], v[vgprLocalReadAddrB] offset:9024 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+56:vgprValuB_X0_I0+56+3], v[vgprLocalReadAddrB] offset:9088 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+60:vgprValuB_X0_I0+60+3], v[vgprLocalReadAddrB] offset:9152 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0 for iteration == 0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
/* localReadsVacancy: latencyLeft 2 */

/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIterDTV] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s28, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s29, s[sgprWrapUA+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
/* localReadsVacancy: latencyLeft 2 */
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s28        // gra SRD += inc(lower)
s_waitcnt lgkmcnt(15)                              // wait for prior local read local write
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
/* localReadsVacancy: latencyLeft 2 */
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s29       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
/* localReadsVacancy: latencyLeft 2 */
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s28 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
/* localReadsVacancy: latencyLeft 2 */
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s29 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
/* localReadsVacancy: latencyLeft 2 */
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
/* localReadsVacancy: latencyLeft 2 */

/* global read inc B loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIter] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s28, s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0] // incLower <- ?
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s29, s[sgprWrapUB+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
/* localReadsVacancy: latencyLeft 2 */
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s28        // gra SRD += inc(lower)
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
/* localReadsVacancy: latencyLeft 2 */
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s29       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
/* localReadsVacancy: latencyLeft 2 */
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s28 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:16  */
/* localReadsVacancy: latencyLeft 2 */
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s29 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[64:67], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:17  */
/* localReadsVacancy: latencyLeft 2 */
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[68:71], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:18  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[72:75], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[76:79], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[80:83], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:21  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[84:87], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:22  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[88:91], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:23  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[92:95], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:24  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[96:99], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:25  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[100:103], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:26  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[104:107], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:27  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LA2+0:vgprG2LA2+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_0_0
v_mfma_f32_16x16x16_f16 acc[108:111], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:28  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[112:115], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:29  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[116:119], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:30  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[120:123], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:31  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[124:127], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:32  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[128:131], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:33  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[132:135], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:34  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[136:139], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:35  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[140:143], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:36  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[144:147], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:37  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[148:151], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:38  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[152:155], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:39  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[156:159], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:40  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[160:163], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:41  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LA2+4:vgprG2LA2+4+3], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16_f16 acc[164:167], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:42  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[168:171], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:43  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[172:175], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:44  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[176:179], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:45  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[180:183], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:46  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[184:187], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:47  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[188:191], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:48  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[192:195], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:49  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[196:199], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:50  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[200:203], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:51  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[204:207], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:52  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[208:211], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:53  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[212:215], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:54  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[216:219], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:55  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LA2+8:vgprG2LA2+8+3], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_2_0
v_mfma_f32_16x16x16_f16 acc[220:223], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
/*  mfmaIndex:56  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[224:227], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[224:227] // left value = acc[224+0:227+0]
/*  mfmaIndex:57  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[228:231], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[228:231] // left value = acc[228+0:231+0]
/*  mfmaIndex:58  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[232:235], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[232:235] // left value = acc[232+0:235+0]
/*  mfmaIndex:59  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[236:239], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[236:239] // left value = acc[236+0:239+0]
/*  mfmaIndex:60  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[240:243], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[240:243] // left value = acc[240+0:243+0]
/*  mfmaIndex:61  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[244:247], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[244:247] // left value = acc[244+0:247+0]
/*  mfmaIndex:62  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[248:251], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[248:251] // left value = acc[248+0:251+0]
/*  mfmaIndex:63  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[252:255], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[252:255] // left value = acc[252+0:255+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=16 */

/* iter 1 (reset local read pointers iteration)  (swap and reset local write pointers iteration)  (swap local read pointers iteration)  */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:26, lwEndMfmaIndex:127  */
/*  numMfmaForLR:30, syncPlrMfmaIndex:0  */
/*  mfmaIndex:64  */
/* localReadsVacancy: latencyLeft 2 */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:65  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:66  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:67  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:68  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:69  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LA2+12:vgprG2LA2+12+3], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_3_0
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:70  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:71  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:72  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:73  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:74  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:75  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:76  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:77  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:78  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:79  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:80  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[64:67], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:81  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[68:71], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:82  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[72:75], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:83  */
/* localReadsVacancy: latencyLeft 2 */
/* sched write - iter 1 writesPerItem=1 */
s_waitcnt vmcnt(7)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+3] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0
v_mfma_f32_16x16x16_f16 acc[76:79], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:84  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LB+0:vgprG2LB+0+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_0_0
v_mfma_f32_16x16x16_f16 acc[80:83], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:85  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[84:87], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:86  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[88:91], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:87  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[92:95], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:88  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[96:99], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:89  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[100:103], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:90  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[104:107], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:91  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[108:111], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:92  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[112:115], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:93  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[116:119], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:94  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[120:123], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:95  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[124:127], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:96  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[128:131], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:97  */
/* localReadsVacancy: latencyLeft 2 */
/* sched write - iter 1 writesPerItem=1 */
s_waitcnt vmcnt(7)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+4:vgprG2LB+4+3] offset:4352 // lwoB_0_0_1_0 = (0*LSCB)*(MT1J+PAD) + (1*LSPB) = 4352
v_mfma_f32_16x16x16_f16 acc[132:135], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:98  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LB+4:vgprG2LB+4+3], v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16_f16 acc[136:139], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:99  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[140:143], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:100  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[144:147], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:101  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[148:151], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:102  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[152:155], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:103  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[156:159], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:104  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[160:163], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:105  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[164:167], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:106  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[168:171], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:107  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[172:175], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:108  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[176:179], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:109  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[180:183], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:110  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[184:187], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:111  */
/* localReadsVacancy: latencyLeft 2 */
/* sched write - iter 1 writesPerItem=1 */
s_waitcnt vmcnt(7)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+8:vgprG2LB+8+3] offset:8704 // lwoB_0_0_2_0 = (0*LSCB)*(MT1J+PAD) + (2*LSPB) = 8704
v_mfma_f32_16x16x16_f16 acc[188:191], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:112  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LB+8:vgprG2LB+8+3], v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_2_0
v_mfma_f32_16x16x16_f16 acc[192:195], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:113  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[196:199], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:114  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[200:203], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:115  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[204:207], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:116  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[208:211], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:117  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[212:215], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:118  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[216:219], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:119  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[220:223], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
/*  mfmaIndex:120  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[224:227], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[224:227] // left value = acc[224+0:227+0]
/*  mfmaIndex:121  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[228:231], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[228:231] // left value = acc[228+0:231+0]
/*  mfmaIndex:122  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[232:235], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[232:235] // left value = acc[232+0:235+0]
/*  mfmaIndex:123  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[236:239], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[236:239] // left value = acc[236+0:239+0]
/*  mfmaIndex:124  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[240:243], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[240:243] // left value = acc[240+0:243+0]
/*  mfmaIndex:125  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[244:247], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[244:247] // left value = acc[244+0:247+0]
/*  mfmaIndex:126  */
/* localReadsVacancy: latencyLeft 2 */
/* sched write - iter 1 writesPerItem=1 */
s_waitcnt vmcnt(7)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+12:vgprG2LB+12+3] offset:13056 // lwoB_0_0_3_0 = (0*LSCB)*(MT1J+PAD) + (3*LSPB) = 13056
v_mfma_f32_16x16x16_f16 acc[248:251], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[248:251] // left value = acc[248+0:251+0]
/*  mfmaIndex:127  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LB+12:vgprG2LB+12+3], v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_3_0

/* local write swap offsets a */

/* local write swap offsets b */
v_xor_b32 v[vgprLocalWriteAddrB+0], 0x8000, v[vgprLocalWriteAddrB+0] // swap Red Blk

/* local read swap offsets a */

/* local read swap offsets b */
v_xor_b32 v[vgprLocalReadAddrB], 0x8000, v[vgprLocalReadAddrB] // swap Red Blk

/* local read init pointers a */

/* local read init pointers b */

/* localReadInitPointers */
v_mfma_f32_16x16x16_f16 acc[252:255], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[252:255] // left value = acc[252+0:255+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=16 */

/******************************************/
/* Unrolled Loop - End 1/2                */
/******************************************/
s_sub_u32 s[sgprLoopCounterL], s[sgprLoopCounterL], 1 // dec counterL
s_cmp_le_u32 s[sgprLoopCounterL], 0x2              // counteL<=2
s_cbranch_scc1 label_NoGRloopAfterABLoop           // exit LoopL

/******************************************/
/* Unrolled Loop 2/2 - Begin              */
/******************************************/
s_waitcnt lgkmcnt(0)                               // 1wait for local write
// Skip force waitcnt0
s_barrier                                          // 4sync for global read

/* Begin Each Unroll: Check VGPR.checkin for INT8 LW */

/* iter 0 */
s_waitcnt vmcnt(4)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:18, lwStartMfmaIndex:26, lwEndMfmaIndex:127  */
/*  numMfmaForLR:30, syncPlrMfmaIndex:0  */
/*  mfmaIndex:0  */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+3], v[vgprLocalReadAddrB] offset:128 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+3], v[vgprLocalReadAddrB] offset:192 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+3], v[vgprLocalReadAddrB] offset:256 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+3], v[vgprLocalReadAddrB] offset:320 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+3], v[vgprLocalReadAddrB] offset:384 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+28:vgprValuB_X0_I0+28+3], v[vgprLocalReadAddrB] offset:448 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+32:vgprValuB_X0_I0+32+3], v[vgprLocalReadAddrB] offset:8704 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+36:vgprValuB_X0_I0+36+3], v[vgprLocalReadAddrB] offset:8768 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+40:vgprValuB_X0_I0+40+3], v[vgprLocalReadAddrB] offset:8832 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+44:vgprValuB_X0_I0+44+3], v[vgprLocalReadAddrB] offset:8896 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+48:vgprValuB_X0_I0+48+3], v[vgprLocalReadAddrB] offset:8960 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+52:vgprValuB_X0_I0+52+3], v[vgprLocalReadAddrB] offset:9024 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+56:vgprValuB_X0_I0+56+3], v[vgprLocalReadAddrB] offset:9088 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+60:vgprValuB_X0_I0+60+3], v[vgprLocalReadAddrB] offset:9152 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0 for iteration == 0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
/* localReadsVacancy: latencyLeft 2 */

/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIterDTV] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s28, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s29, s[sgprWrapUA+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
/* localReadsVacancy: latencyLeft 2 */
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s28        // gra SRD += inc(lower)
s_waitcnt lgkmcnt(15)                              // wait for prior local read local write
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
/* localReadsVacancy: latencyLeft 2 */
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s29       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
/* localReadsVacancy: latencyLeft 2 */
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s28 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
/* localReadsVacancy: latencyLeft 2 */
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s29 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
/* localReadsVacancy: latencyLeft 2 */
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
/* localReadsVacancy: latencyLeft 2 */

/* global read inc B loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIter] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s28, s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0] // incLower <- ?
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s29, s[sgprWrapUB+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
/* localReadsVacancy: latencyLeft 2 */
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s28        // gra SRD += inc(lower)
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
/* localReadsVacancy: latencyLeft 2 */
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s29       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
/* localReadsVacancy: latencyLeft 2 */
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s28 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:16  */
/* localReadsVacancy: latencyLeft 2 */
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s29 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[64:67], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:17  */
/* localReadsVacancy: latencyLeft 2 */
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[68:71], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:18  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[72:75], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[76:79], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[80:83], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:21  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[84:87], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:22  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[88:91], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:23  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[92:95], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:24  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[96:99], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:25  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[100:103], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:26  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[104:107], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:27  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LA+0:vgprG2LA+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_0_0
v_mfma_f32_16x16x16_f16 acc[108:111], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:28  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[112:115], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:29  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[116:119], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:30  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[120:123], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:31  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[124:127], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:32  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[128:131], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:33  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[132:135], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:34  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[136:139], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:35  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[140:143], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:36  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[144:147], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:37  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[148:151], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:38  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[152:155], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:39  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[156:159], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:40  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[160:163], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:41  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LA+4:vgprG2LA+4+3], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16_f16 acc[164:167], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:42  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[168:171], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:43  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[172:175], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:44  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[176:179], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:45  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[180:183], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:46  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[184:187], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:47  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[188:191], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:48  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[192:195], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:49  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[196:199], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:50  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[200:203], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:51  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[204:207], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:52  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[208:211], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:53  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[212:215], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:54  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[216:219], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:55  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LA+8:vgprG2LA+8+3], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_2_0
v_mfma_f32_16x16x16_f16 acc[220:223], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
/*  mfmaIndex:56  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[224:227], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[224:227] // left value = acc[224+0:227+0]
/*  mfmaIndex:57  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[228:231], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[228:231] // left value = acc[228+0:231+0]
/*  mfmaIndex:58  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[232:235], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[232:235] // left value = acc[232+0:235+0]
/*  mfmaIndex:59  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[236:239], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[236:239] // left value = acc[236+0:239+0]
/*  mfmaIndex:60  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[240:243], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[240:243] // left value = acc[240+0:243+0]
/*  mfmaIndex:61  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[244:247], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[244:247] // left value = acc[244+0:247+0]
/*  mfmaIndex:62  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[248:251], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[248:251] // left value = acc[248+0:251+0]
/*  mfmaIndex:63  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[252:255], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[252:255] // left value = acc[252+0:255+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=16 */

/* iter 1 (reset local read pointers iteration)  (swap and reset local write pointers iteration)  (swap local read pointers iteration)  */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:26, lwEndMfmaIndex:127  */
/*  numMfmaForLR:30, syncPlrMfmaIndex:0  */
/*  mfmaIndex:64  */
/* localReadsVacancy: latencyLeft 2 */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:65  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:66  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:67  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:68  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:69  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LA+12:vgprG2LA+12+3], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_3_0
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:70  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:71  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:72  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:73  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:74  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:75  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:76  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:77  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:78  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:79  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:80  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[64:67], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:81  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[68:71], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:82  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[72:75], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:83  */
/* localReadsVacancy: latencyLeft 2 */
/* sched write - iter 1 writesPerItem=1 */
s_waitcnt vmcnt(7)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+3] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0
v_mfma_f32_16x16x16_f16 acc[76:79], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:84  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LB+0:vgprG2LB+0+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_0_0
v_mfma_f32_16x16x16_f16 acc[80:83], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:85  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[84:87], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:86  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[88:91], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:87  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[92:95], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:88  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[96:99], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:89  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[100:103], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:90  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[104:107], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:91  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[108:111], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:92  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[112:115], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:93  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[116:119], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:94  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[120:123], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:95  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[124:127], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:96  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[128:131], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:97  */
/* localReadsVacancy: latencyLeft 2 */
/* sched write - iter 1 writesPerItem=1 */
s_waitcnt vmcnt(7)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+4:vgprG2LB+4+3] offset:4352 // lwoB_0_0_1_0 = (0*LSCB)*(MT1J+PAD) + (1*LSPB) = 4352
v_mfma_f32_16x16x16_f16 acc[132:135], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:98  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LB+4:vgprG2LB+4+3], v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16_f16 acc[136:139], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:99  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[140:143], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:100  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[144:147], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:101  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[148:151], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:102  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[152:155], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:103  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[156:159], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:104  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[160:163], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:105  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[164:167], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:106  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[168:171], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:107  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[172:175], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:108  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[176:179], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:109  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[180:183], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:110  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[184:187], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:111  */
/* localReadsVacancy: latencyLeft 2 */
/* sched write - iter 1 writesPerItem=1 */
s_waitcnt vmcnt(7)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+8:vgprG2LB+8+3] offset:8704 // lwoB_0_0_2_0 = (0*LSCB)*(MT1J+PAD) + (2*LSPB) = 8704
v_mfma_f32_16x16x16_f16 acc[188:191], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:112  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LB+8:vgprG2LB+8+3], v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_2_0
v_mfma_f32_16x16x16_f16 acc[192:195], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:113  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[196:199], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:114  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[200:203], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:115  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[204:207], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:116  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[208:211], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:117  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[212:215], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:118  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[216:219], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:119  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[220:223], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
/*  mfmaIndex:120  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[224:227], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[224:227] // left value = acc[224+0:227+0]
/*  mfmaIndex:121  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[228:231], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[228:231] // left value = acc[228+0:231+0]
/*  mfmaIndex:122  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[232:235], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[232:235] // left value = acc[232+0:235+0]
/*  mfmaIndex:123  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[236:239], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[236:239] // left value = acc[236+0:239+0]
/*  mfmaIndex:124  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[240:243], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[240:243] // left value = acc[240+0:243+0]
/*  mfmaIndex:125  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[244:247], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[244:247] // left value = acc[244+0:247+0]
/*  mfmaIndex:126  */
/* localReadsVacancy: latencyLeft 2 */
/* sched write - iter 1 writesPerItem=1 */
s_waitcnt vmcnt(7)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+12:vgprG2LB+12+3] offset:13056 // lwoB_0_0_3_0 = (0*LSCB)*(MT1J+PAD) + (3*LSPB) = 13056
v_mfma_f32_16x16x16_f16 acc[248:251], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[248:251] // left value = acc[248+0:251+0]
/*  mfmaIndex:127  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LB+12:vgprG2LB+12+3], v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_3_0

/* local write swap offsets a */

/* local write swap offsets b */
v_xor_b32 v[vgprLocalWriteAddrB+0], 0x8000, v[vgprLocalWriteAddrB+0] // swap Red Blk

/* local read swap offsets a */

/* local read swap offsets b */
v_xor_b32 v[vgprLocalReadAddrB], 0x8000, v[vgprLocalReadAddrB] // swap Red Blk

/* local read init pointers a */

/* local read init pointers b */

/* localReadInitPointers */
v_mfma_f32_16x16x16_f16 acc[252:255], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[252:255] // left value = acc[252+0:255+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=16 */

/******************************************/
/* Unrolled Loop - End 2/2 (final)        */
/******************************************/

/* closeLoop loopL finalLoop=1 tailLoop=0 */
s_sub_u32 s[sgprLoopCounterL], s[sgprLoopCounterL], 1 // dec counterL
s_cmp_eq_i32 s[sgprLoopCounterL], 0x2              // counterL==2
s_cbranch_scc0 label_LoopBeginL                    // restart LoopL
label_LoopEndL:

/* Before NLL: Check VGPR.checkin for INT8 LW */

/******************************************/
/* Ord. NoGlobalLoadLoop - Begin 1/2      */
/******************************************/
s_waitcnt lgkmcnt(0)                               // 4wait for local write
// Skip force waitcnt0
s_barrier

/* iter 0 */
s_waitcnt vmcnt(4)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:18, lwStartMfmaIndex:26, lwEndMfmaIndex:127  */
/*  numMfmaForLR:30, syncPlrMfmaIndex:0  */
/*  mfmaIndex:0  */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+3], v[vgprLocalReadAddrB] offset:128 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+3], v[vgprLocalReadAddrB] offset:192 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+3], v[vgprLocalReadAddrB] offset:256 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+3], v[vgprLocalReadAddrB] offset:320 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+3], v[vgprLocalReadAddrB] offset:384 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+28:vgprValuB_X0_I0+28+3], v[vgprLocalReadAddrB] offset:448 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+32:vgprValuB_X0_I0+32+3], v[vgprLocalReadAddrB] offset:8704 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+36:vgprValuB_X0_I0+36+3], v[vgprLocalReadAddrB] offset:8768 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+40:vgprValuB_X0_I0+40+3], v[vgprLocalReadAddrB] offset:8832 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+44:vgprValuB_X0_I0+44+3], v[vgprLocalReadAddrB] offset:8896 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+48:vgprValuB_X0_I0+48+3], v[vgprLocalReadAddrB] offset:8960 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+52:vgprValuB_X0_I0+52+3], v[vgprLocalReadAddrB] offset:9024 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+56:vgprValuB_X0_I0+56+3], v[vgprLocalReadAddrB] offset:9088 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+60:vgprValuB_X0_I0+60+3], v[vgprLocalReadAddrB] offset:9152 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0 for iteration == 0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
/* localReadsVacancy: latencyLeft 2 */

/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIterDTV] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s28, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s29, s[sgprWrapUA+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
/* localReadsVacancy: latencyLeft 2 */
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s28        // gra SRD += inc(lower)
s_waitcnt lgkmcnt(15)                              // wait for prior local read local write
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
/* localReadsVacancy: latencyLeft 2 */
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s29       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
/* localReadsVacancy: latencyLeft 2 */
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s28 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
/* localReadsVacancy: latencyLeft 2 */
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s29 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
/* localReadsVacancy: latencyLeft 2 */
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
/* localReadsVacancy: latencyLeft 2 */

/* global read inc B loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIter] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s28, s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0] // incLower <- ?
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s29, s[sgprWrapUB+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
/* localReadsVacancy: latencyLeft 2 */
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s28        // gra SRD += inc(lower)
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
/* localReadsVacancy: latencyLeft 2 */
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s29       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
/* localReadsVacancy: latencyLeft 2 */
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s28 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:16  */
/* localReadsVacancy: latencyLeft 2 */
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s29 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[64:67], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:17  */
/* localReadsVacancy: latencyLeft 2 */
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[68:71], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:18  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[72:75], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[76:79], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[80:83], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:21  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[84:87], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:22  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[88:91], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:23  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[92:95], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:24  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[96:99], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:25  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[100:103], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:26  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[104:107], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:27  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LA2+0:vgprG2LA2+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_0_0
v_mfma_f32_16x16x16_f16 acc[108:111], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:28  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[112:115], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:29  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[116:119], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:30  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[120:123], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:31  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[124:127], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:32  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[128:131], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:33  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[132:135], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:34  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[136:139], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:35  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[140:143], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:36  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[144:147], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:37  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[148:151], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:38  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[152:155], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:39  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[156:159], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:40  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[160:163], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:41  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LA2+4:vgprG2LA2+4+3], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16_f16 acc[164:167], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:42  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[168:171], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:43  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[172:175], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:44  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[176:179], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:45  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[180:183], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:46  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[184:187], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:47  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[188:191], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:48  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[192:195], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:49  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[196:199], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:50  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[200:203], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:51  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[204:207], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:52  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[208:211], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:53  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[212:215], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:54  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[216:219], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:55  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LA2+8:vgprG2LA2+8+3], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_2_0
v_mfma_f32_16x16x16_f16 acc[220:223], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
/*  mfmaIndex:56  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[224:227], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[224:227] // left value = acc[224+0:227+0]
/*  mfmaIndex:57  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[228:231], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[228:231] // left value = acc[228+0:231+0]
/*  mfmaIndex:58  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[232:235], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[232:235] // left value = acc[232+0:235+0]
/*  mfmaIndex:59  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[236:239], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[236:239] // left value = acc[236+0:239+0]
/*  mfmaIndex:60  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[240:243], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[240:243] // left value = acc[240+0:243+0]
/*  mfmaIndex:61  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[244:247], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[244:247] // left value = acc[244+0:247+0]
/*  mfmaIndex:62  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[248:251], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[248:251] // left value = acc[248+0:251+0]
/*  mfmaIndex:63  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[252:255], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[252:255] // left value = acc[252+0:255+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=16 */

/* iter 1 (reset local read pointers iteration)  (swap and reset local write pointers iteration)  (swap local read pointers iteration)  */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:26, lwEndMfmaIndex:127  */
/*  numMfmaForLR:30, syncPlrMfmaIndex:0  */
/*  mfmaIndex:64  */
/* localReadsVacancy: latencyLeft 2 */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:65  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:66  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:67  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:68  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:69  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LA2+12:vgprG2LA2+12+3], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_3_0
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:70  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:71  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:72  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:73  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:74  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:75  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:76  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:77  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:78  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:79  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:80  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[64:67], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:81  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[68:71], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:82  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[72:75], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:83  */
/* localReadsVacancy: latencyLeft 2 */
/* sched write - iter 1 writesPerItem=1 */
s_waitcnt vmcnt(7)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+3] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0
v_mfma_f32_16x16x16_f16 acc[76:79], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:84  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[80:83], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:85  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[84:87], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:86  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[88:91], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:87  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[92:95], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:88  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[96:99], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:89  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[100:103], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:90  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[104:107], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:91  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[108:111], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:92  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[112:115], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:93  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[116:119], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:94  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[120:123], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:95  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[124:127], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:96  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[128:131], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:97  */
/* localReadsVacancy: latencyLeft 2 */
/* sched write - iter 1 writesPerItem=1 */
s_waitcnt vmcnt(6)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+4:vgprG2LB+4+3] offset:4352 // lwoB_0_0_1_0 = (0*LSCB)*(MT1J+PAD) + (1*LSPB) = 4352
v_mfma_f32_16x16x16_f16 acc[132:135], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:98  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[136:139], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:99  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[140:143], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:100  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[144:147], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:101  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[148:151], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:102  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[152:155], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:103  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[156:159], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:104  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[160:163], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:105  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[164:167], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:106  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[168:171], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:107  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[172:175], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:108  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[176:179], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:109  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[180:183], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:110  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[184:187], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:111  */
/* localReadsVacancy: latencyLeft 2 */
/* sched write - iter 1 writesPerItem=1 */
s_waitcnt vmcnt(5)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+8:vgprG2LB+8+3] offset:8704 // lwoB_0_0_2_0 = (0*LSCB)*(MT1J+PAD) + (2*LSPB) = 8704
v_mfma_f32_16x16x16_f16 acc[188:191], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:112  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[192:195], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:113  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[196:199], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:114  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[200:203], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:115  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[204:207], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:116  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[208:211], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:117  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[212:215], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:118  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[216:219], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:119  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[220:223], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
/*  mfmaIndex:120  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[224:227], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[224:227] // left value = acc[224+0:227+0]
/*  mfmaIndex:121  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[228:231], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[228:231] // left value = acc[228+0:231+0]
/*  mfmaIndex:122  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[232:235], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[232:235] // left value = acc[232+0:235+0]
/*  mfmaIndex:123  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[236:239], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[236:239] // left value = acc[236+0:239+0]
/*  mfmaIndex:124  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[240:243], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[240:243] // left value = acc[240+0:243+0]
/*  mfmaIndex:125  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[244:247], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[244:247] // left value = acc[244+0:247+0]
/*  mfmaIndex:126  */
/* localReadsVacancy: latencyLeft 2 */
/* sched write - iter 1 writesPerItem=1 */
s_waitcnt vmcnt(4)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+12:vgprG2LB+12+3] offset:13056 // lwoB_0_0_3_0 = (0*LSCB)*(MT1J+PAD) + (3*LSPB) = 13056
v_mfma_f32_16x16x16_f16 acc[248:251], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[248:251] // left value = acc[248+0:251+0]
/*  mfmaIndex:127  */
/* localReadsVacancy: latencyLeft 2 */

/* local write swap offsets a */

/* local write swap offsets b */
v_xor_b32 v[vgprLocalWriteAddrB+0], 0x8000, v[vgprLocalWriteAddrB+0] // swap Red Blk

/* local read swap offsets a */

/* local read swap offsets b */
v_xor_b32 v[vgprLocalReadAddrB], 0x8000, v[vgprLocalReadAddrB] // swap Red Blk

/* local read init pointers a */

/* local read init pointers b */

/* localReadInitPointers */
v_mfma_f32_16x16x16_f16 acc[252:255], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[252:255] // left value = acc[252+0:255+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=16 */
s_sub_u32 s[sgprLoopCounterL], s[sgprLoopCounterL], 1 // dec counterL
s_branch label_toPGR1_0                            // Branch to toPGR1
label_NoGRloopAfterABLoop:

/******************************************/
/* Ord. NoGlobalLoadLoop - Begin 2/2      */
/******************************************/
s_waitcnt lgkmcnt(0)                               // 4wait for local write
// Skip force waitcnt0
s_barrier

/* iter 0 */
s_waitcnt vmcnt(4)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:18, lwStartMfmaIndex:26, lwEndMfmaIndex:127  */
/*  numMfmaForLR:30, syncPlrMfmaIndex:0  */
/*  mfmaIndex:0  */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+3], v[vgprLocalReadAddrB] offset:128 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+3], v[vgprLocalReadAddrB] offset:192 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+3], v[vgprLocalReadAddrB] offset:256 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+3], v[vgprLocalReadAddrB] offset:320 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+3], v[vgprLocalReadAddrB] offset:384 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+28:vgprValuB_X0_I0+28+3], v[vgprLocalReadAddrB] offset:448 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+32:vgprValuB_X0_I0+32+3], v[vgprLocalReadAddrB] offset:8704 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+36:vgprValuB_X0_I0+36+3], v[vgprLocalReadAddrB] offset:8768 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+40:vgprValuB_X0_I0+40+3], v[vgprLocalReadAddrB] offset:8832 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+44:vgprValuB_X0_I0+44+3], v[vgprLocalReadAddrB] offset:8896 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+48:vgprValuB_X0_I0+48+3], v[vgprLocalReadAddrB] offset:8960 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+52:vgprValuB_X0_I0+52+3], v[vgprLocalReadAddrB] offset:9024 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+56:vgprValuB_X0_I0+56+3], v[vgprLocalReadAddrB] offset:9088 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+60:vgprValuB_X0_I0+60+3], v[vgprLocalReadAddrB] offset:9152 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0 for iteration == 0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
/* localReadsVacancy: latencyLeft 2 */

/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIterDTV] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s28, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s29, s[sgprWrapUA+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
/* localReadsVacancy: latencyLeft 2 */
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s28        // gra SRD += inc(lower)
s_waitcnt lgkmcnt(15)                              // wait for prior local read local write
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
/* localReadsVacancy: latencyLeft 2 */
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s29       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
/* localReadsVacancy: latencyLeft 2 */
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s28 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
/* localReadsVacancy: latencyLeft 2 */
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s29 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
/* localReadsVacancy: latencyLeft 2 */
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
/* localReadsVacancy: latencyLeft 2 */

/* global read inc B loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIter] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s28, s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0] // incLower <- ?
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s29, s[sgprWrapUB+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
/* localReadsVacancy: latencyLeft 2 */
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s28        // gra SRD += inc(lower)
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
/* localReadsVacancy: latencyLeft 2 */
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s29       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
/* localReadsVacancy: latencyLeft 2 */
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s28 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:16  */
/* localReadsVacancy: latencyLeft 2 */
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s29 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[64:67], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:17  */
/* localReadsVacancy: latencyLeft 2 */
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[68:71], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:18  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[72:75], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[76:79], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[80:83], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:21  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[84:87], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:22  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[88:91], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:23  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[92:95], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:24  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[96:99], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:25  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[100:103], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:26  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[104:107], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:27  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LA+0:vgprG2LA+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_0_0
v_mfma_f32_16x16x16_f16 acc[108:111], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:28  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[112:115], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:29  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[116:119], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:30  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[120:123], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:31  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[124:127], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:32  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[128:131], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:33  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[132:135], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:34  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[136:139], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:35  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[140:143], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:36  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[144:147], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:37  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[148:151], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:38  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[152:155], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:39  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[156:159], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:40  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[160:163], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:41  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LA+4:vgprG2LA+4+3], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16_f16 acc[164:167], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:42  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[168:171], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:43  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[172:175], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:44  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[176:179], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:45  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[180:183], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:46  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[184:187], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:47  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[188:191], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:48  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[192:195], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:49  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[196:199], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:50  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[200:203], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:51  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[204:207], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:52  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[208:211], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:53  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[212:215], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:54  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[216:219], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:55  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LA+8:vgprG2LA+8+3], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_2_0
v_mfma_f32_16x16x16_f16 acc[220:223], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
/*  mfmaIndex:56  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[224:227], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[224:227] // left value = acc[224+0:227+0]
/*  mfmaIndex:57  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[228:231], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[228:231] // left value = acc[228+0:231+0]
/*  mfmaIndex:58  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[232:235], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[232:235] // left value = acc[232+0:235+0]
/*  mfmaIndex:59  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[236:239], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[236:239] // left value = acc[236+0:239+0]
/*  mfmaIndex:60  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[240:243], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[240:243] // left value = acc[240+0:243+0]
/*  mfmaIndex:61  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[244:247], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[244:247] // left value = acc[244+0:247+0]
/*  mfmaIndex:62  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[248:251], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[248:251] // left value = acc[248+0:251+0]
/*  mfmaIndex:63  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[252:255], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[252:255] // left value = acc[252+0:255+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=16 */

/* iter 1 (reset local read pointers iteration)  (swap and reset local write pointers iteration)  (swap local read pointers iteration)  */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:26, lwEndMfmaIndex:127  */
/*  numMfmaForLR:30, syncPlrMfmaIndex:0  */
/*  mfmaIndex:64  */
/* localReadsVacancy: latencyLeft 2 */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:65  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:66  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:67  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:68  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:69  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprG2LA+12:vgprG2LA+12+3], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_3_0
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:70  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:71  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:72  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:73  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:74  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:75  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:76  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:77  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:78  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:79  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:80  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[64:67], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:81  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[68:71], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:82  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[72:75], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:83  */
/* localReadsVacancy: latencyLeft 2 */
/* sched write - iter 1 writesPerItem=1 */
s_waitcnt vmcnt(7)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+3] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0
v_mfma_f32_16x16x16_f16 acc[76:79], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:84  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[80:83], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:85  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[84:87], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:86  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[88:91], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:87  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[92:95], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:88  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[96:99], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:89  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[100:103], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:90  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[104:107], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:91  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[108:111], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:92  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[112:115], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:93  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[116:119], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:94  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[120:123], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:95  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[124:127], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:96  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[128:131], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:97  */
/* localReadsVacancy: latencyLeft 2 */
/* sched write - iter 1 writesPerItem=1 */
s_waitcnt vmcnt(6)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+4:vgprG2LB+4+3] offset:4352 // lwoB_0_0_1_0 = (0*LSCB)*(MT1J+PAD) + (1*LSPB) = 4352
v_mfma_f32_16x16x16_f16 acc[132:135], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:98  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[136:139], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:99  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[140:143], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:100  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[144:147], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:101  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[148:151], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:102  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[152:155], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:103  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[156:159], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:104  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[160:163], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:105  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[164:167], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:106  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[168:171], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:107  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[172:175], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:108  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[176:179], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:109  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[180:183], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:110  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[184:187], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:111  */
/* localReadsVacancy: latencyLeft 2 */
/* sched write - iter 1 writesPerItem=1 */
s_waitcnt vmcnt(5)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+8:vgprG2LB+8+3] offset:8704 // lwoB_0_0_2_0 = (0*LSCB)*(MT1J+PAD) + (2*LSPB) = 8704
v_mfma_f32_16x16x16_f16 acc[188:191], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:112  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[192:195], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:113  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[196:199], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:114  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[200:203], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:115  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[204:207], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:116  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[208:211], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:117  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[212:215], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:118  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[216:219], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:119  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[220:223], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
/*  mfmaIndex:120  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[224:227], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[224:227] // left value = acc[224+0:227+0]
/*  mfmaIndex:121  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[228:231], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[228:231] // left value = acc[228+0:231+0]
/*  mfmaIndex:122  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[232:235], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[232:235] // left value = acc[232+0:235+0]
/*  mfmaIndex:123  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[236:239], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[236:239] // left value = acc[236+0:239+0]
/*  mfmaIndex:124  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[240:243], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[240:243] // left value = acc[240+0:243+0]
/*  mfmaIndex:125  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[244:247], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[244:247] // left value = acc[244+0:247+0]
/*  mfmaIndex:126  */
/* localReadsVacancy: latencyLeft 2 */
/* sched write - iter 1 writesPerItem=1 */
s_waitcnt vmcnt(4)                                 // wait for global read before writing to local
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+12:vgprG2LB+12+3] offset:13056 // lwoB_0_0_3_0 = (0*LSCB)*(MT1J+PAD) + (3*LSPB) = 13056
v_mfma_f32_16x16x16_f16 acc[248:251], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[248:251] // left value = acc[248+0:251+0]
/*  mfmaIndex:127  */
/* localReadsVacancy: latencyLeft 2 */

/* local write swap offsets a */

/* local write swap offsets b */
v_xor_b32 v[vgprLocalWriteAddrB+0], 0x8000, v[vgprLocalWriteAddrB+0] // swap Red Blk

/* local read swap offsets a */

/* local read swap offsets b */
v_xor_b32 v[vgprLocalReadAddrB], 0x8000, v[vgprLocalReadAddrB] // swap Red Blk

/* local read init pointers a */

/* local read init pointers b */

/* localReadInitPointers */
v_mfma_f32_16x16x16_f16 acc[252:255], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[252:255] // left value = acc[252+0:255+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=16 */
s_sub_u32 s[sgprLoopCounterL], s[sgprLoopCounterL], 1 // dec counterL
label_toPGR1_0:
s_and_b32 s8, s[sgprGSU], 0x3fff                   // Restore GSU
s_cmp_eq_u32 s8, 1                                 // GSU == 1 ?
s_cbranch_scc0 label_GSU_3                         // branch if GSU != 1

/******************************************/
/* Opt. NoLoadLoop - Begin 1/2            */
/******************************************/
s_cmpk_eq_u32 s[sgprBeta], 0x0                     // Beta == 0
s_cbranch_scc0 label_OptNLL_End                    // Branch if Beta is not zero

s_cmp_eq_u32 s[sgprAlpha], 1.0                     // Alpha == 1.0 ?
s_cbranch_scc0 label_OptNLL_End                    // branch if alpha != 1

s_and_b32 s28, 255, s[sgprSizeI]                   // s28 = s[sgprSizeI] % 256
s_add_u32 s29, -0x1, s[sgprNumWorkGroups0]
s_cmp_ge_u32 s[sgprWorkGroup0], s29                // wg0 >= nwg0-1 ?
s_cselect_b32 s28, s28, 0                          // set rMT0
s_cmpk_gt_u32 s28, 0x0                             // rMT0 > 0
s_cbranch_scc1 label_OptNLL_End                    // jump if edges required
s_and_b32 s28, 255, s[sgprSizeJ]                   // s28 = s[sgprSizeJ] % 256
s_add_u32 s29, -0x1, s[sgprNumWorkGroups1]
s_cmp_ge_u32 s[sgprWorkGroup1], s29                // wg1 >= nwg1-1
s_cselect_b32 s28, s28, 0                          // set rMT1
s_cmpk_gt_u32 s28, 0x0                             // rMT1 > 0
s_cbranch_scc1 label_OptNLL_End                    // jump if edges required

s_and_b32 s29, 31, s[sgprSizesSum+0]               // s29 = s[sgprSizesSum+0] % 32
s_cmp_eq_u32 s29, 0x0                              // numIterL == 0
s_cbranch_scc0 label_OptNLL_End                    // skip if tail loop required
s_bitcmp1_b32 s[sgprOrigLoopCounter], 0x0          // test if OrigLoopCounter is Odd ?
s_cbranch_scc1 label_OptNLL_second                 // jump to second NoLoadLoop
s_waitcnt lgkmcnt(0)                               // 4wait for local write
// Skip force waitcnt0
s_barrier

/* iter 0 (last unrolled loop) */
s_waitcnt vmcnt(0)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:9, lwStartMfmaIndex:63, lwEndMfmaIndex:63  */
/*  numMfmaForLR:30, syncPlrMfmaIndex:0  */
/*  mfmaIndex:0  */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+3], v[vgprLocalReadAddrB] offset:128 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+3], v[vgprLocalReadAddrB] offset:192 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+3], v[vgprLocalReadAddrB] offset:256 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+3], v[vgprLocalReadAddrB] offset:320 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+3], v[vgprLocalReadAddrB] offset:384 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+28:vgprValuB_X0_I0+28+3], v[vgprLocalReadAddrB] offset:448 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+32:vgprValuB_X0_I0+32+3], v[vgprLocalReadAddrB] offset:8704 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+36:vgprValuB_X0_I0+36+3], v[vgprLocalReadAddrB] offset:8768 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+40:vgprValuB_X0_I0+40+3], v[vgprLocalReadAddrB] offset:8832 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+44:vgprValuB_X0_I0+44+3], v[vgprLocalReadAddrB] offset:8896 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+48:vgprValuB_X0_I0+48+3], v[vgprLocalReadAddrB] offset:8960 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+52:vgprValuB_X0_I0+52+3], v[vgprLocalReadAddrB] offset:9024 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+56:vgprValuB_X0_I0+56+3], v[vgprLocalReadAddrB] offset:9088 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+60:vgprValuB_X0_I0+60+3], v[vgprLocalReadAddrB] offset:9152 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0 for iteration == 0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
/* localReadsVacancy: latencyLeft 2 */

/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIterDTV] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s28, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s29, s[sgprWrapUA+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
/* localReadsVacancy: latencyLeft 2 */
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s28        // gra SRD += inc(lower)
s_waitcnt lgkmcnt(15)                              // wait for prior local read local write
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
/* localReadsVacancy: latencyLeft 2 */
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s29       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
/* localReadsVacancy: latencyLeft 2 */
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s28 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
/* localReadsVacancy: latencyLeft 2 */
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s29 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
/* localReadsVacancy: latencyLeft 2 */
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:16  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[64:67], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:17  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[68:71], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:18  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[72:75], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[76:79], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[80:83], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:21  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[84:87], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:22  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[88:91], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:23  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[92:95], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:24  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[96:99], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:25  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[100:103], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:26  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[104:107], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:27  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[108:111], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:28  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[112:115], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:29  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[116:119], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:30  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[120:123], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:31  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[124:127], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:32  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[128:131], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:33  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[132:135], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:34  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[136:139], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:35  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[140:143], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:36  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[144:147], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:37  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[148:151], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:38  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[152:155], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:39  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[156:159], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:40  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[160:163], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:41  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[164:167], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:42  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[168:171], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:43  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[172:175], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:44  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[176:179], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:45  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[180:183], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:46  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[184:187], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:47  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[188:191], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:48  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[192:195], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:49  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[196:199], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:50  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[200:203], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:51  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[204:207], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:52  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[208:211], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:53  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[212:215], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:54  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[216:219], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:55  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[220:223], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
/*  mfmaIndex:56  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[224:227], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[224:227] // left value = acc[224+0:227+0]
/*  mfmaIndex:57  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[228:231], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[228:231] // left value = acc[228+0:231+0]
/*  mfmaIndex:58  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[232:235], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[232:235] // left value = acc[232+0:235+0]
/*  mfmaIndex:59  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[236:239], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[236:239] // left value = acc[236+0:239+0]
/*  mfmaIndex:60  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[240:243], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[240:243] // left value = acc[240+0:243+0]
/*  mfmaIndex:61  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[244:247], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[244:247] // left value = acc[244+0:247+0]
/*  mfmaIndex:62  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[248:251], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[248:251] // left value = acc[248+0:251+0]
/*  mfmaIndex:63  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[252:255], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[252:255] // left value = acc[252+0:255+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=16 */

/* iter 1 (last unrolled loop) */
/*  grEndMfmaIndex:9, lwStartMfmaIndex:63, lwEndMfmaIndex:63  */
/*  numMfmaForLR:30, syncPlrMfmaIndex:0  */
/*  mfmaIndex:64  */
/* localReadsVacancy: latencyLeft 2 */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:65  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:66  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:67  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:68  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:69  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:70  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:71  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:72  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:73  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:74  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:75  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:76  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:77  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:78  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:79  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:80  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[64:67], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:81  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[68:71], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:82  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[72:75], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:83  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[76:79], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:84  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[80:83], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:85  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[84:87], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:86  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[88:91], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:87  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[92:95], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:88  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[96:99], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:89  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[100:103], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:90  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[104:107], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:91  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[108:111], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:92  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[112:115], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:93  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[116:119], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:94  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[120:123], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:95  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[124:127], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:96  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[128:131], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:97  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[132:135], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:98  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[136:139], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:99  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[140:143], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:100  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[144:147], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:101  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[148:151], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:102  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[152:155], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:103  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[156:159], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:104  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[160:163], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:105  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[164:167], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:106  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[168:171], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:107  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[172:175], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:108  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[176:179], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:109  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[180:183], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:110  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[184:187], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:111  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[188:191], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:112  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[192:195], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:113  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[196:199], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:114  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[200:203], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:115  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[204:207], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:116  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[208:211], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:117  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[212:215], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:118  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[216:219], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:119  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[220:223], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
/*  mfmaIndex:120  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[224:227], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[224:227] // left value = acc[224+0:227+0]
/*  mfmaIndex:121  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[228:231], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[228:231] // left value = acc[228+0:231+0]
/*  mfmaIndex:122  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[232:235], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[232:235] // left value = acc[232+0:235+0]
/*  mfmaIndex:123  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[236:239], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[236:239] // left value = acc[236+0:239+0]
/*  mfmaIndex:124  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[240:243], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[240:243] // left value = acc[240+0:243+0]
/*  mfmaIndex:125  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[244:247], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[244:247] // left value = acc[244+0:247+0]
/*  mfmaIndex:126  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[248:251], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[248:251] // left value = acc[248+0:251+0]
/*  mfmaIndex:127  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[252:255], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[252:255] // left value = acc[252+0:255+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=16 */
s_branch label_toPGR1end_OptNLL_0                  // Branch to toPGR1end

/******************************************/
/* Opt. NoLoadLoop - Begin 2/2            */
/******************************************/
label_OptNLL_second:  /// second Opt NoLoadLoop entry
s_waitcnt lgkmcnt(0)                               // 4wait for local write
// Skip force waitcnt0
s_barrier

/* iter 0 (last unrolled loop) */
s_waitcnt vmcnt(0)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:9, lwStartMfmaIndex:63, lwEndMfmaIndex:63  */
/*  numMfmaForLR:30, syncPlrMfmaIndex:0  */
/*  mfmaIndex:0  */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+3], v[vgprLocalReadAddrB] offset:128 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+3], v[vgprLocalReadAddrB] offset:192 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+3], v[vgprLocalReadAddrB] offset:256 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+3], v[vgprLocalReadAddrB] offset:320 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+3], v[vgprLocalReadAddrB] offset:384 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+28:vgprValuB_X0_I0+28+3], v[vgprLocalReadAddrB] offset:448 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+32:vgprValuB_X0_I0+32+3], v[vgprLocalReadAddrB] offset:8704 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+36:vgprValuB_X0_I0+36+3], v[vgprLocalReadAddrB] offset:8768 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+40:vgprValuB_X0_I0+40+3], v[vgprLocalReadAddrB] offset:8832 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+44:vgprValuB_X0_I0+44+3], v[vgprLocalReadAddrB] offset:8896 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+48:vgprValuB_X0_I0+48+3], v[vgprLocalReadAddrB] offset:8960 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+52:vgprValuB_X0_I0+52+3], v[vgprLocalReadAddrB] offset:9024 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+56:vgprValuB_X0_I0+56+3], v[vgprLocalReadAddrB] offset:9088 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+60:vgprValuB_X0_I0+60+3], v[vgprLocalReadAddrB] offset:9152 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0 for iteration == 0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
/* localReadsVacancy: latencyLeft 2 */

/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIterDTV] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s28, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s29, s[sgprWrapUA+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
/* localReadsVacancy: latencyLeft 2 */
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s28        // gra SRD += inc(lower)
s_waitcnt lgkmcnt(15)                              // wait for prior local read local write
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
/* localReadsVacancy: latencyLeft 2 */
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s29       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
/* localReadsVacancy: latencyLeft 2 */
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s28 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
/* localReadsVacancy: latencyLeft 2 */
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s29 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
/* localReadsVacancy: latencyLeft 2 */
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:16  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[64:67], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:17  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[68:71], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:18  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[72:75], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[76:79], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[80:83], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:21  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[84:87], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:22  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[88:91], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:23  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[92:95], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:24  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[96:99], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:25  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[100:103], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:26  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[104:107], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:27  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[108:111], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:28  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[112:115], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:29  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[116:119], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:30  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[120:123], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:31  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[124:127], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:32  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[128:131], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:33  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[132:135], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:34  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[136:139], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:35  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[140:143], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:36  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[144:147], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:37  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[148:151], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:38  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[152:155], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:39  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[156:159], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:40  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[160:163], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:41  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[164:167], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:42  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[168:171], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:43  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[172:175], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:44  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[176:179], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:45  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[180:183], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:46  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[184:187], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:47  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[188:191], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:48  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[192:195], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:49  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[196:199], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:50  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[200:203], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:51  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[204:207], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:52  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[208:211], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:53  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[212:215], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:54  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[216:219], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:55  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[220:223], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
/*  mfmaIndex:56  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[224:227], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[224:227] // left value = acc[224+0:227+0]
/*  mfmaIndex:57  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[228:231], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[228:231] // left value = acc[228+0:231+0]
/*  mfmaIndex:58  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[232:235], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[232:235] // left value = acc[232+0:235+0]
/*  mfmaIndex:59  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[236:239], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[236:239] // left value = acc[236+0:239+0]
/*  mfmaIndex:60  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[240:243], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[240:243] // left value = acc[240+0:243+0]
/*  mfmaIndex:61  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[244:247], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[244:247] // left value = acc[244+0:247+0]
/*  mfmaIndex:62  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[248:251], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[248:251] // left value = acc[248+0:251+0]
/*  mfmaIndex:63  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[252:255], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[252:255] // left value = acc[252+0:255+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=16 */

/* iter 1 (last unrolled loop) */
/*  grEndMfmaIndex:9, lwStartMfmaIndex:63, lwEndMfmaIndex:63  */
/*  numMfmaForLR:30, syncPlrMfmaIndex:0  */
/*  mfmaIndex:64  */
/* localReadsVacancy: latencyLeft 2 */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:65  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:66  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:67  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:68  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:69  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:70  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:71  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:72  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:73  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:74  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:75  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:76  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:77  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:78  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:79  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:80  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[64:67], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:81  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[68:71], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:82  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[72:75], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:83  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[76:79], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:84  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[80:83], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:85  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[84:87], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:86  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[88:91], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:87  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[92:95], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:88  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[96:99], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:89  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[100:103], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:90  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[104:107], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:91  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[108:111], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:92  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[112:115], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:93  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[116:119], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:94  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[120:123], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:95  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[124:127], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:96  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[128:131], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:97  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[132:135], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:98  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[136:139], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:99  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[140:143], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:100  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[144:147], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:101  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[148:151], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:102  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[152:155], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:103  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[156:159], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:104  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[160:163], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:105  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[164:167], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:106  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[168:171], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:107  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[172:175], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:108  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[176:179], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:109  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[180:183], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:110  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[184:187], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:111  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[188:191], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:112  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[192:195], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:113  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[196:199], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:114  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[200:203], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:115  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[204:207], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:116  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[208:211], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:117  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[212:215], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:118  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[216:219], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:119  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[220:223], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
/*  mfmaIndex:120  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[224:227], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[224:227] // left value = acc[224+0:227+0]
/*  mfmaIndex:121  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[228:231], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[228:231] // left value = acc[228+0:231+0]
/*  mfmaIndex:122  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[232:235], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[232:235] // left value = acc[232+0:235+0]
/*  mfmaIndex:123  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[236:239], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[236:239] // left value = acc[236+0:239+0]
/*  mfmaIndex:124  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[240:243], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[240:243] // left value = acc[240+0:243+0]
/*  mfmaIndex:125  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[244:247], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[244:247] // left value = acc[244+0:247+0]
/*  mfmaIndex:126  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[248:251], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[248:251] // left value = acc[248+0:251+0]
/*  mfmaIndex:127  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[252:255], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[252:255] // left value = acc[252+0:255+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=16 */
label_toPGR1end_OptNLL_0:
/* Stores for OptNLL */
label_Summation_End_OptNLL:
/* endSummation: add vgpr [0...139) to pool */
/* load store sgprs */
.set sgprAddressScaleAlphaVec, 28
.set sgprAddressBias, 30
.set sgprBiasType, 32
.set sgprBiasStride, 33
/* Check if custom structure pointer is null */
s_cmp_eq_u32 s[sgprArgType], 2                     // ArgType == 2 ?
s_cbranch_scc1 label_LoadExternalEpilogueStruct    // branch if ArgType == 2
s_load_dwordx4 s[28:31], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x58
s_load_dwordx2 s[32:33], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x68
s_branch label_LoadExternalEpilogueStructEnd
label_LoadExternalEpilogueStruct:
s_load_dwordx4 s[28:31], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x90
s_load_dwordx2 s[32:33], s[sgprKernArgAddress:sgprKernArgAddress+1], 0xa0
label_LoadExternalEpilogueStructEnd:
.set sgprSrdScaleAlphaVec, 40
.set sgprSrdBias, 48

/* Mapping of Acc register -> C Vgpr register */
/* computeStoreVgprs */
v_lshrrev_b32 v4, 6, v[vgprSerial]                 // v4 = v[vgprSerial] / 64
v_lshrrev_b32 v5, 2, v4                            // v5 = v4 / 4
v_mul_lo_u32 v1, 0x10, v5                          // wave coordination offset 1
v_and_b32 v5, 15, v[vgprSerial]                    // v5 = v[vgprSerial] % 16
v_add_lshl_u32 v1, v5, v1, 3                       // coordination 1 = vwB *(wave_id1 + tid1)
v_mul_lo_u32 v2, v1, s[sgprStrideC1J]              //  offset 1
v_mul_lo_u32 v3, v1, s[sgprStrideD1J]              //  offset 1
v_and_b32 v5, 3, v4                                // v5 = v4 % 4
v_mul_lo_u32 v5, 0x10, v5                          // wave coordination offset 0
v_and_b32 v0, 63, v[vgprSerial]                    // v0 = v[vgprSerial] % 64
v_lshrrev_b32 v0, 4, v0                            // v0 = v0 / 16
v_lshlrev_b32 v0, 0x2, v0                          // thread0 * continuous_output
v_add_lshl_u32 v0, v5, v0, 0                       // coordination 0 = vwA *(wave_id0 + tid0)
s_mul_i32 s8, 256, s[sgprWorkGroup0]               // wgp0 * MT0
v_add_u32 v0, s8, v0                               // coord 0 = (tid0/MI_m)*4 + waveG0*MIB_m + MT0*SG0
s_mul_i32 s8, 256, s[sgprWorkGroup1]               // wgp1 * MT1
v_add_u32 v1, s8, v1                               // coord 1 = (tid0%MI_m) + waveG1*MIB_n + MT1*SG1

/******************************************/
/* Global Write Elements                  */
/******************************************/
s_waitcnt lgkmcnt(0)                               // wait for 24 bytes of kern args.
s_mov_b32 s[sgprSrdScaleAlphaVec+0], s[sgprAddressScaleAlphaVec+0] // init SRD base address (lower)
s_mov_b32 s[sgprSrdScaleAlphaVec+1], s[sgprAddressScaleAlphaVec+1] // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdScaleAlphaVec+3], Srd127_96     // Set bits 127_96 in post-loop SRD
s_cmp_eq_u64 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], 0 // s[AddressScaleAlphaVec] == 0 ?
s_cbranch_scc0 label_ScaleAlphaVecAddrValid        // branch if s[AddressScaleAlphaVec] != 0
s_mov_b32 s[sgprSrdScaleAlphaVec+2], 0
s_branch label_ScaleAlphaVecAddrValid_End
label_ScaleAlphaVecAddrValid:
s_mov_b32 s[sgprSrdScaleAlphaVec+2], s[sgprSizeI]
label_ScaleAlphaVecAddrValid_End:

s_mul_i32 s[sgprSrdScaleAlphaVec+2], 0x4, s[sgprSrdScaleAlphaVec+2] // ScaleAlphaVec scaled by BPE
s_add_u32 s8, s[sgprWorkGroup2], 0x1
s_mul_i32 s8, s[sgprBiasStride], s8                // stride * (wg+1)
s_cmp_eq_u32 s8, 0x0                               // bias stride = 0?
s_cselect_b32 s8, s[sgprSizeI], s8
s_mov_b32 s[sgprSrdBias+0], s[sgprAddressBias+0]   // init SRD base address (lower)
s_mov_b32 s[sgprSrdBias+1], s[sgprAddressBias+1]   // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdBias+3], Srd127_96              // Set bits 127_96 in post-loop SRD
s_cmp_eq_u64 s[sgprAddressBias:sgprAddressBias+1], 0 // s[AddressBias] == 0 ?
s_cbranch_scc0 label_BiasAddrValid                 // branch if s[AddressBias] != 0
s_mov_b32 s[sgprSrdBias+2], 0
s_branch label_BiasAddrValid_End
label_BiasAddrValid:
s_mov_b32 s[sgprSrdBias+2], s8
label_BiasAddrValid_End:

label_Load_Biasf32_0:
s_cmpk_lg_u32 s[sgprBiasType], 0                   // BiasType != 0
s_cbranch_scc1 label_Load_Biasf16_0                // Branch if true

/******************************************/
/* Read vector to LDS                     */
/******************************************/
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_add_u32 v8, s52, v[vgprSerial]                   // coord 0 = wgp0 * MT0 + thread offset
s_mul_i32 s[sgprSrdBias+2], 0x4, s[sgprSrdBias+2]  // scaled by BPE
s_mul_i32 s52, s[sgprBiasStride], s[sgprWorkGroup2] // Stride * WG
v_add_u32 v6, s52, v8                              // coord 0 = wgp0 * MT0 + thread offset + Stride * WG
v_lshlrev_b32 v6, 0x2, v6                          // Global bias address scaled by BPE
v_lshlrev_b32 v7, 0x2, v8                          // Global scaleAlpha address scaled by BPE
s_mul_i32 s52, 256, s[sgprWorkGroup1]              // wgp1 * MT1
v_add_u32 v8, s52, v[vgprSerial]                   // coord 1 = wgp1 * MT1 + thread offset
buffer_load_dword v4, v6, s[sgprSrdBias:sgprSrdBias+3], 0 offen offset:0 // Load Bias
buffer_load_dword v5, v7, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // Load ScaleAlphaVec
v_lshlrev_b32 v8, 0x2, v[vgprSerial]               // Local address scaled by BPE
s_barrier                                          // wait for all global loads.
s_waitcnt vmcnt(1)                                 // wait for global load
ds_write_b32 v8, v4 offset:0                       // store bias
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
s_waitcnt vmcnt(0)                                 // wait for global load
v_cndmask_b32 v5, 1.0, v5, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
ds_write_b32 v8, v5 offset:1024                    // store scaleAlpha
s_branch label_Load_Bias_End                       // Branch to load bias end
label_Load_Biasf16_0:
s_cmpk_lg_u32 s[sgprBiasType], 4                   // BiasType != 4
s_cbranch_scc1 label_Load_Bias_End                 // Branch if true

/******************************************/
/* Read vector to LDS                     */
/******************************************/
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_add_u32 v8, s52, v[vgprSerial]                   // coord 0 = wgp0 * MT0 + thread offset
s_mul_i32 s[sgprSrdBias+2], 0x2, s[sgprSrdBias+2]  // scaled by BPE
s_mul_i32 s52, s[sgprBiasStride], s[sgprWorkGroup2] // Stride * WG
v_add_u32 v6, s52, v8                              // coord 0 = wgp0 * MT0 + thread offset + Stride * WG
v_lshlrev_b32 v6, 0x1, v6                          // Global bias address scaled by BPE
v_lshlrev_b32 v7, 0x2, v8                          // Global scaleAlpha address scaled by BPE
s_mul_i32 s52, 256, s[sgprWorkGroup1]              // wgp1 * MT1
v_add_u32 v8, s52, v[vgprSerial]                   // coord 1 = wgp1 * MT1 + thread offset
buffer_load_short_d16 v4, v6, s[sgprSrdBias:sgprSrdBias+3], 0 offen offset:0 // Load Bias
buffer_load_dword v5, v7, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // Load ScaleAlphaVec
v_lshlrev_b32 v8, 0x2, v[vgprSerial]               // Local address scaled by BPE
s_barrier                                          // wait for all global loads.
s_waitcnt vmcnt(1)                                 // wait for global load
v_cvt_f32_f16 v4, v4                               // convert to FP32
ds_write_b32 v8, v4 offset:0                       // store bias
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
s_waitcnt vmcnt(0)                                 // wait for global load
v_cndmask_b32 v5, 1.0, v5, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
ds_write_b32 v8, v5 offset:1024                    // store scaleAlpha
s_branch label_Load_Bias_End                       // Branch to load bias end
label_Load_Bias_End:
.set sgprAddressScaleAlphaVec, UNDEF
.set sgprSrdScaleAlphaVec, UNDEF
label_GW_B0_E0:

/* edge=0, allocate 2 sgpr. perBatchTmpS=2 perBatchMaskS=0 perElementMaskS=0 elementsPerBatch=18 */
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,1,0,0:vw4); (0,2,0,0:vw4); (0,3,0,0:vw4); (0,0,1,0:vw4); (0,1,1,0:vw4); (0,2,1,0:vw4); (0,3,1,0:vw4); (0,0,2,0:vw4); (0,1,2,0:vw4); (0,2,2,0:vw4); (0,3,2,0:vw4); (0,0,3,0:vw4); (0,1,3,0:vw4); (0,2,3,0:vw4); (0,3,3,0:vw4); (0,0,4,0:vw4); (0,1,4,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s12, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s12
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for LDS write
s_barrier                                          // LDS write barrier
ds_read_b128 v[12:15], v8 offset:0                 // load Bias
v_add_u32 v9, 1024, v8                             // add ScaleAlphaVec offset (1)
ds_read_b128 v[16:19], v9 offset:0                 // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
ds_read_b128 v[24:27], v8 offset:256               // load Bias
ds_read_b128 v[28:31], v9 offset:256               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,0,2,0) */
ds_read_b128 v[36:39], v8 offset:512               // load Bias
ds_read_b128 v[40:43], v9 offset:512               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,0,3,0) */
ds_read_b128 v[48:51], v8 offset:768               // load Bias
ds_read_b128 v[52:55], v9 offset:768               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
/* (d1,vc1,d0,vc0)=(0,1,2,0) */
/* (d1,vc1,d0,vc0)=(0,1,3,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
/* (d1,vc1,d0,vc0)=(0,2,2,0) */
/* (d1,vc1,d0,vc0)=(0,2,3,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
/* (d1,vc1,d0,vc0)=(0,3,2,0) */
/* (d1,vc1,d0,vc0)=(0,3,3,0) */
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
v_add_lshl_u32 v6, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc1           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+44], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+45], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+46], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+47], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+56], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+57], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+58], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+59], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+60], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+61], acc17          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+62], acc18          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+63], acc19          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+64], acc20          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+65], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+66], acc22          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+67], acc23          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+68], acc24          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+69], acc25          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+70], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+71], acc27          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+72], acc28          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+73], acc29          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+74], acc30          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+75], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+76], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+77], acc33          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+78], acc34          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+79], acc35          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+80], acc36          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+81], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+82], acc38          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+83], acc39          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+84], acc40          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+85], acc41          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+86], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+87], acc43          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+88], acc44          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+89], acc45          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+90], acc46          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+91], acc47          // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+92], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+93], acc49          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+94], acc50          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+95], acc51          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+96], acc52          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+97], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+98], acc54          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+99], acc55          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+100], acc56         // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+101], acc57         // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+102], acc58         // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+103], acc59         // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+104], acc60         // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+105], acc61         // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+106], acc62         // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+107], acc63         // copy acc to vreg[63]
v_accvgpr_read_b32 v[vgprValuC+108], acc64         // copy acc to vreg[64]
v_accvgpr_read_b32 v[vgprValuC+109], acc65         // copy acc to vreg[65]
v_accvgpr_read_b32 v[vgprValuC+110], acc66         // copy acc to vreg[66]
v_accvgpr_read_b32 v[vgprValuC+111], acc67         // copy acc to vreg[67]
v_accvgpr_read_b32 v[vgprValuC+112], acc68         // copy acc to vreg[68]
v_accvgpr_read_b32 v[vgprValuC+113], acc69         // copy acc to vreg[69]
v_accvgpr_read_b32 v[vgprValuC+114], acc70         // copy acc to vreg[70]
v_accvgpr_read_b32 v[vgprValuC+115], acc71         // copy acc to vreg[71]

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(6)                               // lgkmcnt(6) = 8 - 1 (bias) - 1 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt lgkmcnt(4)                               // lgkmcnt(4) = 8 - 2 (bias) - 2 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D

s_waitcnt lgkmcnt(2)                               // lgkmcnt(2) = 8 - 3 (bias) - 3 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[40:41], v[vgprValuC+44:vgprValuC+44+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[42:43], v[vgprValuC+46:vgprValuC+46+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[36:37], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[38:39], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt lgkmcnt(0)                               // lgkmcnt(0) = 8 - 4 (bias) - 4 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[52:53], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[54:55], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[48:49], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[50:51], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[12:13], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[14:15], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[60:61], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[28:29], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[30:31], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[40:41], v[vgprValuC+68:vgprValuC+68+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[42:43], v[vgprValuC+70:vgprValuC+70+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+68:vgprValuC+68+1], v[36:37], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[vgprValuC+70:vgprValuC+70+1], v[38:39], v[vgprValuC+70:vgprValuC+70+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+68], v[vgprValuC+68]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+69], v[vgprValuC+69]     // convert C to fp16
v_pack_b32_f16 v68, v[vgprValuC+68], v[vgprValuC+69] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+70], v[vgprValuC+70]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+71], v[vgprValuC+71]     // convert C to fp16
v_pack_b32_f16 v69, v[vgprValuC+70], v[vgprValuC+71] // Pack with neighbor
buffer_store_dwordx2 v[68:69], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[52:53], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[54:55], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[48:49], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[50:51], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[16:17], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[18:19], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[12:13], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[14:15], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[76:77], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[28:29], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[30:31], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[40:41], v[vgprValuC+84:vgprValuC+84+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[42:43], v[vgprValuC+86:vgprValuC+86+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+84:vgprValuC+84+1], v[36:37], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[vgprValuC+86:vgprValuC+86+1], v[38:39], v[vgprValuC+86:vgprValuC+86+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+84], v[vgprValuC+84]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+85], v[vgprValuC+85]     // convert C to fp16
v_pack_b32_f16 v84, v[vgprValuC+84], v[vgprValuC+85] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+86], v[vgprValuC+86]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+87], v[vgprValuC+87]     // convert C to fp16
v_pack_b32_f16 v85, v[vgprValuC+86], v[vgprValuC+87] // Pack with neighbor
buffer_store_dwordx2 v[84:85], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[52:53], v[vgprValuC+88:vgprValuC+88+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[54:55], v[vgprValuC+90:vgprValuC+90+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+88:vgprValuC+88+1], v[48:49], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[vgprValuC+90:vgprValuC+90+1], v[50:51], v[vgprValuC+90:vgprValuC+90+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+88], v[vgprValuC+88]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+89], v[vgprValuC+89]     // convert C to fp16
v_pack_b32_f16 v88, v[vgprValuC+88], v[vgprValuC+89] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+90], v[vgprValuC+90]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+91], v[vgprValuC+91]     // convert C to fp16
v_pack_b32_f16 v89, v[vgprValuC+90], v[vgprValuC+91] // Pack with neighbor
buffer_store_dwordx2 v[88:89], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[16:17], v[vgprValuC+92:vgprValuC+92+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[18:19], v[vgprValuC+94:vgprValuC+94+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+92:vgprValuC+92+1], v[12:13], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[vgprValuC+94:vgprValuC+94+1], v[14:15], v[vgprValuC+94:vgprValuC+94+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+92], v[vgprValuC+92]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+93], v[vgprValuC+93]     // convert C to fp16
v_pack_b32_f16 v92, v[vgprValuC+92], v[vgprValuC+93] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+94], v[vgprValuC+94]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+95], v[vgprValuC+95]     // convert C to fp16
v_pack_b32_f16 v93, v[vgprValuC+94], v[vgprValuC+95] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[92:93], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+96:vgprValuC+96+1], v[28:29], v[vgprValuC+96:vgprValuC+96+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+98:vgprValuC+98+1], v[30:31], v[vgprValuC+98:vgprValuC+98+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+96:vgprValuC+96+1], v[24:25], v[vgprValuC+96:vgprValuC+96+1] // C += bias
v_pk_add_f32 v[vgprValuC+98:vgprValuC+98+1], v[26:27], v[vgprValuC+98:vgprValuC+98+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+96], v[vgprValuC+96]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+97], v[vgprValuC+97]     // convert C to fp16
v_pack_b32_f16 v96, v[vgprValuC+96], v[vgprValuC+97] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+98], v[vgprValuC+98]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+99], v[vgprValuC+99]     // convert C to fp16
v_pack_b32_f16 v97, v[vgprValuC+98], v[vgprValuC+99] // Pack with neighbor
buffer_store_dwordx2 v[96:97], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[40:41], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[42:43], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[36:37], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[38:39], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
buffer_store_dwordx2 v[100:101], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[52:53], v[vgprValuC+104:vgprValuC+104+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[54:55], v[vgprValuC+106:vgprValuC+106+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+104:vgprValuC+104+1], v[48:49], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[vgprValuC+106:vgprValuC+106+1], v[50:51], v[vgprValuC+106:vgprValuC+106+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+104], v[vgprValuC+104]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+105], v[vgprValuC+105]   // convert C to fp16
v_pack_b32_f16 v104, v[vgprValuC+104], v[vgprValuC+105] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+106], v[vgprValuC+106]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+107], v[vgprValuC+107]   // convert C to fp16
v_pack_b32_f16 v105, v[vgprValuC+106], v[vgprValuC+107] // Pack with neighbor
buffer_store_dwordx2 v[104:105], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[16:17], v[vgprValuC+108:vgprValuC+108+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[18:19], v[vgprValuC+110:vgprValuC+110+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+108:vgprValuC+108+1], v[12:13], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[vgprValuC+110:vgprValuC+110+1], v[14:15], v[vgprValuC+110:vgprValuC+110+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+108], v[vgprValuC+108]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+109], v[vgprValuC+109]   // convert C to fp16
v_pack_b32_f16 v108, v[vgprValuC+108], v[vgprValuC+109] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+110], v[vgprValuC+110]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+111], v[vgprValuC+111]   // convert C to fp16
v_pack_b32_f16 v109, v[vgprValuC+110], v[vgprValuC+111] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[108:109], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+112:vgprValuC+112+1], v[28:29], v[vgprValuC+112:vgprValuC+112+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+114:vgprValuC+114+1], v[30:31], v[vgprValuC+114:vgprValuC+114+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+112:vgprValuC+112+1], v[24:25], v[vgprValuC+112:vgprValuC+112+1] // C += bias
v_pk_add_f32 v[vgprValuC+114:vgprValuC+114+1], v[26:27], v[vgprValuC+114:vgprValuC+114+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+112], v[vgprValuC+112]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+113], v[vgprValuC+113]   // convert C to fp16
v_pack_b32_f16 v112, v[vgprValuC+112], v[vgprValuC+113] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+114], v[vgprValuC+114]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+115], v[vgprValuC+115]   // convert C to fp16
v_pack_b32_f16 v113, v[vgprValuC+114], v[vgprValuC+115] // Pack with neighbor
buffer_store_dwordx2 v[112:113], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,2,4,0:vw4); (0,3,4,0:vw4); (0,0,5,0:vw4); (0,1,5,0:vw4); (0,2,5,0:vw4); (0,3,5,0:vw4); (0,0,6,0:vw4); (0,1,6,0:vw4); (0,2,6,0:vw4); (0,3,6,0:vw4); (0,0,7,0:vw4); (0,1,7,0:vw4); (0,2,7,0:vw4); (0,3,7,0:vw4); (1,0,0,0:vw4); (1,1,0,0:vw4); (1,2,0,0:vw4); (1,3,0,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,4,2,0) */
ds_read_b128 v[12:15], v8 offset:512               // load Bias
ds_read_b128 v[16:19], v9 offset:512               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,4,3,0) */
ds_read_b128 v[24:27], v8 offset:768               // load Bias
ds_read_b128 v[28:31], v9 offset:768               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
ds_read_b128 v[36:39], v8 offset:0                 // load Bias
ds_read_b128 v[40:43], v9 offset:0                 // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
ds_read_b128 v[48:51], v8 offset:256               // load Bias
ds_read_b128 v[52:55], v9 offset:256               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,5,2,0) */
/* (d1,vc1,d0,vc0)=(0,5,3,0) */
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
/* (d1,vc1,d0,vc0)=(0,6,2,0) */
/* (d1,vc1,d0,vc0)=(0,6,3,0) */
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
/* (d1,vc1,d0,vc0)=(0,7,2,0) */
/* (d1,vc1,d0,vc0)=(0,7,3,0) */
/* (d1,vc1,d0,vc0)=(1,0,0,0) */
/* (d1,vc1,d0,vc0)=(1,0,1,0) */
/* (d1,vc1,d0,vc0)=(1,0,2,0) */
/* (d1,vc1,d0,vc0)=(1,0,3,0) */
v_accvgpr_read_b32 v[vgprValuC+20], acc72          // copy acc to vreg[72]
v_accvgpr_read_b32 v[vgprValuC+21], acc73          // copy acc to vreg[73]
v_accvgpr_read_b32 v[vgprValuC+22], acc74          // copy acc to vreg[74]
v_accvgpr_read_b32 v[vgprValuC+23], acc75          // copy acc to vreg[75]
v_accvgpr_read_b32 v[vgprValuC+32], acc76          // copy acc to vreg[76]
v_accvgpr_read_b32 v[vgprValuC+33], acc77          // copy acc to vreg[77]
v_accvgpr_read_b32 v[vgprValuC+34], acc78          // copy acc to vreg[78]
v_accvgpr_read_b32 v[vgprValuC+35], acc79          // copy acc to vreg[79]
v_accvgpr_read_b32 v[vgprValuC+44], acc80          // copy acc to vreg[80]
v_accvgpr_read_b32 v[vgprValuC+45], acc81          // copy acc to vreg[81]
v_accvgpr_read_b32 v[vgprValuC+46], acc82          // copy acc to vreg[82]
v_accvgpr_read_b32 v[vgprValuC+47], acc83          // copy acc to vreg[83]
v_accvgpr_read_b32 v[vgprValuC+56], acc84          // copy acc to vreg[84]
v_accvgpr_read_b32 v[vgprValuC+57], acc85          // copy acc to vreg[85]
v_accvgpr_read_b32 v[vgprValuC+58], acc86          // copy acc to vreg[86]
v_accvgpr_read_b32 v[vgprValuC+59], acc87          // copy acc to vreg[87]
v_accvgpr_read_b32 v[vgprValuC+60], acc88          // copy acc to vreg[88]
v_accvgpr_read_b32 v[vgprValuC+61], acc89          // copy acc to vreg[89]
v_accvgpr_read_b32 v[vgprValuC+62], acc90          // copy acc to vreg[90]
v_accvgpr_read_b32 v[vgprValuC+63], acc91          // copy acc to vreg[91]
v_accvgpr_read_b32 v[vgprValuC+64], acc92          // copy acc to vreg[92]
v_accvgpr_read_b32 v[vgprValuC+65], acc93          // copy acc to vreg[93]
v_accvgpr_read_b32 v[vgprValuC+66], acc94          // copy acc to vreg[94]
v_accvgpr_read_b32 v[vgprValuC+67], acc95          // copy acc to vreg[95]
v_accvgpr_read_b32 v[vgprValuC+68], acc96          // copy acc to vreg[96]
v_accvgpr_read_b32 v[vgprValuC+69], acc97          // copy acc to vreg[97]
v_accvgpr_read_b32 v[vgprValuC+70], acc98          // copy acc to vreg[98]
v_accvgpr_read_b32 v[vgprValuC+71], acc99          // copy acc to vreg[99]
v_accvgpr_read_b32 v[vgprValuC+72], acc100         // copy acc to vreg[100]
v_accvgpr_read_b32 v[vgprValuC+73], acc101         // copy acc to vreg[101]
v_accvgpr_read_b32 v[vgprValuC+74], acc102         // copy acc to vreg[102]
v_accvgpr_read_b32 v[vgprValuC+75], acc103         // copy acc to vreg[103]
v_accvgpr_read_b32 v[vgprValuC+76], acc104         // copy acc to vreg[104]
v_accvgpr_read_b32 v[vgprValuC+77], acc105         // copy acc to vreg[105]
v_accvgpr_read_b32 v[vgprValuC+78], acc106         // copy acc to vreg[106]
v_accvgpr_read_b32 v[vgprValuC+79], acc107         // copy acc to vreg[107]
v_accvgpr_read_b32 v[vgprValuC+80], acc108         // copy acc to vreg[108]
v_accvgpr_read_b32 v[vgprValuC+81], acc109         // copy acc to vreg[109]
v_accvgpr_read_b32 v[vgprValuC+82], acc110         // copy acc to vreg[110]
v_accvgpr_read_b32 v[vgprValuC+83], acc111         // copy acc to vreg[111]
v_accvgpr_read_b32 v[vgprValuC+84], acc112         // copy acc to vreg[112]
v_accvgpr_read_b32 v[vgprValuC+85], acc113         // copy acc to vreg[113]
v_accvgpr_read_b32 v[vgprValuC+86], acc114         // copy acc to vreg[114]
v_accvgpr_read_b32 v[vgprValuC+87], acc115         // copy acc to vreg[115]
v_accvgpr_read_b32 v[vgprValuC+88], acc116         // copy acc to vreg[116]
v_accvgpr_read_b32 v[vgprValuC+89], acc117         // copy acc to vreg[117]
v_accvgpr_read_b32 v[vgprValuC+90], acc118         // copy acc to vreg[118]
v_accvgpr_read_b32 v[vgprValuC+91], acc119         // copy acc to vreg[119]
v_accvgpr_read_b32 v[vgprValuC+92], acc120         // copy acc to vreg[120]
v_accvgpr_read_b32 v[vgprValuC+93], acc121         // copy acc to vreg[121]
v_accvgpr_read_b32 v[vgprValuC+94], acc122         // copy acc to vreg[122]
v_accvgpr_read_b32 v[vgprValuC+95], acc123         // copy acc to vreg[123]
v_accvgpr_read_b32 v[vgprValuC+96], acc124         // copy acc to vreg[124]
v_accvgpr_read_b32 v[vgprValuC+97], acc125         // copy acc to vreg[125]
v_accvgpr_read_b32 v[vgprValuC+98], acc126         // copy acc to vreg[126]
v_accvgpr_read_b32 v[vgprValuC+99], acc127         // copy acc to vreg[127]
v_accvgpr_read_b32 v[vgprValuC+100], acc128        // copy acc to vreg[128]
v_accvgpr_read_b32 v[vgprValuC+101], acc129        // copy acc to vreg[129]
v_accvgpr_read_b32 v[vgprValuC+102], acc130        // copy acc to vreg[130]
v_accvgpr_read_b32 v[vgprValuC+103], acc131        // copy acc to vreg[131]
v_accvgpr_read_b32 v[vgprValuC+104], acc132        // copy acc to vreg[132]
v_accvgpr_read_b32 v[vgprValuC+105], acc133        // copy acc to vreg[133]
v_accvgpr_read_b32 v[vgprValuC+106], acc134        // copy acc to vreg[134]
v_accvgpr_read_b32 v[vgprValuC+107], acc135        // copy acc to vreg[135]
v_accvgpr_read_b32 v[vgprValuC+108], acc136        // copy acc to vreg[136]
v_accvgpr_read_b32 v[vgprValuC+109], acc137        // copy acc to vreg[137]
v_accvgpr_read_b32 v[vgprValuC+110], acc138        // copy acc to vreg[138]
v_accvgpr_read_b32 v[vgprValuC+111], acc139        // copy acc to vreg[139]
v_accvgpr_read_b32 v[vgprValuC+112], acc140        // copy acc to vreg[140]
v_accvgpr_read_b32 v[vgprValuC+113], acc141        // copy acc to vreg[141]
v_accvgpr_read_b32 v[vgprValuC+114], acc142        // copy acc to vreg[142]
v_accvgpr_read_b32 v[vgprValuC+115], acc143        // copy acc to vreg[143]

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(6)                               // lgkmcnt(6) = 8 - 1 (bias) - 1 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt lgkmcnt(4)                               // lgkmcnt(4) = 8 - 2 (bias) - 2 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D

s_waitcnt lgkmcnt(2)                               // lgkmcnt(2) = 8 - 3 (bias) - 3 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[40:41], v[vgprValuC+44:vgprValuC+44+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[42:43], v[vgprValuC+46:vgprValuC+46+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[36:37], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[38:39], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt lgkmcnt(0)                               // lgkmcnt(0) = 8 - 4 (bias) - 4 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[52:53], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[54:55], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[48:49], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[50:51], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[12:13], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[14:15], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
buffer_store_dwordx2 v[60:61], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[28:29], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[30:31], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[40:41], v[vgprValuC+68:vgprValuC+68+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[42:43], v[vgprValuC+70:vgprValuC+70+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+68:vgprValuC+68+1], v[36:37], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[vgprValuC+70:vgprValuC+70+1], v[38:39], v[vgprValuC+70:vgprValuC+70+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+68], v[vgprValuC+68]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+69], v[vgprValuC+69]     // convert C to fp16
v_pack_b32_f16 v68, v[vgprValuC+68], v[vgprValuC+69] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+70], v[vgprValuC+70]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+71], v[vgprValuC+71]     // convert C to fp16
v_pack_b32_f16 v69, v[vgprValuC+70], v[vgprValuC+71] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[68:69], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[52:53], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[54:55], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[48:49], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[50:51], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[16:17], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[18:19], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[12:13], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[14:15], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
buffer_store_dwordx2 v[76:77], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[28:29], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[30:31], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[40:41], v[vgprValuC+84:vgprValuC+84+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[42:43], v[vgprValuC+86:vgprValuC+86+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+84:vgprValuC+84+1], v[36:37], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[vgprValuC+86:vgprValuC+86+1], v[38:39], v[vgprValuC+86:vgprValuC+86+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+84], v[vgprValuC+84]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+85], v[vgprValuC+85]     // convert C to fp16
v_pack_b32_f16 v84, v[vgprValuC+84], v[vgprValuC+85] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+86], v[vgprValuC+86]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+87], v[vgprValuC+87]     // convert C to fp16
v_pack_b32_f16 v85, v[vgprValuC+86], v[vgprValuC+87] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[84:85], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[52:53], v[vgprValuC+88:vgprValuC+88+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[54:55], v[vgprValuC+90:vgprValuC+90+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+88:vgprValuC+88+1], v[48:49], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[vgprValuC+90:vgprValuC+90+1], v[50:51], v[vgprValuC+90:vgprValuC+90+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+88], v[vgprValuC+88]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+89], v[vgprValuC+89]     // convert C to fp16
v_pack_b32_f16 v88, v[vgprValuC+88], v[vgprValuC+89] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+90], v[vgprValuC+90]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+91], v[vgprValuC+91]     // convert C to fp16
v_pack_b32_f16 v89, v[vgprValuC+90], v[vgprValuC+91] // Pack with neighbor
buffer_store_dwordx2 v[88:89], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[16:17], v[vgprValuC+92:vgprValuC+92+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[18:19], v[vgprValuC+94:vgprValuC+94+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+92:vgprValuC+92+1], v[12:13], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[vgprValuC+94:vgprValuC+94+1], v[14:15], v[vgprValuC+94:vgprValuC+94+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+92], v[vgprValuC+92]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+93], v[vgprValuC+93]     // convert C to fp16
v_pack_b32_f16 v92, v[vgprValuC+92], v[vgprValuC+93] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+94], v[vgprValuC+94]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+95], v[vgprValuC+95]     // convert C to fp16
v_pack_b32_f16 v93, v[vgprValuC+94], v[vgprValuC+95] // Pack with neighbor
buffer_store_dwordx2 v[92:93], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+96:vgprValuC+96+1], v[28:29], v[vgprValuC+96:vgprValuC+96+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+98:vgprValuC+98+1], v[30:31], v[vgprValuC+98:vgprValuC+98+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+96:vgprValuC+96+1], v[24:25], v[vgprValuC+96:vgprValuC+96+1] // C += bias
v_pk_add_f32 v[vgprValuC+98:vgprValuC+98+1], v[26:27], v[vgprValuC+98:vgprValuC+98+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+96], v[vgprValuC+96]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+97], v[vgprValuC+97]     // convert C to fp16
v_pack_b32_f16 v96, v[vgprValuC+96], v[vgprValuC+97] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+98], v[vgprValuC+98]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+99], v[vgprValuC+99]     // convert C to fp16
v_pack_b32_f16 v97, v[vgprValuC+98], v[vgprValuC+99] // Pack with neighbor
buffer_store_dwordx2 v[96:97], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[40:41], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[42:43], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[36:37], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[38:39], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
s_mul_i32 s12, s[sgprStrideD1J], 242               // scale StrideD *= numRows(121) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[100:101], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[52:53], v[vgprValuC+104:vgprValuC+104+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[54:55], v[vgprValuC+106:vgprValuC+106+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+104:vgprValuC+104+1], v[48:49], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[vgprValuC+106:vgprValuC+106+1], v[50:51], v[vgprValuC+106:vgprValuC+106+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+104], v[vgprValuC+104]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+105], v[vgprValuC+105]   // convert C to fp16
v_pack_b32_f16 v104, v[vgprValuC+104], v[vgprValuC+105] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+106], v[vgprValuC+106]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+107], v[vgprValuC+107]   // convert C to fp16
v_pack_b32_f16 v105, v[vgprValuC+106], v[vgprValuC+107] // Pack with neighbor
buffer_store_dwordx2 v[104:105], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[16:17], v[vgprValuC+108:vgprValuC+108+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[18:19], v[vgprValuC+110:vgprValuC+110+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+108:vgprValuC+108+1], v[12:13], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[vgprValuC+110:vgprValuC+110+1], v[14:15], v[vgprValuC+110:vgprValuC+110+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+108], v[vgprValuC+108]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+109], v[vgprValuC+109]   // convert C to fp16
v_pack_b32_f16 v108, v[vgprValuC+108], v[vgprValuC+109] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+110], v[vgprValuC+110]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+111], v[vgprValuC+111]   // convert C to fp16
v_pack_b32_f16 v109, v[vgprValuC+110], v[vgprValuC+111] // Pack with neighbor
buffer_store_dwordx2 v[108:109], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+112:vgprValuC+112+1], v[28:29], v[vgprValuC+112:vgprValuC+112+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+114:vgprValuC+114+1], v[30:31], v[vgprValuC+114:vgprValuC+114+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+112:vgprValuC+112+1], v[24:25], v[vgprValuC+112:vgprValuC+112+1] // C += bias
v_pk_add_f32 v[vgprValuC+114:vgprValuC+114+1], v[26:27], v[vgprValuC+114:vgprValuC+114+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+112], v[vgprValuC+112]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+113], v[vgprValuC+113]   // convert C to fp16
v_pack_b32_f16 v112, v[vgprValuC+112], v[vgprValuC+113] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+114], v[vgprValuC+114]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+115], v[vgprValuC+115]   // convert C to fp16
v_pack_b32_f16 v113, v[vgprValuC+114], v[vgprValuC+115] // Pack with neighbor
buffer_store_dwordx2 v[112:113], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Batch #2 (d1,d0,vc1,vc0) = */
/*    (1,0,1,0:vw4); (1,1,1,0:vw4); (1,2,1,0:vw4); (1,3,1,0:vw4); (1,0,2,0:vw4); (1,1,2,0:vw4); (1,2,2,0:vw4); (1,3,2,0:vw4); (1,0,3,0:vw4); (1,1,3,0:vw4); (1,2,3,0:vw4); (1,3,3,0:vw4); (1,0,4,0:vw4); (1,1,4,0:vw4); (1,2,4,0:vw4); (1,3,4,0:vw4); (1,0,5,0:vw4); (1,1,5,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(1,1,0,0) */
ds_read_b128 v[12:15], v8 offset:0                 // load Bias
ds_read_b128 v[16:19], v9 offset:0                 // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,1,1,0) */
ds_read_b128 v[24:27], v8 offset:256               // load Bias
ds_read_b128 v[28:31], v9 offset:256               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,1,2,0) */
ds_read_b128 v[36:39], v8 offset:512               // load Bias
ds_read_b128 v[40:43], v9 offset:512               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,1,3,0) */
ds_read_b128 v[48:51], v8 offset:768               // load Bias
ds_read_b128 v[52:55], v9 offset:768               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,2,0,0) */
/* (d1,vc1,d0,vc0)=(1,2,1,0) */
/* (d1,vc1,d0,vc0)=(1,2,2,0) */
/* (d1,vc1,d0,vc0)=(1,2,3,0) */
/* (d1,vc1,d0,vc0)=(1,3,0,0) */
/* (d1,vc1,d0,vc0)=(1,3,1,0) */
/* (d1,vc1,d0,vc0)=(1,3,2,0) */
/* (d1,vc1,d0,vc0)=(1,3,3,0) */
/* (d1,vc1,d0,vc0)=(1,4,0,0) */
/* (d1,vc1,d0,vc0)=(1,4,1,0) */
/* (d1,vc1,d0,vc0)=(1,4,2,0) */
/* (d1,vc1,d0,vc0)=(1,4,3,0) */
/* (d1,vc1,d0,vc0)=(1,5,0,0) */
/* (d1,vc1,d0,vc0)=(1,5,1,0) */
v_accvgpr_read_b32 v[vgprValuC+20], acc144         // copy acc to vreg[144]
v_accvgpr_read_b32 v[vgprValuC+21], acc145         // copy acc to vreg[145]
v_accvgpr_read_b32 v[vgprValuC+22], acc146         // copy acc to vreg[146]
v_accvgpr_read_b32 v[vgprValuC+23], acc147         // copy acc to vreg[147]
v_accvgpr_read_b32 v[vgprValuC+32], acc148         // copy acc to vreg[148]
v_accvgpr_read_b32 v[vgprValuC+33], acc149         // copy acc to vreg[149]
v_accvgpr_read_b32 v[vgprValuC+34], acc150         // copy acc to vreg[150]
v_accvgpr_read_b32 v[vgprValuC+35], acc151         // copy acc to vreg[151]
v_accvgpr_read_b32 v[vgprValuC+44], acc152         // copy acc to vreg[152]
v_accvgpr_read_b32 v[vgprValuC+45], acc153         // copy acc to vreg[153]
v_accvgpr_read_b32 v[vgprValuC+46], acc154         // copy acc to vreg[154]
v_accvgpr_read_b32 v[vgprValuC+47], acc155         // copy acc to vreg[155]
v_accvgpr_read_b32 v[vgprValuC+56], acc156         // copy acc to vreg[156]
v_accvgpr_read_b32 v[vgprValuC+57], acc157         // copy acc to vreg[157]
v_accvgpr_read_b32 v[vgprValuC+58], acc158         // copy acc to vreg[158]
v_accvgpr_read_b32 v[vgprValuC+59], acc159         // copy acc to vreg[159]
v_accvgpr_read_b32 v[vgprValuC+60], acc160         // copy acc to vreg[160]
v_accvgpr_read_b32 v[vgprValuC+61], acc161         // copy acc to vreg[161]
v_accvgpr_read_b32 v[vgprValuC+62], acc162         // copy acc to vreg[162]
v_accvgpr_read_b32 v[vgprValuC+63], acc163         // copy acc to vreg[163]
v_accvgpr_read_b32 v[vgprValuC+64], acc164         // copy acc to vreg[164]
v_accvgpr_read_b32 v[vgprValuC+65], acc165         // copy acc to vreg[165]
v_accvgpr_read_b32 v[vgprValuC+66], acc166         // copy acc to vreg[166]
v_accvgpr_read_b32 v[vgprValuC+67], acc167         // copy acc to vreg[167]
v_accvgpr_read_b32 v[vgprValuC+68], acc168         // copy acc to vreg[168]
v_accvgpr_read_b32 v[vgprValuC+69], acc169         // copy acc to vreg[169]
v_accvgpr_read_b32 v[vgprValuC+70], acc170         // copy acc to vreg[170]
v_accvgpr_read_b32 v[vgprValuC+71], acc171         // copy acc to vreg[171]
v_accvgpr_read_b32 v[vgprValuC+72], acc172         // copy acc to vreg[172]
v_accvgpr_read_b32 v[vgprValuC+73], acc173         // copy acc to vreg[173]
v_accvgpr_read_b32 v[vgprValuC+74], acc174         // copy acc to vreg[174]
v_accvgpr_read_b32 v[vgprValuC+75], acc175         // copy acc to vreg[175]
v_accvgpr_read_b32 v[vgprValuC+76], acc176         // copy acc to vreg[176]
v_accvgpr_read_b32 v[vgprValuC+77], acc177         // copy acc to vreg[177]
v_accvgpr_read_b32 v[vgprValuC+78], acc178         // copy acc to vreg[178]
v_accvgpr_read_b32 v[vgprValuC+79], acc179         // copy acc to vreg[179]
v_accvgpr_read_b32 v[vgprValuC+80], acc180         // copy acc to vreg[180]
v_accvgpr_read_b32 v[vgprValuC+81], acc181         // copy acc to vreg[181]
v_accvgpr_read_b32 v[vgprValuC+82], acc182         // copy acc to vreg[182]
v_accvgpr_read_b32 v[vgprValuC+83], acc183         // copy acc to vreg[183]
v_accvgpr_read_b32 v[vgprValuC+84], acc184         // copy acc to vreg[184]
v_accvgpr_read_b32 v[vgprValuC+85], acc185         // copy acc to vreg[185]
v_accvgpr_read_b32 v[vgprValuC+86], acc186         // copy acc to vreg[186]
v_accvgpr_read_b32 v[vgprValuC+87], acc187         // copy acc to vreg[187]
v_accvgpr_read_b32 v[vgprValuC+88], acc188         // copy acc to vreg[188]
v_accvgpr_read_b32 v[vgprValuC+89], acc189         // copy acc to vreg[189]
v_accvgpr_read_b32 v[vgprValuC+90], acc190         // copy acc to vreg[190]
v_accvgpr_read_b32 v[vgprValuC+91], acc191         // copy acc to vreg[191]
v_accvgpr_read_b32 v[vgprValuC+92], acc192         // copy acc to vreg[192]
v_accvgpr_read_b32 v[vgprValuC+93], acc193         // copy acc to vreg[193]
v_accvgpr_read_b32 v[vgprValuC+94], acc194         // copy acc to vreg[194]
v_accvgpr_read_b32 v[vgprValuC+95], acc195         // copy acc to vreg[195]
v_accvgpr_read_b32 v[vgprValuC+96], acc196         // copy acc to vreg[196]
v_accvgpr_read_b32 v[vgprValuC+97], acc197         // copy acc to vreg[197]
v_accvgpr_read_b32 v[vgprValuC+98], acc198         // copy acc to vreg[198]
v_accvgpr_read_b32 v[vgprValuC+99], acc199         // copy acc to vreg[199]
v_accvgpr_read_b32 v[vgprValuC+100], acc200        // copy acc to vreg[200]
v_accvgpr_read_b32 v[vgprValuC+101], acc201        // copy acc to vreg[201]
v_accvgpr_read_b32 v[vgprValuC+102], acc202        // copy acc to vreg[202]
v_accvgpr_read_b32 v[vgprValuC+103], acc203        // copy acc to vreg[203]
v_accvgpr_read_b32 v[vgprValuC+104], acc204        // copy acc to vreg[204]
v_accvgpr_read_b32 v[vgprValuC+105], acc205        // copy acc to vreg[205]
v_accvgpr_read_b32 v[vgprValuC+106], acc206        // copy acc to vreg[206]
v_accvgpr_read_b32 v[vgprValuC+107], acc207        // copy acc to vreg[207]
v_accvgpr_read_b32 v[vgprValuC+108], acc208        // copy acc to vreg[208]
v_accvgpr_read_b32 v[vgprValuC+109], acc209        // copy acc to vreg[209]
v_accvgpr_read_b32 v[vgprValuC+110], acc210        // copy acc to vreg[210]
v_accvgpr_read_b32 v[vgprValuC+111], acc211        // copy acc to vreg[211]
v_accvgpr_read_b32 v[vgprValuC+112], acc212        // copy acc to vreg[212]
v_accvgpr_read_b32 v[vgprValuC+113], acc213        // copy acc to vreg[213]
v_accvgpr_read_b32 v[vgprValuC+114], acc214        // copy acc to vreg[214]
v_accvgpr_read_b32 v[vgprValuC+115], acc215        // copy acc to vreg[215]

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(6)                               // lgkmcnt(6) = 8 - 1 (bias) - 1 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt lgkmcnt(4)                               // lgkmcnt(4) = 8 - 2 (bias) - 2 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D

s_waitcnt lgkmcnt(2)                               // lgkmcnt(2) = 8 - 3 (bias) - 3 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[40:41], v[vgprValuC+44:vgprValuC+44+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[42:43], v[vgprValuC+46:vgprValuC+46+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[36:37], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[38:39], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt lgkmcnt(0)                               // lgkmcnt(0) = 8 - 4 (bias) - 4 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[52:53], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[54:55], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[48:49], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[50:51], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[12:13], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[14:15], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[60:61], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[28:29], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[30:31], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[40:41], v[vgprValuC+68:vgprValuC+68+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[42:43], v[vgprValuC+70:vgprValuC+70+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+68:vgprValuC+68+1], v[36:37], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[vgprValuC+70:vgprValuC+70+1], v[38:39], v[vgprValuC+70:vgprValuC+70+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+68], v[vgprValuC+68]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+69], v[vgprValuC+69]     // convert C to fp16
v_pack_b32_f16 v68, v[vgprValuC+68], v[vgprValuC+69] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+70], v[vgprValuC+70]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+71], v[vgprValuC+71]     // convert C to fp16
v_pack_b32_f16 v69, v[vgprValuC+70], v[vgprValuC+71] // Pack with neighbor
buffer_store_dwordx2 v[68:69], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[52:53], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[54:55], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[48:49], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[50:51], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[16:17], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[18:19], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[12:13], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[14:15], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[76:77], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[28:29], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[30:31], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[40:41], v[vgprValuC+84:vgprValuC+84+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[42:43], v[vgprValuC+86:vgprValuC+86+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+84:vgprValuC+84+1], v[36:37], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[vgprValuC+86:vgprValuC+86+1], v[38:39], v[vgprValuC+86:vgprValuC+86+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+84], v[vgprValuC+84]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+85], v[vgprValuC+85]     // convert C to fp16
v_pack_b32_f16 v84, v[vgprValuC+84], v[vgprValuC+85] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+86], v[vgprValuC+86]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+87], v[vgprValuC+87]     // convert C to fp16
v_pack_b32_f16 v85, v[vgprValuC+86], v[vgprValuC+87] // Pack with neighbor
buffer_store_dwordx2 v[84:85], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[52:53], v[vgprValuC+88:vgprValuC+88+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[54:55], v[vgprValuC+90:vgprValuC+90+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+88:vgprValuC+88+1], v[48:49], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[vgprValuC+90:vgprValuC+90+1], v[50:51], v[vgprValuC+90:vgprValuC+90+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+88], v[vgprValuC+88]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+89], v[vgprValuC+89]     // convert C to fp16
v_pack_b32_f16 v88, v[vgprValuC+88], v[vgprValuC+89] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+90], v[vgprValuC+90]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+91], v[vgprValuC+91]     // convert C to fp16
v_pack_b32_f16 v89, v[vgprValuC+90], v[vgprValuC+91] // Pack with neighbor
buffer_store_dwordx2 v[88:89], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[16:17], v[vgprValuC+92:vgprValuC+92+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[18:19], v[vgprValuC+94:vgprValuC+94+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+92:vgprValuC+92+1], v[12:13], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[vgprValuC+94:vgprValuC+94+1], v[14:15], v[vgprValuC+94:vgprValuC+94+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+92], v[vgprValuC+92]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+93], v[vgprValuC+93]     // convert C to fp16
v_pack_b32_f16 v92, v[vgprValuC+92], v[vgprValuC+93] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+94], v[vgprValuC+94]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+95], v[vgprValuC+95]     // convert C to fp16
v_pack_b32_f16 v93, v[vgprValuC+94], v[vgprValuC+95] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[92:93], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+96:vgprValuC+96+1], v[28:29], v[vgprValuC+96:vgprValuC+96+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+98:vgprValuC+98+1], v[30:31], v[vgprValuC+98:vgprValuC+98+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+96:vgprValuC+96+1], v[24:25], v[vgprValuC+96:vgprValuC+96+1] // C += bias
v_pk_add_f32 v[vgprValuC+98:vgprValuC+98+1], v[26:27], v[vgprValuC+98:vgprValuC+98+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+96], v[vgprValuC+96]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+97], v[vgprValuC+97]     // convert C to fp16
v_pack_b32_f16 v96, v[vgprValuC+96], v[vgprValuC+97] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+98], v[vgprValuC+98]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+99], v[vgprValuC+99]     // convert C to fp16
v_pack_b32_f16 v97, v[vgprValuC+98], v[vgprValuC+99] // Pack with neighbor
buffer_store_dwordx2 v[96:97], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[40:41], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[42:43], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[36:37], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[38:39], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
buffer_store_dwordx2 v[100:101], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[52:53], v[vgprValuC+104:vgprValuC+104+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[54:55], v[vgprValuC+106:vgprValuC+106+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+104:vgprValuC+104+1], v[48:49], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[vgprValuC+106:vgprValuC+106+1], v[50:51], v[vgprValuC+106:vgprValuC+106+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+104], v[vgprValuC+104]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+105], v[vgprValuC+105]   // convert C to fp16
v_pack_b32_f16 v104, v[vgprValuC+104], v[vgprValuC+105] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+106], v[vgprValuC+106]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+107], v[vgprValuC+107]   // convert C to fp16
v_pack_b32_f16 v105, v[vgprValuC+106], v[vgprValuC+107] // Pack with neighbor
buffer_store_dwordx2 v[104:105], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[16:17], v[vgprValuC+108:vgprValuC+108+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[18:19], v[vgprValuC+110:vgprValuC+110+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+108:vgprValuC+108+1], v[12:13], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[vgprValuC+110:vgprValuC+110+1], v[14:15], v[vgprValuC+110:vgprValuC+110+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+108], v[vgprValuC+108]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+109], v[vgprValuC+109]   // convert C to fp16
v_pack_b32_f16 v108, v[vgprValuC+108], v[vgprValuC+109] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+110], v[vgprValuC+110]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+111], v[vgprValuC+111]   // convert C to fp16
v_pack_b32_f16 v109, v[vgprValuC+110], v[vgprValuC+111] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[108:109], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+112:vgprValuC+112+1], v[28:29], v[vgprValuC+112:vgprValuC+112+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+114:vgprValuC+114+1], v[30:31], v[vgprValuC+114:vgprValuC+114+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+112:vgprValuC+112+1], v[24:25], v[vgprValuC+112:vgprValuC+112+1] // C += bias
v_pk_add_f32 v[vgprValuC+114:vgprValuC+114+1], v[26:27], v[vgprValuC+114:vgprValuC+114+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+112], v[vgprValuC+112]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+113], v[vgprValuC+113]   // convert C to fp16
v_pack_b32_f16 v112, v[vgprValuC+112], v[vgprValuC+113] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+114], v[vgprValuC+114]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+115], v[vgprValuC+115]   // convert C to fp16
v_pack_b32_f16 v113, v[vgprValuC+114], v[vgprValuC+115] // Pack with neighbor
buffer_store_dwordx2 v[112:113], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Batch #3 (d1,d0,vc1,vc0) = */
/*    (1,2,5,0:vw4); (1,3,5,0:vw4); (1,0,6,0:vw4); (1,1,6,0:vw4); (1,2,6,0:vw4); (1,3,6,0:vw4); (1,0,7,0:vw4); (1,1,7,0:vw4); (1,2,7,0:vw4); (1,3,7,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(1,5,2,0) */
ds_read_b128 v[12:15], v8 offset:512               // load Bias
ds_read_b128 v[16:19], v9 offset:512               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,5,3,0) */
ds_read_b128 v[24:27], v8 offset:768               // load Bias
ds_read_b128 v[28:31], v9 offset:768               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,6,0,0) */
ds_read_b128 v[36:39], v8 offset:0                 // load Bias
ds_read_b128 v[40:43], v9 offset:0                 // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,6,1,0) */
ds_read_b128 v[48:51], v8 offset:256               // load Bias
ds_read_b128 v[52:55], v9 offset:256               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,6,2,0) */
/* (d1,vc1,d0,vc0)=(1,6,3,0) */
/* (d1,vc1,d0,vc0)=(1,7,0,0) */
/* (d1,vc1,d0,vc0)=(1,7,1,0) */
/* (d1,vc1,d0,vc0)=(1,7,2,0) */
/* (d1,vc1,d0,vc0)=(1,7,3,0) */
v_accvgpr_read_b32 v[vgprValuC+20], acc216         // copy acc to vreg[216]
v_accvgpr_read_b32 v[vgprValuC+21], acc217         // copy acc to vreg[217]
v_accvgpr_read_b32 v[vgprValuC+22], acc218         // copy acc to vreg[218]
v_accvgpr_read_b32 v[vgprValuC+23], acc219         // copy acc to vreg[219]
v_accvgpr_read_b32 v[vgprValuC+32], acc220         // copy acc to vreg[220]
v_accvgpr_read_b32 v[vgprValuC+33], acc221         // copy acc to vreg[221]
v_accvgpr_read_b32 v[vgprValuC+34], acc222         // copy acc to vreg[222]
v_accvgpr_read_b32 v[vgprValuC+35], acc223         // copy acc to vreg[223]
v_accvgpr_read_b32 v[vgprValuC+44], acc224         // copy acc to vreg[224]
v_accvgpr_read_b32 v[vgprValuC+45], acc225         // copy acc to vreg[225]
v_accvgpr_read_b32 v[vgprValuC+46], acc226         // copy acc to vreg[226]
v_accvgpr_read_b32 v[vgprValuC+47], acc227         // copy acc to vreg[227]
v_accvgpr_read_b32 v[vgprValuC+56], acc228         // copy acc to vreg[228]
v_accvgpr_read_b32 v[vgprValuC+57], acc229         // copy acc to vreg[229]
v_accvgpr_read_b32 v[vgprValuC+58], acc230         // copy acc to vreg[230]
v_accvgpr_read_b32 v[vgprValuC+59], acc231         // copy acc to vreg[231]
v_accvgpr_read_b32 v[vgprValuC+60], acc232         // copy acc to vreg[232]
v_accvgpr_read_b32 v[vgprValuC+61], acc233         // copy acc to vreg[233]
v_accvgpr_read_b32 v[vgprValuC+62], acc234         // copy acc to vreg[234]
v_accvgpr_read_b32 v[vgprValuC+63], acc235         // copy acc to vreg[235]
v_accvgpr_read_b32 v[vgprValuC+64], acc236         // copy acc to vreg[236]
v_accvgpr_read_b32 v[vgprValuC+65], acc237         // copy acc to vreg[237]
v_accvgpr_read_b32 v[vgprValuC+66], acc238         // copy acc to vreg[238]
v_accvgpr_read_b32 v[vgprValuC+67], acc239         // copy acc to vreg[239]
v_accvgpr_read_b32 v[vgprValuC+68], acc240         // copy acc to vreg[240]
v_accvgpr_read_b32 v[vgprValuC+69], acc241         // copy acc to vreg[241]
v_accvgpr_read_b32 v[vgprValuC+70], acc242         // copy acc to vreg[242]
v_accvgpr_read_b32 v[vgprValuC+71], acc243         // copy acc to vreg[243]
v_accvgpr_read_b32 v[vgprValuC+72], acc244         // copy acc to vreg[244]
v_accvgpr_read_b32 v[vgprValuC+73], acc245         // copy acc to vreg[245]
v_accvgpr_read_b32 v[vgprValuC+74], acc246         // copy acc to vreg[246]
v_accvgpr_read_b32 v[vgprValuC+75], acc247         // copy acc to vreg[247]
v_accvgpr_read_b32 v[vgprValuC+76], acc248         // copy acc to vreg[248]
v_accvgpr_read_b32 v[vgprValuC+77], acc249         // copy acc to vreg[249]
v_accvgpr_read_b32 v[vgprValuC+78], acc250         // copy acc to vreg[250]
v_accvgpr_read_b32 v[vgprValuC+79], acc251         // copy acc to vreg[251]
v_accvgpr_read_b32 v[vgprValuC+80], acc252         // copy acc to vreg[252]
v_accvgpr_read_b32 v[vgprValuC+81], acc253         // copy acc to vreg[253]
v_accvgpr_read_b32 v[vgprValuC+82], acc254         // copy acc to vreg[254]
v_accvgpr_read_b32 v[vgprValuC+83], acc255         // copy acc to vreg[255]

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(6)                               // lgkmcnt(6) = 8 - 1 (bias) - 1 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt lgkmcnt(4)                               // lgkmcnt(4) = 8 - 2 (bias) - 2 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D

s_waitcnt lgkmcnt(2)                               // lgkmcnt(2) = 8 - 3 (bias) - 3 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[40:41], v[vgprValuC+44:vgprValuC+44+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[42:43], v[vgprValuC+46:vgprValuC+46+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[36:37], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[38:39], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt lgkmcnt(0)                               // lgkmcnt(0) = 8 - 4 (bias) - 4 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[52:53], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[54:55], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[48:49], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[50:51], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[12:13], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[14:15], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
buffer_store_dwordx2 v[60:61], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[28:29], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[30:31], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[40:41], v[vgprValuC+68:vgprValuC+68+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[42:43], v[vgprValuC+70:vgprValuC+70+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+68:vgprValuC+68+1], v[36:37], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[vgprValuC+70:vgprValuC+70+1], v[38:39], v[vgprValuC+70:vgprValuC+70+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+68], v[vgprValuC+68]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+69], v[vgprValuC+69]     // convert C to fp16
v_pack_b32_f16 v68, v[vgprValuC+68], v[vgprValuC+69] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+70], v[vgprValuC+70]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+71], v[vgprValuC+71]     // convert C to fp16
v_pack_b32_f16 v69, v[vgprValuC+70], v[vgprValuC+71] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[68:69], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[52:53], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[54:55], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[48:49], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[50:51], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[16:17], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[18:19], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[12:13], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[14:15], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
buffer_store_dwordx2 v[76:77], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[28:29], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[30:31], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End                              // jump to end
label_GW_End:

s_endpgm                                           // Kernel End
label_OptNLL_End:
label_GSU_3:

/******************************************/
/* Ord. NoLoadLoop - Begin 1/2            */
/******************************************/
s_bitcmp1_b32 s[sgprOrigLoopCounter], 0x0          // test if OrigLoopCounter is Odd ?
s_cbranch_scc1 label_OrdNLL_second                 // jump to second NoLoadLoop
s_waitcnt lgkmcnt(0)                               // 4wait for local write
// Skip force waitcnt0
s_barrier

/* iter 0 (last unrolled loop) */
s_waitcnt vmcnt(0)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:9, lwStartMfmaIndex:63, lwEndMfmaIndex:63  */
/*  numMfmaForLR:30, syncPlrMfmaIndex:0  */
/*  mfmaIndex:0  */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+3], v[vgprLocalReadAddrB] offset:128 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+3], v[vgprLocalReadAddrB] offset:192 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+3], v[vgprLocalReadAddrB] offset:256 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+3], v[vgprLocalReadAddrB] offset:320 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+3], v[vgprLocalReadAddrB] offset:384 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+28:vgprValuB_X0_I0+28+3], v[vgprLocalReadAddrB] offset:448 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+32:vgprValuB_X0_I0+32+3], v[vgprLocalReadAddrB] offset:8704 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+36:vgprValuB_X0_I0+36+3], v[vgprLocalReadAddrB] offset:8768 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+40:vgprValuB_X0_I0+40+3], v[vgprLocalReadAddrB] offset:8832 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+44:vgprValuB_X0_I0+44+3], v[vgprLocalReadAddrB] offset:8896 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+48:vgprValuB_X0_I0+48+3], v[vgprLocalReadAddrB] offset:8960 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+52:vgprValuB_X0_I0+52+3], v[vgprLocalReadAddrB] offset:9024 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+56:vgprValuB_X0_I0+56+3], v[vgprLocalReadAddrB] offset:9088 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+60:vgprValuB_X0_I0+60+3], v[vgprLocalReadAddrB] offset:9152 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0 for iteration == 0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
/* localReadsVacancy: latencyLeft 2 */

/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIterDTV] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s28, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s29, s[sgprWrapUA+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
/* localReadsVacancy: latencyLeft 2 */
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s28        // gra SRD += inc(lower)
s_waitcnt lgkmcnt(15)                              // wait for prior local read local write
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
/* localReadsVacancy: latencyLeft 2 */
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s29       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
/* localReadsVacancy: latencyLeft 2 */
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s28 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
/* localReadsVacancy: latencyLeft 2 */
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s29 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
/* localReadsVacancy: latencyLeft 2 */
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:16  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[64:67], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:17  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[68:71], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:18  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[72:75], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[76:79], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[80:83], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:21  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[84:87], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:22  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[88:91], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:23  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[92:95], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:24  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[96:99], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:25  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[100:103], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:26  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[104:107], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:27  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[108:111], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:28  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[112:115], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:29  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[116:119], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:30  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[120:123], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:31  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[124:127], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:32  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[128:131], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:33  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[132:135], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:34  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[136:139], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:35  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[140:143], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:36  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[144:147], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:37  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[148:151], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:38  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[152:155], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:39  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[156:159], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:40  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[160:163], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:41  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[164:167], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:42  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[168:171], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:43  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[172:175], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:44  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[176:179], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:45  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[180:183], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:46  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[184:187], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:47  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[188:191], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:48  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[192:195], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:49  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[196:199], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:50  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[200:203], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:51  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[204:207], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:52  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[208:211], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:53  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[212:215], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:54  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[216:219], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:55  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[220:223], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
/*  mfmaIndex:56  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[224:227], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[224:227] // left value = acc[224+0:227+0]
/*  mfmaIndex:57  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[228:231], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[228:231] // left value = acc[228+0:231+0]
/*  mfmaIndex:58  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[232:235], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[232:235] // left value = acc[232+0:235+0]
/*  mfmaIndex:59  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[236:239], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[236:239] // left value = acc[236+0:239+0]
/*  mfmaIndex:60  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[240:243], v[vgprG2LA+16+0:vgprG2LA+16+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[240:243] // left value = acc[240+0:243+0]
/*  mfmaIndex:61  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[244:247], v[vgprG2LA+20+0:vgprG2LA+20+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[244:247] // left value = acc[244+0:247+0]
/*  mfmaIndex:62  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[248:251], v[vgprG2LA+24+0:vgprG2LA+24+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[248:251] // left value = acc[248+0:251+0]
/*  mfmaIndex:63  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[252:255], v[vgprG2LA+28+0:vgprG2LA+28+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[252:255] // left value = acc[252+0:255+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=16 */

/* iter 1 (last unrolled loop) */
/*  grEndMfmaIndex:9, lwStartMfmaIndex:63, lwEndMfmaIndex:63  */
/*  numMfmaForLR:30, syncPlrMfmaIndex:0  */
/*  mfmaIndex:64  */
/* localReadsVacancy: latencyLeft 2 */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:65  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:66  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:67  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:68  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:69  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:70  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:71  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:72  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:73  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:74  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:75  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:76  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:77  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:78  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:79  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:80  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[64:67], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:81  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[68:71], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:82  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[72:75], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:83  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[76:79], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:84  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[80:83], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:85  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[84:87], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:86  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[88:91], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:87  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[92:95], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:88  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[96:99], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:89  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[100:103], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:90  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[104:107], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:91  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[108:111], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:92  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[112:115], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:93  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[116:119], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:94  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[120:123], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:95  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[124:127], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:96  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[128:131], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:97  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[132:135], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:98  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[136:139], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:99  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[140:143], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:100  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[144:147], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:101  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[148:151], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:102  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[152:155], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:103  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[156:159], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:104  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[160:163], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:105  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[164:167], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:106  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[168:171], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:107  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[172:175], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:108  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[176:179], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:109  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[180:183], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:110  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[184:187], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:111  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[188:191], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:112  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[192:195], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:113  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[196:199], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:114  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[200:203], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:115  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[204:207], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:116  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[208:211], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:117  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[212:215], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:118  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[216:219], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:119  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[220:223], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
/*  mfmaIndex:120  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[224:227], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[224:227] // left value = acc[224+0:227+0]
/*  mfmaIndex:121  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[228:231], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[228:231] // left value = acc[228+0:231+0]
/*  mfmaIndex:122  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[232:235], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[232:235] // left value = acc[232+0:235+0]
/*  mfmaIndex:123  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[236:239], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[236:239] // left value = acc[236+0:239+0]
/*  mfmaIndex:124  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[240:243], v[vgprG2LA+16+2:vgprG2LA+16+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[240:243] // left value = acc[240+0:243+0]
/*  mfmaIndex:125  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[244:247], v[vgprG2LA+20+2:vgprG2LA+20+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[244:247] // left value = acc[244+0:247+0]
/*  mfmaIndex:126  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[248:251], v[vgprG2LA+24+2:vgprG2LA+24+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[248:251] // left value = acc[248+0:251+0]
/*  mfmaIndex:127  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[252:255], v[vgprG2LA+28+2:vgprG2LA+28+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[252:255] // left value = acc[252+0:255+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=16 */
s_branch label_toPGR1end_OrdNLL_0                  // Branch to toPGR1end

/******************************************/
/* Ord. NoLoadLoop - Begin 2/2            */
/******************************************/
label_OrdNLL_second:  /// second Ord NoLoadLoop entry
s_waitcnt lgkmcnt(0)                               // 4wait for local write
// Skip force waitcnt0
s_barrier

/* iter 0 (last unrolled loop) */
s_waitcnt vmcnt(0)                                 // global read wait for DirectToVgpr
/*  grEndMfmaIndex:9, lwStartMfmaIndex:63, lwEndMfmaIndex:63  */
/*  numMfmaForLR:30, syncPlrMfmaIndex:0  */
/*  mfmaIndex:0  */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+3], v[vgprLocalReadAddrB] offset:128 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+3], v[vgprLocalReadAddrB] offset:192 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+3], v[vgprLocalReadAddrB] offset:256 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+3], v[vgprLocalReadAddrB] offset:320 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+3], v[vgprLocalReadAddrB] offset:384 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+28:vgprValuB_X0_I0+28+3], v[vgprLocalReadAddrB] offset:448 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+32:vgprValuB_X0_I0+32+3], v[vgprLocalReadAddrB] offset:8704 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+36:vgprValuB_X0_I0+36+3], v[vgprLocalReadAddrB] offset:8768 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+40:vgprValuB_X0_I0+40+3], v[vgprLocalReadAddrB] offset:8832 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+44:vgprValuB_X0_I0+44+3], v[vgprLocalReadAddrB] offset:8896 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+48:vgprValuB_X0_I0+48+3], v[vgprLocalReadAddrB] offset:8960 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+52:vgprValuB_X0_I0+52+3], v[vgprLocalReadAddrB] offset:9024 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+56:vgprValuB_X0_I0+56+3], v[vgprLocalReadAddrB] offset:9088 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+60:vgprValuB_X0_I0+60+3], v[vgprLocalReadAddrB] offset:9152 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0 for iteration == 0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
/* localReadsVacancy: latencyLeft 2 */

/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIterDTV] // Is this the wrapIter?
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s28, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s29, s[sgprWrapUA+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
/* localReadsVacancy: latencyLeft 2 */
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s28        // gra SRD += inc(lower)
s_waitcnt lgkmcnt(15)                              // wait for prior local read local write
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
/* localReadsVacancy: latencyLeft 2 */
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s29       // gra SRD += inc(upper)
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
/* localReadsVacancy: latencyLeft 2 */
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s28 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
/* localReadsVacancy: latencyLeft 2 */
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s29 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
/* localReadsVacancy: latencyLeft 2 */
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
/* localReadsVacancy: latencyLeft 2 */
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:16  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[64:67], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:17  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[68:71], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:18  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[72:75], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[76:79], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[80:83], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:21  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[84:87], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:22  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[88:91], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:23  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[92:95], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:24  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[96:99], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:25  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[100:103], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:26  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[104:107], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:27  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[108:111], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:28  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[112:115], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:29  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[116:119], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:30  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[120:123], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:31  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[124:127], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:32  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[128:131], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:33  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[132:135], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:34  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[136:139], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:35  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[140:143], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:36  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[144:147], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:37  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[148:151], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:38  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[152:155], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:39  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[156:159], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:40  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[160:163], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:41  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[164:167], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:42  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[168:171], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:43  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[172:175], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:44  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[176:179], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:45  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[180:183], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:46  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[184:187], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:47  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[188:191], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:48  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[192:195], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:49  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[196:199], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:50  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[200:203], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:51  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[204:207], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:52  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[208:211], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:53  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[212:215], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:54  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[216:219], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:55  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[220:223], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
/*  mfmaIndex:56  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[224:227], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[224:227] // left value = acc[224+0:227+0]
/*  mfmaIndex:57  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[228:231], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[228:231] // left value = acc[228+0:231+0]
/*  mfmaIndex:58  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[232:235], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[232:235] // left value = acc[232+0:235+0]
/*  mfmaIndex:59  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[236:239], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[236:239] // left value = acc[236+0:239+0]
/*  mfmaIndex:60  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[240:243], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[240:243] // left value = acc[240+0:243+0]
/*  mfmaIndex:61  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[244:247], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[244:247] // left value = acc[244+0:247+0]
/*  mfmaIndex:62  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[248:251], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[248:251] // left value = acc[248+0:251+0]
/*  mfmaIndex:63  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[252:255], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[252:255] // left value = acc[252+0:255+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=16 */

/* iter 1 (last unrolled loop) */
/*  grEndMfmaIndex:9, lwStartMfmaIndex:63, lwEndMfmaIndex:63  */
/*  numMfmaForLR:30, syncPlrMfmaIndex:0  */
/*  mfmaIndex:64  */
/* localReadsVacancy: latencyLeft 2 */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:65  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:66  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:67  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:68  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:69  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:70  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:71  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:72  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:73  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:74  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:75  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:76  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:77  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:78  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:79  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:80  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[64:67], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:81  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[68:71], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:82  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[72:75], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:83  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[76:79], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:84  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[80:83], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:85  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[84:87], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:86  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[88:91], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:87  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[92:95], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:88  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[96:99], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:89  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[100:103], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:90  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[104:107], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:91  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[108:111], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:92  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[112:115], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:93  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[116:119], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:94  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[120:123], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:95  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[124:127], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:96  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[128:131], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:97  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[132:135], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:98  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[136:139], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:99  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[140:143], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:100  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[144:147], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:101  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[148:151], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:102  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[152:155], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:103  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[156:159], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:104  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[160:163], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:105  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[164:167], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:106  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[168:171], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:107  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[172:175], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:108  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[176:179], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:109  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[180:183], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:110  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[184:187], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:111  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[188:191], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:112  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[192:195], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:113  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[196:199], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:114  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[200:203], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:115  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[204:207], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:116  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[208:211], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:117  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[212:215], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:118  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[216:219], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:119  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[220:223], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
/*  mfmaIndex:120  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[224:227], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[224:227] // left value = acc[224+0:227+0]
/*  mfmaIndex:121  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[228:231], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[228:231] // left value = acc[228+0:231+0]
/*  mfmaIndex:122  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[232:235], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[232:235] // left value = acc[232+0:235+0]
/*  mfmaIndex:123  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[236:239], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[236:239] // left value = acc[236+0:239+0]
/*  mfmaIndex:124  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[240:243], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[240:243] // left value = acc[240+0:243+0]
/*  mfmaIndex:125  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[244:247], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[244:247] // left value = acc[244+0:247+0]
/*  mfmaIndex:126  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[248:251], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[248:251] // left value = acc[248+0:251+0]
/*  mfmaIndex:127  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16_f16 acc[252:255], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[252:255] // left value = acc[252+0:255+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=16 */
label_toPGR1end_OrdNLL_0:
label_PrefetchGlobalLastIterEnd:

/******************************************/
/* Tail Loop                              */
/******************************************/

/* Tail: add ValuA/B vgpr buffer [0...80) to pool */

/* local write reset offsets a */

/* local write reset offsets b */
v_and_b32 v[vgprLocalWriteAddrB], 0xf07fff, v[vgprLocalWriteAddrB] // reset to Red

// numIterL = LOCAL_SPLITU * min(sizeL % LOCAL_DEPTHU, DEPTHU / LOCAL_SPLITU)
s_and_b32 s[sgprLoopCounterL], 31, s[sgprSizesSum+0] // s[sgprLoopCounterL] = s[sgprSizesSum+0] % 32
s_and_b32 s28, s[sgprGSU], 0x8000                  // SCC = (GSUC == 1) ?
s_cbranch_scc1 label_GSUC_TL                       // branch if GSUC == 1
s_cmp_lg_u32 s[sgprGSUSumIdx], s[sgprGSUSumIdx+1]  // gsuSumIdx == numIterPerWgRemainder
s_cmov_b32 s[sgprLoopCounterL], 0x0                // numIter=0 if gsuSimIdx != numIterPerWgRemainder
s_branch label_GSUC_TL_End
label_GSUC_TL:
s_lshr_b32 s29, s[sgprSizesSum], 5                 // s29 = s[sgprSizesSum] / 32
s_and_b32 s30, s[sgprGSU], 0x3fff                  // Restore GSU
v_cvt_f32_u32 v0, s30                              // s28 = s29 / s30
v_rcp_iflag_f32 v0, v0                             // s28 = s29 / s30
v_cvt_f32_u32 v1, s29                              // s28 = s29 / s30
v_mul_f32 v0, v0, v1                               // s28 = s29 / s30
v_cvt_u32_f32 v0, v0                               // s28 = s29 / s30
v_mul_u32_u24 v1, v0, s30                          // s28 = s29 / s30
v_sub_u32 v1, s29, v1                              // s28 = s29 / s30
v_cmpx_eq_u32 exec, v1, s30                        // s28 = s29 / s30
v_add_u32 v0, 1, v0                                // s28 = s29 / s30
v_mov_b32 v1, 0                                    // s[sgprGSUSumIdx+1] = s29 % s30
s_mov_b64 exec, -1                                 // s28 = s29 / s30
v_readfirstlane_b32 s28, v0                        // quotient
v_readfirstlane_b32 s[sgprGSUSumIdx+1], v1         // remainder
s_sub_u32 s29, s30, 1                              // GSU-1
s_cmp_eq_u32 s28, 0                                // quotient == 0
s_cselect_b32 s28, s[sgprGSUSumIdx+1], s29         // lastWg = (quotient==0) ? numIterPerWgRemainder : GSU-1
s_cmp_lg_u32 s[sgprGSUSumIdx], s28                 // gsuSumIdx == lastWg
s_cmov_b32 s[sgprLoopCounterL], 0x0                // numIter=0 if gsuSumIdx != lastWg
label_GSUC_TL_End:
s_cmp_eq_u32 s[sgprLoopCounterL], 0x0              // numIterL == 0
s_mov_b32 s[sgprOrigLoopCounter], 0                // repurpose to count each localRead increment
s_cbranch_scc1 label_SkipTailLoopL                 // skip to end of tail loop b/c numIter==0

/* remove stagger offsets for tail loop */
s_sub_i32 s28, 3, s[sgprStaggerUIter]
s_mul_hi_i32 s29, s28, s[sgprGlobalReadIncsA+0]    // start offset S in bytes
s_mul_i32 s28, s28, s[sgprGlobalReadIncsA+0]       // start offset S in bytes
s_sub_u32 s28, s28, s[sgprWrapUA]                  // S - WrapU
s_subb_u32 s29, s29, s[sgprWrapUA+1]               // S - WrapU
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s28        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s29       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s28 // limit -= inc)
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s29 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
s_sub_i32 s28, 3, s[sgprStaggerUIter]
s_mul_hi_i32 s29, s28, s[sgprGlobalReadIncsB+0]    // start offset S in bytes
s_mul_i32 s28, s28, s[sgprGlobalReadIncsB+0]       // start offset S in bytes
s_sub_u32 s28, s28, s[sgprWrapUB]                  // S - WrapU
s_subb_u32 s29, s29, s[sgprWrapUB+1]               // S - WrapU
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s28        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s29       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s28 // limit -= inc)
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s29 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32

/* Update M0 for DTLDS */

/* global read A */
/* g2l=0, load component 0 */
buffer_load_short_d16 v[vgprG2LA+0+0], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // load one buffer value
/* g2l=0, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+0+0], v[vgprG2LA+0+0], v0      // HasEccHalf: pack
/* g2l=0, load component 2 */
buffer_load_short_d16 v[vgprG2LA+0+1], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:4 // load one buffer value
/* g2l=0, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+0+1], v[vgprG2LA+0+1], v0      // HasEccHalf: pack
/* g2l=0, load component 4 */
buffer_load_short_d16 v[vgprG2LA+0+2], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:8 // load one buffer value
/* g2l=0, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+0+2], v[vgprG2LA+0+2], v0      // HasEccHalf: pack
/* g2l=0, load component 6 */
buffer_load_short_d16 v[vgprG2LA+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:12 // load one buffer value
/* g2l=0, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+0+3], v[vgprG2LA+0+3], v0      // HasEccHalf: pack
/* g2l=4, load component 0 */
buffer_load_short_d16 v[vgprG2LA+4+0], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // load one buffer value
/* g2l=4, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+4+0], v[vgprG2LA+4+0], v0      // HasEccHalf: pack
/* g2l=4, load component 2 */
buffer_load_short_d16 v[vgprG2LA+4+1], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:4 // load one buffer value
/* g2l=4, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+4+1], v[vgprG2LA+4+1], v0      // HasEccHalf: pack
/* g2l=4, load component 4 */
buffer_load_short_d16 v[vgprG2LA+4+2], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:8 // load one buffer value
/* g2l=4, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+4+2], v[vgprG2LA+4+2], v0      // HasEccHalf: pack
/* g2l=4, load component 6 */
buffer_load_short_d16 v[vgprG2LA+4+3], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:12 // load one buffer value
/* g2l=4, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+4+3], v[vgprG2LA+4+3], v0      // HasEccHalf: pack
/* g2l=8, load component 0 */
buffer_load_short_d16 v[vgprG2LA+8+0], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // load one buffer value
/* g2l=8, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+8+0], v[vgprG2LA+8+0], v0      // HasEccHalf: pack
/* g2l=8, load component 2 */
buffer_load_short_d16 v[vgprG2LA+8+1], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:4 // load one buffer value
/* g2l=8, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+8+1], v[vgprG2LA+8+1], v0      // HasEccHalf: pack
/* g2l=8, load component 4 */
buffer_load_short_d16 v[vgprG2LA+8+2], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:8 // load one buffer value
/* g2l=8, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+8+2], v[vgprG2LA+8+2], v0      // HasEccHalf: pack
/* g2l=8, load component 6 */
buffer_load_short_d16 v[vgprG2LA+8+3], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:12 // load one buffer value
/* g2l=8, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+8+3], v[vgprG2LA+8+3], v0      // HasEccHalf: pack
/* g2l=12, load component 0 */
buffer_load_short_d16 v[vgprG2LA+12+0], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // load one buffer value
/* g2l=12, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+12+0], v[vgprG2LA+12+0], v0    // HasEccHalf: pack
/* g2l=12, load component 2 */
buffer_load_short_d16 v[vgprG2LA+12+1], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:4 // load one buffer value
/* g2l=12, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+12+1], v[vgprG2LA+12+1], v0    // HasEccHalf: pack
/* g2l=12, load component 4 */
buffer_load_short_d16 v[vgprG2LA+12+2], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:8 // load one buffer value
/* g2l=12, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+12+2], v[vgprG2LA+12+2], v0    // HasEccHalf: pack
/* g2l=12, load component 6 */
buffer_load_short_d16 v[vgprG2LA+12+3], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:12 // load one buffer value
/* g2l=12, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LA+12+3], v[vgprG2LA+12+3], v0    // HasEccHalf: pack

/* Update M0 for DTLDS */

/* global read B */
/* g2l=0, load component 0 */
buffer_load_short_d16 v[vgprG2LB+0+0], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // load one buffer value
/* g2l=0, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+0+0], v[vgprG2LB+0+0], v0      // HasEccHalf: pack
/* g2l=0, load component 2 */
buffer_load_short_d16 v[vgprG2LB+0+1], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:4 // load one buffer value
/* g2l=0, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+0+1], v[vgprG2LB+0+1], v0      // HasEccHalf: pack
/* g2l=0, load component 4 */
buffer_load_short_d16 v[vgprG2LB+0+2], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:8 // load one buffer value
/* g2l=0, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+0+2], v[vgprG2LB+0+2], v0      // HasEccHalf: pack
/* g2l=0, load component 6 */
buffer_load_short_d16 v[vgprG2LB+0+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:12 // load one buffer value
/* g2l=0, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+0+3], v[vgprG2LB+0+3], v0      // HasEccHalf: pack
/* g2l=4, load component 0 */
buffer_load_short_d16 v[vgprG2LB+4+0], v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // load one buffer value
/* g2l=4, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+4+0], v[vgprG2LB+4+0], v0      // HasEccHalf: pack
/* g2l=4, load component 2 */
buffer_load_short_d16 v[vgprG2LB+4+1], v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:4 // load one buffer value
/* g2l=4, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+4+1], v[vgprG2LB+4+1], v0      // HasEccHalf: pack
/* g2l=4, load component 4 */
buffer_load_short_d16 v[vgprG2LB+4+2], v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:8 // load one buffer value
/* g2l=4, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+4+2], v[vgprG2LB+4+2], v0      // HasEccHalf: pack
/* g2l=4, load component 6 */
buffer_load_short_d16 v[vgprG2LB+4+3], v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:12 // load one buffer value
/* g2l=4, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+1], s[sgprSrdB:sgprSrdB+3], 0 offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+4+3], v[vgprG2LB+4+3], v0      // HasEccHalf: pack
/* g2l=8, load component 0 */
buffer_load_short_d16 v[vgprG2LB+8+0], v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // load one buffer value
/* g2l=8, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+8+0], v[vgprG2LB+8+0], v0      // HasEccHalf: pack
/* g2l=8, load component 2 */
buffer_load_short_d16 v[vgprG2LB+8+1], v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:4 // load one buffer value
/* g2l=8, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+8+1], v[vgprG2LB+8+1], v0      // HasEccHalf: pack
/* g2l=8, load component 4 */
buffer_load_short_d16 v[vgprG2LB+8+2], v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:8 // load one buffer value
/* g2l=8, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+8+2], v[vgprG2LB+8+2], v0      // HasEccHalf: pack
/* g2l=8, load component 6 */
buffer_load_short_d16 v[vgprG2LB+8+3], v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:12 // load one buffer value
/* g2l=8, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+2], s[sgprSrdB:sgprSrdB+3], 0 offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+8+3], v[vgprG2LB+8+3], v0      // HasEccHalf: pack
/* g2l=12, load component 0 */
buffer_load_short_d16 v[vgprG2LB+12+0], v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // load one buffer value
/* g2l=12, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+12+0], v[vgprG2LB+12+0], v0    // HasEccHalf: pack
/* g2l=12, load component 2 */
buffer_load_short_d16 v[vgprG2LB+12+1], v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:4 // load one buffer value
/* g2l=12, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+12+1], v[vgprG2LB+12+1], v0    // HasEccHalf: pack
/* g2l=12, load component 4 */
buffer_load_short_d16 v[vgprG2LB+12+2], v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:8 // load one buffer value
/* g2l=12, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+12+2], v[vgprG2LB+12+2], v0    // HasEccHalf: pack
/* g2l=12, load component 6 */
buffer_load_short_d16 v[vgprG2LB+12+3], v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:12 // load one buffer value
/* g2l=12, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+3], s[sgprSrdB:sgprSrdB+3], 0 offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+12+3], v[vgprG2LB+12+3], v0    // HasEccHalf: pack
s_waitcnt vmcnt(0)                                 // 2wait for global read
// Skip force waitcnt0
s_barrier

/* local write a */

/* local write b */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+3] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+4:vgprG2LB+4+3] offset:4352 // lwoB_0_0_1_0 = (0*LSCB)*(MT1J+PAD) + (1*LSPB) = 4352
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+8:vgprG2LB+8+3] offset:8704 // lwoB_0_0_2_0 = (0*LSCB)*(MT1J+PAD) + (2*LSPB) = 8704
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+12:vgprG2LB+12+3] offset:13056 // lwoB_0_0_3_0 = (0*LSCB)*(MT1J+PAD) + (3*LSPB) = 13056
s_waitcnt lgkmcnt(0)                               // 5wait for local write
// Skip force waitcnt0
s_barrier

/* local read reset offsets a */

/* local read reset offsets b */

/* localReadResetOffsets */
/* handled internally */
v_and_b32 v[vgprLocalReadAddrB], 0x7fff, v[vgprLocalReadAddrB] // reset Red,Blk -> Red

/* local read init pointers a */

/* local read init pointers b */

/* localReadInitPointers */

/* tail loop: macs */
label_TailLoopBeginL:

/* Tail: remove ValuA/B vgpr buffer [0...80) from pool */

/* Tail: add address/G2L vgpr [80...139) to pool */

/* tail loop unroll iter 0 */

/* local read a */

/* local read b */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+3], v[vgprLocalReadAddrB] offset:128 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+3], v[vgprLocalReadAddrB] offset:192 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+3], v[vgprLocalReadAddrB] offset:256 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+3], v[vgprLocalReadAddrB] offset:320 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+3], v[vgprLocalReadAddrB] offset:384 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+28:vgprValuB_X0_I0+28+3], v[vgprLocalReadAddrB] offset:448 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=0 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+32:vgprValuB_X0_I0+32+3], v[vgprLocalReadAddrB] offset:8704 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+36:vgprValuB_X0_I0+36+3], v[vgprLocalReadAddrB] offset:8768 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=1 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+40:vgprValuB_X0_I0+40+3], v[vgprLocalReadAddrB] offset:8832 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=2 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+44:vgprValuB_X0_I0+44+3], v[vgprLocalReadAddrB] offset:8896 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=3 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+48:vgprValuB_X0_I0+48+3], v[vgprLocalReadAddrB] offset:8960 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=4 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+52:vgprValuB_X0_I0+52+3], v[vgprLocalReadAddrB] offset:9024 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=5 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+56:vgprValuB_X0_I0+56+3], v[vgprLocalReadAddrB] offset:9088 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=6 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+60:vgprValuB_X0_I0+60+3], v[vgprLocalReadAddrB] offset:9152 // L -> Reg lro=0 swapByteOffset=0 ti=128 vIdx=1 eIdx=7 rIdx=0 oIdx=0 buffer=0 iui=0

/* local read inc a */

/* local read inc b */
s_mov_b32 s8, 0x40                                 // inc
v_add_co_u32 v[vgprLocalReadAddrB], vcc, s8, v[vgprLocalReadAddrB] // lrB += 64 (bpeDS)
s_waitcnt lgkmcnt(0)                               // 4wait for local read
v_and_b32 v80, 63, v[vgprSerial]                   // v80 = v[vgprSerial] % 64
v_lshrrev_b32 v80, 4, v80                          // v80 = v80 / 16
v_lshlrev_b32 v80, 0x3, v80                        // v80 = v80 * 8
v_cmp_ge_i32 s[28:29], v80, s[sgprLoopCounterL]    // check K index >= Size L
v_cndmask_b32 v[vgprG2LA+0+0+0], v[vgprG2LA+0+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+4+0+0], v[vgprG2LA+4+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+8+0+0], v[vgprG2LA+8+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+12+0+0], v[vgprG2LA+12+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+0+0+1], v[vgprG2LA+0+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+4+0+1], v[vgprG2LA+4+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+8+0+1], v[vgprG2LA+8+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+12+0+1], v[vgprG2LA+12+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+0+0+0+0], v[vgprValuB_X0_I0+0+0+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+4+0+0+0], v[vgprValuB_X0_I0+4+0+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+8+0+0+0], v[vgprValuB_X0_I0+8+0+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+12+0+0+0], v[vgprValuB_X0_I0+12+0+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+16+0+0+0], v[vgprValuB_X0_I0+16+0+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+20+0+0+0], v[vgprValuB_X0_I0+20+0+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+24+0+0+0], v[vgprValuB_X0_I0+24+0+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+28+0+0+0], v[vgprValuB_X0_I0+28+0+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+32+0+0+0], v[vgprValuB_X0_I0+32+0+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+36+0+0+0], v[vgprValuB_X0_I0+36+0+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+40+0+0+0], v[vgprValuB_X0_I0+40+0+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+44+0+0+0], v[vgprValuB_X0_I0+44+0+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+48+0+0+0], v[vgprValuB_X0_I0+48+0+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+52+0+0+0], v[vgprValuB_X0_I0+52+0+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+56+0+0+0], v[vgprValuB_X0_I0+56+0+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+60+0+0+0], v[vgprValuB_X0_I0+60+0+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+0+0+0+1], v[vgprValuB_X0_I0+0+0+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+4+0+0+1], v[vgprValuB_X0_I0+4+0+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+8+0+0+1], v[vgprValuB_X0_I0+8+0+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+12+0+0+1], v[vgprValuB_X0_I0+12+0+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+16+0+0+1], v[vgprValuB_X0_I0+16+0+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+20+0+0+1], v[vgprValuB_X0_I0+20+0+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+24+0+0+1], v[vgprValuB_X0_I0+24+0+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+28+0+0+1], v[vgprValuB_X0_I0+28+0+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+32+0+0+1], v[vgprValuB_X0_I0+32+0+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+36+0+0+1], v[vgprValuB_X0_I0+36+0+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+40+0+0+1], v[vgprValuB_X0_I0+40+0+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+44+0+0+1], v[vgprValuB_X0_I0+44+0+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+48+0+0+1], v[vgprValuB_X0_I0+48+0+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+52+0+0+1], v[vgprValuB_X0_I0+52+0+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+56+0+0+1], v[vgprValuB_X0_I0+56+0+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+60+0+0+1], v[vgprValuB_X0_I0+60+0+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_sub_u32 v80, s[sgprLoopCounterL], v80            // get distance between size and k index
v_cmp_lt_i32 s[28:29], v80, 4                      // set partial 0 if distance less than input per thread
s_and_b32 s30, s[sgprLoopCounterL], 3              // get inputs for edge thread
s_sub_u32 s30, 4, s30                              // use shift to fill 0 for outside element
s_lshl_b32 s30, s30, 4                             // use shift to fill 0 for outside element
v_lshlrev_b64 v[82:83], s30, v[vgprG2LA+0+0:vgprG2LA+0+0+1]
v_cndmask_b32 v[vgprG2LA+0+0+0], v[vgprG2LA+0+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprG2LA+0+0+1], v[vgprG2LA+0+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprG2LA+4+0:vgprG2LA+4+0+1]
v_cndmask_b32 v[vgprG2LA+4+0+0], v[vgprG2LA+4+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprG2LA+4+0+1], v[vgprG2LA+4+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprG2LA+8+0:vgprG2LA+8+0+1]
v_cndmask_b32 v[vgprG2LA+8+0+0], v[vgprG2LA+8+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprG2LA+8+0+1], v[vgprG2LA+8+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprG2LA+12+0:vgprG2LA+12+0+1]
v_cndmask_b32 v[vgprG2LA+12+0+0], v[vgprG2LA+12+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprG2LA+12+0+1], v[vgprG2LA+12+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+0+0+0+0], v[vgprValuB_X0_I0+0+0+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+0+0+0+1], v[vgprValuB_X0_I0+0+0+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+4+0+0+0], v[vgprValuB_X0_I0+4+0+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+4+0+0+1], v[vgprValuB_X0_I0+4+0+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+8+0+0+0], v[vgprValuB_X0_I0+8+0+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+8+0+0+1], v[vgprValuB_X0_I0+8+0+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+12+0+0+0], v[vgprValuB_X0_I0+12+0+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+12+0+0+1], v[vgprValuB_X0_I0+12+0+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+16+0+0+0], v[vgprValuB_X0_I0+16+0+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+16+0+0+1], v[vgprValuB_X0_I0+16+0+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+20+0+0+0], v[vgprValuB_X0_I0+20+0+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+20+0+0+1], v[vgprValuB_X0_I0+20+0+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+24+0+0+0], v[vgprValuB_X0_I0+24+0+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+24+0+0+1], v[vgprValuB_X0_I0+24+0+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+28+0+0+0], v[vgprValuB_X0_I0+28+0+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+28+0+0+1], v[vgprValuB_X0_I0+28+0+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+32+0+0+0], v[vgprValuB_X0_I0+32+0+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+32+0+0+1], v[vgprValuB_X0_I0+32+0+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+36+0+0+0], v[vgprValuB_X0_I0+36+0+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+36+0+0+1], v[vgprValuB_X0_I0+36+0+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+40+0+0+0], v[vgprValuB_X0_I0+40+0+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+40+0+0+1], v[vgprValuB_X0_I0+40+0+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+44+0+0+0], v[vgprValuB_X0_I0+44+0+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+44+0+0+1], v[vgprValuB_X0_I0+44+0+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+48+0+0+0], v[vgprValuB_X0_I0+48+0+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+48+0+0+1], v[vgprValuB_X0_I0+48+0+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+52+0+0+0], v[vgprValuB_X0_I0+52+0+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+52+0+0+1], v[vgprValuB_X0_I0+52+0+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+56+0+0+0], v[vgprValuB_X0_I0+56+0+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+56+0+0+1], v[vgprValuB_X0_I0+56+0+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+60+0+0+0], v[vgprValuB_X0_I0+60+0+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+60+0+0+1], v[vgprValuB_X0_I0+60+0+0+1], v83, s[28:29]
s_nop 1
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
v_mfma_f32_16x16x16_f16 acc[64:67], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
v_mfma_f32_16x16x16_f16 acc[68:71], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
v_mfma_f32_16x16x16_f16 acc[72:75], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
v_mfma_f32_16x16x16_f16 acc[76:79], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
v_mfma_f32_16x16x16_f16 acc[80:83], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
v_mfma_f32_16x16x16_f16 acc[84:87], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
v_mfma_f32_16x16x16_f16 acc[88:91], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
v_mfma_f32_16x16x16_f16 acc[92:95], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
v_mfma_f32_16x16x16_f16 acc[96:99], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
v_mfma_f32_16x16x16_f16 acc[100:103], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
v_mfma_f32_16x16x16_f16 acc[104:107], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
v_mfma_f32_16x16x16_f16 acc[108:111], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
v_mfma_f32_16x16x16_f16 acc[112:115], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
v_mfma_f32_16x16x16_f16 acc[116:119], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
v_mfma_f32_16x16x16_f16 acc[120:123], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
v_mfma_f32_16x16x16_f16 acc[124:127], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
v_mfma_f32_16x16x16_f16 acc[128:131], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
v_mfma_f32_16x16x16_f16 acc[132:135], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
v_mfma_f32_16x16x16_f16 acc[136:139], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
v_mfma_f32_16x16x16_f16 acc[140:143], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
v_mfma_f32_16x16x16_f16 acc[144:147], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
v_mfma_f32_16x16x16_f16 acc[148:151], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
v_mfma_f32_16x16x16_f16 acc[152:155], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
v_mfma_f32_16x16x16_f16 acc[156:159], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
v_mfma_f32_16x16x16_f16 acc[160:163], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
v_mfma_f32_16x16x16_f16 acc[164:167], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
v_mfma_f32_16x16x16_f16 acc[168:171], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
v_mfma_f32_16x16x16_f16 acc[172:175], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
v_mfma_f32_16x16x16_f16 acc[176:179], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
v_mfma_f32_16x16x16_f16 acc[180:183], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
v_mfma_f32_16x16x16_f16 acc[184:187], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
v_mfma_f32_16x16x16_f16 acc[188:191], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
v_mfma_f32_16x16x16_f16 acc[192:195], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
v_mfma_f32_16x16x16_f16 acc[196:199], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
v_mfma_f32_16x16x16_f16 acc[200:203], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
v_mfma_f32_16x16x16_f16 acc[204:207], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
v_mfma_f32_16x16x16_f16 acc[208:211], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
v_mfma_f32_16x16x16_f16 acc[212:215], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
v_mfma_f32_16x16x16_f16 acc[216:219], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
v_mfma_f32_16x16x16_f16 acc[220:223], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
v_mfma_f32_16x16x16_f16 acc[224:227], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[224:227] // left value = acc[224+0:227+0]
v_mfma_f32_16x16x16_f16 acc[228:231], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[228:231] // left value = acc[228+0:231+0]
v_mfma_f32_16x16x16_f16 acc[232:235], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[232:235] // left value = acc[232+0:235+0]
v_mfma_f32_16x16x16_f16 acc[236:239], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+56+0+0:vgprValuB_X0_I0+56+0+0+1], acc[236:239] // left value = acc[236+0:239+0]
v_mfma_f32_16x16x16_f16 acc[240:243], v[vgprG2LA+0+0:vgprG2LA+0+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[240:243] // left value = acc[240+0:243+0]
v_mfma_f32_16x16x16_f16 acc[244:247], v[vgprG2LA+4+0:vgprG2LA+4+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[244:247] // left value = acc[244+0:247+0]
v_mfma_f32_16x16x16_f16 acc[248:251], v[vgprG2LA+8+0:vgprG2LA+8+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[248:251] // left value = acc[248+0:251+0]
v_mfma_f32_16x16x16_f16 acc[252:255], v[vgprG2LA+12+0:vgprG2LA+12+0+1], v[vgprValuB_X0_I0+60+0+0:vgprValuB_X0_I0+60+0+0+1], acc[252:255] // left value = acc[252+0:255+0]

/* tail loop unroll iter 1 */

/* local read inc a */

/* local read inc b */
s_mov_b32 s8, 0x40                                 // inc
v_add_co_u32 v[vgprLocalReadAddrB], vcc, s8, v[vgprLocalReadAddrB] // lrB += 64 (bpeDS)
s_waitcnt lgkmcnt(0)                               // 4wait for local read
v_and_b32 v80, 63, v[vgprSerial]                   // v80 = v[vgprSerial] % 64
v_lshrrev_b32 v80, 4, v80                          // v80 = v80 / 16
v_lshlrev_b32 v80, 0x3, v80                        // v80 = v80 * 8
v_add_u32 v80, 0x4, v80                            // k += (u%%numReadsIterCoalesced) * numMIInput
v_cmp_ge_i32 s[28:29], v80, s[sgprLoopCounterL]    // check K index >= Size L
v_cndmask_b32 v[vgprG2LA+0+2+0], v[vgprG2LA+0+2+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+4+2+0], v[vgprG2LA+4+2+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+8+2+0], v[vgprG2LA+8+2+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+12+2+0], v[vgprG2LA+12+2+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+0+2+1], v[vgprG2LA+0+2+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+4+2+1], v[vgprG2LA+4+2+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+8+2+1], v[vgprG2LA+8+2+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprG2LA+12+2+1], v[vgprG2LA+12+2+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+0+2+0+0], v[vgprValuB_X0_I0+0+2+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+4+2+0+0], v[vgprValuB_X0_I0+4+2+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+8+2+0+0], v[vgprValuB_X0_I0+8+2+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+12+2+0+0], v[vgprValuB_X0_I0+12+2+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+16+2+0+0], v[vgprValuB_X0_I0+16+2+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+20+2+0+0], v[vgprValuB_X0_I0+20+2+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+24+2+0+0], v[vgprValuB_X0_I0+24+2+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+28+2+0+0], v[vgprValuB_X0_I0+28+2+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+32+2+0+0], v[vgprValuB_X0_I0+32+2+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+36+2+0+0], v[vgprValuB_X0_I0+36+2+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+40+2+0+0], v[vgprValuB_X0_I0+40+2+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+44+2+0+0], v[vgprValuB_X0_I0+44+2+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+48+2+0+0], v[vgprValuB_X0_I0+48+2+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+52+2+0+0], v[vgprValuB_X0_I0+52+2+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+56+2+0+0], v[vgprValuB_X0_I0+56+2+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+60+2+0+0], v[vgprValuB_X0_I0+60+2+0+0], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+0+2+0+1], v[vgprValuB_X0_I0+0+2+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+4+2+0+1], v[vgprValuB_X0_I0+4+2+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+8+2+0+1], v[vgprValuB_X0_I0+8+2+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+12+2+0+1], v[vgprValuB_X0_I0+12+2+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+16+2+0+1], v[vgprValuB_X0_I0+16+2+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+20+2+0+1], v[vgprValuB_X0_I0+20+2+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+24+2+0+1], v[vgprValuB_X0_I0+24+2+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+28+2+0+1], v[vgprValuB_X0_I0+28+2+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+32+2+0+1], v[vgprValuB_X0_I0+32+2+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+36+2+0+1], v[vgprValuB_X0_I0+36+2+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+40+2+0+1], v[vgprValuB_X0_I0+40+2+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+44+2+0+1], v[vgprValuB_X0_I0+44+2+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+48+2+0+1], v[vgprValuB_X0_I0+48+2+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+52+2+0+1], v[vgprValuB_X0_I0+52+2+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+56+2+0+1], v[vgprValuB_X0_I0+56+2+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+60+2+0+1], v[vgprValuB_X0_I0+60+2+0+1], 0x0, s[28:29] // set 0 if K_idx >= sizeL
v_sub_u32 v80, s[sgprLoopCounterL], v80            // get distance between size and k index
v_cmp_lt_i32 s[28:29], v80, 4                      // set partial 0 if distance less than input per thread
s_and_b32 s30, s[sgprLoopCounterL], 3              // get inputs for edge thread
s_sub_u32 s30, 4, s30                              // use shift to fill 0 for outside element
s_lshl_b32 s30, s30, 4                             // use shift to fill 0 for outside element
v_lshlrev_b64 v[82:83], s30, v[vgprG2LA+0+2:vgprG2LA+0+2+1]
v_cndmask_b32 v[vgprG2LA+0+2+0], v[vgprG2LA+0+2+0], v82, s[28:29]
v_cndmask_b32 v[vgprG2LA+0+2+1], v[vgprG2LA+0+2+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprG2LA+4+2:vgprG2LA+4+2+1]
v_cndmask_b32 v[vgprG2LA+4+2+0], v[vgprG2LA+4+2+0], v82, s[28:29]
v_cndmask_b32 v[vgprG2LA+4+2+1], v[vgprG2LA+4+2+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprG2LA+8+2:vgprG2LA+8+2+1]
v_cndmask_b32 v[vgprG2LA+8+2+0], v[vgprG2LA+8+2+0], v82, s[28:29]
v_cndmask_b32 v[vgprG2LA+8+2+1], v[vgprG2LA+8+2+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprG2LA+12+2:vgprG2LA+12+2+1]
v_cndmask_b32 v[vgprG2LA+12+2+0], v[vgprG2LA+12+2+0], v82, s[28:29]
v_cndmask_b32 v[vgprG2LA+12+2+1], v[vgprG2LA+12+2+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+0+2+0+0], v[vgprValuB_X0_I0+0+2+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+0+2+0+1], v[vgprValuB_X0_I0+0+2+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+4+2+0+0], v[vgprValuB_X0_I0+4+2+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+4+2+0+1], v[vgprValuB_X0_I0+4+2+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+8+2+0+0], v[vgprValuB_X0_I0+8+2+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+8+2+0+1], v[vgprValuB_X0_I0+8+2+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+12+2+0+0], v[vgprValuB_X0_I0+12+2+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+12+2+0+1], v[vgprValuB_X0_I0+12+2+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+16+2+0+0], v[vgprValuB_X0_I0+16+2+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+16+2+0+1], v[vgprValuB_X0_I0+16+2+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+20+2+0+0], v[vgprValuB_X0_I0+20+2+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+20+2+0+1], v[vgprValuB_X0_I0+20+2+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+24+2+0+0], v[vgprValuB_X0_I0+24+2+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+24+2+0+1], v[vgprValuB_X0_I0+24+2+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+28+2+0+0], v[vgprValuB_X0_I0+28+2+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+28+2+0+1], v[vgprValuB_X0_I0+28+2+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+32+2+0+0], v[vgprValuB_X0_I0+32+2+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+32+2+0+1], v[vgprValuB_X0_I0+32+2+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+36+2+0+0], v[vgprValuB_X0_I0+36+2+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+36+2+0+1], v[vgprValuB_X0_I0+36+2+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+40+2+0+0], v[vgprValuB_X0_I0+40+2+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+40+2+0+1], v[vgprValuB_X0_I0+40+2+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+44+2+0+0], v[vgprValuB_X0_I0+44+2+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+44+2+0+1], v[vgprValuB_X0_I0+44+2+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+48+2+0+0], v[vgprValuB_X0_I0+48+2+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+48+2+0+1], v[vgprValuB_X0_I0+48+2+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+52+2+0+0], v[vgprValuB_X0_I0+52+2+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+52+2+0+1], v[vgprValuB_X0_I0+52+2+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+56+2+0+0], v[vgprValuB_X0_I0+56+2+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+56+2+0+1], v[vgprValuB_X0_I0+56+2+0+1], v83, s[28:29]
v_lshlrev_b64 v[82:83], s30, v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+60+2+0+0], v[vgprValuB_X0_I0+60+2+0+0], v82, s[28:29]
v_cndmask_b32 v[vgprValuB_X0_I0+60+2+0+1], v[vgprValuB_X0_I0+60+2+0+1], v83, s[28:29]
s_nop 1
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
v_mfma_f32_16x16x16_f16 acc[16:19], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
v_mfma_f32_16x16x16_f16 acc[20:23], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
v_mfma_f32_16x16x16_f16 acc[24:27], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
v_mfma_f32_16x16x16_f16 acc[28:31], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
v_mfma_f32_16x16x16_f16 acc[32:35], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
v_mfma_f32_16x16x16_f16 acc[36:39], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
v_mfma_f32_16x16x16_f16 acc[40:43], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
v_mfma_f32_16x16x16_f16 acc[44:47], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
v_mfma_f32_16x16x16_f16 acc[48:51], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
v_mfma_f32_16x16x16_f16 acc[52:55], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
v_mfma_f32_16x16x16_f16 acc[56:59], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
v_mfma_f32_16x16x16_f16 acc[60:63], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
v_mfma_f32_16x16x16_f16 acc[64:67], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
v_mfma_f32_16x16x16_f16 acc[68:71], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
v_mfma_f32_16x16x16_f16 acc[72:75], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
v_mfma_f32_16x16x16_f16 acc[76:79], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
v_mfma_f32_16x16x16_f16 acc[80:83], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
v_mfma_f32_16x16x16_f16 acc[84:87], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
v_mfma_f32_16x16x16_f16 acc[88:91], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
v_mfma_f32_16x16x16_f16 acc[92:95], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
v_mfma_f32_16x16x16_f16 acc[96:99], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
v_mfma_f32_16x16x16_f16 acc[100:103], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
v_mfma_f32_16x16x16_f16 acc[104:107], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
v_mfma_f32_16x16x16_f16 acc[108:111], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
v_mfma_f32_16x16x16_f16 acc[112:115], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
v_mfma_f32_16x16x16_f16 acc[116:119], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
v_mfma_f32_16x16x16_f16 acc[120:123], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
v_mfma_f32_16x16x16_f16 acc[124:127], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
v_mfma_f32_16x16x16_f16 acc[128:131], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
v_mfma_f32_16x16x16_f16 acc[132:135], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
v_mfma_f32_16x16x16_f16 acc[136:139], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
v_mfma_f32_16x16x16_f16 acc[140:143], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
v_mfma_f32_16x16x16_f16 acc[144:147], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
v_mfma_f32_16x16x16_f16 acc[148:151], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
v_mfma_f32_16x16x16_f16 acc[152:155], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
v_mfma_f32_16x16x16_f16 acc[156:159], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
v_mfma_f32_16x16x16_f16 acc[160:163], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
v_mfma_f32_16x16x16_f16 acc[164:167], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
v_mfma_f32_16x16x16_f16 acc[168:171], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
v_mfma_f32_16x16x16_f16 acc[172:175], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
v_mfma_f32_16x16x16_f16 acc[176:179], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
v_mfma_f32_16x16x16_f16 acc[180:183], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
v_mfma_f32_16x16x16_f16 acc[184:187], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
v_mfma_f32_16x16x16_f16 acc[188:191], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
v_mfma_f32_16x16x16_f16 acc[192:195], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
v_mfma_f32_16x16x16_f16 acc[196:199], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
v_mfma_f32_16x16x16_f16 acc[200:203], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
v_mfma_f32_16x16x16_f16 acc[204:207], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
v_mfma_f32_16x16x16_f16 acc[208:211], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
v_mfma_f32_16x16x16_f16 acc[212:215], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
v_mfma_f32_16x16x16_f16 acc[216:219], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
v_mfma_f32_16x16x16_f16 acc[220:223], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
v_mfma_f32_16x16x16_f16 acc[224:227], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[224:227] // left value = acc[224+0:227+0]
v_mfma_f32_16x16x16_f16 acc[228:231], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[228:231] // left value = acc[228+0:231+0]
v_mfma_f32_16x16x16_f16 acc[232:235], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[232:235] // left value = acc[232+0:235+0]
v_mfma_f32_16x16x16_f16 acc[236:239], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+56+2+0:vgprValuB_X0_I0+56+2+0+1], acc[236:239] // left value = acc[236+0:239+0]
v_mfma_f32_16x16x16_f16 acc[240:243], v[vgprG2LA+0+2:vgprG2LA+0+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[240:243] // left value = acc[240+0:243+0]
v_mfma_f32_16x16x16_f16 acc[244:247], v[vgprG2LA+4+2:vgprG2LA+4+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[244:247] // left value = acc[244+0:247+0]
v_mfma_f32_16x16x16_f16 acc[248:251], v[vgprG2LA+8+2:vgprG2LA+8+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[248:251] // left value = acc[248+0:251+0]
v_mfma_f32_16x16x16_f16 acc[252:255], v[vgprG2LA+12+2:vgprG2LA+12+2+1], v[vgprValuB_X0_I0+60+2+0:vgprValuB_X0_I0+60+2+0+1], acc[252:255] // left value = acc[252+0:255+0]

/* closeLoop loopL finalLoop=1 tailLoop=1 */
s_sub_i32 s[sgprLoopCounterL], s[sgprLoopCounterL], 0x20 // dec counterL (tailLoop)
s_add_u32 s[sgprOrigLoopCounter], s[sgprOrigLoopCounter], 0x20 // inc counterL
s_cmp_le_i32 s[sgprLoopCounterL], 0x0              // counterL<=0
s_cbranch_scc0 label_TailLoopBeginL                // restart LoopL
label_TailLoopEndL:
label_SkipTailLoopL:

/* Tail: remove address/G2L [80...139) from pool */
label_Summation_End_NKNWPA1S08SCB151_0:
/* endSummation: add vgpr [0...139) to pool */
.set sgprWGM, UNDEF
.set sgprLoopCounterL, UNDEF
.set sgprOrigLoopCounter, UNDEF
.set sgprAddressA, UNDEF
.set sgprAddressB, UNDEF
.set sgprStridesA, UNDEF
.set sgprStridesB, UNDEF
.set sgprStaggerUIter, UNDEF
.set sgprSrdA, UNDEF
.set sgprSrdB, UNDEF
.set sgprShadowLimitA, UNDEF
.set sgprShadowLimitB, UNDEF
.set sgprStaggerUIterDTV, UNDEF
.set sgprWrapUA, UNDEF
.set sgprWrapUB, UNDEF
.set sgprGlobalReadIncsA, UNDEF
.set sgprGlobalReadIncsB, UNDEF
/* load store sgprs */
.set sgprAddressScaleAlphaVec, 28
.set sgprAddressBias, 30
.set sgprBiasType, 32
.set sgprBiasStride, 33
s_and_b32 s8, s[sgprGSU], 0x3fff                   // Restore GSU
s_cmp_eq_u32 s8, 1                                 // GSU == 1 ?
s_cbranch_scc0 label_GSU_4                         // branch if GSU != 1
/* Check if custom structure pointer is null */
s_cmp_eq_u32 s[sgprArgType], 2                     // ArgType == 2 ?
s_cbranch_scc1 label_LoadExternalEpilogueStruct_1  // branch if ArgType == 2
s_load_dwordx4 s[28:31], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x58
s_load_dwordx2 s[32:33], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x68
s_branch label_LoadExternalEpilogueStructEnd_1
label_LoadExternalEpilogueStruct_1:
s_load_dwordx4 s[28:31], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x90
s_load_dwordx2 s[32:33], s[sgprKernArgAddress:sgprKernArgAddress+1], 0xa0
label_LoadExternalEpilogueStructEnd_1:
label_GSU_4:
.set sgprSrdScaleAlphaVec, 40
.set sgprSrdBias, 48

/* Mapping of Acc register -> C Vgpr register */

/* not-LocalSplitU: global write indices */
/* computeStoreVgprs */
v_lshrrev_b32 v4, 6, v[vgprSerial]                 // v4 = v[vgprSerial] / 64
v_lshrrev_b32 v5, 2, v4                            // v5 = v4 / 4
v_mul_lo_u32 v1, 0x10, v5                          // wave coordination offset 1
v_and_b32 v5, 15, v[vgprSerial]                    // v5 = v[vgprSerial] % 16
v_add_lshl_u32 v1, v5, v1, 3                       // coordination 1 = vwB *(wave_id1 + tid1)
v_mul_lo_u32 v2, v1, s[sgprStrideC1J]              //  offset 1
v_mul_lo_u32 v3, v1, s[sgprStrideD1J]              //  offset 1
v_and_b32 v5, 3, v4                                // v5 = v4 % 4
v_mul_lo_u32 v5, 0x10, v5                          // wave coordination offset 0
v_and_b32 v0, 63, v[vgprSerial]                    // v0 = v[vgprSerial] % 64
v_lshrrev_b32 v0, 4, v0                            // v0 = v0 / 16
v_lshlrev_b32 v0, 0x2, v0                          // thread0 * continuous_output
v_add_lshl_u32 v0, v5, v0, 0                       // coordination 0 = vwA *(wave_id0 + tid0)
s_mul_i32 s8, 256, s[sgprWorkGroup0]               // wgp0 * MT0
v_add_u32 v0, s8, v0                               // coord 0 = (tid0/MI_m)*4 + waveG0*MIB_m + MT0*SG0
s_mul_i32 s8, 256, s[sgprWorkGroup1]               // wgp1 * MT1
v_add_u32 v1, s8, v1                               // coord 1 = (tid0%MI_m) + waveG1*MIB_n + MT1*SG1

/* not-LocalSplitU: global write */

/******************************************/
/* Global Write Elements                  */
/******************************************/
s_waitcnt lgkmcnt(0)                               // wait for 24 bytes of kern args.
s_and_b32 s8, s[sgprGSU], 0x3fff                   // Restore GSU
s_cmp_eq_u32 s8, 1                                 // GSU == 1 ?
s_cbranch_scc1 label_GSU_5                         // branch if GSU == 1
.set sgprAddressScaleAlphaVec, UNDEF
.set sgprSrdScaleAlphaVec, UNDEF
s_and_b32 s40, 255, s[sgprSizeI]                   // s40 = s[sgprSizeI] % 256
s_add_u32 s41, -0x1, s[sgprNumWorkGroups0]
s_cmp_ge_u32 s[sgprWorkGroup0], s41                // wg0 >= nwg0-1 ?
s_cselect_b32 s40, s40, 0                          // set rMT0
s_cmpk_gt_u32 s40, 0x0                             // rMT0 > 0
s_cbranch_scc1 label_GW_B0_E1_M                    // jump if edges required
s_and_b32 s40, 255, s[sgprSizeJ]                   // s40 = s[sgprSizeJ] % 256
s_add_u32 s41, -0x1, s[sgprNumWorkGroups1]
s_cmp_ge_u32 s[sgprWorkGroup1], s41                // wg1 >= nwg1-1
s_cselect_b32 s40, s40, 0                          // set rMT1
s_cmpk_gt_u32 s40, 0x0                             // rMT1 > 0
s_cbranch_scc1 label_GW_B0_E1_N                    // jump if edges required
label_GW_B0_E0_1:

/* edge=0, allocate 2 sgpr. perBatchTmpS=2 perBatchMaskS=0 perElementMaskS=0 elementsPerBatch=60 */
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,1,0,0:vw4); (0,2,0,0:vw4); (0,3,0,0:vw4); (0,0,1,0:vw4); (0,1,1,0:vw4); (0,2,1,0:vw4); (0,3,1,0:vw4); (0,0,2,0:vw4); (0,1,2,0:vw4); (0,2,2,0:vw4); (0,3,2,0:vw4); (0,0,3,0:vw4); (0,1,3,0:vw4); (0,2,3,0:vw4); (0,3,3,0:vw4); (0,0,4,0:vw4); (0,1,4,0:vw4); (0,2,4,0:vw4); (0,3,4,0:vw4); (0,0,5,0:vw4); (0,1,5,0:vw4); (0,2,5,0:vw4); (0,3,5,0:vw4); (0,0,6,0:vw4); (0,1,6,0:vw4); (0,2,6,0:vw4); (0,3,6,0:vw4); (0,0,7,0:vw4); (0,1,7,0:vw4); (0,2,7,0:vw4); (0,3,7,0:vw4); (1,0,0,0:vw4); (1,1,0,0:vw4); (1,2,0,0:vw4); (1,3,0,0:vw4); (1,0,1,0:vw4); (1,1,1,0:vw4); (1,2,1,0:vw4); (1,3,1,0:vw4); (1,0,2,0:vw4); (1,1,2,0:vw4); (1,2,2,0:vw4); (1,3,2,0:vw4); (1,0,3,0:vw4); (1,1,3,0:vw4); (1,2,3,0:vw4); (1,3,3,0:vw4); (1,0,4,0:vw4); (1,1,4,0:vw4); (1,2,4,0:vw4); (1,3,4,0:vw4); (1,0,5,0:vw4); (1,1,5,0:vw4); (1,2,5,0:vw4); (1,3,5,0:vw4); (1,0,6,0:vw4); (1,1,6,0:vw4); (1,2,6,0:vw4); (1,3,6,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
/* (d1,vc1,d0,vc0)=(0,0,2,0) */
/* (d1,vc1,d0,vc0)=(0,0,3,0) */
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
/* (d1,vc1,d0,vc0)=(0,1,2,0) */
/* (d1,vc1,d0,vc0)=(0,1,3,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
/* (d1,vc1,d0,vc0)=(0,2,2,0) */
/* (d1,vc1,d0,vc0)=(0,2,3,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
/* (d1,vc1,d0,vc0)=(0,3,2,0) */
/* (d1,vc1,d0,vc0)=(0,3,3,0) */
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
/* (d1,vc1,d0,vc0)=(0,4,2,0) */
/* (d1,vc1,d0,vc0)=(0,4,3,0) */
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
/* (d1,vc1,d0,vc0)=(0,5,2,0) */
/* (d1,vc1,d0,vc0)=(0,5,3,0) */
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
/* (d1,vc1,d0,vc0)=(0,6,2,0) */
/* (d1,vc1,d0,vc0)=(0,6,3,0) */
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
/* (d1,vc1,d0,vc0)=(0,7,2,0) */
/* (d1,vc1,d0,vc0)=(0,7,3,0) */
/* (d1,vc1,d0,vc0)=(1,0,0,0) */
/* (d1,vc1,d0,vc0)=(1,0,1,0) */
/* (d1,vc1,d0,vc0)=(1,0,2,0) */
/* (d1,vc1,d0,vc0)=(1,0,3,0) */
/* (d1,vc1,d0,vc0)=(1,1,0,0) */
/* (d1,vc1,d0,vc0)=(1,1,1,0) */
/* (d1,vc1,d0,vc0)=(1,1,2,0) */
/* (d1,vc1,d0,vc0)=(1,1,3,0) */
/* (d1,vc1,d0,vc0)=(1,2,0,0) */
/* (d1,vc1,d0,vc0)=(1,2,1,0) */
/* (d1,vc1,d0,vc0)=(1,2,2,0) */
/* (d1,vc1,d0,vc0)=(1,2,3,0) */
/* (d1,vc1,d0,vc0)=(1,3,0,0) */
/* (d1,vc1,d0,vc0)=(1,3,1,0) */
/* (d1,vc1,d0,vc0)=(1,3,2,0) */
/* (d1,vc1,d0,vc0)=(1,3,3,0) */
/* (d1,vc1,d0,vc0)=(1,4,0,0) */
/* (d1,vc1,d0,vc0)=(1,4,1,0) */
/* (d1,vc1,d0,vc0)=(1,4,2,0) */
/* (d1,vc1,d0,vc0)=(1,4,3,0) */
/* (d1,vc1,d0,vc0)=(1,5,0,0) */
/* (d1,vc1,d0,vc0)=(1,5,1,0) */
/* (d1,vc1,d0,vc0)=(1,5,2,0) */
/* (d1,vc1,d0,vc0)=(1,5,3,0) */
/* (d1,vc1,d0,vc0)=(1,6,0,0) */
/* (d1,vc1,d0,vc0)=(1,6,1,0) */
/* (d1,vc1,d0,vc0)=(1,6,2,0) */
/* (d1,vc1,d0,vc0)=(1,6,3,0) */
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+8], acc0            // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+9], acc1            // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+10], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+11], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+12], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+13], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+14], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+15], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+16], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+17], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+18], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+19], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+20], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+21], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+22], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+23], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+24], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+25], acc17          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+26], acc18          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+27], acc19          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+28], acc20          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+29], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+30], acc22          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+31], acc23          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+32], acc24          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+33], acc25          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+34], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+35], acc27          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+36], acc28          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+37], acc29          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+38], acc30          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+39], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+40], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+41], acc33          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+42], acc34          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+43], acc35          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+44], acc36          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+45], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+46], acc38          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+47], acc39          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+48], acc40          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+49], acc41          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+50], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+51], acc43          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+52], acc44          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+53], acc45          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+54], acc46          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+55], acc47          // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+56], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+57], acc49          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+58], acc50          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+59], acc51          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+60], acc52          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+61], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+62], acc54          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+63], acc55          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+64], acc56          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+65], acc57          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+66], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+67], acc59          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+68], acc60          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+69], acc61          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+70], acc62          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+71], acc63          // copy acc to vreg[63]
v_accvgpr_read_b32 v[vgprValuC+72], acc64          // copy acc to vreg[64]
v_accvgpr_read_b32 v[vgprValuC+73], acc65          // copy acc to vreg[65]
v_accvgpr_read_b32 v[vgprValuC+74], acc66          // copy acc to vreg[66]
v_accvgpr_read_b32 v[vgprValuC+75], acc67          // copy acc to vreg[67]
v_accvgpr_read_b32 v[vgprValuC+76], acc68          // copy acc to vreg[68]
v_accvgpr_read_b32 v[vgprValuC+77], acc69          // copy acc to vreg[69]
v_accvgpr_read_b32 v[vgprValuC+78], acc70          // copy acc to vreg[70]
v_accvgpr_read_b32 v[vgprValuC+79], acc71          // copy acc to vreg[71]
v_accvgpr_read_b32 v[vgprValuC+80], acc72          // copy acc to vreg[72]
v_accvgpr_read_b32 v[vgprValuC+81], acc73          // copy acc to vreg[73]
v_accvgpr_read_b32 v[vgprValuC+82], acc74          // copy acc to vreg[74]
v_accvgpr_read_b32 v[vgprValuC+83], acc75          // copy acc to vreg[75]
v_accvgpr_read_b32 v[vgprValuC+84], acc76          // copy acc to vreg[76]
v_accvgpr_read_b32 v[vgprValuC+85], acc77          // copy acc to vreg[77]
v_accvgpr_read_b32 v[vgprValuC+86], acc78          // copy acc to vreg[78]
v_accvgpr_read_b32 v[vgprValuC+87], acc79          // copy acc to vreg[79]
v_accvgpr_read_b32 v[vgprValuC+88], acc80          // copy acc to vreg[80]
v_accvgpr_read_b32 v[vgprValuC+89], acc81          // copy acc to vreg[81]
v_accvgpr_read_b32 v[vgprValuC+90], acc82          // copy acc to vreg[82]
v_accvgpr_read_b32 v[vgprValuC+91], acc83          // copy acc to vreg[83]
v_accvgpr_read_b32 v[vgprValuC+92], acc84          // copy acc to vreg[84]
v_accvgpr_read_b32 v[vgprValuC+93], acc85          // copy acc to vreg[85]
v_accvgpr_read_b32 v[vgprValuC+94], acc86          // copy acc to vreg[86]
v_accvgpr_read_b32 v[vgprValuC+95], acc87          // copy acc to vreg[87]
v_accvgpr_read_b32 v[vgprValuC+96], acc88          // copy acc to vreg[88]
v_accvgpr_read_b32 v[vgprValuC+97], acc89          // copy acc to vreg[89]
v_accvgpr_read_b32 v[vgprValuC+98], acc90          // copy acc to vreg[90]
v_accvgpr_read_b32 v[vgprValuC+99], acc91          // copy acc to vreg[91]
v_accvgpr_read_b32 v[vgprValuC+100], acc92         // copy acc to vreg[92]
v_accvgpr_read_b32 v[vgprValuC+101], acc93         // copy acc to vreg[93]
v_accvgpr_read_b32 v[vgprValuC+102], acc94         // copy acc to vreg[94]
v_accvgpr_read_b32 v[vgprValuC+103], acc95         // copy acc to vreg[95]
v_accvgpr_read_b32 v[vgprValuC+104], acc96         // copy acc to vreg[96]
v_accvgpr_read_b32 v[vgprValuC+105], acc97         // copy acc to vreg[97]
v_accvgpr_read_b32 v[vgprValuC+106], acc98         // copy acc to vreg[98]
v_accvgpr_read_b32 v[vgprValuC+107], acc99         // copy acc to vreg[99]
v_accvgpr_read_b32 v[vgprValuC+108], acc100        // copy acc to vreg[100]
v_accvgpr_read_b32 v[vgprValuC+109], acc101        // copy acc to vreg[101]
v_accvgpr_read_b32 v[vgprValuC+110], acc102        // copy acc to vreg[102]
v_accvgpr_read_b32 v[vgprValuC+111], acc103        // copy acc to vreg[103]
v_accvgpr_read_b32 v[vgprValuC+112], acc104        // copy acc to vreg[104]
v_accvgpr_read_b32 v[vgprValuC+113], acc105        // copy acc to vreg[105]
v_accvgpr_read_b32 v[vgprValuC+114], acc106        // copy acc to vreg[106]
v_accvgpr_read_b32 v[vgprValuC+115], acc107        // copy acc to vreg[107]
v_accvgpr_read_b32 v[vgprValuC+116], acc108        // copy acc to vreg[108]
v_accvgpr_read_b32 v[vgprValuC+117], acc109        // copy acc to vreg[109]
v_accvgpr_read_b32 v[vgprValuC+118], acc110        // copy acc to vreg[110]
v_accvgpr_read_b32 v[vgprValuC+119], acc111        // copy acc to vreg[111]
v_accvgpr_read_b32 v[vgprValuC+120], acc112        // copy acc to vreg[112]
v_accvgpr_read_b32 v[vgprValuC+121], acc113        // copy acc to vreg[113]
v_accvgpr_read_b32 v[vgprValuC+122], acc114        // copy acc to vreg[114]
v_accvgpr_read_b32 v[vgprValuC+123], acc115        // copy acc to vreg[115]
v_accvgpr_read_b32 v[vgprValuC+124], acc116        // copy acc to vreg[116]
v_accvgpr_read_b32 v[vgprValuC+125], acc117        // copy acc to vreg[117]
v_accvgpr_read_b32 v[vgprValuC+126], acc118        // copy acc to vreg[118]
v_accvgpr_read_b32 v[vgprValuC+127], acc119        // copy acc to vreg[119]
v_accvgpr_read_b32 v[vgprValuC+128], acc120        // copy acc to vreg[120]
v_accvgpr_read_b32 v[vgprValuC+129], acc121        // copy acc to vreg[121]
v_accvgpr_read_b32 v[vgprValuC+130], acc122        // copy acc to vreg[122]
v_accvgpr_read_b32 v[vgprValuC+131], acc123        // copy acc to vreg[123]
v_accvgpr_read_b32 v[vgprValuC+132], acc124        // copy acc to vreg[124]
v_accvgpr_read_b32 v[vgprValuC+133], acc125        // copy acc to vreg[125]
v_accvgpr_read_b32 v[vgprValuC+134], acc126        // copy acc to vreg[126]
v_accvgpr_read_b32 v[vgprValuC+135], acc127        // copy acc to vreg[127]
v_accvgpr_read_b32 v[vgprValuC+140], acc128        // copy acc to vreg[128]
v_accvgpr_read_b32 v[vgprValuC+141], acc129        // copy acc to vreg[129]
v_accvgpr_read_b32 v[vgprValuC+142], acc130        // copy acc to vreg[130]
v_accvgpr_read_b32 v[vgprValuC+143], acc131        // copy acc to vreg[131]
v_accvgpr_read_b32 v[vgprValuC+144], acc132        // copy acc to vreg[132]
v_accvgpr_read_b32 v[vgprValuC+145], acc133        // copy acc to vreg[133]
v_accvgpr_read_b32 v[vgprValuC+146], acc134        // copy acc to vreg[134]
v_accvgpr_read_b32 v[vgprValuC+147], acc135        // copy acc to vreg[135]
v_accvgpr_read_b32 v[vgprValuC+148], acc136        // copy acc to vreg[136]
v_accvgpr_read_b32 v[vgprValuC+149], acc137        // copy acc to vreg[137]
v_accvgpr_read_b32 v[vgprValuC+150], acc138        // copy acc to vreg[138]
v_accvgpr_read_b32 v[vgprValuC+151], acc139        // copy acc to vreg[139]
v_accvgpr_read_b32 v[vgprValuC+152], acc140        // copy acc to vreg[140]
v_accvgpr_read_b32 v[vgprValuC+153], acc141        // copy acc to vreg[141]
v_accvgpr_read_b32 v[vgprValuC+154], acc142        // copy acc to vreg[142]
v_accvgpr_read_b32 v[vgprValuC+155], acc143        // copy acc to vreg[143]
v_accvgpr_read_b32 v[vgprValuC+156], acc144        // copy acc to vreg[144]
v_accvgpr_read_b32 v[vgprValuC+157], acc145        // copy acc to vreg[145]
v_accvgpr_read_b32 v[vgprValuC+158], acc146        // copy acc to vreg[146]
v_accvgpr_read_b32 v[vgprValuC+159], acc147        // copy acc to vreg[147]
v_accvgpr_read_b32 v[vgprValuC+160], acc148        // copy acc to vreg[148]
v_accvgpr_read_b32 v[vgprValuC+161], acc149        // copy acc to vreg[149]
v_accvgpr_read_b32 v[vgprValuC+162], acc150        // copy acc to vreg[150]
v_accvgpr_read_b32 v[vgprValuC+163], acc151        // copy acc to vreg[151]
v_accvgpr_read_b32 v[vgprValuC+164], acc152        // copy acc to vreg[152]
v_accvgpr_read_b32 v[vgprValuC+165], acc153        // copy acc to vreg[153]
v_accvgpr_read_b32 v[vgprValuC+166], acc154        // copy acc to vreg[154]
v_accvgpr_read_b32 v[vgprValuC+167], acc155        // copy acc to vreg[155]
v_accvgpr_read_b32 v[vgprValuC+168], acc156        // copy acc to vreg[156]
v_accvgpr_read_b32 v[vgprValuC+169], acc157        // copy acc to vreg[157]
v_accvgpr_read_b32 v[vgprValuC+170], acc158        // copy acc to vreg[158]
v_accvgpr_read_b32 v[vgprValuC+171], acc159        // copy acc to vreg[159]
v_accvgpr_read_b32 v[vgprValuC+172], acc160        // copy acc to vreg[160]
v_accvgpr_read_b32 v[vgprValuC+173], acc161        // copy acc to vreg[161]
v_accvgpr_read_b32 v[vgprValuC+174], acc162        // copy acc to vreg[162]
v_accvgpr_read_b32 v[vgprValuC+175], acc163        // copy acc to vreg[163]
v_accvgpr_read_b32 v[vgprValuC+176], acc164        // copy acc to vreg[164]
v_accvgpr_read_b32 v[vgprValuC+177], acc165        // copy acc to vreg[165]
v_accvgpr_read_b32 v[vgprValuC+178], acc166        // copy acc to vreg[166]
v_accvgpr_read_b32 v[vgprValuC+179], acc167        // copy acc to vreg[167]
v_accvgpr_read_b32 v[vgprValuC+180], acc168        // copy acc to vreg[168]
v_accvgpr_read_b32 v[vgprValuC+181], acc169        // copy acc to vreg[169]
v_accvgpr_read_b32 v[vgprValuC+182], acc170        // copy acc to vreg[170]
v_accvgpr_read_b32 v[vgprValuC+183], acc171        // copy acc to vreg[171]
v_accvgpr_read_b32 v[vgprValuC+184], acc172        // copy acc to vreg[172]
v_accvgpr_read_b32 v[vgprValuC+185], acc173        // copy acc to vreg[173]
v_accvgpr_read_b32 v[vgprValuC+186], acc174        // copy acc to vreg[174]
v_accvgpr_read_b32 v[vgprValuC+187], acc175        // copy acc to vreg[175]
v_accvgpr_read_b32 v[vgprValuC+188], acc176        // copy acc to vreg[176]
v_accvgpr_read_b32 v[vgprValuC+189], acc177        // copy acc to vreg[177]
v_accvgpr_read_b32 v[vgprValuC+190], acc178        // copy acc to vreg[178]
v_accvgpr_read_b32 v[vgprValuC+191], acc179        // copy acc to vreg[179]
v_accvgpr_read_b32 v[vgprValuC+192], acc180        // copy acc to vreg[180]
v_accvgpr_read_b32 v[vgprValuC+193], acc181        // copy acc to vreg[181]
v_accvgpr_read_b32 v[vgprValuC+194], acc182        // copy acc to vreg[182]
v_accvgpr_read_b32 v[vgprValuC+195], acc183        // copy acc to vreg[183]
v_accvgpr_read_b32 v[vgprValuC+196], acc184        // copy acc to vreg[184]
v_accvgpr_read_b32 v[vgprValuC+197], acc185        // copy acc to vreg[185]
v_accvgpr_read_b32 v[vgprValuC+198], acc186        // copy acc to vreg[186]
v_accvgpr_read_b32 v[vgprValuC+199], acc187        // copy acc to vreg[187]
v_accvgpr_read_b32 v[vgprValuC+200], acc188        // copy acc to vreg[188]
v_accvgpr_read_b32 v[vgprValuC+201], acc189        // copy acc to vreg[189]
v_accvgpr_read_b32 v[vgprValuC+202], acc190        // copy acc to vreg[190]
v_accvgpr_read_b32 v[vgprValuC+203], acc191        // copy acc to vreg[191]
v_accvgpr_read_b32 v[vgprValuC+204], acc192        // copy acc to vreg[192]
v_accvgpr_read_b32 v[vgprValuC+205], acc193        // copy acc to vreg[193]
v_accvgpr_read_b32 v[vgprValuC+206], acc194        // copy acc to vreg[194]
v_accvgpr_read_b32 v[vgprValuC+207], acc195        // copy acc to vreg[195]
v_accvgpr_read_b32 v[vgprValuC+208], acc196        // copy acc to vreg[196]
v_accvgpr_read_b32 v[vgprValuC+209], acc197        // copy acc to vreg[197]
v_accvgpr_read_b32 v[vgprValuC+210], acc198        // copy acc to vreg[198]
v_accvgpr_read_b32 v[vgprValuC+211], acc199        // copy acc to vreg[199]
v_accvgpr_read_b32 v[vgprValuC+212], acc200        // copy acc to vreg[200]
v_accvgpr_read_b32 v[vgprValuC+213], acc201        // copy acc to vreg[201]
v_accvgpr_read_b32 v[vgprValuC+214], acc202        // copy acc to vreg[202]
v_accvgpr_read_b32 v[vgprValuC+215], acc203        // copy acc to vreg[203]
v_accvgpr_read_b32 v[vgprValuC+216], acc204        // copy acc to vreg[204]
v_accvgpr_read_b32 v[vgprValuC+217], acc205        // copy acc to vreg[205]
v_accvgpr_read_b32 v[vgprValuC+218], acc206        // copy acc to vreg[206]
v_accvgpr_read_b32 v[vgprValuC+219], acc207        // copy acc to vreg[207]
v_accvgpr_read_b32 v[vgprValuC+220], acc208        // copy acc to vreg[208]
v_accvgpr_read_b32 v[vgprValuC+221], acc209        // copy acc to vreg[209]
v_accvgpr_read_b32 v[vgprValuC+222], acc210        // copy acc to vreg[210]
v_accvgpr_read_b32 v[vgprValuC+223], acc211        // copy acc to vreg[211]
v_accvgpr_read_b32 v[vgprValuC+224], acc212        // copy acc to vreg[212]
v_accvgpr_read_b32 v[vgprValuC+225], acc213        // copy acc to vreg[213]
v_accvgpr_read_b32 v[vgprValuC+226], acc214        // copy acc to vreg[214]
v_accvgpr_read_b32 v[vgprValuC+227], acc215        // copy acc to vreg[215]
v_accvgpr_read_b32 v[vgprValuC+228], acc216        // copy acc to vreg[216]
v_accvgpr_read_b32 v[vgprValuC+229], acc217        // copy acc to vreg[217]
v_accvgpr_read_b32 v[vgprValuC+230], acc218        // copy acc to vreg[218]
v_accvgpr_read_b32 v[vgprValuC+231], acc219        // copy acc to vreg[219]
v_accvgpr_read_b32 v[vgprValuC+232], acc220        // copy acc to vreg[220]
v_accvgpr_read_b32 v[vgprValuC+233], acc221        // copy acc to vreg[221]
v_accvgpr_read_b32 v[vgprValuC+234], acc222        // copy acc to vreg[222]
v_accvgpr_read_b32 v[vgprValuC+235], acc223        // copy acc to vreg[223]
v_accvgpr_read_b32 v[vgprValuC+236], acc224        // copy acc to vreg[224]
v_accvgpr_read_b32 v[vgprValuC+237], acc225        // copy acc to vreg[225]
v_accvgpr_read_b32 v[vgprValuC+238], acc226        // copy acc to vreg[226]
v_accvgpr_read_b32 v[vgprValuC+239], acc227        // copy acc to vreg[227]
v_accvgpr_read_b32 v[vgprValuC+240], acc228        // copy acc to vreg[228]
v_accvgpr_read_b32 v[vgprValuC+241], acc229        // copy acc to vreg[229]
v_accvgpr_read_b32 v[vgprValuC+242], acc230        // copy acc to vreg[230]
v_accvgpr_read_b32 v[vgprValuC+243], acc231        // copy acc to vreg[231]
v_accvgpr_read_b32 v[vgprValuC+244], acc232        // copy acc to vreg[232]
v_accvgpr_read_b32 v[vgprValuC+245], acc233        // copy acc to vreg[233]
v_accvgpr_read_b32 v[vgprValuC+246], acc234        // copy acc to vreg[234]
v_accvgpr_read_b32 v[vgprValuC+247], acc235        // copy acc to vreg[235]
v_accvgpr_read_b32 v[vgprValuC+248], acc236        // copy acc to vreg[236]
v_accvgpr_read_b32 v[vgprValuC+249], acc237        // copy acc to vreg[237]
v_accvgpr_read_b32 v[vgprValuC+250], acc238        // copy acc to vreg[238]
v_accvgpr_read_b32 v[vgprValuC+251], acc239        // copy acc to vreg[239]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 1, 0, 0), (0, 2, 0, 0), (0, 3, 0, 0), (0, 0, 1, 0), (0, 1, 1, 0), (0, 2, 1, 0), (0, 3, 1, 0), (0, 0, 2, 0), (0, 1, 2, 0), (0, 2, 2, 0), (0, 3, 2, 0), (0, 0, 3, 0), (0, 1, 3, 0), (0, 2, 3, 0), (0, 3, 3, 0), (0, 0, 4, 0), (0, 1, 4, 0), (0, 2, 4, 0), (0, 3, 4, 0), (0, 0, 5, 0), (0, 1, 5, 0), (0, 2, 5, 0), (0, 3, 5, 0), (0, 0, 6, 0), (0, 1, 6, 0), (0, 2, 6, 0), (0, 3, 6, 0), (0, 0, 7, 0), (0, 1, 7, 0), (0, 2, 7, 0), (0, 3, 7, 0), (1, 0, 0, 0), (1, 1, 0, 0), (1, 2, 0, 0), (1, 3, 0, 0), (1, 0, 1, 0), (1, 1, 1, 0), (1, 2, 1, 0), (1, 3, 1, 0), (1, 0, 2, 0), (1, 1, 2, 0), (1, 2, 2, 0), (1, 3, 2, 0), (1, 0, 3, 0), (1, 1, 3, 0), (1, 2, 3, 0), (1, 3, 3, 0), (1, 0, 4, 0), (1, 1, 4, 0), (1, 2, 4, 0), (1, 3, 4, 0), (1, 0, 5, 0), (1, 1, 5, 0), (1, 2, 5, 0), (1, 3, 5, 0), (1, 0, 6, 0), (1, 1, 6, 0), (1, 2, 6, 0), (1, 3, 6, 0)] */

/* apply mask, calc new C and issue writes */
buffer_store_dwordx4 v[8:11], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[12:15], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
buffer_store_dwordx4 v[16:19], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:512 // store D
buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:768 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[28:31], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:512 // store D
buffer_store_dwordx4 v[36:39], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:768 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[40:43], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[44:47], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
buffer_store_dwordx4 v[48:51], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:512 // store D
buffer_store_dwordx4 v[52:55], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:768 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[56:59], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[60:63], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
buffer_store_dwordx4 v[64:67], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:512 // store D
buffer_store_dwordx4 v[68:71], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:768 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[72:75], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[76:79], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
buffer_store_dwordx4 v[80:83], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:512 // store D
buffer_store_dwordx4 v[84:87], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:768 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[88:91], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[92:95], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
buffer_store_dwordx4 v[96:99], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:512 // store D
buffer_store_dwordx4 v[100:103], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:768 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[104:107], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[108:111], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
buffer_store_dwordx4 v[112:115], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:512 // store D
buffer_store_dwordx4 v[116:119], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:768 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[120:123], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[124:127], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
buffer_store_dwordx4 v[128:131], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:512 // store D
buffer_store_dwordx4 v[132:135], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:768 // store D
s_mul_i32 s12, s[sgprStrideD1J], 484               // scale StrideD *= numRows(121) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[140:143], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[144:147], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
buffer_store_dwordx4 v[148:151], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:512 // store D
buffer_store_dwordx4 v[152:155], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:768 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[156:159], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[160:163], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
buffer_store_dwordx4 v[164:167], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:512 // store D
buffer_store_dwordx4 v[168:171], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:768 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[172:175], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[176:179], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
buffer_store_dwordx4 v[180:183], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:512 // store D
buffer_store_dwordx4 v[184:187], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:768 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[188:191], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[192:195], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
buffer_store_dwordx4 v[196:199], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:512 // store D
buffer_store_dwordx4 v[200:203], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:768 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[204:207], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[208:211], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
buffer_store_dwordx4 v[212:215], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:512 // store D
buffer_store_dwordx4 v[216:219], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:768 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[220:223], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[224:227], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
buffer_store_dwordx4 v[228:231], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:512 // store D
buffer_store_dwordx4 v[232:235], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:768 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[236:239], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[240:243], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
buffer_store_dwordx4 v[244:247], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:512 // store D
buffer_store_dwordx4 v[248:251], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:768 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Batch #1 (d1,d0,vc1,vc0) = */
/*    (1,0,7,0:vw4); (1,1,7,0:vw4); (1,2,7,0:vw4); (1,3,7,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(1,7,0,0) */
/* (d1,vc1,d0,vc0)=(1,7,1,0) */
/* (d1,vc1,d0,vc0)=(1,7,2,0) */
/* (d1,vc1,d0,vc0)=(1,7,3,0) */
v_accvgpr_read_b32 v[vgprValuC+8], acc240          // copy acc to vreg[240]
v_accvgpr_read_b32 v[vgprValuC+9], acc241          // copy acc to vreg[241]
v_accvgpr_read_b32 v[vgprValuC+10], acc242         // copy acc to vreg[242]
v_accvgpr_read_b32 v[vgprValuC+11], acc243         // copy acc to vreg[243]
v_accvgpr_read_b32 v[vgprValuC+12], acc244         // copy acc to vreg[244]
v_accvgpr_read_b32 v[vgprValuC+13], acc245         // copy acc to vreg[245]
v_accvgpr_read_b32 v[vgprValuC+14], acc246         // copy acc to vreg[246]
v_accvgpr_read_b32 v[vgprValuC+15], acc247         // copy acc to vreg[247]
v_accvgpr_read_b32 v[vgprValuC+16], acc248         // copy acc to vreg[248]
v_accvgpr_read_b32 v[vgprValuC+17], acc249         // copy acc to vreg[249]
v_accvgpr_read_b32 v[vgprValuC+18], acc250         // copy acc to vreg[250]
v_accvgpr_read_b32 v[vgprValuC+19], acc251         // copy acc to vreg[251]
v_accvgpr_read_b32 v[vgprValuC+20], acc252         // copy acc to vreg[252]
v_accvgpr_read_b32 v[vgprValuC+21], acc253         // copy acc to vreg[253]
v_accvgpr_read_b32 v[vgprValuC+22], acc254         // copy acc to vreg[254]
v_accvgpr_read_b32 v[vgprValuC+23], acc255         // copy acc to vreg[255]

/* rC *= alpha batchElements=[(1, 0, 7, 0), (1, 1, 7, 0), (1, 2, 7, 0), (1, 3, 7, 0)] */

/* apply mask, calc new C and issue writes */
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[8:11], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[12:15], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
buffer_store_dwordx4 v[16:19], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:512 // store D
buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:768 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_1                            // jump to end
label_GW_B0_E1_N:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=48 */
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,1,0,0:vw4); (0,2,0,0:vw4); (0,3,0,0:vw4); (0,0,1,0:vw4); (0,1,1,0:vw4); (0,2,1,0:vw4); (0,3,1,0:vw4); (0,0,2,0:vw4); (0,1,2,0:vw4); (0,2,2,0:vw4); (0,3,2,0:vw4); (0,0,3,0:vw4); (0,1,3,0:vw4); (0,2,3,0:vw4); (0,3,3,0:vw4); (0,0,4,0:vw4); (0,1,4,0:vw4); (0,2,4,0:vw4); (0,3,4,0:vw4); (0,0,5,0:vw4); (0,1,5,0:vw4); (0,2,5,0:vw4); (0,3,5,0:vw4); (0,0,6,0:vw4); (0,1,6,0:vw4); (0,2,6,0:vw4); (0,3,6,0:vw4); (0,0,7,0:vw4); (0,1,7,0:vw4); (0,2,7,0:vw4); (0,3,7,0:vw4); (1,0,0,0:vw4); (1,1,0,0:vw4); (1,2,0,0:vw4); (1,3,0,0:vw4); (1,0,1,0:vw4); (1,1,1,0:vw4); (1,2,1,0:vw4); (1,3,1,0:vw4); (1,0,2,0:vw4); (1,1,2,0:vw4); (1,2,2,0:vw4); (1,3,2,0:vw4); (1,0,3,0:vw4); (1,1,3,0:vw4); (1,2,3,0:vw4); (1,3,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v235, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v235, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v7, v3, v4, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v235, v7, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v16, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v16, v235, v16, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v17, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v17, v235, v17, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v18, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v235, v18, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v19, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v19, v235, v19, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v36, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v235, v36, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v37, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v37, v235, v37, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v38, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v235, v38, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v39, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v39, v235, v39, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v56, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v235, v56, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v57, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v57, v235, v57, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v58, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v58, v235, v58, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v59, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v59, v235, v59, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v76, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v235, v76, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v77, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v235, v77, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v78, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v78, v235, v78, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v79, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v79, v235, v79, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v96, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v235, v96, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v97, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v235, v97, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v98, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v98, v235, v98, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v99, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v99, v235, v99, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v116, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v116, v235, v116, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v117, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v117, v235, v117, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v118, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v235, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v119, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v119, v235, v119, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v136, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v136, v235, v136, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v137, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v137, v235, v137, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v138, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v235, v138, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v152, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v152, v235, v152, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v153, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v153, v235, v153, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v154, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v154, v235, v154, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,0) */
s_mov_b32 s52, 121                                 // rowInc d1=0 vc1=0
v_add_co_u32 v1, vcc, v1, s52                      // coord1.2: coord1 += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s52, s[sgprStrideC1J], 121               // scale stride
v_add_i32 v2, v2, s52                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s52, s[sgprStrideD1J], 121               // scale stride
v_add_i32 v3, v3, s52                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v155, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v155, v235, v155, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v172, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v172, v235, v172, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v173, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v173, v235, v173, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v174, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v174, v235, v174, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v175, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v175, v235, v175, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v192, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v192, v235, v192, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v193, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v193, v235, v193, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v194, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v194, v235, v194, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v195, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v195, v235, v195, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v212, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v212, v235, v212, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v213, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v213, v235, v213, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v214, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v214, v235, v214, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v215, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v215, v235, v215, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v232, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v232, v235, v232, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v233, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v233, v235, v233, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v234, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v234, v235, v234, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+8], acc0            // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+9], acc1            // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+10], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+11], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+12], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+13], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+14], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+15], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+20], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+21], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+22], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+23], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+24], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+25], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+26], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+27], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+28], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+29], acc17          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+30], acc18          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+31], acc19          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+32], acc20          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+33], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+34], acc22          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+35], acc23          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+40], acc24          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+41], acc25          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+42], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+43], acc27          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+44], acc28          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+45], acc29          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+46], acc30          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+47], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+48], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+49], acc33          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+50], acc34          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+51], acc35          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+52], acc36          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+53], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+54], acc38          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+55], acc39          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+60], acc40          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+61], acc41          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+62], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+63], acc43          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+64], acc44          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+65], acc45          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+66], acc46          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+67], acc47          // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+68], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+69], acc49          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+70], acc50          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+71], acc51          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+72], acc52          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+73], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+74], acc54          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+75], acc55          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+80], acc56          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+81], acc57          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+82], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+83], acc59          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+84], acc60          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+85], acc61          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+86], acc62          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+87], acc63          // copy acc to vreg[63]
v_accvgpr_read_b32 v[vgprValuC+88], acc64          // copy acc to vreg[64]
v_accvgpr_read_b32 v[vgprValuC+89], acc65          // copy acc to vreg[65]
v_accvgpr_read_b32 v[vgprValuC+90], acc66          // copy acc to vreg[66]
v_accvgpr_read_b32 v[vgprValuC+91], acc67          // copy acc to vreg[67]
v_accvgpr_read_b32 v[vgprValuC+92], acc68          // copy acc to vreg[68]
v_accvgpr_read_b32 v[vgprValuC+93], acc69          // copy acc to vreg[69]
v_accvgpr_read_b32 v[vgprValuC+94], acc70          // copy acc to vreg[70]
v_accvgpr_read_b32 v[vgprValuC+95], acc71          // copy acc to vreg[71]
v_accvgpr_read_b32 v[vgprValuC+100], acc72         // copy acc to vreg[72]
v_accvgpr_read_b32 v[vgprValuC+101], acc73         // copy acc to vreg[73]
v_accvgpr_read_b32 v[vgprValuC+102], acc74         // copy acc to vreg[74]
v_accvgpr_read_b32 v[vgprValuC+103], acc75         // copy acc to vreg[75]
v_accvgpr_read_b32 v[vgprValuC+104], acc76         // copy acc to vreg[76]
v_accvgpr_read_b32 v[vgprValuC+105], acc77         // copy acc to vreg[77]
v_accvgpr_read_b32 v[vgprValuC+106], acc78         // copy acc to vreg[78]
v_accvgpr_read_b32 v[vgprValuC+107], acc79         // copy acc to vreg[79]
v_accvgpr_read_b32 v[vgprValuC+108], acc80         // copy acc to vreg[80]
v_accvgpr_read_b32 v[vgprValuC+109], acc81         // copy acc to vreg[81]
v_accvgpr_read_b32 v[vgprValuC+110], acc82         // copy acc to vreg[82]
v_accvgpr_read_b32 v[vgprValuC+111], acc83         // copy acc to vreg[83]
v_accvgpr_read_b32 v[vgprValuC+112], acc84         // copy acc to vreg[84]
v_accvgpr_read_b32 v[vgprValuC+113], acc85         // copy acc to vreg[85]
v_accvgpr_read_b32 v[vgprValuC+114], acc86         // copy acc to vreg[86]
v_accvgpr_read_b32 v[vgprValuC+115], acc87         // copy acc to vreg[87]
v_accvgpr_read_b32 v[vgprValuC+120], acc88         // copy acc to vreg[88]
v_accvgpr_read_b32 v[vgprValuC+121], acc89         // copy acc to vreg[89]
v_accvgpr_read_b32 v[vgprValuC+122], acc90         // copy acc to vreg[90]
v_accvgpr_read_b32 v[vgprValuC+123], acc91         // copy acc to vreg[91]
v_accvgpr_read_b32 v[vgprValuC+124], acc92         // copy acc to vreg[92]
v_accvgpr_read_b32 v[vgprValuC+125], acc93         // copy acc to vreg[93]
v_accvgpr_read_b32 v[vgprValuC+126], acc94         // copy acc to vreg[94]
v_accvgpr_read_b32 v[vgprValuC+127], acc95         // copy acc to vreg[95]
v_accvgpr_read_b32 v[vgprValuC+128], acc96         // copy acc to vreg[96]
v_accvgpr_read_b32 v[vgprValuC+129], acc97         // copy acc to vreg[97]
v_accvgpr_read_b32 v[vgprValuC+130], acc98         // copy acc to vreg[98]
v_accvgpr_read_b32 v[vgprValuC+131], acc99         // copy acc to vreg[99]
v_accvgpr_read_b32 v[vgprValuC+132], acc100        // copy acc to vreg[100]
v_accvgpr_read_b32 v[vgprValuC+133], acc101        // copy acc to vreg[101]
v_accvgpr_read_b32 v[vgprValuC+134], acc102        // copy acc to vreg[102]
v_accvgpr_read_b32 v[vgprValuC+135], acc103        // copy acc to vreg[103]
v_accvgpr_read_b32 v[vgprValuC+140], acc104        // copy acc to vreg[104]
v_accvgpr_read_b32 v[vgprValuC+141], acc105        // copy acc to vreg[105]
v_accvgpr_read_b32 v[vgprValuC+142], acc106        // copy acc to vreg[106]
v_accvgpr_read_b32 v[vgprValuC+143], acc107        // copy acc to vreg[107]
v_accvgpr_read_b32 v[vgprValuC+144], acc108        // copy acc to vreg[108]
v_accvgpr_read_b32 v[vgprValuC+145], acc109        // copy acc to vreg[109]
v_accvgpr_read_b32 v[vgprValuC+146], acc110        // copy acc to vreg[110]
v_accvgpr_read_b32 v[vgprValuC+147], acc111        // copy acc to vreg[111]
v_accvgpr_read_b32 v[vgprValuC+148], acc112        // copy acc to vreg[112]
v_accvgpr_read_b32 v[vgprValuC+149], acc113        // copy acc to vreg[113]
v_accvgpr_read_b32 v[vgprValuC+150], acc114        // copy acc to vreg[114]
v_accvgpr_read_b32 v[vgprValuC+151], acc115        // copy acc to vreg[115]
v_accvgpr_read_b32 v[vgprValuC+156], acc116        // copy acc to vreg[116]
v_accvgpr_read_b32 v[vgprValuC+157], acc117        // copy acc to vreg[117]
v_accvgpr_read_b32 v[vgprValuC+158], acc118        // copy acc to vreg[118]
v_accvgpr_read_b32 v[vgprValuC+159], acc119        // copy acc to vreg[119]
v_accvgpr_read_b32 v[vgprValuC+160], acc120        // copy acc to vreg[120]
v_accvgpr_read_b32 v[vgprValuC+161], acc121        // copy acc to vreg[121]
v_accvgpr_read_b32 v[vgprValuC+162], acc122        // copy acc to vreg[122]
v_accvgpr_read_b32 v[vgprValuC+163], acc123        // copy acc to vreg[123]
v_accvgpr_read_b32 v[vgprValuC+164], acc124        // copy acc to vreg[124]
v_accvgpr_read_b32 v[vgprValuC+165], acc125        // copy acc to vreg[125]
v_accvgpr_read_b32 v[vgprValuC+166], acc126        // copy acc to vreg[126]
v_accvgpr_read_b32 v[vgprValuC+167], acc127        // copy acc to vreg[127]
v_accvgpr_read_b32 v[vgprValuC+168], acc128        // copy acc to vreg[128]
v_accvgpr_read_b32 v[vgprValuC+169], acc129        // copy acc to vreg[129]
v_accvgpr_read_b32 v[vgprValuC+170], acc130        // copy acc to vreg[130]
v_accvgpr_read_b32 v[vgprValuC+171], acc131        // copy acc to vreg[131]
v_accvgpr_read_b32 v[vgprValuC+176], acc132        // copy acc to vreg[132]
v_accvgpr_read_b32 v[vgprValuC+177], acc133        // copy acc to vreg[133]
v_accvgpr_read_b32 v[vgprValuC+178], acc134        // copy acc to vreg[134]
v_accvgpr_read_b32 v[vgprValuC+179], acc135        // copy acc to vreg[135]
v_accvgpr_read_b32 v[vgprValuC+180], acc136        // copy acc to vreg[136]
v_accvgpr_read_b32 v[vgprValuC+181], acc137        // copy acc to vreg[137]
v_accvgpr_read_b32 v[vgprValuC+182], acc138        // copy acc to vreg[138]
v_accvgpr_read_b32 v[vgprValuC+183], acc139        // copy acc to vreg[139]
v_accvgpr_read_b32 v[vgprValuC+184], acc140        // copy acc to vreg[140]
v_accvgpr_read_b32 v[vgprValuC+185], acc141        // copy acc to vreg[141]
v_accvgpr_read_b32 v[vgprValuC+186], acc142        // copy acc to vreg[142]
v_accvgpr_read_b32 v[vgprValuC+187], acc143        // copy acc to vreg[143]
v_accvgpr_read_b32 v[vgprValuC+188], acc144        // copy acc to vreg[144]
v_accvgpr_read_b32 v[vgprValuC+189], acc145        // copy acc to vreg[145]
v_accvgpr_read_b32 v[vgprValuC+190], acc146        // copy acc to vreg[146]
v_accvgpr_read_b32 v[vgprValuC+191], acc147        // copy acc to vreg[147]
v_accvgpr_read_b32 v[vgprValuC+196], acc148        // copy acc to vreg[148]
v_accvgpr_read_b32 v[vgprValuC+197], acc149        // copy acc to vreg[149]
v_accvgpr_read_b32 v[vgprValuC+198], acc150        // copy acc to vreg[150]
v_accvgpr_read_b32 v[vgprValuC+199], acc151        // copy acc to vreg[151]
v_accvgpr_read_b32 v[vgprValuC+200], acc152        // copy acc to vreg[152]
v_accvgpr_read_b32 v[vgprValuC+201], acc153        // copy acc to vreg[153]
v_accvgpr_read_b32 v[vgprValuC+202], acc154        // copy acc to vreg[154]
v_accvgpr_read_b32 v[vgprValuC+203], acc155        // copy acc to vreg[155]
v_accvgpr_read_b32 v[vgprValuC+204], acc156        // copy acc to vreg[156]
v_accvgpr_read_b32 v[vgprValuC+205], acc157        // copy acc to vreg[157]
v_accvgpr_read_b32 v[vgprValuC+206], acc158        // copy acc to vreg[158]
v_accvgpr_read_b32 v[vgprValuC+207], acc159        // copy acc to vreg[159]
v_accvgpr_read_b32 v[vgprValuC+208], acc160        // copy acc to vreg[160]
v_accvgpr_read_b32 v[vgprValuC+209], acc161        // copy acc to vreg[161]
v_accvgpr_read_b32 v[vgprValuC+210], acc162        // copy acc to vreg[162]
v_accvgpr_read_b32 v[vgprValuC+211], acc163        // copy acc to vreg[163]
v_accvgpr_read_b32 v[vgprValuC+216], acc164        // copy acc to vreg[164]
v_accvgpr_read_b32 v[vgprValuC+217], acc165        // copy acc to vreg[165]
v_accvgpr_read_b32 v[vgprValuC+218], acc166        // copy acc to vreg[166]
v_accvgpr_read_b32 v[vgprValuC+219], acc167        // copy acc to vreg[167]
v_accvgpr_read_b32 v[vgprValuC+220], acc168        // copy acc to vreg[168]
v_accvgpr_read_b32 v[vgprValuC+221], acc169        // copy acc to vreg[169]
v_accvgpr_read_b32 v[vgprValuC+222], acc170        // copy acc to vreg[170]
v_accvgpr_read_b32 v[vgprValuC+223], acc171        // copy acc to vreg[171]
v_accvgpr_read_b32 v[vgprValuC+224], acc172        // copy acc to vreg[172]
v_accvgpr_read_b32 v[vgprValuC+225], acc173        // copy acc to vreg[173]
v_accvgpr_read_b32 v[vgprValuC+226], acc174        // copy acc to vreg[174]
v_accvgpr_read_b32 v[vgprValuC+227], acc175        // copy acc to vreg[175]
v_accvgpr_read_b32 v[vgprValuC+228], acc176        // copy acc to vreg[176]
v_accvgpr_read_b32 v[vgprValuC+229], acc177        // copy acc to vreg[177]
v_accvgpr_read_b32 v[vgprValuC+230], acc178        // copy acc to vreg[178]
v_accvgpr_read_b32 v[vgprValuC+231], acc179        // copy acc to vreg[179]
v_accvgpr_read_b32 v[vgprValuC+236], acc180        // copy acc to vreg[180]
v_accvgpr_read_b32 v[vgprValuC+237], acc181        // copy acc to vreg[181]
v_accvgpr_read_b32 v[vgprValuC+238], acc182        // copy acc to vreg[182]
v_accvgpr_read_b32 v[vgprValuC+239], acc183        // copy acc to vreg[183]
v_accvgpr_read_b32 v[vgprValuC+240], acc184        // copy acc to vreg[184]
v_accvgpr_read_b32 v[vgprValuC+241], acc185        // copy acc to vreg[185]
v_accvgpr_read_b32 v[vgprValuC+242], acc186        // copy acc to vreg[186]
v_accvgpr_read_b32 v[vgprValuC+243], acc187        // copy acc to vreg[187]
v_accvgpr_read_b32 v[vgprValuC+244], acc188        // copy acc to vreg[188]
v_accvgpr_read_b32 v[vgprValuC+245], acc189        // copy acc to vreg[189]
v_accvgpr_read_b32 v[vgprValuC+246], acc190        // copy acc to vreg[190]
v_accvgpr_read_b32 v[vgprValuC+247], acc191        // copy acc to vreg[191]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 1, 0, 0), (0, 2, 0, 0), (0, 3, 0, 0), (0, 0, 1, 0), (0, 1, 1, 0), (0, 2, 1, 0), (0, 3, 1, 0), (0, 0, 2, 0), (0, 1, 2, 0), (0, 2, 2, 0), (0, 3, 2, 0), (0, 0, 3, 0), (0, 1, 3, 0), (0, 2, 3, 0), (0, 3, 3, 0), (0, 0, 4, 0), (0, 1, 4, 0), (0, 2, 4, 0), (0, 3, 4, 0), (0, 0, 5, 0), (0, 1, 5, 0), (0, 2, 5, 0), (0, 3, 5, 0), (0, 0, 6, 0), (0, 1, 6, 0), (0, 2, 6, 0), (0, 3, 6, 0), (0, 0, 7, 0), (0, 1, 7, 0), (0, 2, 7, 0), (0, 3, 7, 0), (1, 0, 0, 0), (1, 1, 0, 0), (1, 2, 0, 0), (1, 3, 0, 0), (1, 0, 1, 0), (1, 1, 1, 0), (1, 2, 1, 0), (1, 3, 1, 0), (1, 0, 2, 0), (1, 1, 2, 0), (1, 2, 2, 0), (1, 3, 2, 0), (1, 0, 3, 0), (1, 1, 3, 0), (1, 2, 3, 0), (1, 3, 3, 0)] */

/* apply mask, calc new C and issue writes */
buffer_store_dwordx4 v[8:11], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[12:15], v7, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[20:23], v16, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[24:27], v17, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[28:31], v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[32:35], v19, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[40:43], v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[44:47], v37, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[48:51], v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[52:55], v39, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[60:63], v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[64:67], v57, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[68:71], v58, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[72:75], v59, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[80:83], v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[84:87], v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[88:91], v78, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[92:95], v79, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[100:103], v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[104:107], v97, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[108:111], v98, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[112:115], v99, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[120:123], v116, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[124:127], v117, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[128:131], v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[132:135], v119, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[140:143], v136, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[144:147], v137, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[148:151], v138, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[156:159], v152, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[160:163], v153, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[164:167], v154, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[168:171], v155, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[176:179], v172, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[180:183], v173, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[184:187], v174, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[188:191], v175, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[196:199], v192, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[200:203], v193, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[204:207], v194, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[208:211], v195, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[216:219], v212, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[220:223], v213, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[224:227], v214, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[228:231], v215, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[236:239], v232, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[240:243], v233, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[244:247], v234, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #1 (d1,d0,vc1,vc0) = */
/*    (1,0,4,0:vw4); (1,1,4,0:vw4); (1,2,4,0:vw4); (1,3,4,0:vw4); (1,0,5,0:vw4); (1,1,5,0:vw4); (1,2,5,0:vw4); (1,3,5,0:vw4); (1,0,6,0:vw4); (1,1,6,0:vw4); (1,2,6,0:vw4); (1,3,6,0:vw4); (1,0,7,0:vw4); (1,1,7,0:vw4); (1,2,7,0:vw4); (1,3,7,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v78, BufferOOB
/* (d1,vc1,d0,vc0)=(1,4,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v78, v6, s[56:57]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v7, v3, v4, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v78, v7, s[56:57]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v16, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v16, v78, v16, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v17, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v17, v78, v17, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v18, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v78, v18, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v19, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v19, v78, v19, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v36, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v78, v36, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v37, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v37, v78, v37, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v38, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v78, v38, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v39, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v39, v78, v39, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v56, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v78, v56, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v57, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v57, v78, v57, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v58, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v58, v78, v58, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v59, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v59, v78, v59, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v76, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v78, v76, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v77, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v78, v77, s[56:57]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+8], acc192          // copy acc to vreg[192]
v_accvgpr_read_b32 v[vgprValuC+9], acc193          // copy acc to vreg[193]
v_accvgpr_read_b32 v[vgprValuC+10], acc194         // copy acc to vreg[194]
v_accvgpr_read_b32 v[vgprValuC+11], acc195         // copy acc to vreg[195]
v_accvgpr_read_b32 v[vgprValuC+12], acc196         // copy acc to vreg[196]
v_accvgpr_read_b32 v[vgprValuC+13], acc197         // copy acc to vreg[197]
v_accvgpr_read_b32 v[vgprValuC+14], acc198         // copy acc to vreg[198]
v_accvgpr_read_b32 v[vgprValuC+15], acc199         // copy acc to vreg[199]
v_accvgpr_read_b32 v[vgprValuC+20], acc200         // copy acc to vreg[200]
v_accvgpr_read_b32 v[vgprValuC+21], acc201         // copy acc to vreg[201]
v_accvgpr_read_b32 v[vgprValuC+22], acc202         // copy acc to vreg[202]
v_accvgpr_read_b32 v[vgprValuC+23], acc203         // copy acc to vreg[203]
v_accvgpr_read_b32 v[vgprValuC+24], acc204         // copy acc to vreg[204]
v_accvgpr_read_b32 v[vgprValuC+25], acc205         // copy acc to vreg[205]
v_accvgpr_read_b32 v[vgprValuC+26], acc206         // copy acc to vreg[206]
v_accvgpr_read_b32 v[vgprValuC+27], acc207         // copy acc to vreg[207]
v_accvgpr_read_b32 v[vgprValuC+28], acc208         // copy acc to vreg[208]
v_accvgpr_read_b32 v[vgprValuC+29], acc209         // copy acc to vreg[209]
v_accvgpr_read_b32 v[vgprValuC+30], acc210         // copy acc to vreg[210]
v_accvgpr_read_b32 v[vgprValuC+31], acc211         // copy acc to vreg[211]
v_accvgpr_read_b32 v[vgprValuC+32], acc212         // copy acc to vreg[212]
v_accvgpr_read_b32 v[vgprValuC+33], acc213         // copy acc to vreg[213]
v_accvgpr_read_b32 v[vgprValuC+34], acc214         // copy acc to vreg[214]
v_accvgpr_read_b32 v[vgprValuC+35], acc215         // copy acc to vreg[215]
v_accvgpr_read_b32 v[vgprValuC+40], acc216         // copy acc to vreg[216]
v_accvgpr_read_b32 v[vgprValuC+41], acc217         // copy acc to vreg[217]
v_accvgpr_read_b32 v[vgprValuC+42], acc218         // copy acc to vreg[218]
v_accvgpr_read_b32 v[vgprValuC+43], acc219         // copy acc to vreg[219]
v_accvgpr_read_b32 v[vgprValuC+44], acc220         // copy acc to vreg[220]
v_accvgpr_read_b32 v[vgprValuC+45], acc221         // copy acc to vreg[221]
v_accvgpr_read_b32 v[vgprValuC+46], acc222         // copy acc to vreg[222]
v_accvgpr_read_b32 v[vgprValuC+47], acc223         // copy acc to vreg[223]
v_accvgpr_read_b32 v[vgprValuC+48], acc224         // copy acc to vreg[224]
v_accvgpr_read_b32 v[vgprValuC+49], acc225         // copy acc to vreg[225]
v_accvgpr_read_b32 v[vgprValuC+50], acc226         // copy acc to vreg[226]
v_accvgpr_read_b32 v[vgprValuC+51], acc227         // copy acc to vreg[227]
v_accvgpr_read_b32 v[vgprValuC+52], acc228         // copy acc to vreg[228]
v_accvgpr_read_b32 v[vgprValuC+53], acc229         // copy acc to vreg[229]
v_accvgpr_read_b32 v[vgprValuC+54], acc230         // copy acc to vreg[230]
v_accvgpr_read_b32 v[vgprValuC+55], acc231         // copy acc to vreg[231]
v_accvgpr_read_b32 v[vgprValuC+60], acc232         // copy acc to vreg[232]
v_accvgpr_read_b32 v[vgprValuC+61], acc233         // copy acc to vreg[233]
v_accvgpr_read_b32 v[vgprValuC+62], acc234         // copy acc to vreg[234]
v_accvgpr_read_b32 v[vgprValuC+63], acc235         // copy acc to vreg[235]
v_accvgpr_read_b32 v[vgprValuC+64], acc236         // copy acc to vreg[236]
v_accvgpr_read_b32 v[vgprValuC+65], acc237         // copy acc to vreg[237]
v_accvgpr_read_b32 v[vgprValuC+66], acc238         // copy acc to vreg[238]
v_accvgpr_read_b32 v[vgprValuC+67], acc239         // copy acc to vreg[239]
v_accvgpr_read_b32 v[vgprValuC+68], acc240         // copy acc to vreg[240]
v_accvgpr_read_b32 v[vgprValuC+69], acc241         // copy acc to vreg[241]
v_accvgpr_read_b32 v[vgprValuC+70], acc242         // copy acc to vreg[242]
v_accvgpr_read_b32 v[vgprValuC+71], acc243         // copy acc to vreg[243]
v_accvgpr_read_b32 v[vgprValuC+72], acc244         // copy acc to vreg[244]
v_accvgpr_read_b32 v[vgprValuC+73], acc245         // copy acc to vreg[245]
v_accvgpr_read_b32 v[vgprValuC+74], acc246         // copy acc to vreg[246]
v_accvgpr_read_b32 v[vgprValuC+75], acc247         // copy acc to vreg[247]
v_accvgpr_read_b32 v[vgprValuC+80], acc248         // copy acc to vreg[248]
v_accvgpr_read_b32 v[vgprValuC+81], acc249         // copy acc to vreg[249]
v_accvgpr_read_b32 v[vgprValuC+82], acc250         // copy acc to vreg[250]
v_accvgpr_read_b32 v[vgprValuC+83], acc251         // copy acc to vreg[251]
v_accvgpr_read_b32 v[vgprValuC+84], acc252         // copy acc to vreg[252]
v_accvgpr_read_b32 v[vgprValuC+85], acc253         // copy acc to vreg[253]
v_accvgpr_read_b32 v[vgprValuC+86], acc254         // copy acc to vreg[254]
v_accvgpr_read_b32 v[vgprValuC+87], acc255         // copy acc to vreg[255]

/* rC *= alpha batchElements=[(1, 0, 4, 0), (1, 1, 4, 0), (1, 2, 4, 0), (1, 3, 4, 0), (1, 0, 5, 0), (1, 1, 5, 0), (1, 2, 5, 0), (1, 3, 5, 0), (1, 0, 6, 0), (1, 1, 6, 0), (1, 2, 6, 0), (1, 3, 6, 0), (1, 0, 7, 0), (1, 1, 7, 0), (1, 2, 7, 0), (1, 3, 7, 0)] */

/* apply mask, calc new C and issue writes */
buffer_store_dwordx4 v[8:11], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[12:15], v7, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[20:23], v16, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[24:27], v17, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[28:31], v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[32:35], v19, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[40:43], v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[44:47], v37, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[48:51], v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[52:55], v39, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[60:63], v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[64:67], v57, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[68:71], v58, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[72:75], v59, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[80:83], v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[84:87], v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_1                            // jump to end
label_GW_B0_E1_M:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=124 */
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw1); (0,0,0,1:vw1); (0,0,0,2:vw1); (0,0,0,3:vw1); (0,1,0,0:vw1); (0,1,0,1:vw1); (0,1,0,2:vw1); (0,1,0,3:vw1); (0,2,0,0:vw1); (0,2,0,1:vw1); (0,2,0,2:vw1); (0,2,0,3:vw1); (0,3,0,0:vw1); (0,3,0,1:vw1); (0,3,0,2:vw1); (0,3,0,3:vw1); (0,0,1,0:vw1); (0,0,1,1:vw1); (0,0,1,2:vw1); (0,0,1,3:vw1); (0,1,1,0:vw1); (0,1,1,1:vw1); (0,1,1,2:vw1); (0,1,1,3:vw1); (0,2,1,0:vw1); (0,2,1,1:vw1); (0,2,1,2:vw1); (0,2,1,3:vw1); (0,3,1,0:vw1); (0,3,1,1:vw1); (0,3,1,2:vw1); (0,3,1,3:vw1); (0,0,2,0:vw1); (0,0,2,1:vw1); (0,0,2,2:vw1); (0,0,2,3:vw1); (0,1,2,0:vw1); (0,1,2,1:vw1); (0,1,2,2:vw1); (0,1,2,3:vw1); (0,2,2,0:vw1); (0,2,2,1:vw1); (0,2,2,2:vw1); (0,2,2,3:vw1); (0,3,2,0:vw1); (0,3,2,1:vw1); (0,3,2,2:vw1); (0,3,2,3:vw1); (0,0,3,0:vw1); (0,0,3,1:vw1); (0,0,3,2:vw1); (0,0,3,3:vw1); (0,1,3,0:vw1); (0,1,3,1:vw1); (0,1,3,2:vw1); (0,1,3,3:vw1); (0,2,3,0:vw1); (0,2,3,1:vw1); (0,2,3,2:vw1); (0,2,3,3:vw1); (0,3,3,0:vw1); (0,3,3,1:vw1); (0,3,3,2:vw1); (0,3,3,3:vw1); (0,0,4,0:vw1); (0,0,4,1:vw1); (0,0,4,2:vw1); (0,0,4,3:vw1); (0,1,4,0:vw1); (0,1,4,1:vw1); (0,1,4,2:vw1); (0,1,4,3:vw1); (0,2,4,0:vw1); (0,2,4,1:vw1); (0,2,4,2:vw1); (0,2,4,3:vw1); (0,3,4,0:vw1); (0,3,4,1:vw1); (0,3,4,2:vw1); (0,3,4,3:vw1); (0,0,5,0:vw1); (0,0,5,1:vw1); (0,0,5,2:vw1); (0,0,5,3:vw1); (0,1,5,0:vw1); (0,1,5,1:vw1); (0,1,5,2:vw1); (0,1,5,3:vw1); (0,2,5,0:vw1); (0,2,5,1:vw1); (0,2,5,2:vw1); (0,2,5,3:vw1); (0,3,5,0:vw1); (0,3,5,1:vw1); (0,3,5,2:vw1); (0,3,5,3:vw1); (0,0,6,0:vw1); (0,0,6,1:vw1); (0,0,6,2:vw1); (0,0,6,3:vw1); (0,1,6,0:vw1); (0,1,6,1:vw1); (0,1,6,2:vw1); (0,1,6,3:vw1); (0,2,6,0:vw1); (0,2,6,1:vw1); (0,2,6,2:vw1); (0,2,6,3:vw1); (0,3,6,0:vw1); (0,3,6,1:vw1); (0,3,6,2:vw1); (0,3,6,3:vw1); (0,0,7,0:vw1); (0,0,7,1:vw1); (0,0,7,2:vw1); (0,0,7,3:vw1); (0,1,7,0:vw1); (0,1,7,1:vw1); (0,1,7,2:vw1); (0,1,7,3:vw1); (0,2,7,0:vw1); (0,2,7,1:vw1); (0,2,7,2:vw1); (0,2,7,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v255, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v255, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v8, v3, v4, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v8, v255, v8, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v10, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v255, v10, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v12, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v255, v12, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v14, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v255, v14, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v16, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v16, v255, v16, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v18, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v255, v18, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v20, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v255, v20, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v22, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v255, v22, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v24, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v255, v24, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v26, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v255, v26, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v28, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v255, v28, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v30, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v255, v30, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v32, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v32, v255, v32, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v34, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v255, v34, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v36, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v255, v36, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v38, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v255, v38, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v40, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v255, v40, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v42, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v255, v42, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v44, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v44, v255, v44, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v46, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v255, v46, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v48, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v255, v48, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v50, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v50, v255, v50, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v52, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v52, v255, v52, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v54, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v255, v54, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v56, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v255, v56, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v58, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v58, v255, v58, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v60, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v60, v255, v60, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v62, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v255, v62, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v64, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v64, v255, v64, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v66, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v255, v66, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v68, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v68, v255, v68, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v70, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v70, v255, v70, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v72, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v72, v255, v72, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v74, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v74, v255, v74, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v76, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v255, v76, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v78, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v78, v255, v78, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v80, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v80, v255, v80, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v82, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v82, v255, v82, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v84, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v255, v84, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v86, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v255, v86, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v88, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v88, v255, v88, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v90, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v255, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v92, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v92, v255, v92, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v94, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v94, v255, v94, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v96, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v255, v96, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v98, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v98, v255, v98, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v100, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v100, v255, v100, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v102, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v102, v255, v102, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v104, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v104, v255, v104, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v106, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v255, v106, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v108, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v108, v255, v108, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v110, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v110, v255, v110, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v112, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v112, v255, v112, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v114, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v114, v255, v114, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v116, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v116, v255, v116, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v118, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v255, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v120, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v120, v255, v120, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v122, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v122, v255, v122, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v124, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v124, v255, v124, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v126, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v126, v255, v126, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v128, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v255, v128, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v130, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v130, v255, v130, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v132, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v132, v255, v132, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v134, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v134, v255, v134, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v136, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v136, v255, v136, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v138, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v255, v138, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v141, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v141, v255, v141, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v143, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v143, v255, v143, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v145, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v145, v255, v145, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v147, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v147, v255, v147, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v149, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v255, v149, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v151, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v151, v255, v151, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v153, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v153, v255, v153, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v155, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v155, v255, v155, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v157, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v157, v255, v157, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v159, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v255, v159, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v161, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v161, v255, v161, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v163, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v163, v255, v163, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v165, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v165, v255, v165, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v167, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v167, v255, v167, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v169, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v169, v255, v169, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v171, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v171, v255, v171, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v173, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v173, v255, v173, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v175, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v175, v255, v175, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v177, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v177, v255, v177, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v179, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v255, v179, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v181, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v181, v255, v181, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v183, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v183, v255, v183, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v185, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v185, v255, v185, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v187, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v187, v255, v187, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v189, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v189, v255, v189, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v191, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v191, v255, v191, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v193, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v193, v255, v193, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v195, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v195, v255, v195, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v197, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v197, v255, v197, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v199, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v199, v255, v199, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v201, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v201, v255, v201, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v203, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v203, v255, v203, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v205, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v205, v255, v205, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v207, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v207, v255, v207, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v209, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v209, v255, v209, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v211, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v211, v255, v211, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v213, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v213, v255, v213, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v215, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v215, v255, v215, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v217, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v217, v255, v217, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v219, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v219, v255, v219, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v221, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v221, v255, v221, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v223, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v223, v255, v223, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v225, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v225, v255, v225, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v227, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v227, v255, v227, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v229, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v229, v255, v229, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v231, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v231, v255, v231, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v233, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v233, v255, v233, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v235, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v235, v255, v235, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v237, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v237, v255, v237, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v239, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v239, v255, v239, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v241, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v241, v255, v241, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v243, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v243, v255, v243, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v245, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v245, v255, v245, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v247, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v247, v255, v247, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v249, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v249, v255, v249, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v251, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v251, v255, v251, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v253, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v253, v255, v253, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+7], acc0            // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+9], acc1            // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+11], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+13], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+15], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+17], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+19], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+21], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+23], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+25], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+27], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+29], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+31], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+33], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+35], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+37], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+39], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+41], acc17          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+43], acc18          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+45], acc19          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+47], acc20          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+49], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+51], acc22          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+53], acc23          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+55], acc24          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+57], acc25          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+59], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+61], acc27          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+63], acc28          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+65], acc29          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+67], acc30          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+69], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+71], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+73], acc33          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+75], acc34          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+77], acc35          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+79], acc36          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+81], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+83], acc38          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+85], acc39          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+87], acc40          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+89], acc41          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+91], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+93], acc43          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+95], acc44          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+97], acc45          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+99], acc46          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+101], acc47         // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+103], acc48         // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+105], acc49         // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+107], acc50         // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+109], acc51         // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+111], acc52         // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+113], acc53         // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+115], acc54         // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+117], acc55         // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+119], acc56         // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+121], acc57         // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+123], acc58         // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+125], acc59         // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+127], acc60         // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+129], acc61         // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+131], acc62         // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+133], acc63         // copy acc to vreg[63]
v_accvgpr_read_b32 v[vgprValuC+135], acc64         // copy acc to vreg[64]
v_accvgpr_read_b32 v[vgprValuC+137], acc65         // copy acc to vreg[65]
v_accvgpr_read_b32 v[vgprValuC+140], acc66         // copy acc to vreg[66]
v_accvgpr_read_b32 v[vgprValuC+142], acc67         // copy acc to vreg[67]
v_accvgpr_read_b32 v[vgprValuC+144], acc68         // copy acc to vreg[68]
v_accvgpr_read_b32 v[vgprValuC+146], acc69         // copy acc to vreg[69]
v_accvgpr_read_b32 v[vgprValuC+148], acc70         // copy acc to vreg[70]
v_accvgpr_read_b32 v[vgprValuC+150], acc71         // copy acc to vreg[71]
v_accvgpr_read_b32 v[vgprValuC+152], acc72         // copy acc to vreg[72]
v_accvgpr_read_b32 v[vgprValuC+154], acc73         // copy acc to vreg[73]
v_accvgpr_read_b32 v[vgprValuC+156], acc74         // copy acc to vreg[74]
v_accvgpr_read_b32 v[vgprValuC+158], acc75         // copy acc to vreg[75]
v_accvgpr_read_b32 v[vgprValuC+160], acc76         // copy acc to vreg[76]
v_accvgpr_read_b32 v[vgprValuC+162], acc77         // copy acc to vreg[77]
v_accvgpr_read_b32 v[vgprValuC+164], acc78         // copy acc to vreg[78]
v_accvgpr_read_b32 v[vgprValuC+166], acc79         // copy acc to vreg[79]
v_accvgpr_read_b32 v[vgprValuC+168], acc80         // copy acc to vreg[80]
v_accvgpr_read_b32 v[vgprValuC+170], acc81         // copy acc to vreg[81]
v_accvgpr_read_b32 v[vgprValuC+172], acc82         // copy acc to vreg[82]
v_accvgpr_read_b32 v[vgprValuC+174], acc83         // copy acc to vreg[83]
v_accvgpr_read_b32 v[vgprValuC+176], acc84         // copy acc to vreg[84]
v_accvgpr_read_b32 v[vgprValuC+178], acc85         // copy acc to vreg[85]
v_accvgpr_read_b32 v[vgprValuC+180], acc86         // copy acc to vreg[86]
v_accvgpr_read_b32 v[vgprValuC+182], acc87         // copy acc to vreg[87]
v_accvgpr_read_b32 v[vgprValuC+184], acc88         // copy acc to vreg[88]
v_accvgpr_read_b32 v[vgprValuC+186], acc89         // copy acc to vreg[89]
v_accvgpr_read_b32 v[vgprValuC+188], acc90         // copy acc to vreg[90]
v_accvgpr_read_b32 v[vgprValuC+190], acc91         // copy acc to vreg[91]
v_accvgpr_read_b32 v[vgprValuC+192], acc92         // copy acc to vreg[92]
v_accvgpr_read_b32 v[vgprValuC+194], acc93         // copy acc to vreg[93]
v_accvgpr_read_b32 v[vgprValuC+196], acc94         // copy acc to vreg[94]
v_accvgpr_read_b32 v[vgprValuC+198], acc95         // copy acc to vreg[95]
v_accvgpr_read_b32 v[vgprValuC+200], acc96         // copy acc to vreg[96]
v_accvgpr_read_b32 v[vgprValuC+202], acc97         // copy acc to vreg[97]
v_accvgpr_read_b32 v[vgprValuC+204], acc98         // copy acc to vreg[98]
v_accvgpr_read_b32 v[vgprValuC+206], acc99         // copy acc to vreg[99]
v_accvgpr_read_b32 v[vgprValuC+208], acc100        // copy acc to vreg[100]
v_accvgpr_read_b32 v[vgprValuC+210], acc101        // copy acc to vreg[101]
v_accvgpr_read_b32 v[vgprValuC+212], acc102        // copy acc to vreg[102]
v_accvgpr_read_b32 v[vgprValuC+214], acc103        // copy acc to vreg[103]
v_accvgpr_read_b32 v[vgprValuC+216], acc104        // copy acc to vreg[104]
v_accvgpr_read_b32 v[vgprValuC+218], acc105        // copy acc to vreg[105]
v_accvgpr_read_b32 v[vgprValuC+220], acc106        // copy acc to vreg[106]
v_accvgpr_read_b32 v[vgprValuC+222], acc107        // copy acc to vreg[107]
v_accvgpr_read_b32 v[vgprValuC+224], acc108        // copy acc to vreg[108]
v_accvgpr_read_b32 v[vgprValuC+226], acc109        // copy acc to vreg[109]
v_accvgpr_read_b32 v[vgprValuC+228], acc110        // copy acc to vreg[110]
v_accvgpr_read_b32 v[vgprValuC+230], acc111        // copy acc to vreg[111]
v_accvgpr_read_b32 v[vgprValuC+232], acc112        // copy acc to vreg[112]
v_accvgpr_read_b32 v[vgprValuC+234], acc113        // copy acc to vreg[113]
v_accvgpr_read_b32 v[vgprValuC+236], acc114        // copy acc to vreg[114]
v_accvgpr_read_b32 v[vgprValuC+238], acc115        // copy acc to vreg[115]
v_accvgpr_read_b32 v[vgprValuC+240], acc116        // copy acc to vreg[116]
v_accvgpr_read_b32 v[vgprValuC+242], acc117        // copy acc to vreg[117]
v_accvgpr_read_b32 v[vgprValuC+244], acc118        // copy acc to vreg[118]
v_accvgpr_read_b32 v[vgprValuC+246], acc119        // copy acc to vreg[119]
v_accvgpr_read_b32 v[vgprValuC+248], acc120        // copy acc to vreg[120]
v_accvgpr_read_b32 v[vgprValuC+250], acc121        // copy acc to vreg[121]
v_accvgpr_read_b32 v[vgprValuC+252], acc122        // copy acc to vreg[122]
v_accvgpr_read_b32 v[vgprValuC+254], acc123        // copy acc to vreg[123]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 0, 1), (0, 0, 0, 2), (0, 0, 0, 3), (0, 1, 0, 0), (0, 1, 0, 1), (0, 1, 0, 2), (0, 1, 0, 3), (0, 2, 0, 0), (0, 2, 0, 1), (0, 2, 0, 2), (0, 2, 0, 3), (0, 3, 0, 0), (0, 3, 0, 1), (0, 3, 0, 2), (0, 3, 0, 3), (0, 0, 1, 0), (0, 0, 1, 1), (0, 0, 1, 2), (0, 0, 1, 3), (0, 1, 1, 0), (0, 1, 1, 1), (0, 1, 1, 2), (0, 1, 1, 3), (0, 2, 1, 0), (0, 2, 1, 1), (0, 2, 1, 2), (0, 2, 1, 3), (0, 3, 1, 0), (0, 3, 1, 1), (0, 3, 1, 2), (0, 3, 1, 3), (0, 0, 2, 0), (0, 0, 2, 1), (0, 0, 2, 2), (0, 0, 2, 3), (0, 1, 2, 0), (0, 1, 2, 1), (0, 1, 2, 2), (0, 1, 2, 3), (0, 2, 2, 0), (0, 2, 2, 1), (0, 2, 2, 2), (0, 2, 2, 3), (0, 3, 2, 0), (0, 3, 2, 1), (0, 3, 2, 2), (0, 3, 2, 3), (0, 0, 3, 0), (0, 0, 3, 1), (0, 0, 3, 2), (0, 0, 3, 3), (0, 1, 3, 0), (0, 1, 3, 1), (0, 1, 3, 2), (0, 1, 3, 3), (0, 2, 3, 0), (0, 2, 3, 1), (0, 2, 3, 2), (0, 2, 3, 3), (0, 3, 3, 0), (0, 3, 3, 1), (0, 3, 3, 2), (0, 3, 3, 3), (0, 0, 4, 0), (0, 0, 4, 1), (0, 0, 4, 2), (0, 0, 4, 3), (0, 1, 4, 0), (0, 1, 4, 1), (0, 1, 4, 2), (0, 1, 4, 3), (0, 2, 4, 0), (0, 2, 4, 1), (0, 2, 4, 2), (0, 2, 4, 3), (0, 3, 4, 0), (0, 3, 4, 1), (0, 3, 4, 2), (0, 3, 4, 3), (0, 0, 5, 0), (0, 0, 5, 1), (0, 0, 5, 2), (0, 0, 5, 3), (0, 1, 5, 0), (0, 1, 5, 1), (0, 1, 5, 2), (0, 1, 5, 3), (0, 2, 5, 0), (0, 2, 5, 1), (0, 2, 5, 2), (0, 2, 5, 3), (0, 3, 5, 0), (0, 3, 5, 1), (0, 3, 5, 2), (0, 3, 5, 3), (0, 0, 6, 0), (0, 0, 6, 1), (0, 0, 6, 2), (0, 0, 6, 3), (0, 1, 6, 0), (0, 1, 6, 1), (0, 1, 6, 2), (0, 1, 6, 3), (0, 2, 6, 0), (0, 2, 6, 1), (0, 2, 6, 2), (0, 2, 6, 3), (0, 3, 6, 0), (0, 3, 6, 1), (0, 3, 6, 2), (0, 3, 6, 3), (0, 0, 7, 0), (0, 0, 7, 1), (0, 0, 7, 2), (0, 0, 7, 3), (0, 1, 7, 0), (0, 1, 7, 1), (0, 1, 7, 2), (0, 1, 7, 3), (0, 2, 7, 0), (0, 2, 7, 1), (0, 2, 7, 2), (0, 2, 7, 3)] */

/* apply mask, calc new C and issue writes */
buffer_store_dword v7, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v9, v8, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v11, v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v13, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v15, v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v17, v16, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v19, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v21, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v23, v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v25, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v27, v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v29, v28, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v31, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v33, v32, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v35, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v37, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v39, v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v41, v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v43, v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v45, v44, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v47, v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v49, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v51, v50, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v53, v52, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v55, v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v57, v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v59, v58, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v61, v60, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v63, v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v65, v64, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v67, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v69, v68, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v71, v70, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v73, v72, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v75, v74, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v77, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v79, v78, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v81, v80, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v83, v82, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v85, v84, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v87, v86, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v89, v88, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v91, v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v93, v92, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v95, v94, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v97, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v99, v98, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v101, v100, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v103, v102, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v105, v104, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v107, v106, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v109, v108, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v111, v110, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v113, v112, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v115, v114, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v117, v116, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v119, v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v121, v120, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v123, v122, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v125, v124, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v127, v126, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v129, v128, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v131, v130, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v133, v132, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v135, v134, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v137, v136, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v140, v138, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v142, v141, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v144, v143, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v146, v145, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v148, v147, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v150, v149, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v152, v151, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v154, v153, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v156, v155, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v158, v157, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v160, v159, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v162, v161, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v164, v163, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v166, v165, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v168, v167, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v170, v169, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v172, v171, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v174, v173, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v176, v175, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v178, v177, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v180, v179, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v182, v181, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v184, v183, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v186, v185, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v188, v187, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v190, v189, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v192, v191, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v194, v193, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v196, v195, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v198, v197, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v200, v199, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v202, v201, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v204, v203, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v206, v205, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v208, v207, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v210, v209, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v212, v211, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v214, v213, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v216, v215, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v218, v217, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v220, v219, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v222, v221, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v224, v223, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v226, v225, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v228, v227, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v230, v229, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v232, v231, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v234, v233, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v236, v235, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v238, v237, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v240, v239, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v242, v241, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v244, v243, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v246, v245, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v248, v247, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v250, v249, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v252, v251, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v254, v253, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,3,7,0:vw1); (0,3,7,1:vw1); (0,3,7,2:vw1); (0,3,7,3:vw1); (1,0,0,0:vw1); (1,0,0,1:vw1); (1,0,0,2:vw1); (1,0,0,3:vw1); (1,1,0,0:vw1); (1,1,0,1:vw1); (1,1,0,2:vw1); (1,1,0,3:vw1); (1,2,0,0:vw1); (1,2,0,1:vw1); (1,2,0,2:vw1); (1,2,0,3:vw1); (1,3,0,0:vw1); (1,3,0,1:vw1); (1,3,0,2:vw1); (1,3,0,3:vw1); (1,0,1,0:vw1); (1,0,1,1:vw1); (1,0,1,2:vw1); (1,0,1,3:vw1); (1,1,1,0:vw1); (1,1,1,1:vw1); (1,1,1,2:vw1); (1,1,1,3:vw1); (1,2,1,0:vw1); (1,2,1,1:vw1); (1,2,1,2:vw1); (1,2,1,3:vw1); (1,3,1,0:vw1); (1,3,1,1:vw1); (1,3,1,2:vw1); (1,3,1,3:vw1); (1,0,2,0:vw1); (1,0,2,1:vw1); (1,0,2,2:vw1); (1,0,2,3:vw1); (1,1,2,0:vw1); (1,1,2,1:vw1); (1,1,2,2:vw1); (1,1,2,3:vw1); (1,2,2,0:vw1); (1,2,2,1:vw1); (1,2,2,2:vw1); (1,2,2,3:vw1); (1,3,2,0:vw1); (1,3,2,1:vw1); (1,3,2,2:vw1); (1,3,2,3:vw1); (1,0,3,0:vw1); (1,0,3,1:vw1); (1,0,3,2:vw1); (1,0,3,3:vw1); (1,1,3,0:vw1); (1,1,3,1:vw1); (1,1,3,2:vw1); (1,1,3,3:vw1); (1,2,3,0:vw1); (1,2,3,1:vw1); (1,2,3,2:vw1); (1,2,3,3:vw1); (1,3,3,0:vw1); (1,3,3,1:vw1); (1,3,3,2:vw1); (1,3,3,3:vw1); (1,0,4,0:vw1); (1,0,4,1:vw1); (1,0,4,2:vw1); (1,0,4,3:vw1); (1,1,4,0:vw1); (1,1,4,1:vw1); (1,1,4,2:vw1); (1,1,4,3:vw1); (1,2,4,0:vw1); (1,2,4,1:vw1); (1,2,4,2:vw1); (1,2,4,3:vw1); (1,3,4,0:vw1); (1,3,4,1:vw1); (1,3,4,2:vw1); (1,3,4,3:vw1); (1,0,5,0:vw1); (1,0,5,1:vw1); (1,0,5,2:vw1); (1,0,5,3:vw1); (1,1,5,0:vw1); (1,1,5,1:vw1); (1,1,5,2:vw1); (1,1,5,3:vw1); (1,2,5,0:vw1); (1,2,5,1:vw1); (1,2,5,2:vw1); (1,2,5,3:vw1); (1,3,5,0:vw1); (1,3,5,1:vw1); (1,3,5,2:vw1); (1,3,5,3:vw1); (1,0,6,0:vw1); (1,0,6,1:vw1); (1,0,6,2:vw1); (1,0,6,3:vw1); (1,1,6,0:vw1); (1,1,6,1:vw1); (1,1,6,2:vw1); (1,1,6,3:vw1); (1,2,6,0:vw1); (1,2,6,1:vw1); (1,2,6,2:vw1); (1,2,6,3:vw1); (1,3,6,0:vw1); (1,3,6,1:vw1); (1,3,6,2:vw1); (1,3,6,3:vw1); (1,0,7,0:vw1); (1,0,7,1:vw1); (1,0,7,2:vw1); (1,0,7,3:vw1); (1,1,7,0:vw1); (1,1,7,1:vw1); (1,1,7,2:vw1); (1,1,7,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v255, BufferOOB
/* (d1,vc1,d0,vc0)=(0,7,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v6, v3, v4, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v255, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v8, v3, v4, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v8, v255, v8, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v10, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v255, v10, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v12, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v255, v12, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,0) */
s_mov_b32 s52, 121                                 // rowInc d1=0 vc1=0
v_add_co_u32 v1, vcc, v1, s52                      // coord1.2: coord1 += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s52, s[sgprStrideC1J], 121               // scale stride
v_add_i32 v2, v2, s52                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s52, s[sgprStrideD1J], 121               // scale stride
v_add_i32 v3, v3, s52                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v14, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v255, v14, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v16, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v16, v255, v16, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v18, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v255, v18, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v20, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v255, v20, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v22, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v255, v22, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v24, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v255, v24, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v26, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v255, v26, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v28, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v255, v28, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v30, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v255, v30, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v32, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v32, v255, v32, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v34, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v255, v34, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v36, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v255, v36, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v38, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v255, v38, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v40, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v255, v40, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v42, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v255, v42, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v44, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v44, v255, v44, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v46, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v255, v46, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v48, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v255, v48, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v50, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v50, v255, v50, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v52, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v52, v255, v52, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v54, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v255, v54, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v56, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v255, v56, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v58, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v58, v255, v58, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v60, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v60, v255, v60, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v62, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v255, v62, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v64, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v64, v255, v64, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v66, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v255, v66, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v68, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v68, v255, v68, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v70, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v70, v255, v70, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v72, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v72, v255, v72, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v74, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v74, v255, v74, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v76, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v255, v76, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v78, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v78, v255, v78, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v80, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v80, v255, v80, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v82, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v82, v255, v82, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v84, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v255, v84, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v86, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v255, v86, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v88, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v88, v255, v88, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v90, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v255, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v92, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v92, v255, v92, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v94, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v94, v255, v94, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v96, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v255, v96, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v98, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v98, v255, v98, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v100, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v100, v255, v100, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v102, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v102, v255, v102, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v104, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v104, v255, v104, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v106, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v255, v106, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v108, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v108, v255, v108, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v110, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v110, v255, v110, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v112, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v112, v255, v112, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v114, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v114, v255, v114, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v116, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v116, v255, v116, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v118, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v255, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v120, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v120, v255, v120, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v122, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v122, v255, v122, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v124, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v124, v255, v124, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v126, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v126, v255, v126, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v128, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v255, v128, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v130, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v130, v255, v130, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v132, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v132, v255, v132, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v134, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v134, v255, v134, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v136, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v136, v255, v136, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v138, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v255, v138, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v141, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v141, v255, v141, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v143, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v143, v255, v143, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v145, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v145, v255, v145, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v147, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v147, v255, v147, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v149, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v255, v149, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v151, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v151, v255, v151, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v153, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v153, v255, v153, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v155, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v155, v255, v155, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v157, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v157, v255, v157, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v159, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v255, v159, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v161, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v161, v255, v161, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v163, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v163, v255, v163, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v165, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v165, v255, v165, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v167, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v167, v255, v167, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v169, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v169, v255, v169, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v171, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v171, v255, v171, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v173, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v173, v255, v173, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v175, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v175, v255, v175, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v177, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v177, v255, v177, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v179, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v255, v179, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v181, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v181, v255, v181, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v183, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v183, v255, v183, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v185, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v185, v255, v185, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v187, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v187, v255, v187, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v189, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v189, v255, v189, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v191, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v191, v255, v191, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v193, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v193, v255, v193, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v195, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v195, v255, v195, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v197, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v197, v255, v197, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v199, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v199, v255, v199, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v201, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v201, v255, v201, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v203, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v203, v255, v203, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v205, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v205, v255, v205, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v207, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v207, v255, v207, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v209, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v209, v255, v209, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v211, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v211, v255, v211, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v213, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v213, v255, v213, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v215, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v215, v255, v215, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v217, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v217, v255, v217, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v219, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v219, v255, v219, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v221, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v221, v255, v221, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v223, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v223, v255, v223, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v225, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v225, v255, v225, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v227, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v227, v255, v227, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v229, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v229, v255, v229, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v231, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v231, v255, v231, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v233, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v233, v255, v233, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v235, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v235, v255, v235, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v237, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v237, v255, v237, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v239, v3, v0, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v239, v255, v239, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v241, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v241, v255, v241, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v243, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v243, v255, v243, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v245, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v245, v255, v245, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v247, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v247, v255, v247, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v249, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v249, v255, v249, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v251, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v251, v255, v251, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v253, v3, v4, 0x2                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v253, v255, v253, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+7], acc124          // copy acc to vreg[124]
v_accvgpr_read_b32 v[vgprValuC+9], acc125          // copy acc to vreg[125]
v_accvgpr_read_b32 v[vgprValuC+11], acc126         // copy acc to vreg[126]
v_accvgpr_read_b32 v[vgprValuC+13], acc127         // copy acc to vreg[127]
v_accvgpr_read_b32 v[vgprValuC+15], acc128         // copy acc to vreg[128]
v_accvgpr_read_b32 v[vgprValuC+17], acc129         // copy acc to vreg[129]
v_accvgpr_read_b32 v[vgprValuC+19], acc130         // copy acc to vreg[130]
v_accvgpr_read_b32 v[vgprValuC+21], acc131         // copy acc to vreg[131]
v_accvgpr_read_b32 v[vgprValuC+23], acc132         // copy acc to vreg[132]
v_accvgpr_read_b32 v[vgprValuC+25], acc133         // copy acc to vreg[133]
v_accvgpr_read_b32 v[vgprValuC+27], acc134         // copy acc to vreg[134]
v_accvgpr_read_b32 v[vgprValuC+29], acc135         // copy acc to vreg[135]
v_accvgpr_read_b32 v[vgprValuC+31], acc136         // copy acc to vreg[136]
v_accvgpr_read_b32 v[vgprValuC+33], acc137         // copy acc to vreg[137]
v_accvgpr_read_b32 v[vgprValuC+35], acc138         // copy acc to vreg[138]
v_accvgpr_read_b32 v[vgprValuC+37], acc139         // copy acc to vreg[139]
v_accvgpr_read_b32 v[vgprValuC+39], acc140         // copy acc to vreg[140]
v_accvgpr_read_b32 v[vgprValuC+41], acc141         // copy acc to vreg[141]
v_accvgpr_read_b32 v[vgprValuC+43], acc142         // copy acc to vreg[142]
v_accvgpr_read_b32 v[vgprValuC+45], acc143         // copy acc to vreg[143]
v_accvgpr_read_b32 v[vgprValuC+47], acc144         // copy acc to vreg[144]
v_accvgpr_read_b32 v[vgprValuC+49], acc145         // copy acc to vreg[145]
v_accvgpr_read_b32 v[vgprValuC+51], acc146         // copy acc to vreg[146]
v_accvgpr_read_b32 v[vgprValuC+53], acc147         // copy acc to vreg[147]
v_accvgpr_read_b32 v[vgprValuC+55], acc148         // copy acc to vreg[148]
v_accvgpr_read_b32 v[vgprValuC+57], acc149         // copy acc to vreg[149]
v_accvgpr_read_b32 v[vgprValuC+59], acc150         // copy acc to vreg[150]
v_accvgpr_read_b32 v[vgprValuC+61], acc151         // copy acc to vreg[151]
v_accvgpr_read_b32 v[vgprValuC+63], acc152         // copy acc to vreg[152]
v_accvgpr_read_b32 v[vgprValuC+65], acc153         // copy acc to vreg[153]
v_accvgpr_read_b32 v[vgprValuC+67], acc154         // copy acc to vreg[154]
v_accvgpr_read_b32 v[vgprValuC+69], acc155         // copy acc to vreg[155]
v_accvgpr_read_b32 v[vgprValuC+71], acc156         // copy acc to vreg[156]
v_accvgpr_read_b32 v[vgprValuC+73], acc157         // copy acc to vreg[157]
v_accvgpr_read_b32 v[vgprValuC+75], acc158         // copy acc to vreg[158]
v_accvgpr_read_b32 v[vgprValuC+77], acc159         // copy acc to vreg[159]
v_accvgpr_read_b32 v[vgprValuC+79], acc160         // copy acc to vreg[160]
v_accvgpr_read_b32 v[vgprValuC+81], acc161         // copy acc to vreg[161]
v_accvgpr_read_b32 v[vgprValuC+83], acc162         // copy acc to vreg[162]
v_accvgpr_read_b32 v[vgprValuC+85], acc163         // copy acc to vreg[163]
v_accvgpr_read_b32 v[vgprValuC+87], acc164         // copy acc to vreg[164]
v_accvgpr_read_b32 v[vgprValuC+89], acc165         // copy acc to vreg[165]
v_accvgpr_read_b32 v[vgprValuC+91], acc166         // copy acc to vreg[166]
v_accvgpr_read_b32 v[vgprValuC+93], acc167         // copy acc to vreg[167]
v_accvgpr_read_b32 v[vgprValuC+95], acc168         // copy acc to vreg[168]
v_accvgpr_read_b32 v[vgprValuC+97], acc169         // copy acc to vreg[169]
v_accvgpr_read_b32 v[vgprValuC+99], acc170         // copy acc to vreg[170]
v_accvgpr_read_b32 v[vgprValuC+101], acc171        // copy acc to vreg[171]
v_accvgpr_read_b32 v[vgprValuC+103], acc172        // copy acc to vreg[172]
v_accvgpr_read_b32 v[vgprValuC+105], acc173        // copy acc to vreg[173]
v_accvgpr_read_b32 v[vgprValuC+107], acc174        // copy acc to vreg[174]
v_accvgpr_read_b32 v[vgprValuC+109], acc175        // copy acc to vreg[175]
v_accvgpr_read_b32 v[vgprValuC+111], acc176        // copy acc to vreg[176]
v_accvgpr_read_b32 v[vgprValuC+113], acc177        // copy acc to vreg[177]
v_accvgpr_read_b32 v[vgprValuC+115], acc178        // copy acc to vreg[178]
v_accvgpr_read_b32 v[vgprValuC+117], acc179        // copy acc to vreg[179]
v_accvgpr_read_b32 v[vgprValuC+119], acc180        // copy acc to vreg[180]
v_accvgpr_read_b32 v[vgprValuC+121], acc181        // copy acc to vreg[181]
v_accvgpr_read_b32 v[vgprValuC+123], acc182        // copy acc to vreg[182]
v_accvgpr_read_b32 v[vgprValuC+125], acc183        // copy acc to vreg[183]
v_accvgpr_read_b32 v[vgprValuC+127], acc184        // copy acc to vreg[184]
v_accvgpr_read_b32 v[vgprValuC+129], acc185        // copy acc to vreg[185]
v_accvgpr_read_b32 v[vgprValuC+131], acc186        // copy acc to vreg[186]
v_accvgpr_read_b32 v[vgprValuC+133], acc187        // copy acc to vreg[187]
v_accvgpr_read_b32 v[vgprValuC+135], acc188        // copy acc to vreg[188]
v_accvgpr_read_b32 v[vgprValuC+137], acc189        // copy acc to vreg[189]
v_accvgpr_read_b32 v[vgprValuC+140], acc190        // copy acc to vreg[190]
v_accvgpr_read_b32 v[vgprValuC+142], acc191        // copy acc to vreg[191]
v_accvgpr_read_b32 v[vgprValuC+144], acc192        // copy acc to vreg[192]
v_accvgpr_read_b32 v[vgprValuC+146], acc193        // copy acc to vreg[193]
v_accvgpr_read_b32 v[vgprValuC+148], acc194        // copy acc to vreg[194]
v_accvgpr_read_b32 v[vgprValuC+150], acc195        // copy acc to vreg[195]
v_accvgpr_read_b32 v[vgprValuC+152], acc196        // copy acc to vreg[196]
v_accvgpr_read_b32 v[vgprValuC+154], acc197        // copy acc to vreg[197]
v_accvgpr_read_b32 v[vgprValuC+156], acc198        // copy acc to vreg[198]
v_accvgpr_read_b32 v[vgprValuC+158], acc199        // copy acc to vreg[199]
v_accvgpr_read_b32 v[vgprValuC+160], acc200        // copy acc to vreg[200]
v_accvgpr_read_b32 v[vgprValuC+162], acc201        // copy acc to vreg[201]
v_accvgpr_read_b32 v[vgprValuC+164], acc202        // copy acc to vreg[202]
v_accvgpr_read_b32 v[vgprValuC+166], acc203        // copy acc to vreg[203]
v_accvgpr_read_b32 v[vgprValuC+168], acc204        // copy acc to vreg[204]
v_accvgpr_read_b32 v[vgprValuC+170], acc205        // copy acc to vreg[205]
v_accvgpr_read_b32 v[vgprValuC+172], acc206        // copy acc to vreg[206]
v_accvgpr_read_b32 v[vgprValuC+174], acc207        // copy acc to vreg[207]
v_accvgpr_read_b32 v[vgprValuC+176], acc208        // copy acc to vreg[208]
v_accvgpr_read_b32 v[vgprValuC+178], acc209        // copy acc to vreg[209]
v_accvgpr_read_b32 v[vgprValuC+180], acc210        // copy acc to vreg[210]
v_accvgpr_read_b32 v[vgprValuC+182], acc211        // copy acc to vreg[211]
v_accvgpr_read_b32 v[vgprValuC+184], acc212        // copy acc to vreg[212]
v_accvgpr_read_b32 v[vgprValuC+186], acc213        // copy acc to vreg[213]
v_accvgpr_read_b32 v[vgprValuC+188], acc214        // copy acc to vreg[214]
v_accvgpr_read_b32 v[vgprValuC+190], acc215        // copy acc to vreg[215]
v_accvgpr_read_b32 v[vgprValuC+192], acc216        // copy acc to vreg[216]
v_accvgpr_read_b32 v[vgprValuC+194], acc217        // copy acc to vreg[217]
v_accvgpr_read_b32 v[vgprValuC+196], acc218        // copy acc to vreg[218]
v_accvgpr_read_b32 v[vgprValuC+198], acc219        // copy acc to vreg[219]
v_accvgpr_read_b32 v[vgprValuC+200], acc220        // copy acc to vreg[220]
v_accvgpr_read_b32 v[vgprValuC+202], acc221        // copy acc to vreg[221]
v_accvgpr_read_b32 v[vgprValuC+204], acc222        // copy acc to vreg[222]
v_accvgpr_read_b32 v[vgprValuC+206], acc223        // copy acc to vreg[223]
v_accvgpr_read_b32 v[vgprValuC+208], acc224        // copy acc to vreg[224]
v_accvgpr_read_b32 v[vgprValuC+210], acc225        // copy acc to vreg[225]
v_accvgpr_read_b32 v[vgprValuC+212], acc226        // copy acc to vreg[226]
v_accvgpr_read_b32 v[vgprValuC+214], acc227        // copy acc to vreg[227]
v_accvgpr_read_b32 v[vgprValuC+216], acc228        // copy acc to vreg[228]
v_accvgpr_read_b32 v[vgprValuC+218], acc229        // copy acc to vreg[229]
v_accvgpr_read_b32 v[vgprValuC+220], acc230        // copy acc to vreg[230]
v_accvgpr_read_b32 v[vgprValuC+222], acc231        // copy acc to vreg[231]
v_accvgpr_read_b32 v[vgprValuC+224], acc232        // copy acc to vreg[232]
v_accvgpr_read_b32 v[vgprValuC+226], acc233        // copy acc to vreg[233]
v_accvgpr_read_b32 v[vgprValuC+228], acc234        // copy acc to vreg[234]
v_accvgpr_read_b32 v[vgprValuC+230], acc235        // copy acc to vreg[235]
v_accvgpr_read_b32 v[vgprValuC+232], acc236        // copy acc to vreg[236]
v_accvgpr_read_b32 v[vgprValuC+234], acc237        // copy acc to vreg[237]
v_accvgpr_read_b32 v[vgprValuC+236], acc238        // copy acc to vreg[238]
v_accvgpr_read_b32 v[vgprValuC+238], acc239        // copy acc to vreg[239]
v_accvgpr_read_b32 v[vgprValuC+240], acc240        // copy acc to vreg[240]
v_accvgpr_read_b32 v[vgprValuC+242], acc241        // copy acc to vreg[241]
v_accvgpr_read_b32 v[vgprValuC+244], acc242        // copy acc to vreg[242]
v_accvgpr_read_b32 v[vgprValuC+246], acc243        // copy acc to vreg[243]
v_accvgpr_read_b32 v[vgprValuC+248], acc244        // copy acc to vreg[244]
v_accvgpr_read_b32 v[vgprValuC+250], acc245        // copy acc to vreg[245]
v_accvgpr_read_b32 v[vgprValuC+252], acc246        // copy acc to vreg[246]
v_accvgpr_read_b32 v[vgprValuC+254], acc247        // copy acc to vreg[247]

/* rC *= alpha batchElements=[(0, 3, 7, 0), (0, 3, 7, 1), (0, 3, 7, 2), (0, 3, 7, 3), (1, 0, 0, 0), (1, 0, 0, 1), (1, 0, 0, 2), (1, 0, 0, 3), (1, 1, 0, 0), (1, 1, 0, 1), (1, 1, 0, 2), (1, 1, 0, 3), (1, 2, 0, 0), (1, 2, 0, 1), (1, 2, 0, 2), (1, 2, 0, 3), (1, 3, 0, 0), (1, 3, 0, 1), (1, 3, 0, 2), (1, 3, 0, 3), (1, 0, 1, 0), (1, 0, 1, 1), (1, 0, 1, 2), (1, 0, 1, 3), (1, 1, 1, 0), (1, 1, 1, 1), (1, 1, 1, 2), (1, 1, 1, 3), (1, 2, 1, 0), (1, 2, 1, 1), (1, 2, 1, 2), (1, 2, 1, 3), (1, 3, 1, 0), (1, 3, 1, 1), (1, 3, 1, 2), (1, 3, 1, 3), (1, 0, 2, 0), (1, 0, 2, 1), (1, 0, 2, 2), (1, 0, 2, 3), (1, 1, 2, 0), (1, 1, 2, 1), (1, 1, 2, 2), (1, 1, 2, 3), (1, 2, 2, 0), (1, 2, 2, 1), (1, 2, 2, 2), (1, 2, 2, 3), (1, 3, 2, 0), (1, 3, 2, 1), (1, 3, 2, 2), (1, 3, 2, 3), (1, 0, 3, 0), (1, 0, 3, 1), (1, 0, 3, 2), (1, 0, 3, 3), (1, 1, 3, 0), (1, 1, 3, 1), (1, 1, 3, 2), (1, 1, 3, 3), (1, 2, 3, 0), (1, 2, 3, 1), (1, 2, 3, 2), (1, 2, 3, 3), (1, 3, 3, 0), (1, 3, 3, 1), (1, 3, 3, 2), (1, 3, 3, 3), (1, 0, 4, 0), (1, 0, 4, 1), (1, 0, 4, 2), (1, 0, 4, 3), (1, 1, 4, 0), (1, 1, 4, 1), (1, 1, 4, 2), (1, 1, 4, 3), (1, 2, 4, 0), (1, 2, 4, 1), (1, 2, 4, 2), (1, 2, 4, 3), (1, 3, 4, 0), (1, 3, 4, 1), (1, 3, 4, 2), (1, 3, 4, 3), (1, 0, 5, 0), (1, 0, 5, 1), (1, 0, 5, 2), (1, 0, 5, 3), (1, 1, 5, 0), (1, 1, 5, 1), (1, 1, 5, 2), (1, 1, 5, 3), (1, 2, 5, 0), (1, 2, 5, 1), (1, 2, 5, 2), (1, 2, 5, 3), (1, 3, 5, 0), (1, 3, 5, 1), (1, 3, 5, 2), (1, 3, 5, 3), (1, 0, 6, 0), (1, 0, 6, 1), (1, 0, 6, 2), (1, 0, 6, 3), (1, 1, 6, 0), (1, 1, 6, 1), (1, 1, 6, 2), (1, 1, 6, 3), (1, 2, 6, 0), (1, 2, 6, 1), (1, 2, 6, 2), (1, 2, 6, 3), (1, 3, 6, 0), (1, 3, 6, 1), (1, 3, 6, 2), (1, 3, 6, 3), (1, 0, 7, 0), (1, 0, 7, 1), (1, 0, 7, 2), (1, 0, 7, 3), (1, 1, 7, 0), (1, 1, 7, 1), (1, 1, 7, 2), (1, 1, 7, 3)] */

/* apply mask, calc new C and issue writes */
buffer_store_dword v7, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v9, v8, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v11, v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v13, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v15, v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v17, v16, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v19, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v21, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v23, v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v25, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v27, v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v29, v28, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v31, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v33, v32, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v35, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v37, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v39, v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v41, v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v43, v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v45, v44, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v47, v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v49, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v51, v50, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v53, v52, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v55, v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v57, v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v59, v58, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v61, v60, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v63, v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v65, v64, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v67, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v69, v68, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v71, v70, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v73, v72, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v75, v74, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v77, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v79, v78, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v81, v80, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v83, v82, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v85, v84, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v87, v86, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v89, v88, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v91, v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v93, v92, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v95, v94, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v97, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v99, v98, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v101, v100, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v103, v102, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v105, v104, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v107, v106, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v109, v108, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v111, v110, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v113, v112, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v115, v114, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v117, v116, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v119, v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v121, v120, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v123, v122, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v125, v124, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v127, v126, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v129, v128, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v131, v130, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v133, v132, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v135, v134, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v137, v136, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v140, v138, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v142, v141, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v144, v143, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v146, v145, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v148, v147, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v150, v149, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v152, v151, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v154, v153, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v156, v155, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v158, v157, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v160, v159, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v162, v161, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v164, v163, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v166, v165, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v168, v167, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v170, v169, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v172, v171, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v174, v173, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v176, v175, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v178, v177, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v180, v179, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v182, v181, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v184, v183, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v186, v185, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v188, v187, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v190, v189, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v192, v191, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v194, v193, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v196, v195, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v198, v197, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v200, v199, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v202, v201, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v204, v203, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v206, v205, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v208, v207, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v210, v209, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v212, v211, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v214, v213, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v216, v215, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v218, v217, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v220, v219, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v222, v221, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v224, v223, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v226, v225, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v228, v227, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v230, v229, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v232, v231, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v234, v233, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v236, v235, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v238, v237, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v240, v239, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v242, v241, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v244, v243, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v246, v245, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v248, v247, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v250, v249, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v252, v251, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v254, v253, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #2 (d1,d0,vc1,vc0) = */
/*    (1,2,7,0:vw1); (1,2,7,1:vw1); (1,2,7,2:vw1); (1,2,7,3:vw1); (1,3,7,0:vw1); (1,3,7,1:vw1); (1,3,7,2:vw1); (1,3,7,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v22, BufferOOB
/* (d1,vc1,d0,vc0)=(1,7,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v6, v3, v4, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v22, v6, s[56:57]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v8, v3, v4, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v8, v22, v8, s[56:57]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v10, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v22, v10, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v12, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v22, v12, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v14, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v22, v14, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v16, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v16, v22, v16, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v18, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v22, v18, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v20, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v22, v20, s[56:57]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+7], acc248          // copy acc to vreg[248]
v_accvgpr_read_b32 v[vgprValuC+9], acc249          // copy acc to vreg[249]
v_accvgpr_read_b32 v[vgprValuC+11], acc250         // copy acc to vreg[250]
v_accvgpr_read_b32 v[vgprValuC+13], acc251         // copy acc to vreg[251]
v_accvgpr_read_b32 v[vgprValuC+15], acc252         // copy acc to vreg[252]
v_accvgpr_read_b32 v[vgprValuC+17], acc253         // copy acc to vreg[253]
v_accvgpr_read_b32 v[vgprValuC+19], acc254         // copy acc to vreg[254]
v_accvgpr_read_b32 v[vgprValuC+21], acc255         // copy acc to vreg[255]

/* rC *= alpha batchElements=[(1, 2, 7, 0), (1, 2, 7, 1), (1, 2, 7, 2), (1, 2, 7, 3), (1, 3, 7, 0), (1, 3, 7, 1), (1, 3, 7, 2), (1, 3, 7, 3)] */

/* apply mask, calc new C and issue writes */
buffer_store_dword v7, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v9, v8, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v11, v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v13, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v15, v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v17, v16, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v19, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v21, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_1                            // jump to end
label_GW_End_1:
s_getpc_b64 s[40:41]                               // addr of next instr
s_add_i32 s42, label_KernelEnd, 0x4                // target branch offset
s_add_u32 s40, s40, s42                            // add target branch offset
s_addc_u32 s41, s41, 0                             // add high and carry
s_setpc_b64 s[40:41]                               // branch to label_KernelEnd
label_GSU_5:
.set sgprAddressScaleAlphaVec, 28
.set sgprSrdScaleAlphaVec, 40
s_mov_b32 s[sgprSrdScaleAlphaVec+0], s[sgprAddressScaleAlphaVec+0] // init SRD base address (lower)
s_mov_b32 s[sgprSrdScaleAlphaVec+1], s[sgprAddressScaleAlphaVec+1] // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdScaleAlphaVec+3], Srd127_96     // Set bits 127_96 in post-loop SRD
s_cmp_eq_u64 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], 0 // s[AddressScaleAlphaVec] == 0 ?
s_cbranch_scc0 label_ScaleAlphaVec_1AddrValid      // branch if s[AddressScaleAlphaVec] != 0
s_mov_b32 s[sgprSrdScaleAlphaVec+2], 0
s_branch label_ScaleAlphaVec_1AddrValid_End
label_ScaleAlphaVec_1AddrValid:
s_mov_b32 s[sgprSrdScaleAlphaVec+2], s[sgprSizeI]
label_ScaleAlphaVec_1AddrValid_End:

s_mul_i32 s[sgprSrdScaleAlphaVec+2], 0x4, s[sgprSrdScaleAlphaVec+2] // ScaleAlphaVec scaled by BPE
s_add_u32 s8, s[sgprWorkGroup2], 0x1
s_mul_i32 s8, s[sgprBiasStride], s8                // stride * (wg+1)
s_cmp_eq_u32 s8, 0x0                               // bias stride = 0?
s_cselect_b32 s8, s[sgprSizeI], s8
s_mov_b32 s[sgprSrdBias+0], s[sgprAddressBias+0]   // init SRD base address (lower)
s_mov_b32 s[sgprSrdBias+1], s[sgprAddressBias+1]   // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdBias+3], Srd127_96              // Set bits 127_96 in post-loop SRD
s_cmp_eq_u64 s[sgprAddressBias:sgprAddressBias+1], 0 // s[AddressBias] == 0 ?
s_cbranch_scc0 label_Bias_1AddrValid               // branch if s[AddressBias] != 0
s_mov_b32 s[sgprSrdBias+2], 0
s_branch label_Bias_1AddrValid_End
label_Bias_1AddrValid:
s_mov_b32 s[sgprSrdBias+2], s8
label_Bias_1AddrValid_End:

label_Load_Biasf32_0_1:
s_cmpk_lg_u32 s[sgprBiasType], 0                   // BiasType != 0
s_cbranch_scc1 label_Load_Biasf16_0_1              // Branch if true

/******************************************/
/* Read vector to LDS                     */
/******************************************/
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_add_u32 v8, s52, v[vgprSerial]                   // coord 0 = wgp0 * MT0 + thread offset
s_mul_i32 s[sgprSrdBias+2], 0x4, s[sgprSrdBias+2]  // scaled by BPE
s_mul_i32 s52, s[sgprBiasStride], s[sgprWorkGroup2] // Stride * WG
v_add_u32 v6, s52, v8                              // coord 0 = wgp0 * MT0 + thread offset + Stride * WG
v_lshlrev_b32 v6, 0x2, v6                          // Global bias address scaled by BPE
v_lshlrev_b32 v7, 0x2, v8                          // Global scaleAlpha address scaled by BPE
s_mul_i32 s52, 256, s[sgprWorkGroup1]              // wgp1 * MT1
v_add_u32 v8, s52, v[vgprSerial]                   // coord 1 = wgp1 * MT1 + thread offset
buffer_load_dword v4, v6, s[sgprSrdBias:sgprSrdBias+3], 0 offen offset:0 // Load Bias
buffer_load_dword v5, v7, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // Load ScaleAlphaVec
v_lshlrev_b32 v8, 0x2, v[vgprSerial]               // Local address scaled by BPE
s_barrier                                          // wait for all global loads.
s_waitcnt vmcnt(1)                                 // wait for global load
ds_write_b32 v8, v4 offset:0                       // store bias
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
s_waitcnt vmcnt(0)                                 // wait for global load
v_cndmask_b32 v5, 1.0, v5, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
ds_write_b32 v8, v5 offset:1024                    // store scaleAlpha
s_branch label_Load_Bias_End_1                     // Branch to load bias end
label_Load_Biasf16_0_1:
s_cmpk_lg_u32 s[sgprBiasType], 4                   // BiasType != 4
s_cbranch_scc1 label_Load_Bias_End_1               // Branch if true

/******************************************/
/* Read vector to LDS                     */
/******************************************/
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_add_u32 v8, s52, v[vgprSerial]                   // coord 0 = wgp0 * MT0 + thread offset
s_mul_i32 s[sgprSrdBias+2], 0x2, s[sgprSrdBias+2]  // scaled by BPE
s_mul_i32 s52, s[sgprBiasStride], s[sgprWorkGroup2] // Stride * WG
v_add_u32 v6, s52, v8                              // coord 0 = wgp0 * MT0 + thread offset + Stride * WG
v_lshlrev_b32 v6, 0x1, v6                          // Global bias address scaled by BPE
v_lshlrev_b32 v7, 0x2, v8                          // Global scaleAlpha address scaled by BPE
s_mul_i32 s52, 256, s[sgprWorkGroup1]              // wgp1 * MT1
v_add_u32 v8, s52, v[vgprSerial]                   // coord 1 = wgp1 * MT1 + thread offset
buffer_load_short_d16 v4, v6, s[sgprSrdBias:sgprSrdBias+3], 0 offen offset:0 // Load Bias
buffer_load_dword v5, v7, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // Load ScaleAlphaVec
v_lshlrev_b32 v8, 0x2, v[vgprSerial]               // Local address scaled by BPE
s_barrier                                          // wait for all global loads.
s_waitcnt vmcnt(1)                                 // wait for global load
v_cvt_f32_f16 v4, v4                               // convert to FP32
ds_write_b32 v8, v4 offset:0                       // store bias
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
s_waitcnt vmcnt(0)                                 // wait for global load
v_cndmask_b32 v5, 1.0, v5, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
ds_write_b32 v8, v5 offset:1024                    // store scaleAlpha
s_branch label_Load_Bias_End_1                     // Branch to load bias end
label_Load_Bias_End_1:
.set sgprAddressScaleAlphaVec, UNDEF
.set sgprSrdScaleAlphaVec, UNDEF
s_cmpk_eq_u32 s[sgprBeta], 0x0                     // Beta == 0
s_cbranch_scc0 label_GW_Beta_2                     // Branch if Beta is not zero

s_and_b32 s40, 255, s[sgprSizeI]                   // s40 = s[sgprSizeI] % 256
s_add_u32 s41, -0x1, s[sgprNumWorkGroups0]
s_cmp_ge_u32 s[sgprWorkGroup0], s41                // wg0 >= nwg0-1 ?
s_cselect_b32 s40, s40, 0                          // set rMT0
s_cmpk_gt_u32 s40, 0x0                             // rMT0 > 0
s_cbranch_scc0 label_NoBranch_MNRAEB4P53IF2YFY_0   // Only branch on scc1
// jump if edges required
s_getpc_b64 s[40:41]                               // addr of next instr
s_add_i32 s42, label_GW_B0_E1_M_1, 0x4             // target branch offset
s_add_u32 s40, s40, s42                            // add target branch offset
s_addc_u32 s41, s41, 0                             // add high and carry
s_setpc_b64 s[40:41]                               // branch to label_GW_B0_E1_M_1
label_NoBranch_MNRAEB4P53IF2YFY_0:
s_and_b32 s40, 255, s[sgprSizeJ]                   // s40 = s[sgprSizeJ] % 256
s_add_u32 s41, -0x1, s[sgprNumWorkGroups1]
s_cmp_ge_u32 s[sgprWorkGroup1], s41                // wg1 >= nwg1-1
s_cselect_b32 s40, s40, 0                          // set rMT1
s_cmpk_gt_u32 s40, 0x0                             // rMT1 > 0
s_cbranch_scc0 label_NoBranch_2PSBPCILZMFAPXL0_0   // Only branch on scc1
// jump if edges required
s_getpc_b64 s[40:41]                               // addr of next instr
s_add_i32 s42, label_GW_B0_E1_N_1, 0x4             // target branch offset
s_add_u32 s40, s40, s42                            // add target branch offset
s_addc_u32 s41, s41, 0                             // add high and carry
s_setpc_b64 s[40:41]                               // branch to label_GW_B0_E1_N_1
label_NoBranch_2PSBPCILZMFAPXL0_0:
label_GW_B0_E0_2:

/* edge=0, allocate 2 sgpr. perBatchTmpS=2 perBatchMaskS=0 perElementMaskS=0 elementsPerBatch=18 */
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,1,0,0:vw4); (0,2,0,0:vw4); (0,3,0,0:vw4); (0,0,1,0:vw4); (0,1,1,0:vw4); (0,2,1,0:vw4); (0,3,1,0:vw4); (0,0,2,0:vw4); (0,1,2,0:vw4); (0,2,2,0:vw4); (0,3,2,0:vw4); (0,0,3,0:vw4); (0,1,3,0:vw4); (0,2,3,0:vw4); (0,3,3,0:vw4); (0,0,4,0:vw4); (0,1,4,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s12, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s12
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for LDS write
s_barrier                                          // LDS write barrier
ds_read_b128 v[12:15], v8 offset:0                 // load Bias
v_add_u32 v9, 1024, v8                             // add ScaleAlphaVec offset (1)
ds_read_b128 v[16:19], v9 offset:0                 // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
ds_read_b128 v[24:27], v8 offset:256               // load Bias
ds_read_b128 v[28:31], v9 offset:256               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,0,2,0) */
ds_read_b128 v[36:39], v8 offset:512               // load Bias
ds_read_b128 v[40:43], v9 offset:512               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,0,3,0) */
ds_read_b128 v[48:51], v8 offset:768               // load Bias
ds_read_b128 v[52:55], v9 offset:768               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
/* (d1,vc1,d0,vc0)=(0,1,2,0) */
/* (d1,vc1,d0,vc0)=(0,1,3,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
/* (d1,vc1,d0,vc0)=(0,2,2,0) */
/* (d1,vc1,d0,vc0)=(0,2,3,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
/* (d1,vc1,d0,vc0)=(0,3,2,0) */
/* (d1,vc1,d0,vc0)=(0,3,3,0) */
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
v_add_lshl_u32 v6, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc1           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+44], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+45], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+46], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+47], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+56], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+57], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+58], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+59], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+60], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+61], acc17          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+62], acc18          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+63], acc19          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+64], acc20          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+65], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+66], acc22          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+67], acc23          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+68], acc24          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+69], acc25          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+70], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+71], acc27          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+72], acc28          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+73], acc29          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+74], acc30          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+75], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+76], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+77], acc33          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+78], acc34          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+79], acc35          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+80], acc36          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+81], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+82], acc38          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+83], acc39          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+84], acc40          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+85], acc41          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+86], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+87], acc43          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+88], acc44          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+89], acc45          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+90], acc46          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+91], acc47          // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+92], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+93], acc49          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+94], acc50          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+95], acc51          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+96], acc52          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+97], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+98], acc54          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+99], acc55          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+100], acc56         // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+101], acc57         // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+102], acc58         // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+103], acc59         // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+104], acc60         // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+105], acc61         // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+106], acc62         // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+107], acc63         // copy acc to vreg[63]
v_accvgpr_read_b32 v[vgprValuC+108], acc64         // copy acc to vreg[64]
v_accvgpr_read_b32 v[vgprValuC+109], acc65         // copy acc to vreg[65]
v_accvgpr_read_b32 v[vgprValuC+110], acc66         // copy acc to vreg[66]
v_accvgpr_read_b32 v[vgprValuC+111], acc67         // copy acc to vreg[67]
v_accvgpr_read_b32 v[vgprValuC+112], acc68         // copy acc to vreg[68]
v_accvgpr_read_b32 v[vgprValuC+113], acc69         // copy acc to vreg[69]
v_accvgpr_read_b32 v[vgprValuC+114], acc70         // copy acc to vreg[70]
v_accvgpr_read_b32 v[vgprValuC+115], acc71         // copy acc to vreg[71]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 1, 0, 0), (0, 2, 0, 0), (0, 3, 0, 0), (0, 0, 1, 0), (0, 1, 1, 0), (0, 2, 1, 0), (0, 3, 1, 0), (0, 0, 2, 0), (0, 1, 2, 0), (0, 2, 2, 0), (0, 3, 2, 0), (0, 0, 3, 0), (0, 1, 3, 0), (0, 2, 3, 0), (0, 3, 3, 0), (0, 0, 4, 0), (0, 1, 4, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+97], s[sgprAlpha], v[vgprValuC+97] // *= alpha
v_mul_f32 v[vgprValuC+98], s[sgprAlpha], v[vgprValuC+98] // *= alpha
v_mul_f32 v[vgprValuC+99], s[sgprAlpha], v[vgprValuC+99] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+104], s[sgprAlpha], v[vgprValuC+104] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+107], s[sgprAlpha], v[vgprValuC+107] // *= alpha
v_mul_f32 v[vgprValuC+108], s[sgprAlpha], v[vgprValuC+108] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+112], s[sgprAlpha], v[vgprValuC+112] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+114], s[sgprAlpha], v[vgprValuC+114] // *= alpha
v_mul_f32 v[vgprValuC+115], s[sgprAlpha], v[vgprValuC+115] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(6)                               // lgkmcnt(6) = 8 - 1 (bias) - 1 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt lgkmcnt(4)                               // lgkmcnt(4) = 8 - 2 (bias) - 2 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D

s_waitcnt lgkmcnt(2)                               // lgkmcnt(2) = 8 - 3 (bias) - 3 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[40:41], v[vgprValuC+44:vgprValuC+44+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[42:43], v[vgprValuC+46:vgprValuC+46+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[36:37], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[38:39], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt lgkmcnt(0)                               // lgkmcnt(0) = 8 - 4 (bias) - 4 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[52:53], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[54:55], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[48:49], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[50:51], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[12:13], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[14:15], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[60:61], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[28:29], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[30:31], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[40:41], v[vgprValuC+68:vgprValuC+68+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[42:43], v[vgprValuC+70:vgprValuC+70+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+68:vgprValuC+68+1], v[36:37], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[vgprValuC+70:vgprValuC+70+1], v[38:39], v[vgprValuC+70:vgprValuC+70+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+68], v[vgprValuC+68]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+69], v[vgprValuC+69]     // convert C to fp16
v_pack_b32_f16 v68, v[vgprValuC+68], v[vgprValuC+69] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+70], v[vgprValuC+70]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+71], v[vgprValuC+71]     // convert C to fp16
v_pack_b32_f16 v69, v[vgprValuC+70], v[vgprValuC+71] // Pack with neighbor
buffer_store_dwordx2 v[68:69], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[52:53], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[54:55], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[48:49], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[50:51], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[16:17], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[18:19], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[12:13], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[14:15], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[76:77], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[28:29], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[30:31], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[40:41], v[vgprValuC+84:vgprValuC+84+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[42:43], v[vgprValuC+86:vgprValuC+86+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+84:vgprValuC+84+1], v[36:37], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[vgprValuC+86:vgprValuC+86+1], v[38:39], v[vgprValuC+86:vgprValuC+86+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+84], v[vgprValuC+84]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+85], v[vgprValuC+85]     // convert C to fp16
v_pack_b32_f16 v84, v[vgprValuC+84], v[vgprValuC+85] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+86], v[vgprValuC+86]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+87], v[vgprValuC+87]     // convert C to fp16
v_pack_b32_f16 v85, v[vgprValuC+86], v[vgprValuC+87] // Pack with neighbor
buffer_store_dwordx2 v[84:85], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[52:53], v[vgprValuC+88:vgprValuC+88+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[54:55], v[vgprValuC+90:vgprValuC+90+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+88:vgprValuC+88+1], v[48:49], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[vgprValuC+90:vgprValuC+90+1], v[50:51], v[vgprValuC+90:vgprValuC+90+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+88], v[vgprValuC+88]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+89], v[vgprValuC+89]     // convert C to fp16
v_pack_b32_f16 v88, v[vgprValuC+88], v[vgprValuC+89] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+90], v[vgprValuC+90]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+91], v[vgprValuC+91]     // convert C to fp16
v_pack_b32_f16 v89, v[vgprValuC+90], v[vgprValuC+91] // Pack with neighbor
buffer_store_dwordx2 v[88:89], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[16:17], v[vgprValuC+92:vgprValuC+92+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[18:19], v[vgprValuC+94:vgprValuC+94+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+92:vgprValuC+92+1], v[12:13], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[vgprValuC+94:vgprValuC+94+1], v[14:15], v[vgprValuC+94:vgprValuC+94+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+92], v[vgprValuC+92]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+93], v[vgprValuC+93]     // convert C to fp16
v_pack_b32_f16 v92, v[vgprValuC+92], v[vgprValuC+93] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+94], v[vgprValuC+94]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+95], v[vgprValuC+95]     // convert C to fp16
v_pack_b32_f16 v93, v[vgprValuC+94], v[vgprValuC+95] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[92:93], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+96:vgprValuC+96+1], v[28:29], v[vgprValuC+96:vgprValuC+96+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+98:vgprValuC+98+1], v[30:31], v[vgprValuC+98:vgprValuC+98+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+96:vgprValuC+96+1], v[24:25], v[vgprValuC+96:vgprValuC+96+1] // C += bias
v_pk_add_f32 v[vgprValuC+98:vgprValuC+98+1], v[26:27], v[vgprValuC+98:vgprValuC+98+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+96], v[vgprValuC+96]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+97], v[vgprValuC+97]     // convert C to fp16
v_pack_b32_f16 v96, v[vgprValuC+96], v[vgprValuC+97] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+98], v[vgprValuC+98]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+99], v[vgprValuC+99]     // convert C to fp16
v_pack_b32_f16 v97, v[vgprValuC+98], v[vgprValuC+99] // Pack with neighbor
buffer_store_dwordx2 v[96:97], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[40:41], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[42:43], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[36:37], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[38:39], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
buffer_store_dwordx2 v[100:101], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[52:53], v[vgprValuC+104:vgprValuC+104+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[54:55], v[vgprValuC+106:vgprValuC+106+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+104:vgprValuC+104+1], v[48:49], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[vgprValuC+106:vgprValuC+106+1], v[50:51], v[vgprValuC+106:vgprValuC+106+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+104], v[vgprValuC+104]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+105], v[vgprValuC+105]   // convert C to fp16
v_pack_b32_f16 v104, v[vgprValuC+104], v[vgprValuC+105] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+106], v[vgprValuC+106]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+107], v[vgprValuC+107]   // convert C to fp16
v_pack_b32_f16 v105, v[vgprValuC+106], v[vgprValuC+107] // Pack with neighbor
buffer_store_dwordx2 v[104:105], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[16:17], v[vgprValuC+108:vgprValuC+108+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[18:19], v[vgprValuC+110:vgprValuC+110+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+108:vgprValuC+108+1], v[12:13], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[vgprValuC+110:vgprValuC+110+1], v[14:15], v[vgprValuC+110:vgprValuC+110+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+108], v[vgprValuC+108]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+109], v[vgprValuC+109]   // convert C to fp16
v_pack_b32_f16 v108, v[vgprValuC+108], v[vgprValuC+109] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+110], v[vgprValuC+110]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+111], v[vgprValuC+111]   // convert C to fp16
v_pack_b32_f16 v109, v[vgprValuC+110], v[vgprValuC+111] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[108:109], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+112:vgprValuC+112+1], v[28:29], v[vgprValuC+112:vgprValuC+112+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+114:vgprValuC+114+1], v[30:31], v[vgprValuC+114:vgprValuC+114+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+112:vgprValuC+112+1], v[24:25], v[vgprValuC+112:vgprValuC+112+1] // C += bias
v_pk_add_f32 v[vgprValuC+114:vgprValuC+114+1], v[26:27], v[vgprValuC+114:vgprValuC+114+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+112], v[vgprValuC+112]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+113], v[vgprValuC+113]   // convert C to fp16
v_pack_b32_f16 v112, v[vgprValuC+112], v[vgprValuC+113] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+114], v[vgprValuC+114]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+115], v[vgprValuC+115]   // convert C to fp16
v_pack_b32_f16 v113, v[vgprValuC+114], v[vgprValuC+115] // Pack with neighbor
buffer_store_dwordx2 v[112:113], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,2,4,0:vw4); (0,3,4,0:vw4); (0,0,5,0:vw4); (0,1,5,0:vw4); (0,2,5,0:vw4); (0,3,5,0:vw4); (0,0,6,0:vw4); (0,1,6,0:vw4); (0,2,6,0:vw4); (0,3,6,0:vw4); (0,0,7,0:vw4); (0,1,7,0:vw4); (0,2,7,0:vw4); (0,3,7,0:vw4); (1,0,0,0:vw4); (1,1,0,0:vw4); (1,2,0,0:vw4); (1,3,0,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,4,2,0) */
ds_read_b128 v[12:15], v8 offset:512               // load Bias
ds_read_b128 v[16:19], v9 offset:512               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,4,3,0) */
ds_read_b128 v[24:27], v8 offset:768               // load Bias
ds_read_b128 v[28:31], v9 offset:768               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
ds_read_b128 v[36:39], v8 offset:0                 // load Bias
ds_read_b128 v[40:43], v9 offset:0                 // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
ds_read_b128 v[48:51], v8 offset:256               // load Bias
ds_read_b128 v[52:55], v9 offset:256               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,5,2,0) */
/* (d1,vc1,d0,vc0)=(0,5,3,0) */
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
/* (d1,vc1,d0,vc0)=(0,6,2,0) */
/* (d1,vc1,d0,vc0)=(0,6,3,0) */
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
/* (d1,vc1,d0,vc0)=(0,7,2,0) */
/* (d1,vc1,d0,vc0)=(0,7,3,0) */
/* (d1,vc1,d0,vc0)=(1,0,0,0) */
/* (d1,vc1,d0,vc0)=(1,0,1,0) */
/* (d1,vc1,d0,vc0)=(1,0,2,0) */
/* (d1,vc1,d0,vc0)=(1,0,3,0) */
v_accvgpr_read_b32 v[vgprValuC+20], acc72          // copy acc to vreg[72]
v_accvgpr_read_b32 v[vgprValuC+21], acc73          // copy acc to vreg[73]
v_accvgpr_read_b32 v[vgprValuC+22], acc74          // copy acc to vreg[74]
v_accvgpr_read_b32 v[vgprValuC+23], acc75          // copy acc to vreg[75]
v_accvgpr_read_b32 v[vgprValuC+32], acc76          // copy acc to vreg[76]
v_accvgpr_read_b32 v[vgprValuC+33], acc77          // copy acc to vreg[77]
v_accvgpr_read_b32 v[vgprValuC+34], acc78          // copy acc to vreg[78]
v_accvgpr_read_b32 v[vgprValuC+35], acc79          // copy acc to vreg[79]
v_accvgpr_read_b32 v[vgprValuC+44], acc80          // copy acc to vreg[80]
v_accvgpr_read_b32 v[vgprValuC+45], acc81          // copy acc to vreg[81]
v_accvgpr_read_b32 v[vgprValuC+46], acc82          // copy acc to vreg[82]
v_accvgpr_read_b32 v[vgprValuC+47], acc83          // copy acc to vreg[83]
v_accvgpr_read_b32 v[vgprValuC+56], acc84          // copy acc to vreg[84]
v_accvgpr_read_b32 v[vgprValuC+57], acc85          // copy acc to vreg[85]
v_accvgpr_read_b32 v[vgprValuC+58], acc86          // copy acc to vreg[86]
v_accvgpr_read_b32 v[vgprValuC+59], acc87          // copy acc to vreg[87]
v_accvgpr_read_b32 v[vgprValuC+60], acc88          // copy acc to vreg[88]
v_accvgpr_read_b32 v[vgprValuC+61], acc89          // copy acc to vreg[89]
v_accvgpr_read_b32 v[vgprValuC+62], acc90          // copy acc to vreg[90]
v_accvgpr_read_b32 v[vgprValuC+63], acc91          // copy acc to vreg[91]
v_accvgpr_read_b32 v[vgprValuC+64], acc92          // copy acc to vreg[92]
v_accvgpr_read_b32 v[vgprValuC+65], acc93          // copy acc to vreg[93]
v_accvgpr_read_b32 v[vgprValuC+66], acc94          // copy acc to vreg[94]
v_accvgpr_read_b32 v[vgprValuC+67], acc95          // copy acc to vreg[95]
v_accvgpr_read_b32 v[vgprValuC+68], acc96          // copy acc to vreg[96]
v_accvgpr_read_b32 v[vgprValuC+69], acc97          // copy acc to vreg[97]
v_accvgpr_read_b32 v[vgprValuC+70], acc98          // copy acc to vreg[98]
v_accvgpr_read_b32 v[vgprValuC+71], acc99          // copy acc to vreg[99]
v_accvgpr_read_b32 v[vgprValuC+72], acc100         // copy acc to vreg[100]
v_accvgpr_read_b32 v[vgprValuC+73], acc101         // copy acc to vreg[101]
v_accvgpr_read_b32 v[vgprValuC+74], acc102         // copy acc to vreg[102]
v_accvgpr_read_b32 v[vgprValuC+75], acc103         // copy acc to vreg[103]
v_accvgpr_read_b32 v[vgprValuC+76], acc104         // copy acc to vreg[104]
v_accvgpr_read_b32 v[vgprValuC+77], acc105         // copy acc to vreg[105]
v_accvgpr_read_b32 v[vgprValuC+78], acc106         // copy acc to vreg[106]
v_accvgpr_read_b32 v[vgprValuC+79], acc107         // copy acc to vreg[107]
v_accvgpr_read_b32 v[vgprValuC+80], acc108         // copy acc to vreg[108]
v_accvgpr_read_b32 v[vgprValuC+81], acc109         // copy acc to vreg[109]
v_accvgpr_read_b32 v[vgprValuC+82], acc110         // copy acc to vreg[110]
v_accvgpr_read_b32 v[vgprValuC+83], acc111         // copy acc to vreg[111]
v_accvgpr_read_b32 v[vgprValuC+84], acc112         // copy acc to vreg[112]
v_accvgpr_read_b32 v[vgprValuC+85], acc113         // copy acc to vreg[113]
v_accvgpr_read_b32 v[vgprValuC+86], acc114         // copy acc to vreg[114]
v_accvgpr_read_b32 v[vgprValuC+87], acc115         // copy acc to vreg[115]
v_accvgpr_read_b32 v[vgprValuC+88], acc116         // copy acc to vreg[116]
v_accvgpr_read_b32 v[vgprValuC+89], acc117         // copy acc to vreg[117]
v_accvgpr_read_b32 v[vgprValuC+90], acc118         // copy acc to vreg[118]
v_accvgpr_read_b32 v[vgprValuC+91], acc119         // copy acc to vreg[119]
v_accvgpr_read_b32 v[vgprValuC+92], acc120         // copy acc to vreg[120]
v_accvgpr_read_b32 v[vgprValuC+93], acc121         // copy acc to vreg[121]
v_accvgpr_read_b32 v[vgprValuC+94], acc122         // copy acc to vreg[122]
v_accvgpr_read_b32 v[vgprValuC+95], acc123         // copy acc to vreg[123]
v_accvgpr_read_b32 v[vgprValuC+96], acc124         // copy acc to vreg[124]
v_accvgpr_read_b32 v[vgprValuC+97], acc125         // copy acc to vreg[125]
v_accvgpr_read_b32 v[vgprValuC+98], acc126         // copy acc to vreg[126]
v_accvgpr_read_b32 v[vgprValuC+99], acc127         // copy acc to vreg[127]
v_accvgpr_read_b32 v[vgprValuC+100], acc128        // copy acc to vreg[128]
v_accvgpr_read_b32 v[vgprValuC+101], acc129        // copy acc to vreg[129]
v_accvgpr_read_b32 v[vgprValuC+102], acc130        // copy acc to vreg[130]
v_accvgpr_read_b32 v[vgprValuC+103], acc131        // copy acc to vreg[131]
v_accvgpr_read_b32 v[vgprValuC+104], acc132        // copy acc to vreg[132]
v_accvgpr_read_b32 v[vgprValuC+105], acc133        // copy acc to vreg[133]
v_accvgpr_read_b32 v[vgprValuC+106], acc134        // copy acc to vreg[134]
v_accvgpr_read_b32 v[vgprValuC+107], acc135        // copy acc to vreg[135]
v_accvgpr_read_b32 v[vgprValuC+108], acc136        // copy acc to vreg[136]
v_accvgpr_read_b32 v[vgprValuC+109], acc137        // copy acc to vreg[137]
v_accvgpr_read_b32 v[vgprValuC+110], acc138        // copy acc to vreg[138]
v_accvgpr_read_b32 v[vgprValuC+111], acc139        // copy acc to vreg[139]
v_accvgpr_read_b32 v[vgprValuC+112], acc140        // copy acc to vreg[140]
v_accvgpr_read_b32 v[vgprValuC+113], acc141        // copy acc to vreg[141]
v_accvgpr_read_b32 v[vgprValuC+114], acc142        // copy acc to vreg[142]
v_accvgpr_read_b32 v[vgprValuC+115], acc143        // copy acc to vreg[143]

/* rC *= alpha batchElements=[(0, 2, 4, 0), (0, 3, 4, 0), (0, 0, 5, 0), (0, 1, 5, 0), (0, 2, 5, 0), (0, 3, 5, 0), (0, 0, 6, 0), (0, 1, 6, 0), (0, 2, 6, 0), (0, 3, 6, 0), (0, 0, 7, 0), (0, 1, 7, 0), (0, 2, 7, 0), (0, 3, 7, 0), (1, 0, 0, 0), (1, 1, 0, 0), (1, 2, 0, 0), (1, 3, 0, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+97], s[sgprAlpha], v[vgprValuC+97] // *= alpha
v_mul_f32 v[vgprValuC+98], s[sgprAlpha], v[vgprValuC+98] // *= alpha
v_mul_f32 v[vgprValuC+99], s[sgprAlpha], v[vgprValuC+99] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+104], s[sgprAlpha], v[vgprValuC+104] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+107], s[sgprAlpha], v[vgprValuC+107] // *= alpha
v_mul_f32 v[vgprValuC+108], s[sgprAlpha], v[vgprValuC+108] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+112], s[sgprAlpha], v[vgprValuC+112] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+114], s[sgprAlpha], v[vgprValuC+114] // *= alpha
v_mul_f32 v[vgprValuC+115], s[sgprAlpha], v[vgprValuC+115] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(6)                               // lgkmcnt(6) = 8 - 1 (bias) - 1 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt lgkmcnt(4)                               // lgkmcnt(4) = 8 - 2 (bias) - 2 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D

s_waitcnt lgkmcnt(2)                               // lgkmcnt(2) = 8 - 3 (bias) - 3 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[40:41], v[vgprValuC+44:vgprValuC+44+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[42:43], v[vgprValuC+46:vgprValuC+46+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[36:37], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[38:39], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt lgkmcnt(0)                               // lgkmcnt(0) = 8 - 4 (bias) - 4 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[52:53], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[54:55], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[48:49], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[50:51], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[12:13], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[14:15], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
buffer_store_dwordx2 v[60:61], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[28:29], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[30:31], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[40:41], v[vgprValuC+68:vgprValuC+68+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[42:43], v[vgprValuC+70:vgprValuC+70+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+68:vgprValuC+68+1], v[36:37], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[vgprValuC+70:vgprValuC+70+1], v[38:39], v[vgprValuC+70:vgprValuC+70+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+68], v[vgprValuC+68]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+69], v[vgprValuC+69]     // convert C to fp16
v_pack_b32_f16 v68, v[vgprValuC+68], v[vgprValuC+69] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+70], v[vgprValuC+70]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+71], v[vgprValuC+71]     // convert C to fp16
v_pack_b32_f16 v69, v[vgprValuC+70], v[vgprValuC+71] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[68:69], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[52:53], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[54:55], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[48:49], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[50:51], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[16:17], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[18:19], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[12:13], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[14:15], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
buffer_store_dwordx2 v[76:77], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[28:29], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[30:31], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[40:41], v[vgprValuC+84:vgprValuC+84+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[42:43], v[vgprValuC+86:vgprValuC+86+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+84:vgprValuC+84+1], v[36:37], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[vgprValuC+86:vgprValuC+86+1], v[38:39], v[vgprValuC+86:vgprValuC+86+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+84], v[vgprValuC+84]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+85], v[vgprValuC+85]     // convert C to fp16
v_pack_b32_f16 v84, v[vgprValuC+84], v[vgprValuC+85] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+86], v[vgprValuC+86]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+87], v[vgprValuC+87]     // convert C to fp16
v_pack_b32_f16 v85, v[vgprValuC+86], v[vgprValuC+87] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[84:85], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[52:53], v[vgprValuC+88:vgprValuC+88+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[54:55], v[vgprValuC+90:vgprValuC+90+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+88:vgprValuC+88+1], v[48:49], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[vgprValuC+90:vgprValuC+90+1], v[50:51], v[vgprValuC+90:vgprValuC+90+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+88], v[vgprValuC+88]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+89], v[vgprValuC+89]     // convert C to fp16
v_pack_b32_f16 v88, v[vgprValuC+88], v[vgprValuC+89] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+90], v[vgprValuC+90]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+91], v[vgprValuC+91]     // convert C to fp16
v_pack_b32_f16 v89, v[vgprValuC+90], v[vgprValuC+91] // Pack with neighbor
buffer_store_dwordx2 v[88:89], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[16:17], v[vgprValuC+92:vgprValuC+92+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[18:19], v[vgprValuC+94:vgprValuC+94+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+92:vgprValuC+92+1], v[12:13], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[vgprValuC+94:vgprValuC+94+1], v[14:15], v[vgprValuC+94:vgprValuC+94+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+92], v[vgprValuC+92]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+93], v[vgprValuC+93]     // convert C to fp16
v_pack_b32_f16 v92, v[vgprValuC+92], v[vgprValuC+93] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+94], v[vgprValuC+94]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+95], v[vgprValuC+95]     // convert C to fp16
v_pack_b32_f16 v93, v[vgprValuC+94], v[vgprValuC+95] // Pack with neighbor
buffer_store_dwordx2 v[92:93], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+96:vgprValuC+96+1], v[28:29], v[vgprValuC+96:vgprValuC+96+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+98:vgprValuC+98+1], v[30:31], v[vgprValuC+98:vgprValuC+98+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+96:vgprValuC+96+1], v[24:25], v[vgprValuC+96:vgprValuC+96+1] // C += bias
v_pk_add_f32 v[vgprValuC+98:vgprValuC+98+1], v[26:27], v[vgprValuC+98:vgprValuC+98+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+96], v[vgprValuC+96]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+97], v[vgprValuC+97]     // convert C to fp16
v_pack_b32_f16 v96, v[vgprValuC+96], v[vgprValuC+97] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+98], v[vgprValuC+98]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+99], v[vgprValuC+99]     // convert C to fp16
v_pack_b32_f16 v97, v[vgprValuC+98], v[vgprValuC+99] // Pack with neighbor
buffer_store_dwordx2 v[96:97], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[40:41], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[42:43], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[36:37], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[38:39], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
s_mul_i32 s12, s[sgprStrideD1J], 242               // scale StrideD *= numRows(121) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[100:101], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[52:53], v[vgprValuC+104:vgprValuC+104+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[54:55], v[vgprValuC+106:vgprValuC+106+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+104:vgprValuC+104+1], v[48:49], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[vgprValuC+106:vgprValuC+106+1], v[50:51], v[vgprValuC+106:vgprValuC+106+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+104], v[vgprValuC+104]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+105], v[vgprValuC+105]   // convert C to fp16
v_pack_b32_f16 v104, v[vgprValuC+104], v[vgprValuC+105] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+106], v[vgprValuC+106]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+107], v[vgprValuC+107]   // convert C to fp16
v_pack_b32_f16 v105, v[vgprValuC+106], v[vgprValuC+107] // Pack with neighbor
buffer_store_dwordx2 v[104:105], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[16:17], v[vgprValuC+108:vgprValuC+108+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[18:19], v[vgprValuC+110:vgprValuC+110+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+108:vgprValuC+108+1], v[12:13], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[vgprValuC+110:vgprValuC+110+1], v[14:15], v[vgprValuC+110:vgprValuC+110+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+108], v[vgprValuC+108]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+109], v[vgprValuC+109]   // convert C to fp16
v_pack_b32_f16 v108, v[vgprValuC+108], v[vgprValuC+109] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+110], v[vgprValuC+110]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+111], v[vgprValuC+111]   // convert C to fp16
v_pack_b32_f16 v109, v[vgprValuC+110], v[vgprValuC+111] // Pack with neighbor
buffer_store_dwordx2 v[108:109], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+112:vgprValuC+112+1], v[28:29], v[vgprValuC+112:vgprValuC+112+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+114:vgprValuC+114+1], v[30:31], v[vgprValuC+114:vgprValuC+114+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+112:vgprValuC+112+1], v[24:25], v[vgprValuC+112:vgprValuC+112+1] // C += bias
v_pk_add_f32 v[vgprValuC+114:vgprValuC+114+1], v[26:27], v[vgprValuC+114:vgprValuC+114+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+112], v[vgprValuC+112]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+113], v[vgprValuC+113]   // convert C to fp16
v_pack_b32_f16 v112, v[vgprValuC+112], v[vgprValuC+113] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+114], v[vgprValuC+114]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+115], v[vgprValuC+115]   // convert C to fp16
v_pack_b32_f16 v113, v[vgprValuC+114], v[vgprValuC+115] // Pack with neighbor
buffer_store_dwordx2 v[112:113], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Batch #2 (d1,d0,vc1,vc0) = */
/*    (1,0,1,0:vw4); (1,1,1,0:vw4); (1,2,1,0:vw4); (1,3,1,0:vw4); (1,0,2,0:vw4); (1,1,2,0:vw4); (1,2,2,0:vw4); (1,3,2,0:vw4); (1,0,3,0:vw4); (1,1,3,0:vw4); (1,2,3,0:vw4); (1,3,3,0:vw4); (1,0,4,0:vw4); (1,1,4,0:vw4); (1,2,4,0:vw4); (1,3,4,0:vw4); (1,0,5,0:vw4); (1,1,5,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(1,1,0,0) */
ds_read_b128 v[12:15], v8 offset:0                 // load Bias
ds_read_b128 v[16:19], v9 offset:0                 // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,1,1,0) */
ds_read_b128 v[24:27], v8 offset:256               // load Bias
ds_read_b128 v[28:31], v9 offset:256               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,1,2,0) */
ds_read_b128 v[36:39], v8 offset:512               // load Bias
ds_read_b128 v[40:43], v9 offset:512               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,1,3,0) */
ds_read_b128 v[48:51], v8 offset:768               // load Bias
ds_read_b128 v[52:55], v9 offset:768               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,2,0,0) */
/* (d1,vc1,d0,vc0)=(1,2,1,0) */
/* (d1,vc1,d0,vc0)=(1,2,2,0) */
/* (d1,vc1,d0,vc0)=(1,2,3,0) */
/* (d1,vc1,d0,vc0)=(1,3,0,0) */
/* (d1,vc1,d0,vc0)=(1,3,1,0) */
/* (d1,vc1,d0,vc0)=(1,3,2,0) */
/* (d1,vc1,d0,vc0)=(1,3,3,0) */
/* (d1,vc1,d0,vc0)=(1,4,0,0) */
/* (d1,vc1,d0,vc0)=(1,4,1,0) */
/* (d1,vc1,d0,vc0)=(1,4,2,0) */
/* (d1,vc1,d0,vc0)=(1,4,3,0) */
/* (d1,vc1,d0,vc0)=(1,5,0,0) */
/* (d1,vc1,d0,vc0)=(1,5,1,0) */
v_accvgpr_read_b32 v[vgprValuC+20], acc144         // copy acc to vreg[144]
v_accvgpr_read_b32 v[vgprValuC+21], acc145         // copy acc to vreg[145]
v_accvgpr_read_b32 v[vgprValuC+22], acc146         // copy acc to vreg[146]
v_accvgpr_read_b32 v[vgprValuC+23], acc147         // copy acc to vreg[147]
v_accvgpr_read_b32 v[vgprValuC+32], acc148         // copy acc to vreg[148]
v_accvgpr_read_b32 v[vgprValuC+33], acc149         // copy acc to vreg[149]
v_accvgpr_read_b32 v[vgprValuC+34], acc150         // copy acc to vreg[150]
v_accvgpr_read_b32 v[vgprValuC+35], acc151         // copy acc to vreg[151]
v_accvgpr_read_b32 v[vgprValuC+44], acc152         // copy acc to vreg[152]
v_accvgpr_read_b32 v[vgprValuC+45], acc153         // copy acc to vreg[153]
v_accvgpr_read_b32 v[vgprValuC+46], acc154         // copy acc to vreg[154]
v_accvgpr_read_b32 v[vgprValuC+47], acc155         // copy acc to vreg[155]
v_accvgpr_read_b32 v[vgprValuC+56], acc156         // copy acc to vreg[156]
v_accvgpr_read_b32 v[vgprValuC+57], acc157         // copy acc to vreg[157]
v_accvgpr_read_b32 v[vgprValuC+58], acc158         // copy acc to vreg[158]
v_accvgpr_read_b32 v[vgprValuC+59], acc159         // copy acc to vreg[159]
v_accvgpr_read_b32 v[vgprValuC+60], acc160         // copy acc to vreg[160]
v_accvgpr_read_b32 v[vgprValuC+61], acc161         // copy acc to vreg[161]
v_accvgpr_read_b32 v[vgprValuC+62], acc162         // copy acc to vreg[162]
v_accvgpr_read_b32 v[vgprValuC+63], acc163         // copy acc to vreg[163]
v_accvgpr_read_b32 v[vgprValuC+64], acc164         // copy acc to vreg[164]
v_accvgpr_read_b32 v[vgprValuC+65], acc165         // copy acc to vreg[165]
v_accvgpr_read_b32 v[vgprValuC+66], acc166         // copy acc to vreg[166]
v_accvgpr_read_b32 v[vgprValuC+67], acc167         // copy acc to vreg[167]
v_accvgpr_read_b32 v[vgprValuC+68], acc168         // copy acc to vreg[168]
v_accvgpr_read_b32 v[vgprValuC+69], acc169         // copy acc to vreg[169]
v_accvgpr_read_b32 v[vgprValuC+70], acc170         // copy acc to vreg[170]
v_accvgpr_read_b32 v[vgprValuC+71], acc171         // copy acc to vreg[171]
v_accvgpr_read_b32 v[vgprValuC+72], acc172         // copy acc to vreg[172]
v_accvgpr_read_b32 v[vgprValuC+73], acc173         // copy acc to vreg[173]
v_accvgpr_read_b32 v[vgprValuC+74], acc174         // copy acc to vreg[174]
v_accvgpr_read_b32 v[vgprValuC+75], acc175         // copy acc to vreg[175]
v_accvgpr_read_b32 v[vgprValuC+76], acc176         // copy acc to vreg[176]
v_accvgpr_read_b32 v[vgprValuC+77], acc177         // copy acc to vreg[177]
v_accvgpr_read_b32 v[vgprValuC+78], acc178         // copy acc to vreg[178]
v_accvgpr_read_b32 v[vgprValuC+79], acc179         // copy acc to vreg[179]
v_accvgpr_read_b32 v[vgprValuC+80], acc180         // copy acc to vreg[180]
v_accvgpr_read_b32 v[vgprValuC+81], acc181         // copy acc to vreg[181]
v_accvgpr_read_b32 v[vgprValuC+82], acc182         // copy acc to vreg[182]
v_accvgpr_read_b32 v[vgprValuC+83], acc183         // copy acc to vreg[183]
v_accvgpr_read_b32 v[vgprValuC+84], acc184         // copy acc to vreg[184]
v_accvgpr_read_b32 v[vgprValuC+85], acc185         // copy acc to vreg[185]
v_accvgpr_read_b32 v[vgprValuC+86], acc186         // copy acc to vreg[186]
v_accvgpr_read_b32 v[vgprValuC+87], acc187         // copy acc to vreg[187]
v_accvgpr_read_b32 v[vgprValuC+88], acc188         // copy acc to vreg[188]
v_accvgpr_read_b32 v[vgprValuC+89], acc189         // copy acc to vreg[189]
v_accvgpr_read_b32 v[vgprValuC+90], acc190         // copy acc to vreg[190]
v_accvgpr_read_b32 v[vgprValuC+91], acc191         // copy acc to vreg[191]
v_accvgpr_read_b32 v[vgprValuC+92], acc192         // copy acc to vreg[192]
v_accvgpr_read_b32 v[vgprValuC+93], acc193         // copy acc to vreg[193]
v_accvgpr_read_b32 v[vgprValuC+94], acc194         // copy acc to vreg[194]
v_accvgpr_read_b32 v[vgprValuC+95], acc195         // copy acc to vreg[195]
v_accvgpr_read_b32 v[vgprValuC+96], acc196         // copy acc to vreg[196]
v_accvgpr_read_b32 v[vgprValuC+97], acc197         // copy acc to vreg[197]
v_accvgpr_read_b32 v[vgprValuC+98], acc198         // copy acc to vreg[198]
v_accvgpr_read_b32 v[vgprValuC+99], acc199         // copy acc to vreg[199]
v_accvgpr_read_b32 v[vgprValuC+100], acc200        // copy acc to vreg[200]
v_accvgpr_read_b32 v[vgprValuC+101], acc201        // copy acc to vreg[201]
v_accvgpr_read_b32 v[vgprValuC+102], acc202        // copy acc to vreg[202]
v_accvgpr_read_b32 v[vgprValuC+103], acc203        // copy acc to vreg[203]
v_accvgpr_read_b32 v[vgprValuC+104], acc204        // copy acc to vreg[204]
v_accvgpr_read_b32 v[vgprValuC+105], acc205        // copy acc to vreg[205]
v_accvgpr_read_b32 v[vgprValuC+106], acc206        // copy acc to vreg[206]
v_accvgpr_read_b32 v[vgprValuC+107], acc207        // copy acc to vreg[207]
v_accvgpr_read_b32 v[vgprValuC+108], acc208        // copy acc to vreg[208]
v_accvgpr_read_b32 v[vgprValuC+109], acc209        // copy acc to vreg[209]
v_accvgpr_read_b32 v[vgprValuC+110], acc210        // copy acc to vreg[210]
v_accvgpr_read_b32 v[vgprValuC+111], acc211        // copy acc to vreg[211]
v_accvgpr_read_b32 v[vgprValuC+112], acc212        // copy acc to vreg[212]
v_accvgpr_read_b32 v[vgprValuC+113], acc213        // copy acc to vreg[213]
v_accvgpr_read_b32 v[vgprValuC+114], acc214        // copy acc to vreg[214]
v_accvgpr_read_b32 v[vgprValuC+115], acc215        // copy acc to vreg[215]

/* rC *= alpha batchElements=[(1, 0, 1, 0), (1, 1, 1, 0), (1, 2, 1, 0), (1, 3, 1, 0), (1, 0, 2, 0), (1, 1, 2, 0), (1, 2, 2, 0), (1, 3, 2, 0), (1, 0, 3, 0), (1, 1, 3, 0), (1, 2, 3, 0), (1, 3, 3, 0), (1, 0, 4, 0), (1, 1, 4, 0), (1, 2, 4, 0), (1, 3, 4, 0), (1, 0, 5, 0), (1, 1, 5, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+97], s[sgprAlpha], v[vgprValuC+97] // *= alpha
v_mul_f32 v[vgprValuC+98], s[sgprAlpha], v[vgprValuC+98] // *= alpha
v_mul_f32 v[vgprValuC+99], s[sgprAlpha], v[vgprValuC+99] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+104], s[sgprAlpha], v[vgprValuC+104] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+107], s[sgprAlpha], v[vgprValuC+107] // *= alpha
v_mul_f32 v[vgprValuC+108], s[sgprAlpha], v[vgprValuC+108] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+112], s[sgprAlpha], v[vgprValuC+112] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+114], s[sgprAlpha], v[vgprValuC+114] // *= alpha
v_mul_f32 v[vgprValuC+115], s[sgprAlpha], v[vgprValuC+115] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(6)                               // lgkmcnt(6) = 8 - 1 (bias) - 1 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt lgkmcnt(4)                               // lgkmcnt(4) = 8 - 2 (bias) - 2 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D

s_waitcnt lgkmcnt(2)                               // lgkmcnt(2) = 8 - 3 (bias) - 3 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[40:41], v[vgprValuC+44:vgprValuC+44+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[42:43], v[vgprValuC+46:vgprValuC+46+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[36:37], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[38:39], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt lgkmcnt(0)                               // lgkmcnt(0) = 8 - 4 (bias) - 4 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[52:53], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[54:55], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[48:49], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[50:51], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[12:13], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[14:15], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[60:61], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[28:29], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[30:31], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[40:41], v[vgprValuC+68:vgprValuC+68+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[42:43], v[vgprValuC+70:vgprValuC+70+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+68:vgprValuC+68+1], v[36:37], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[vgprValuC+70:vgprValuC+70+1], v[38:39], v[vgprValuC+70:vgprValuC+70+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+68], v[vgprValuC+68]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+69], v[vgprValuC+69]     // convert C to fp16
v_pack_b32_f16 v68, v[vgprValuC+68], v[vgprValuC+69] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+70], v[vgprValuC+70]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+71], v[vgprValuC+71]     // convert C to fp16
v_pack_b32_f16 v69, v[vgprValuC+70], v[vgprValuC+71] // Pack with neighbor
buffer_store_dwordx2 v[68:69], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[52:53], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[54:55], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[48:49], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[50:51], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[16:17], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[18:19], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[12:13], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[14:15], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[76:77], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[28:29], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[30:31], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[40:41], v[vgprValuC+84:vgprValuC+84+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[42:43], v[vgprValuC+86:vgprValuC+86+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+84:vgprValuC+84+1], v[36:37], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[vgprValuC+86:vgprValuC+86+1], v[38:39], v[vgprValuC+86:vgprValuC+86+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+84], v[vgprValuC+84]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+85], v[vgprValuC+85]     // convert C to fp16
v_pack_b32_f16 v84, v[vgprValuC+84], v[vgprValuC+85] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+86], v[vgprValuC+86]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+87], v[vgprValuC+87]     // convert C to fp16
v_pack_b32_f16 v85, v[vgprValuC+86], v[vgprValuC+87] // Pack with neighbor
buffer_store_dwordx2 v[84:85], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[52:53], v[vgprValuC+88:vgprValuC+88+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[54:55], v[vgprValuC+90:vgprValuC+90+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+88:vgprValuC+88+1], v[48:49], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[vgprValuC+90:vgprValuC+90+1], v[50:51], v[vgprValuC+90:vgprValuC+90+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+88], v[vgprValuC+88]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+89], v[vgprValuC+89]     // convert C to fp16
v_pack_b32_f16 v88, v[vgprValuC+88], v[vgprValuC+89] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+90], v[vgprValuC+90]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+91], v[vgprValuC+91]     // convert C to fp16
v_pack_b32_f16 v89, v[vgprValuC+90], v[vgprValuC+91] // Pack with neighbor
buffer_store_dwordx2 v[88:89], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[16:17], v[vgprValuC+92:vgprValuC+92+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[18:19], v[vgprValuC+94:vgprValuC+94+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+92:vgprValuC+92+1], v[12:13], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[vgprValuC+94:vgprValuC+94+1], v[14:15], v[vgprValuC+94:vgprValuC+94+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+92], v[vgprValuC+92]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+93], v[vgprValuC+93]     // convert C to fp16
v_pack_b32_f16 v92, v[vgprValuC+92], v[vgprValuC+93] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+94], v[vgprValuC+94]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+95], v[vgprValuC+95]     // convert C to fp16
v_pack_b32_f16 v93, v[vgprValuC+94], v[vgprValuC+95] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[92:93], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+96:vgprValuC+96+1], v[28:29], v[vgprValuC+96:vgprValuC+96+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+98:vgprValuC+98+1], v[30:31], v[vgprValuC+98:vgprValuC+98+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+96:vgprValuC+96+1], v[24:25], v[vgprValuC+96:vgprValuC+96+1] // C += bias
v_pk_add_f32 v[vgprValuC+98:vgprValuC+98+1], v[26:27], v[vgprValuC+98:vgprValuC+98+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+96], v[vgprValuC+96]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+97], v[vgprValuC+97]     // convert C to fp16
v_pack_b32_f16 v96, v[vgprValuC+96], v[vgprValuC+97] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+98], v[vgprValuC+98]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+99], v[vgprValuC+99]     // convert C to fp16
v_pack_b32_f16 v97, v[vgprValuC+98], v[vgprValuC+99] // Pack with neighbor
buffer_store_dwordx2 v[96:97], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[40:41], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[42:43], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[36:37], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[38:39], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
buffer_store_dwordx2 v[100:101], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[52:53], v[vgprValuC+104:vgprValuC+104+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[54:55], v[vgprValuC+106:vgprValuC+106+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+104:vgprValuC+104+1], v[48:49], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[vgprValuC+106:vgprValuC+106+1], v[50:51], v[vgprValuC+106:vgprValuC+106+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+104], v[vgprValuC+104]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+105], v[vgprValuC+105]   // convert C to fp16
v_pack_b32_f16 v104, v[vgprValuC+104], v[vgprValuC+105] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+106], v[vgprValuC+106]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+107], v[vgprValuC+107]   // convert C to fp16
v_pack_b32_f16 v105, v[vgprValuC+106], v[vgprValuC+107] // Pack with neighbor
buffer_store_dwordx2 v[104:105], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[16:17], v[vgprValuC+108:vgprValuC+108+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[18:19], v[vgprValuC+110:vgprValuC+110+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+108:vgprValuC+108+1], v[12:13], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[vgprValuC+110:vgprValuC+110+1], v[14:15], v[vgprValuC+110:vgprValuC+110+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+108], v[vgprValuC+108]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+109], v[vgprValuC+109]   // convert C to fp16
v_pack_b32_f16 v108, v[vgprValuC+108], v[vgprValuC+109] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+110], v[vgprValuC+110]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+111], v[vgprValuC+111]   // convert C to fp16
v_pack_b32_f16 v109, v[vgprValuC+110], v[vgprValuC+111] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[108:109], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+112:vgprValuC+112+1], v[28:29], v[vgprValuC+112:vgprValuC+112+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+114:vgprValuC+114+1], v[30:31], v[vgprValuC+114:vgprValuC+114+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+112:vgprValuC+112+1], v[24:25], v[vgprValuC+112:vgprValuC+112+1] // C += bias
v_pk_add_f32 v[vgprValuC+114:vgprValuC+114+1], v[26:27], v[vgprValuC+114:vgprValuC+114+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+112], v[vgprValuC+112]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+113], v[vgprValuC+113]   // convert C to fp16
v_pack_b32_f16 v112, v[vgprValuC+112], v[vgprValuC+113] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+114], v[vgprValuC+114]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+115], v[vgprValuC+115]   // convert C to fp16
v_pack_b32_f16 v113, v[vgprValuC+114], v[vgprValuC+115] // Pack with neighbor
buffer_store_dwordx2 v[112:113], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Batch #3 (d1,d0,vc1,vc0) = */
/*    (1,2,5,0:vw4); (1,3,5,0:vw4); (1,0,6,0:vw4); (1,1,6,0:vw4); (1,2,6,0:vw4); (1,3,6,0:vw4); (1,0,7,0:vw4); (1,1,7,0:vw4); (1,2,7,0:vw4); (1,3,7,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(1,5,2,0) */
ds_read_b128 v[12:15], v8 offset:512               // load Bias
ds_read_b128 v[16:19], v9 offset:512               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,5,3,0) */
ds_read_b128 v[24:27], v8 offset:768               // load Bias
ds_read_b128 v[28:31], v9 offset:768               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,6,0,0) */
ds_read_b128 v[36:39], v8 offset:0                 // load Bias
ds_read_b128 v[40:43], v9 offset:0                 // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,6,1,0) */
ds_read_b128 v[48:51], v8 offset:256               // load Bias
ds_read_b128 v[52:55], v9 offset:256               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,6,2,0) */
/* (d1,vc1,d0,vc0)=(1,6,3,0) */
/* (d1,vc1,d0,vc0)=(1,7,0,0) */
/* (d1,vc1,d0,vc0)=(1,7,1,0) */
/* (d1,vc1,d0,vc0)=(1,7,2,0) */
/* (d1,vc1,d0,vc0)=(1,7,3,0) */
v_accvgpr_read_b32 v[vgprValuC+20], acc216         // copy acc to vreg[216]
v_accvgpr_read_b32 v[vgprValuC+21], acc217         // copy acc to vreg[217]
v_accvgpr_read_b32 v[vgprValuC+22], acc218         // copy acc to vreg[218]
v_accvgpr_read_b32 v[vgprValuC+23], acc219         // copy acc to vreg[219]
v_accvgpr_read_b32 v[vgprValuC+32], acc220         // copy acc to vreg[220]
v_accvgpr_read_b32 v[vgprValuC+33], acc221         // copy acc to vreg[221]
v_accvgpr_read_b32 v[vgprValuC+34], acc222         // copy acc to vreg[222]
v_accvgpr_read_b32 v[vgprValuC+35], acc223         // copy acc to vreg[223]
v_accvgpr_read_b32 v[vgprValuC+44], acc224         // copy acc to vreg[224]
v_accvgpr_read_b32 v[vgprValuC+45], acc225         // copy acc to vreg[225]
v_accvgpr_read_b32 v[vgprValuC+46], acc226         // copy acc to vreg[226]
v_accvgpr_read_b32 v[vgprValuC+47], acc227         // copy acc to vreg[227]
v_accvgpr_read_b32 v[vgprValuC+56], acc228         // copy acc to vreg[228]
v_accvgpr_read_b32 v[vgprValuC+57], acc229         // copy acc to vreg[229]
v_accvgpr_read_b32 v[vgprValuC+58], acc230         // copy acc to vreg[230]
v_accvgpr_read_b32 v[vgprValuC+59], acc231         // copy acc to vreg[231]
v_accvgpr_read_b32 v[vgprValuC+60], acc232         // copy acc to vreg[232]
v_accvgpr_read_b32 v[vgprValuC+61], acc233         // copy acc to vreg[233]
v_accvgpr_read_b32 v[vgprValuC+62], acc234         // copy acc to vreg[234]
v_accvgpr_read_b32 v[vgprValuC+63], acc235         // copy acc to vreg[235]
v_accvgpr_read_b32 v[vgprValuC+64], acc236         // copy acc to vreg[236]
v_accvgpr_read_b32 v[vgprValuC+65], acc237         // copy acc to vreg[237]
v_accvgpr_read_b32 v[vgprValuC+66], acc238         // copy acc to vreg[238]
v_accvgpr_read_b32 v[vgprValuC+67], acc239         // copy acc to vreg[239]
v_accvgpr_read_b32 v[vgprValuC+68], acc240         // copy acc to vreg[240]
v_accvgpr_read_b32 v[vgprValuC+69], acc241         // copy acc to vreg[241]
v_accvgpr_read_b32 v[vgprValuC+70], acc242         // copy acc to vreg[242]
v_accvgpr_read_b32 v[vgprValuC+71], acc243         // copy acc to vreg[243]
v_accvgpr_read_b32 v[vgprValuC+72], acc244         // copy acc to vreg[244]
v_accvgpr_read_b32 v[vgprValuC+73], acc245         // copy acc to vreg[245]
v_accvgpr_read_b32 v[vgprValuC+74], acc246         // copy acc to vreg[246]
v_accvgpr_read_b32 v[vgprValuC+75], acc247         // copy acc to vreg[247]
v_accvgpr_read_b32 v[vgprValuC+76], acc248         // copy acc to vreg[248]
v_accvgpr_read_b32 v[vgprValuC+77], acc249         // copy acc to vreg[249]
v_accvgpr_read_b32 v[vgprValuC+78], acc250         // copy acc to vreg[250]
v_accvgpr_read_b32 v[vgprValuC+79], acc251         // copy acc to vreg[251]
v_accvgpr_read_b32 v[vgprValuC+80], acc252         // copy acc to vreg[252]
v_accvgpr_read_b32 v[vgprValuC+81], acc253         // copy acc to vreg[253]
v_accvgpr_read_b32 v[vgprValuC+82], acc254         // copy acc to vreg[254]
v_accvgpr_read_b32 v[vgprValuC+83], acc255         // copy acc to vreg[255]

/* rC *= alpha batchElements=[(1, 2, 5, 0), (1, 3, 5, 0), (1, 0, 6, 0), (1, 1, 6, 0), (1, 2, 6, 0), (1, 3, 6, 0), (1, 0, 7, 0), (1, 1, 7, 0), (1, 2, 7, 0), (1, 3, 7, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(6)                               // lgkmcnt(6) = 8 - 1 (bias) - 1 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt lgkmcnt(4)                               // lgkmcnt(4) = 8 - 2 (bias) - 2 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D

s_waitcnt lgkmcnt(2)                               // lgkmcnt(2) = 8 - 3 (bias) - 3 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[40:41], v[vgprValuC+44:vgprValuC+44+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[42:43], v[vgprValuC+46:vgprValuC+46+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[36:37], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[38:39], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt lgkmcnt(0)                               // lgkmcnt(0) = 8 - 4 (bias) - 4 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[52:53], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[54:55], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[48:49], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[50:51], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[12:13], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[14:15], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
buffer_store_dwordx2 v[60:61], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[28:29], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[30:31], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[40:41], v[vgprValuC+68:vgprValuC+68+1] // *= ScaleAlphaVecVMulPK(40)(0)
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[42:43], v[vgprValuC+70:vgprValuC+70+1] // *= ScaleAlphaVecVMulPK(40)(2)
v_pk_add_f32 v[vgprValuC+68:vgprValuC+68+1], v[36:37], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[vgprValuC+70:vgprValuC+70+1], v[38:39], v[vgprValuC+70:vgprValuC+70+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+68], v[vgprValuC+68]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+69], v[vgprValuC+69]     // convert C to fp16
v_pack_b32_f16 v68, v[vgprValuC+68], v[vgprValuC+69] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+70], v[vgprValuC+70]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+71], v[vgprValuC+71]     // convert C to fp16
v_pack_b32_f16 v69, v[vgprValuC+70], v[vgprValuC+71] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[68:69], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[52:53], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[54:55], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[48:49], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[50:51], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[16:17], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[18:19], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[12:13], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[14:15], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
buffer_store_dwordx2 v[76:77], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[28:29], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[30:31], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_2                            // jump to end
label_GW_B0_E1_N_1:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=14 */
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,1,0,0:vw4); (0,2,0,0:vw4); (0,3,0,0:vw4); (0,0,1,0:vw4); (0,1,1,0:vw4); (0,2,1,0:vw4); (0,3,1,0:vw4); (0,0,2,0:vw4); (0,1,2,0:vw4); (0,2,2,0:vw4); (0,3,2,0:vw4); (0,0,3,0:vw4); (0,1,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v136, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v0, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for LDS write
s_barrier                                          // LDS write barrier
ds_read_b128 v[12:15], v7 offset:0                 // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b128 v[16:19], v8 offset:0                 // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v136, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v4, s52
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
ds_read_b128 v[24:27], v10 offset:0                // load Bias
v_add_u32 v11, 1024, v10                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[28:31], v11 offset:0                // load scaleAlpha
v_add_lshl_u32 v9, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v136, v9, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v37, v4, s52
v_lshlrev_b32 v37, 0x2, v37                        // Bias address scaled by BPE
ds_read_b128 v[40:43], v37 offset:0                // load Bias
v_add_u32 v38, 1024, v37                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[44:47], v38 offset:0                // load scaleAlpha
v_add_lshl_u32 v36, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v136, v36, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v4, s52
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
ds_read_b128 v[56:59], v52 offset:0                // load Bias
v_add_u32 v53, 1024, v52                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[60:63], v53 offset:0                // load scaleAlpha
v_add_lshl_u32 v39, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v39, v136, v39, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v55, v0, s52
v_lshlrev_b32 v55, 0x2, v55                        // Bias address scaled by BPE
v_add_u32 v68, 1024, v55                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v54, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v136, v54, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v4, s52
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
v_add_u32 v71, 1024, v70                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v69, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v136, v69, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v81, v4, s52
v_lshlrev_b32 v81, 0x2, v81                        // Bias address scaled by BPE
v_add_u32 v82, 1024, v81                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v80, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v80, v136, v80, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v88, v4, s52
v_lshlrev_b32 v88, 0x2, v88                        // Bias address scaled by BPE
v_add_u32 v89, 1024, v88                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v83, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v136, v83, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v0, s52
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
v_add_u32 v96, 1024, v91                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v90, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v136, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v98, v4, s52
v_lshlrev_b32 v98, 0x2, v98                        // Bias address scaled by BPE
v_add_u32 v99, 1024, v98                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v97, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v136, v97, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v109, v4, s52
v_lshlrev_b32 v109, 0x2, v109                      // Bias address scaled by BPE
v_add_u32 v110, 1024, v109                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v108, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v108, v136, v108, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v116, v4, s52
v_lshlrev_b32 v116, 0x2, v116                      // Bias address scaled by BPE
v_add_u32 v117, 1024, v116                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v111, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v111, v136, v111, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v0, s52
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v124, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v136, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v126, v4, s52
v_lshlrev_b32 v126, 0x2, v126                      // Bias address scaled by BPE
v_add_u32 v127, 1024, v126                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v125, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v125, v136, v125, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc1           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+48], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+49], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+50], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+51], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+64], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+65], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+66], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+67], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+72], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+73], acc17          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+74], acc18          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+75], acc19          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+76], acc20          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+77], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+78], acc22          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+79], acc23          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+84], acc24          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+85], acc25          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+86], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+87], acc27          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+92], acc28          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+93], acc29          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+94], acc30          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+95], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+100], acc32         // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+101], acc33         // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+102], acc34         // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+103], acc35         // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+104], acc36         // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+105], acc37         // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+106], acc38         // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+107], acc39         // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+112], acc40         // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+113], acc41         // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+114], acc42         // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+115], acc43         // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+120], acc44         // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+121], acc45         // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+122], acc46         // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+123], acc47         // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+128], acc48         // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+129], acc49         // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+130], acc50         // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+131], acc51         // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+132], acc52         // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+133], acc53         // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+134], acc54         // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+135], acc55         // copy acc to vreg[55]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 1, 0, 0), (0, 2, 0, 0), (0, 3, 0, 0), (0, 0, 1, 0), (0, 1, 1, 0), (0, 2, 1, 0), (0, 3, 1, 0), (0, 0, 2, 0), (0, 1, 2, 0), (0, 2, 2, 0), (0, 3, 2, 0), (0, 0, 3, 0), (0, 1, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+104], s[sgprAlpha], v[vgprValuC+104] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+107], s[sgprAlpha], v[vgprValuC+107] // *= alpha
v_mul_f32 v[vgprValuC+112], s[sgprAlpha], v[vgprValuC+112] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+114], s[sgprAlpha], v[vgprValuC+114] // *= alpha
v_mul_f32 v[vgprValuC+115], s[sgprAlpha], v[vgprValuC+115] // *= alpha
v_mul_f32 v[vgprValuC+120], s[sgprAlpha], v[vgprValuC+120] // *= alpha
v_mul_f32 v[vgprValuC+121], s[sgprAlpha], v[vgprValuC+121] // *= alpha
v_mul_f32 v[vgprValuC+122], s[sgprAlpha], v[vgprValuC+122] // *= alpha
v_mul_f32 v[vgprValuC+123], s[sgprAlpha], v[vgprValuC+123] // *= alpha
v_mul_f32 v[vgprValuC+128], s[sgprAlpha], v[vgprValuC+128] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+130], s[sgprAlpha], v[vgprValuC+130] // *= alpha
v_mul_f32 v[vgprValuC+131], s[sgprAlpha], v[vgprValuC+131] // *= alpha
v_mul_f32 v[vgprValuC+132], s[sgprAlpha], v[vgprValuC+132] // *= alpha
v_mul_f32 v[vgprValuC+133], s[sgprAlpha], v[vgprValuC+133] // *= alpha
v_mul_f32 v[vgprValuC+134], s[sgprAlpha], v[vgprValuC+134] // *= alpha
v_mul_f32 v[vgprValuC+135], s[sgprAlpha], v[vgprValuC+135] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[44:45], v[vgprValuC+48:vgprValuC+48+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[46:47], v[vgprValuC+50:vgprValuC+50+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[40:41], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[42:43], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[60:61], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[62:63], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[56:57], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[58:59], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v39, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[16:17], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[18:19], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[12:13], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[14:15], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[28:29], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[30:31], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[24:25], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[26:27], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
buffer_store_dwordx2 v[76:77], v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[44:45], v[vgprValuC+84:vgprValuC+84+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[46:47], v[vgprValuC+86:vgprValuC+86+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_pk_add_f32 v[vgprValuC+84:vgprValuC+84+1], v[40:41], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[vgprValuC+86:vgprValuC+86+1], v[42:43], v[vgprValuC+86:vgprValuC+86+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+84], v[vgprValuC+84]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+85], v[vgprValuC+85]     // convert C to fp16
v_pack_b32_f16 v84, v[vgprValuC+84], v[vgprValuC+85] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+86], v[vgprValuC+86]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+87], v[vgprValuC+87]     // convert C to fp16
v_pack_b32_f16 v85, v[vgprValuC+86], v[vgprValuC+87] // Pack with neighbor
buffer_store_dwordx2 v[84:85], v80, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[60:61], v[vgprValuC+92:vgprValuC+92+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[62:63], v[vgprValuC+94:vgprValuC+94+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_pk_add_f32 v[vgprValuC+92:vgprValuC+92+1], v[56:57], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[vgprValuC+94:vgprValuC+94+1], v[58:59], v[vgprValuC+94:vgprValuC+94+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+92], v[vgprValuC+92]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+93], v[vgprValuC+93]     // convert C to fp16
v_pack_b32_f16 v92, v[vgprValuC+92], v[vgprValuC+93] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+94], v[vgprValuC+94]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+95], v[vgprValuC+95]     // convert C to fp16
v_pack_b32_f16 v93, v[vgprValuC+94], v[vgprValuC+95] // Pack with neighbor
buffer_store_dwordx2 v[92:93], v83, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[16:17], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[18:19], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[12:13], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[14:15], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
buffer_store_dwordx2 v[100:101], v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[28:29], v[vgprValuC+104:vgprValuC+104+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[30:31], v[vgprValuC+106:vgprValuC+106+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+104:vgprValuC+104+1], v[24:25], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[vgprValuC+106:vgprValuC+106+1], v[26:27], v[vgprValuC+106:vgprValuC+106+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+104], v[vgprValuC+104]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+105], v[vgprValuC+105]   // convert C to fp16
v_pack_b32_f16 v104, v[vgprValuC+104], v[vgprValuC+105] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+106], v[vgprValuC+106]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+107], v[vgprValuC+107]   // convert C to fp16
v_pack_b32_f16 v105, v[vgprValuC+106], v[vgprValuC+107] // Pack with neighbor
buffer_store_dwordx2 v[104:105], v97, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+112:vgprValuC+112+1], v[44:45], v[vgprValuC+112:vgprValuC+112+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+114:vgprValuC+114+1], v[46:47], v[vgprValuC+114:vgprValuC+114+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_pk_add_f32 v[vgprValuC+112:vgprValuC+112+1], v[40:41], v[vgprValuC+112:vgprValuC+112+1] // C += bias
v_pk_add_f32 v[vgprValuC+114:vgprValuC+114+1], v[42:43], v[vgprValuC+114:vgprValuC+114+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+112], v[vgprValuC+112]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+113], v[vgprValuC+113]   // convert C to fp16
v_pack_b32_f16 v112, v[vgprValuC+112], v[vgprValuC+113] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+114], v[vgprValuC+114]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+115], v[vgprValuC+115]   // convert C to fp16
v_pack_b32_f16 v113, v[vgprValuC+114], v[vgprValuC+115] // Pack with neighbor
buffer_store_dwordx2 v[112:113], v108, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+120:vgprValuC+120+1], v[60:61], v[vgprValuC+120:vgprValuC+120+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+122:vgprValuC+122+1], v[62:63], v[vgprValuC+122:vgprValuC+122+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_pk_add_f32 v[vgprValuC+120:vgprValuC+120+1], v[56:57], v[vgprValuC+120:vgprValuC+120+1] // C += bias
v_pk_add_f32 v[vgprValuC+122:vgprValuC+122+1], v[58:59], v[vgprValuC+122:vgprValuC+122+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+120], v[vgprValuC+120]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+121], v[vgprValuC+121]   // convert C to fp16
v_pack_b32_f16 v120, v[vgprValuC+120], v[vgprValuC+121] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+122], v[vgprValuC+122]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+123], v[vgprValuC+123]   // convert C to fp16
v_pack_b32_f16 v121, v[vgprValuC+122], v[vgprValuC+123] // Pack with neighbor
buffer_store_dwordx2 v[120:121], v111, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+128:vgprValuC+128+1], v[16:17], v[vgprValuC+128:vgprValuC+128+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+130:vgprValuC+130+1], v[18:19], v[vgprValuC+130:vgprValuC+130+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+128:vgprValuC+128+1], v[12:13], v[vgprValuC+128:vgprValuC+128+1] // C += bias
v_pk_add_f32 v[vgprValuC+130:vgprValuC+130+1], v[14:15], v[vgprValuC+130:vgprValuC+130+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+128], v[vgprValuC+128]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+129], v[vgprValuC+129]   // convert C to fp16
v_pack_b32_f16 v128, v[vgprValuC+128], v[vgprValuC+129] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+130], v[vgprValuC+130]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+131], v[vgprValuC+131]   // convert C to fp16
v_pack_b32_f16 v129, v[vgprValuC+130], v[vgprValuC+131] // Pack with neighbor
buffer_store_dwordx2 v[128:129], v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+132:vgprValuC+132+1], v[28:29], v[vgprValuC+132:vgprValuC+132+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+134:vgprValuC+134+1], v[30:31], v[vgprValuC+134:vgprValuC+134+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+132:vgprValuC+132+1], v[24:25], v[vgprValuC+132:vgprValuC+132+1] // C += bias
v_pk_add_f32 v[vgprValuC+134:vgprValuC+134+1], v[26:27], v[vgprValuC+134:vgprValuC+134+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+132], v[vgprValuC+132]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+133], v[vgprValuC+133]   // convert C to fp16
v_pack_b32_f16 v132, v[vgprValuC+132], v[vgprValuC+133] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+134], v[vgprValuC+134]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+135], v[vgprValuC+135]   // convert C to fp16
v_pack_b32_f16 v133, v[vgprValuC+134], v[vgprValuC+135] // Pack with neighbor
buffer_store_dwordx2 v[132:133], v125, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,2,3,0:vw4); (0,3,3,0:vw4); (0,0,4,0:vw4); (0,1,4,0:vw4); (0,2,4,0:vw4); (0,3,4,0:vw4); (0,0,5,0:vw4); (0,1,5,0:vw4); (0,2,5,0:vw4); (0,3,5,0:vw4); (0,0,6,0:vw4); (0,1,6,0:vw4); (0,2,6,0:vw4); (0,3,6,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v136, BufferOOB
/* (d1,vc1,d0,vc0)=(0,3,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v4, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b128 v[12:15], v7 offset:0                 // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b128 v[16:19], v8 offset:0                 // load scaleAlpha
v_add_lshl_u32 v6, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v136, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v4, s52
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
ds_read_b128 v[24:27], v10 offset:0                // load Bias
v_add_u32 v11, 1024, v10                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[28:31], v11 offset:0                // load scaleAlpha
v_add_lshl_u32 v9, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v136, v9, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v37, v0, s52
v_lshlrev_b32 v37, 0x2, v37                        // Bias address scaled by BPE
ds_read_b128 v[40:43], v37 offset:0                // load Bias
v_add_u32 v38, 1024, v37                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[44:47], v38 offset:0                // load scaleAlpha
v_add_lshl_u32 v36, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v136, v36, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v4, s52
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
ds_read_b128 v[56:59], v52 offset:0                // load Bias
v_add_u32 v53, 1024, v52                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[60:63], v53 offset:0                // load scaleAlpha
v_add_lshl_u32 v39, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v39, v136, v39, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v55, v4, s52
v_lshlrev_b32 v55, 0x2, v55                        // Bias address scaled by BPE
v_add_u32 v68, 1024, v55                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v54, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v136, v54, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v4, s52
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
v_add_u32 v71, 1024, v70                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v69, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v136, v69, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v81, v0, s52
v_lshlrev_b32 v81, 0x2, v81                        // Bias address scaled by BPE
v_add_u32 v82, 1024, v81                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v80, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v80, v136, v80, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v88, v4, s52
v_lshlrev_b32 v88, 0x2, v88                        // Bias address scaled by BPE
v_add_u32 v89, 1024, v88                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v83, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v136, v83, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v4, s52
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
v_add_u32 v96, 1024, v91                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v90, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v136, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v98, v4, s52
v_lshlrev_b32 v98, 0x2, v98                        // Bias address scaled by BPE
v_add_u32 v99, 1024, v98                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v97, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v136, v97, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v109, v0, s52
v_lshlrev_b32 v109, 0x2, v109                      // Bias address scaled by BPE
v_add_u32 v110, 1024, v109                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v108, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v108, v136, v108, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v116, v4, s52
v_lshlrev_b32 v116, 0x2, v116                      // Bias address scaled by BPE
v_add_u32 v117, 1024, v116                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v111, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v111, v136, v111, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v4, s52
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v124, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v136, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v126, v4, s52
v_lshlrev_b32 v126, 0x2, v126                      // Bias address scaled by BPE
v_add_u32 v127, 1024, v126                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v125, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v125, v136, v125, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc56          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+21], acc57          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+22], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+23], acc59          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+32], acc60          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+33], acc61          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+34], acc62          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+35], acc63          // copy acc to vreg[63]
v_accvgpr_read_b32 v[vgprValuC+48], acc64          // copy acc to vreg[64]
v_accvgpr_read_b32 v[vgprValuC+49], acc65          // copy acc to vreg[65]
v_accvgpr_read_b32 v[vgprValuC+50], acc66          // copy acc to vreg[66]
v_accvgpr_read_b32 v[vgprValuC+51], acc67          // copy acc to vreg[67]
v_accvgpr_read_b32 v[vgprValuC+64], acc68          // copy acc to vreg[68]
v_accvgpr_read_b32 v[vgprValuC+65], acc69          // copy acc to vreg[69]
v_accvgpr_read_b32 v[vgprValuC+66], acc70          // copy acc to vreg[70]
v_accvgpr_read_b32 v[vgprValuC+67], acc71          // copy acc to vreg[71]
v_accvgpr_read_b32 v[vgprValuC+72], acc72          // copy acc to vreg[72]
v_accvgpr_read_b32 v[vgprValuC+73], acc73          // copy acc to vreg[73]
v_accvgpr_read_b32 v[vgprValuC+74], acc74          // copy acc to vreg[74]
v_accvgpr_read_b32 v[vgprValuC+75], acc75          // copy acc to vreg[75]
v_accvgpr_read_b32 v[vgprValuC+76], acc76          // copy acc to vreg[76]
v_accvgpr_read_b32 v[vgprValuC+77], acc77          // copy acc to vreg[77]
v_accvgpr_read_b32 v[vgprValuC+78], acc78          // copy acc to vreg[78]
v_accvgpr_read_b32 v[vgprValuC+79], acc79          // copy acc to vreg[79]
v_accvgpr_read_b32 v[vgprValuC+84], acc80          // copy acc to vreg[80]
v_accvgpr_read_b32 v[vgprValuC+85], acc81          // copy acc to vreg[81]
v_accvgpr_read_b32 v[vgprValuC+86], acc82          // copy acc to vreg[82]
v_accvgpr_read_b32 v[vgprValuC+87], acc83          // copy acc to vreg[83]
v_accvgpr_read_b32 v[vgprValuC+92], acc84          // copy acc to vreg[84]
v_accvgpr_read_b32 v[vgprValuC+93], acc85          // copy acc to vreg[85]
v_accvgpr_read_b32 v[vgprValuC+94], acc86          // copy acc to vreg[86]
v_accvgpr_read_b32 v[vgprValuC+95], acc87          // copy acc to vreg[87]
v_accvgpr_read_b32 v[vgprValuC+100], acc88         // copy acc to vreg[88]
v_accvgpr_read_b32 v[vgprValuC+101], acc89         // copy acc to vreg[89]
v_accvgpr_read_b32 v[vgprValuC+102], acc90         // copy acc to vreg[90]
v_accvgpr_read_b32 v[vgprValuC+103], acc91         // copy acc to vreg[91]
v_accvgpr_read_b32 v[vgprValuC+104], acc92         // copy acc to vreg[92]
v_accvgpr_read_b32 v[vgprValuC+105], acc93         // copy acc to vreg[93]
v_accvgpr_read_b32 v[vgprValuC+106], acc94         // copy acc to vreg[94]
v_accvgpr_read_b32 v[vgprValuC+107], acc95         // copy acc to vreg[95]
v_accvgpr_read_b32 v[vgprValuC+112], acc96         // copy acc to vreg[96]
v_accvgpr_read_b32 v[vgprValuC+113], acc97         // copy acc to vreg[97]
v_accvgpr_read_b32 v[vgprValuC+114], acc98         // copy acc to vreg[98]
v_accvgpr_read_b32 v[vgprValuC+115], acc99         // copy acc to vreg[99]
v_accvgpr_read_b32 v[vgprValuC+120], acc100        // copy acc to vreg[100]
v_accvgpr_read_b32 v[vgprValuC+121], acc101        // copy acc to vreg[101]
v_accvgpr_read_b32 v[vgprValuC+122], acc102        // copy acc to vreg[102]
v_accvgpr_read_b32 v[vgprValuC+123], acc103        // copy acc to vreg[103]
v_accvgpr_read_b32 v[vgprValuC+128], acc104        // copy acc to vreg[104]
v_accvgpr_read_b32 v[vgprValuC+129], acc105        // copy acc to vreg[105]
v_accvgpr_read_b32 v[vgprValuC+130], acc106        // copy acc to vreg[106]
v_accvgpr_read_b32 v[vgprValuC+131], acc107        // copy acc to vreg[107]
v_accvgpr_read_b32 v[vgprValuC+132], acc108        // copy acc to vreg[108]
v_accvgpr_read_b32 v[vgprValuC+133], acc109        // copy acc to vreg[109]
v_accvgpr_read_b32 v[vgprValuC+134], acc110        // copy acc to vreg[110]
v_accvgpr_read_b32 v[vgprValuC+135], acc111        // copy acc to vreg[111]

/* rC *= alpha batchElements=[(0, 2, 3, 0), (0, 3, 3, 0), (0, 0, 4, 0), (0, 1, 4, 0), (0, 2, 4, 0), (0, 3, 4, 0), (0, 0, 5, 0), (0, 1, 5, 0), (0, 2, 5, 0), (0, 3, 5, 0), (0, 0, 6, 0), (0, 1, 6, 0), (0, 2, 6, 0), (0, 3, 6, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+104], s[sgprAlpha], v[vgprValuC+104] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+107], s[sgprAlpha], v[vgprValuC+107] // *= alpha
v_mul_f32 v[vgprValuC+112], s[sgprAlpha], v[vgprValuC+112] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+114], s[sgprAlpha], v[vgprValuC+114] // *= alpha
v_mul_f32 v[vgprValuC+115], s[sgprAlpha], v[vgprValuC+115] // *= alpha
v_mul_f32 v[vgprValuC+120], s[sgprAlpha], v[vgprValuC+120] // *= alpha
v_mul_f32 v[vgprValuC+121], s[sgprAlpha], v[vgprValuC+121] // *= alpha
v_mul_f32 v[vgprValuC+122], s[sgprAlpha], v[vgprValuC+122] // *= alpha
v_mul_f32 v[vgprValuC+123], s[sgprAlpha], v[vgprValuC+123] // *= alpha
v_mul_f32 v[vgprValuC+128], s[sgprAlpha], v[vgprValuC+128] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+130], s[sgprAlpha], v[vgprValuC+130] // *= alpha
v_mul_f32 v[vgprValuC+131], s[sgprAlpha], v[vgprValuC+131] // *= alpha
v_mul_f32 v[vgprValuC+132], s[sgprAlpha], v[vgprValuC+132] // *= alpha
v_mul_f32 v[vgprValuC+133], s[sgprAlpha], v[vgprValuC+133] // *= alpha
v_mul_f32 v[vgprValuC+134], s[sgprAlpha], v[vgprValuC+134] // *= alpha
v_mul_f32 v[vgprValuC+135], s[sgprAlpha], v[vgprValuC+135] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[44:45], v[vgprValuC+48:vgprValuC+48+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[46:47], v[vgprValuC+50:vgprValuC+50+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[40:41], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[42:43], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[60:61], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[62:63], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[56:57], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[58:59], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v39, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[16:17], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[18:19], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[12:13], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[14:15], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[28:29], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[30:31], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[24:25], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[26:27], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
buffer_store_dwordx2 v[76:77], v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[44:45], v[vgprValuC+84:vgprValuC+84+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[46:47], v[vgprValuC+86:vgprValuC+86+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_pk_add_f32 v[vgprValuC+84:vgprValuC+84+1], v[40:41], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[vgprValuC+86:vgprValuC+86+1], v[42:43], v[vgprValuC+86:vgprValuC+86+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+84], v[vgprValuC+84]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+85], v[vgprValuC+85]     // convert C to fp16
v_pack_b32_f16 v84, v[vgprValuC+84], v[vgprValuC+85] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+86], v[vgprValuC+86]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+87], v[vgprValuC+87]     // convert C to fp16
v_pack_b32_f16 v85, v[vgprValuC+86], v[vgprValuC+87] // Pack with neighbor
buffer_store_dwordx2 v[84:85], v80, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[60:61], v[vgprValuC+92:vgprValuC+92+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[62:63], v[vgprValuC+94:vgprValuC+94+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_pk_add_f32 v[vgprValuC+92:vgprValuC+92+1], v[56:57], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[vgprValuC+94:vgprValuC+94+1], v[58:59], v[vgprValuC+94:vgprValuC+94+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+92], v[vgprValuC+92]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+93], v[vgprValuC+93]     // convert C to fp16
v_pack_b32_f16 v92, v[vgprValuC+92], v[vgprValuC+93] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+94], v[vgprValuC+94]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+95], v[vgprValuC+95]     // convert C to fp16
v_pack_b32_f16 v93, v[vgprValuC+94], v[vgprValuC+95] // Pack with neighbor
buffer_store_dwordx2 v[92:93], v83, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[16:17], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[18:19], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[12:13], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[14:15], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
buffer_store_dwordx2 v[100:101], v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[28:29], v[vgprValuC+104:vgprValuC+104+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[30:31], v[vgprValuC+106:vgprValuC+106+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+104:vgprValuC+104+1], v[24:25], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[vgprValuC+106:vgprValuC+106+1], v[26:27], v[vgprValuC+106:vgprValuC+106+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+104], v[vgprValuC+104]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+105], v[vgprValuC+105]   // convert C to fp16
v_pack_b32_f16 v104, v[vgprValuC+104], v[vgprValuC+105] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+106], v[vgprValuC+106]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+107], v[vgprValuC+107]   // convert C to fp16
v_pack_b32_f16 v105, v[vgprValuC+106], v[vgprValuC+107] // Pack with neighbor
buffer_store_dwordx2 v[104:105], v97, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+112:vgprValuC+112+1], v[44:45], v[vgprValuC+112:vgprValuC+112+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+114:vgprValuC+114+1], v[46:47], v[vgprValuC+114:vgprValuC+114+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_pk_add_f32 v[vgprValuC+112:vgprValuC+112+1], v[40:41], v[vgprValuC+112:vgprValuC+112+1] // C += bias
v_pk_add_f32 v[vgprValuC+114:vgprValuC+114+1], v[42:43], v[vgprValuC+114:vgprValuC+114+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+112], v[vgprValuC+112]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+113], v[vgprValuC+113]   // convert C to fp16
v_pack_b32_f16 v112, v[vgprValuC+112], v[vgprValuC+113] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+114], v[vgprValuC+114]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+115], v[vgprValuC+115]   // convert C to fp16
v_pack_b32_f16 v113, v[vgprValuC+114], v[vgprValuC+115] // Pack with neighbor
buffer_store_dwordx2 v[112:113], v108, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+120:vgprValuC+120+1], v[60:61], v[vgprValuC+120:vgprValuC+120+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+122:vgprValuC+122+1], v[62:63], v[vgprValuC+122:vgprValuC+122+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_pk_add_f32 v[vgprValuC+120:vgprValuC+120+1], v[56:57], v[vgprValuC+120:vgprValuC+120+1] // C += bias
v_pk_add_f32 v[vgprValuC+122:vgprValuC+122+1], v[58:59], v[vgprValuC+122:vgprValuC+122+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+120], v[vgprValuC+120]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+121], v[vgprValuC+121]   // convert C to fp16
v_pack_b32_f16 v120, v[vgprValuC+120], v[vgprValuC+121] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+122], v[vgprValuC+122]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+123], v[vgprValuC+123]   // convert C to fp16
v_pack_b32_f16 v121, v[vgprValuC+122], v[vgprValuC+123] // Pack with neighbor
buffer_store_dwordx2 v[120:121], v111, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+128:vgprValuC+128+1], v[16:17], v[vgprValuC+128:vgprValuC+128+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+130:vgprValuC+130+1], v[18:19], v[vgprValuC+130:vgprValuC+130+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+128:vgprValuC+128+1], v[12:13], v[vgprValuC+128:vgprValuC+128+1] // C += bias
v_pk_add_f32 v[vgprValuC+130:vgprValuC+130+1], v[14:15], v[vgprValuC+130:vgprValuC+130+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+128], v[vgprValuC+128]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+129], v[vgprValuC+129]   // convert C to fp16
v_pack_b32_f16 v128, v[vgprValuC+128], v[vgprValuC+129] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+130], v[vgprValuC+130]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+131], v[vgprValuC+131]   // convert C to fp16
v_pack_b32_f16 v129, v[vgprValuC+130], v[vgprValuC+131] // Pack with neighbor
buffer_store_dwordx2 v[128:129], v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+132:vgprValuC+132+1], v[28:29], v[vgprValuC+132:vgprValuC+132+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+134:vgprValuC+134+1], v[30:31], v[vgprValuC+134:vgprValuC+134+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+132:vgprValuC+132+1], v[24:25], v[vgprValuC+132:vgprValuC+132+1] // C += bias
v_pk_add_f32 v[vgprValuC+134:vgprValuC+134+1], v[26:27], v[vgprValuC+134:vgprValuC+134+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+132], v[vgprValuC+132]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+133], v[vgprValuC+133]   // convert C to fp16
v_pack_b32_f16 v132, v[vgprValuC+132], v[vgprValuC+133] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+134], v[vgprValuC+134]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+135], v[vgprValuC+135]   // convert C to fp16
v_pack_b32_f16 v133, v[vgprValuC+134], v[vgprValuC+135] // Pack with neighbor
buffer_store_dwordx2 v[132:133], v125, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #2 (d1,d0,vc1,vc0) = */
/*    (0,0,7,0:vw4); (0,1,7,0:vw4); (0,2,7,0:vw4); (0,3,7,0:vw4); (1,0,0,0:vw4); (1,1,0,0:vw4); (1,2,0,0:vw4); (1,3,0,0:vw4); (1,0,1,0:vw4); (1,1,1,0:vw4); (1,2,1,0:vw4); (1,3,1,0:vw4); (1,0,2,0:vw4); (1,1,2,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v136, BufferOOB
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v0, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b128 v[12:15], v7 offset:0                 // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b128 v[16:19], v8 offset:0                 // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v136, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v4, s52
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
ds_read_b128 v[24:27], v10 offset:0                // load Bias
v_add_u32 v11, 1024, v10                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[28:31], v11 offset:0                // load scaleAlpha
v_add_lshl_u32 v9, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v136, v9, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v37, v4, s52
v_lshlrev_b32 v37, 0x2, v37                        // Bias address scaled by BPE
ds_read_b128 v[40:43], v37 offset:0                // load Bias
v_add_u32 v38, 1024, v37                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[44:47], v38 offset:0                // load scaleAlpha
v_add_lshl_u32 v36, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v136, v36, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v4, s52
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
ds_read_b128 v[56:59], v52 offset:0                // load Bias
v_add_u32 v53, 1024, v52                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[60:63], v53 offset:0                // load scaleAlpha
v_add_lshl_u32 v39, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v39, v136, v39, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,0) */
s_mov_b32 s52, 121                                 // rowInc d1=0 vc1=0
v_add_co_u32 v1, vcc, v1, s52                      // coord1.2: coord1 += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s52, s[sgprStrideC1J], 121               // scale stride
v_add_i32 v2, v2, s52                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s52, s[sgprStrideD1J], 121               // scale stride
v_add_i32 v3, v3, s52                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v55, v0, s52
v_lshlrev_b32 v55, 0x2, v55                        // Bias address scaled by BPE
v_add_u32 v68, 1024, v55                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v54, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v136, v54, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v4, s52
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
v_add_u32 v71, 1024, v70                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v69, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v136, v69, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v81, v4, s52
v_lshlrev_b32 v81, 0x2, v81                        // Bias address scaled by BPE
v_add_u32 v82, 1024, v81                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v80, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v80, v136, v80, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v88, v4, s52
v_lshlrev_b32 v88, 0x2, v88                        // Bias address scaled by BPE
v_add_u32 v89, 1024, v88                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v83, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v136, v83, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v0, s52
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
v_add_u32 v96, 1024, v91                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v90, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v136, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v98, v4, s52
v_lshlrev_b32 v98, 0x2, v98                        // Bias address scaled by BPE
v_add_u32 v99, 1024, v98                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v97, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v136, v97, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v109, v4, s52
v_lshlrev_b32 v109, 0x2, v109                      // Bias address scaled by BPE
v_add_u32 v110, 1024, v109                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v108, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v108, v136, v108, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v116, v4, s52
v_lshlrev_b32 v116, 0x2, v116                      // Bias address scaled by BPE
v_add_u32 v117, 1024, v116                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v111, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v111, v136, v111, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v0, s52
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v124, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v136, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v126, v4, s52
v_lshlrev_b32 v126, 0x2, v126                      // Bias address scaled by BPE
v_add_u32 v127, 1024, v126                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v125, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v125, v136, v125, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc112         // copy acc to vreg[112]
v_accvgpr_read_b32 v[vgprValuC+21], acc113         // copy acc to vreg[113]
v_accvgpr_read_b32 v[vgprValuC+22], acc114         // copy acc to vreg[114]
v_accvgpr_read_b32 v[vgprValuC+23], acc115         // copy acc to vreg[115]
v_accvgpr_read_b32 v[vgprValuC+32], acc116         // copy acc to vreg[116]
v_accvgpr_read_b32 v[vgprValuC+33], acc117         // copy acc to vreg[117]
v_accvgpr_read_b32 v[vgprValuC+34], acc118         // copy acc to vreg[118]
v_accvgpr_read_b32 v[vgprValuC+35], acc119         // copy acc to vreg[119]
v_accvgpr_read_b32 v[vgprValuC+48], acc120         // copy acc to vreg[120]
v_accvgpr_read_b32 v[vgprValuC+49], acc121         // copy acc to vreg[121]
v_accvgpr_read_b32 v[vgprValuC+50], acc122         // copy acc to vreg[122]
v_accvgpr_read_b32 v[vgprValuC+51], acc123         // copy acc to vreg[123]
v_accvgpr_read_b32 v[vgprValuC+64], acc124         // copy acc to vreg[124]
v_accvgpr_read_b32 v[vgprValuC+65], acc125         // copy acc to vreg[125]
v_accvgpr_read_b32 v[vgprValuC+66], acc126         // copy acc to vreg[126]
v_accvgpr_read_b32 v[vgprValuC+67], acc127         // copy acc to vreg[127]
v_accvgpr_read_b32 v[vgprValuC+72], acc128         // copy acc to vreg[128]
v_accvgpr_read_b32 v[vgprValuC+73], acc129         // copy acc to vreg[129]
v_accvgpr_read_b32 v[vgprValuC+74], acc130         // copy acc to vreg[130]
v_accvgpr_read_b32 v[vgprValuC+75], acc131         // copy acc to vreg[131]
v_accvgpr_read_b32 v[vgprValuC+76], acc132         // copy acc to vreg[132]
v_accvgpr_read_b32 v[vgprValuC+77], acc133         // copy acc to vreg[133]
v_accvgpr_read_b32 v[vgprValuC+78], acc134         // copy acc to vreg[134]
v_accvgpr_read_b32 v[vgprValuC+79], acc135         // copy acc to vreg[135]
v_accvgpr_read_b32 v[vgprValuC+84], acc136         // copy acc to vreg[136]
v_accvgpr_read_b32 v[vgprValuC+85], acc137         // copy acc to vreg[137]
v_accvgpr_read_b32 v[vgprValuC+86], acc138         // copy acc to vreg[138]
v_accvgpr_read_b32 v[vgprValuC+87], acc139         // copy acc to vreg[139]
v_accvgpr_read_b32 v[vgprValuC+92], acc140         // copy acc to vreg[140]
v_accvgpr_read_b32 v[vgprValuC+93], acc141         // copy acc to vreg[141]
v_accvgpr_read_b32 v[vgprValuC+94], acc142         // copy acc to vreg[142]
v_accvgpr_read_b32 v[vgprValuC+95], acc143         // copy acc to vreg[143]
v_accvgpr_read_b32 v[vgprValuC+100], acc144        // copy acc to vreg[144]
v_accvgpr_read_b32 v[vgprValuC+101], acc145        // copy acc to vreg[145]
v_accvgpr_read_b32 v[vgprValuC+102], acc146        // copy acc to vreg[146]
v_accvgpr_read_b32 v[vgprValuC+103], acc147        // copy acc to vreg[147]
v_accvgpr_read_b32 v[vgprValuC+104], acc148        // copy acc to vreg[148]
v_accvgpr_read_b32 v[vgprValuC+105], acc149        // copy acc to vreg[149]
v_accvgpr_read_b32 v[vgprValuC+106], acc150        // copy acc to vreg[150]
v_accvgpr_read_b32 v[vgprValuC+107], acc151        // copy acc to vreg[151]
v_accvgpr_read_b32 v[vgprValuC+112], acc152        // copy acc to vreg[152]
v_accvgpr_read_b32 v[vgprValuC+113], acc153        // copy acc to vreg[153]
v_accvgpr_read_b32 v[vgprValuC+114], acc154        // copy acc to vreg[154]
v_accvgpr_read_b32 v[vgprValuC+115], acc155        // copy acc to vreg[155]
v_accvgpr_read_b32 v[vgprValuC+120], acc156        // copy acc to vreg[156]
v_accvgpr_read_b32 v[vgprValuC+121], acc157        // copy acc to vreg[157]
v_accvgpr_read_b32 v[vgprValuC+122], acc158        // copy acc to vreg[158]
v_accvgpr_read_b32 v[vgprValuC+123], acc159        // copy acc to vreg[159]
v_accvgpr_read_b32 v[vgprValuC+128], acc160        // copy acc to vreg[160]
v_accvgpr_read_b32 v[vgprValuC+129], acc161        // copy acc to vreg[161]
v_accvgpr_read_b32 v[vgprValuC+130], acc162        // copy acc to vreg[162]
v_accvgpr_read_b32 v[vgprValuC+131], acc163        // copy acc to vreg[163]
v_accvgpr_read_b32 v[vgprValuC+132], acc164        // copy acc to vreg[164]
v_accvgpr_read_b32 v[vgprValuC+133], acc165        // copy acc to vreg[165]
v_accvgpr_read_b32 v[vgprValuC+134], acc166        // copy acc to vreg[166]
v_accvgpr_read_b32 v[vgprValuC+135], acc167        // copy acc to vreg[167]

/* rC *= alpha batchElements=[(0, 0, 7, 0), (0, 1, 7, 0), (0, 2, 7, 0), (0, 3, 7, 0), (1, 0, 0, 0), (1, 1, 0, 0), (1, 2, 0, 0), (1, 3, 0, 0), (1, 0, 1, 0), (1, 1, 1, 0), (1, 2, 1, 0), (1, 3, 1, 0), (1, 0, 2, 0), (1, 1, 2, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+104], s[sgprAlpha], v[vgprValuC+104] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+107], s[sgprAlpha], v[vgprValuC+107] // *= alpha
v_mul_f32 v[vgprValuC+112], s[sgprAlpha], v[vgprValuC+112] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+114], s[sgprAlpha], v[vgprValuC+114] // *= alpha
v_mul_f32 v[vgprValuC+115], s[sgprAlpha], v[vgprValuC+115] // *= alpha
v_mul_f32 v[vgprValuC+120], s[sgprAlpha], v[vgprValuC+120] // *= alpha
v_mul_f32 v[vgprValuC+121], s[sgprAlpha], v[vgprValuC+121] // *= alpha
v_mul_f32 v[vgprValuC+122], s[sgprAlpha], v[vgprValuC+122] // *= alpha
v_mul_f32 v[vgprValuC+123], s[sgprAlpha], v[vgprValuC+123] // *= alpha
v_mul_f32 v[vgprValuC+128], s[sgprAlpha], v[vgprValuC+128] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+130], s[sgprAlpha], v[vgprValuC+130] // *= alpha
v_mul_f32 v[vgprValuC+131], s[sgprAlpha], v[vgprValuC+131] // *= alpha
v_mul_f32 v[vgprValuC+132], s[sgprAlpha], v[vgprValuC+132] // *= alpha
v_mul_f32 v[vgprValuC+133], s[sgprAlpha], v[vgprValuC+133] // *= alpha
v_mul_f32 v[vgprValuC+134], s[sgprAlpha], v[vgprValuC+134] // *= alpha
v_mul_f32 v[vgprValuC+135], s[sgprAlpha], v[vgprValuC+135] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[44:45], v[vgprValuC+48:vgprValuC+48+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[46:47], v[vgprValuC+50:vgprValuC+50+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[40:41], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[42:43], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[60:61], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[62:63], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[56:57], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[58:59], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v39, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[16:17], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[18:19], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[12:13], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[14:15], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[28:29], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[30:31], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[24:25], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[26:27], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
buffer_store_dwordx2 v[76:77], v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[44:45], v[vgprValuC+84:vgprValuC+84+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[46:47], v[vgprValuC+86:vgprValuC+86+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_pk_add_f32 v[vgprValuC+84:vgprValuC+84+1], v[40:41], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[vgprValuC+86:vgprValuC+86+1], v[42:43], v[vgprValuC+86:vgprValuC+86+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+84], v[vgprValuC+84]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+85], v[vgprValuC+85]     // convert C to fp16
v_pack_b32_f16 v84, v[vgprValuC+84], v[vgprValuC+85] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+86], v[vgprValuC+86]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+87], v[vgprValuC+87]     // convert C to fp16
v_pack_b32_f16 v85, v[vgprValuC+86], v[vgprValuC+87] // Pack with neighbor
buffer_store_dwordx2 v[84:85], v80, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[60:61], v[vgprValuC+92:vgprValuC+92+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[62:63], v[vgprValuC+94:vgprValuC+94+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_pk_add_f32 v[vgprValuC+92:vgprValuC+92+1], v[56:57], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[vgprValuC+94:vgprValuC+94+1], v[58:59], v[vgprValuC+94:vgprValuC+94+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+92], v[vgprValuC+92]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+93], v[vgprValuC+93]     // convert C to fp16
v_pack_b32_f16 v92, v[vgprValuC+92], v[vgprValuC+93] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+94], v[vgprValuC+94]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+95], v[vgprValuC+95]     // convert C to fp16
v_pack_b32_f16 v93, v[vgprValuC+94], v[vgprValuC+95] // Pack with neighbor
buffer_store_dwordx2 v[92:93], v83, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[16:17], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[18:19], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[12:13], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[14:15], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
buffer_store_dwordx2 v[100:101], v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[28:29], v[vgprValuC+104:vgprValuC+104+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[30:31], v[vgprValuC+106:vgprValuC+106+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+104:vgprValuC+104+1], v[24:25], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[vgprValuC+106:vgprValuC+106+1], v[26:27], v[vgprValuC+106:vgprValuC+106+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+104], v[vgprValuC+104]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+105], v[vgprValuC+105]   // convert C to fp16
v_pack_b32_f16 v104, v[vgprValuC+104], v[vgprValuC+105] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+106], v[vgprValuC+106]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+107], v[vgprValuC+107]   // convert C to fp16
v_pack_b32_f16 v105, v[vgprValuC+106], v[vgprValuC+107] // Pack with neighbor
buffer_store_dwordx2 v[104:105], v97, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+112:vgprValuC+112+1], v[44:45], v[vgprValuC+112:vgprValuC+112+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+114:vgprValuC+114+1], v[46:47], v[vgprValuC+114:vgprValuC+114+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_pk_add_f32 v[vgprValuC+112:vgprValuC+112+1], v[40:41], v[vgprValuC+112:vgprValuC+112+1] // C += bias
v_pk_add_f32 v[vgprValuC+114:vgprValuC+114+1], v[42:43], v[vgprValuC+114:vgprValuC+114+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+112], v[vgprValuC+112]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+113], v[vgprValuC+113]   // convert C to fp16
v_pack_b32_f16 v112, v[vgprValuC+112], v[vgprValuC+113] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+114], v[vgprValuC+114]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+115], v[vgprValuC+115]   // convert C to fp16
v_pack_b32_f16 v113, v[vgprValuC+114], v[vgprValuC+115] // Pack with neighbor
buffer_store_dwordx2 v[112:113], v108, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+120:vgprValuC+120+1], v[60:61], v[vgprValuC+120:vgprValuC+120+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+122:vgprValuC+122+1], v[62:63], v[vgprValuC+122:vgprValuC+122+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_pk_add_f32 v[vgprValuC+120:vgprValuC+120+1], v[56:57], v[vgprValuC+120:vgprValuC+120+1] // C += bias
v_pk_add_f32 v[vgprValuC+122:vgprValuC+122+1], v[58:59], v[vgprValuC+122:vgprValuC+122+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+120], v[vgprValuC+120]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+121], v[vgprValuC+121]   // convert C to fp16
v_pack_b32_f16 v120, v[vgprValuC+120], v[vgprValuC+121] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+122], v[vgprValuC+122]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+123], v[vgprValuC+123]   // convert C to fp16
v_pack_b32_f16 v121, v[vgprValuC+122], v[vgprValuC+123] // Pack with neighbor
buffer_store_dwordx2 v[120:121], v111, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+128:vgprValuC+128+1], v[16:17], v[vgprValuC+128:vgprValuC+128+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+130:vgprValuC+130+1], v[18:19], v[vgprValuC+130:vgprValuC+130+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+128:vgprValuC+128+1], v[12:13], v[vgprValuC+128:vgprValuC+128+1] // C += bias
v_pk_add_f32 v[vgprValuC+130:vgprValuC+130+1], v[14:15], v[vgprValuC+130:vgprValuC+130+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+128], v[vgprValuC+128]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+129], v[vgprValuC+129]   // convert C to fp16
v_pack_b32_f16 v128, v[vgprValuC+128], v[vgprValuC+129] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+130], v[vgprValuC+130]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+131], v[vgprValuC+131]   // convert C to fp16
v_pack_b32_f16 v129, v[vgprValuC+130], v[vgprValuC+131] // Pack with neighbor
buffer_store_dwordx2 v[128:129], v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+132:vgprValuC+132+1], v[28:29], v[vgprValuC+132:vgprValuC+132+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+134:vgprValuC+134+1], v[30:31], v[vgprValuC+134:vgprValuC+134+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+132:vgprValuC+132+1], v[24:25], v[vgprValuC+132:vgprValuC+132+1] // C += bias
v_pk_add_f32 v[vgprValuC+134:vgprValuC+134+1], v[26:27], v[vgprValuC+134:vgprValuC+134+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+132], v[vgprValuC+132]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+133], v[vgprValuC+133]   // convert C to fp16
v_pack_b32_f16 v132, v[vgprValuC+132], v[vgprValuC+133] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+134], v[vgprValuC+134]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+135], v[vgprValuC+135]   // convert C to fp16
v_pack_b32_f16 v133, v[vgprValuC+134], v[vgprValuC+135] // Pack with neighbor
buffer_store_dwordx2 v[132:133], v125, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #3 (d1,d0,vc1,vc0) = */
/*    (1,2,2,0:vw4); (1,3,2,0:vw4); (1,0,3,0:vw4); (1,1,3,0:vw4); (1,2,3,0:vw4); (1,3,3,0:vw4); (1,0,4,0:vw4); (1,1,4,0:vw4); (1,2,4,0:vw4); (1,3,4,0:vw4); (1,0,5,0:vw4); (1,1,5,0:vw4); (1,2,5,0:vw4); (1,3,5,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v136, BufferOOB
/* (d1,vc1,d0,vc0)=(1,2,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v4, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b128 v[12:15], v7 offset:0                 // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b128 v[16:19], v8 offset:0                 // load scaleAlpha
v_add_lshl_u32 v6, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v136, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v4, s52
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
ds_read_b128 v[24:27], v10 offset:0                // load Bias
v_add_u32 v11, 1024, v10                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[28:31], v11 offset:0                // load scaleAlpha
v_add_lshl_u32 v9, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v136, v9, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v37, v0, s52
v_lshlrev_b32 v37, 0x2, v37                        // Bias address scaled by BPE
ds_read_b128 v[40:43], v37 offset:0                // load Bias
v_add_u32 v38, 1024, v37                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[44:47], v38 offset:0                // load scaleAlpha
v_add_lshl_u32 v36, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v136, v36, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v4, s52
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
ds_read_b128 v[56:59], v52 offset:0                // load Bias
v_add_u32 v53, 1024, v52                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[60:63], v53 offset:0                // load scaleAlpha
v_add_lshl_u32 v39, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v39, v136, v39, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v55, v4, s52
v_lshlrev_b32 v55, 0x2, v55                        // Bias address scaled by BPE
v_add_u32 v68, 1024, v55                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v54, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v136, v54, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v4, s52
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
v_add_u32 v71, 1024, v70                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v69, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v136, v69, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v81, v0, s52
v_lshlrev_b32 v81, 0x2, v81                        // Bias address scaled by BPE
v_add_u32 v82, 1024, v81                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v80, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v80, v136, v80, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v88, v4, s52
v_lshlrev_b32 v88, 0x2, v88                        // Bias address scaled by BPE
v_add_u32 v89, 1024, v88                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v83, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v136, v83, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v4, s52
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
v_add_u32 v96, 1024, v91                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v90, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v136, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v98, v4, s52
v_lshlrev_b32 v98, 0x2, v98                        // Bias address scaled by BPE
v_add_u32 v99, 1024, v98                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v97, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v136, v97, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v109, v0, s52
v_lshlrev_b32 v109, 0x2, v109                      // Bias address scaled by BPE
v_add_u32 v110, 1024, v109                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v108, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v108, v136, v108, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v116, v4, s52
v_lshlrev_b32 v116, 0x2, v116                      // Bias address scaled by BPE
v_add_u32 v117, 1024, v116                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v111, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v111, v136, v111, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v4, s52
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v124, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v136, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v126, v4, s52
v_lshlrev_b32 v126, 0x2, v126                      // Bias address scaled by BPE
v_add_u32 v127, 1024, v126                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v125, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v125, v136, v125, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc168         // copy acc to vreg[168]
v_accvgpr_read_b32 v[vgprValuC+21], acc169         // copy acc to vreg[169]
v_accvgpr_read_b32 v[vgprValuC+22], acc170         // copy acc to vreg[170]
v_accvgpr_read_b32 v[vgprValuC+23], acc171         // copy acc to vreg[171]
v_accvgpr_read_b32 v[vgprValuC+32], acc172         // copy acc to vreg[172]
v_accvgpr_read_b32 v[vgprValuC+33], acc173         // copy acc to vreg[173]
v_accvgpr_read_b32 v[vgprValuC+34], acc174         // copy acc to vreg[174]
v_accvgpr_read_b32 v[vgprValuC+35], acc175         // copy acc to vreg[175]
v_accvgpr_read_b32 v[vgprValuC+48], acc176         // copy acc to vreg[176]
v_accvgpr_read_b32 v[vgprValuC+49], acc177         // copy acc to vreg[177]
v_accvgpr_read_b32 v[vgprValuC+50], acc178         // copy acc to vreg[178]
v_accvgpr_read_b32 v[vgprValuC+51], acc179         // copy acc to vreg[179]
v_accvgpr_read_b32 v[vgprValuC+64], acc180         // copy acc to vreg[180]
v_accvgpr_read_b32 v[vgprValuC+65], acc181         // copy acc to vreg[181]
v_accvgpr_read_b32 v[vgprValuC+66], acc182         // copy acc to vreg[182]
v_accvgpr_read_b32 v[vgprValuC+67], acc183         // copy acc to vreg[183]
v_accvgpr_read_b32 v[vgprValuC+72], acc184         // copy acc to vreg[184]
v_accvgpr_read_b32 v[vgprValuC+73], acc185         // copy acc to vreg[185]
v_accvgpr_read_b32 v[vgprValuC+74], acc186         // copy acc to vreg[186]
v_accvgpr_read_b32 v[vgprValuC+75], acc187         // copy acc to vreg[187]
v_accvgpr_read_b32 v[vgprValuC+76], acc188         // copy acc to vreg[188]
v_accvgpr_read_b32 v[vgprValuC+77], acc189         // copy acc to vreg[189]
v_accvgpr_read_b32 v[vgprValuC+78], acc190         // copy acc to vreg[190]
v_accvgpr_read_b32 v[vgprValuC+79], acc191         // copy acc to vreg[191]
v_accvgpr_read_b32 v[vgprValuC+84], acc192         // copy acc to vreg[192]
v_accvgpr_read_b32 v[vgprValuC+85], acc193         // copy acc to vreg[193]
v_accvgpr_read_b32 v[vgprValuC+86], acc194         // copy acc to vreg[194]
v_accvgpr_read_b32 v[vgprValuC+87], acc195         // copy acc to vreg[195]
v_accvgpr_read_b32 v[vgprValuC+92], acc196         // copy acc to vreg[196]
v_accvgpr_read_b32 v[vgprValuC+93], acc197         // copy acc to vreg[197]
v_accvgpr_read_b32 v[vgprValuC+94], acc198         // copy acc to vreg[198]
v_accvgpr_read_b32 v[vgprValuC+95], acc199         // copy acc to vreg[199]
v_accvgpr_read_b32 v[vgprValuC+100], acc200        // copy acc to vreg[200]
v_accvgpr_read_b32 v[vgprValuC+101], acc201        // copy acc to vreg[201]
v_accvgpr_read_b32 v[vgprValuC+102], acc202        // copy acc to vreg[202]
v_accvgpr_read_b32 v[vgprValuC+103], acc203        // copy acc to vreg[203]
v_accvgpr_read_b32 v[vgprValuC+104], acc204        // copy acc to vreg[204]
v_accvgpr_read_b32 v[vgprValuC+105], acc205        // copy acc to vreg[205]
v_accvgpr_read_b32 v[vgprValuC+106], acc206        // copy acc to vreg[206]
v_accvgpr_read_b32 v[vgprValuC+107], acc207        // copy acc to vreg[207]
v_accvgpr_read_b32 v[vgprValuC+112], acc208        // copy acc to vreg[208]
v_accvgpr_read_b32 v[vgprValuC+113], acc209        // copy acc to vreg[209]
v_accvgpr_read_b32 v[vgprValuC+114], acc210        // copy acc to vreg[210]
v_accvgpr_read_b32 v[vgprValuC+115], acc211        // copy acc to vreg[211]
v_accvgpr_read_b32 v[vgprValuC+120], acc212        // copy acc to vreg[212]
v_accvgpr_read_b32 v[vgprValuC+121], acc213        // copy acc to vreg[213]
v_accvgpr_read_b32 v[vgprValuC+122], acc214        // copy acc to vreg[214]
v_accvgpr_read_b32 v[vgprValuC+123], acc215        // copy acc to vreg[215]
v_accvgpr_read_b32 v[vgprValuC+128], acc216        // copy acc to vreg[216]
v_accvgpr_read_b32 v[vgprValuC+129], acc217        // copy acc to vreg[217]
v_accvgpr_read_b32 v[vgprValuC+130], acc218        // copy acc to vreg[218]
v_accvgpr_read_b32 v[vgprValuC+131], acc219        // copy acc to vreg[219]
v_accvgpr_read_b32 v[vgprValuC+132], acc220        // copy acc to vreg[220]
v_accvgpr_read_b32 v[vgprValuC+133], acc221        // copy acc to vreg[221]
v_accvgpr_read_b32 v[vgprValuC+134], acc222        // copy acc to vreg[222]
v_accvgpr_read_b32 v[vgprValuC+135], acc223        // copy acc to vreg[223]

/* rC *= alpha batchElements=[(1, 2, 2, 0), (1, 3, 2, 0), (1, 0, 3, 0), (1, 1, 3, 0), (1, 2, 3, 0), (1, 3, 3, 0), (1, 0, 4, 0), (1, 1, 4, 0), (1, 2, 4, 0), (1, 3, 4, 0), (1, 0, 5, 0), (1, 1, 5, 0), (1, 2, 5, 0), (1, 3, 5, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+104], s[sgprAlpha], v[vgprValuC+104] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+107], s[sgprAlpha], v[vgprValuC+107] // *= alpha
v_mul_f32 v[vgprValuC+112], s[sgprAlpha], v[vgprValuC+112] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+114], s[sgprAlpha], v[vgprValuC+114] // *= alpha
v_mul_f32 v[vgprValuC+115], s[sgprAlpha], v[vgprValuC+115] // *= alpha
v_mul_f32 v[vgprValuC+120], s[sgprAlpha], v[vgprValuC+120] // *= alpha
v_mul_f32 v[vgprValuC+121], s[sgprAlpha], v[vgprValuC+121] // *= alpha
v_mul_f32 v[vgprValuC+122], s[sgprAlpha], v[vgprValuC+122] // *= alpha
v_mul_f32 v[vgprValuC+123], s[sgprAlpha], v[vgprValuC+123] // *= alpha
v_mul_f32 v[vgprValuC+128], s[sgprAlpha], v[vgprValuC+128] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+130], s[sgprAlpha], v[vgprValuC+130] // *= alpha
v_mul_f32 v[vgprValuC+131], s[sgprAlpha], v[vgprValuC+131] // *= alpha
v_mul_f32 v[vgprValuC+132], s[sgprAlpha], v[vgprValuC+132] // *= alpha
v_mul_f32 v[vgprValuC+133], s[sgprAlpha], v[vgprValuC+133] // *= alpha
v_mul_f32 v[vgprValuC+134], s[sgprAlpha], v[vgprValuC+134] // *= alpha
v_mul_f32 v[vgprValuC+135], s[sgprAlpha], v[vgprValuC+135] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[44:45], v[vgprValuC+48:vgprValuC+48+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[46:47], v[vgprValuC+50:vgprValuC+50+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[40:41], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[42:43], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[60:61], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[62:63], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[56:57], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[58:59], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v39, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[16:17], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[18:19], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[12:13], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[14:15], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[28:29], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[30:31], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[24:25], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[26:27], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
buffer_store_dwordx2 v[76:77], v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[44:45], v[vgprValuC+84:vgprValuC+84+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[46:47], v[vgprValuC+86:vgprValuC+86+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_pk_add_f32 v[vgprValuC+84:vgprValuC+84+1], v[40:41], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[vgprValuC+86:vgprValuC+86+1], v[42:43], v[vgprValuC+86:vgprValuC+86+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+84], v[vgprValuC+84]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+85], v[vgprValuC+85]     // convert C to fp16
v_pack_b32_f16 v84, v[vgprValuC+84], v[vgprValuC+85] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+86], v[vgprValuC+86]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+87], v[vgprValuC+87]     // convert C to fp16
v_pack_b32_f16 v85, v[vgprValuC+86], v[vgprValuC+87] // Pack with neighbor
buffer_store_dwordx2 v[84:85], v80, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[60:61], v[vgprValuC+92:vgprValuC+92+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[62:63], v[vgprValuC+94:vgprValuC+94+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_pk_add_f32 v[vgprValuC+92:vgprValuC+92+1], v[56:57], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[vgprValuC+94:vgprValuC+94+1], v[58:59], v[vgprValuC+94:vgprValuC+94+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+92], v[vgprValuC+92]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+93], v[vgprValuC+93]     // convert C to fp16
v_pack_b32_f16 v92, v[vgprValuC+92], v[vgprValuC+93] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+94], v[vgprValuC+94]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+95], v[vgprValuC+95]     // convert C to fp16
v_pack_b32_f16 v93, v[vgprValuC+94], v[vgprValuC+95] // Pack with neighbor
buffer_store_dwordx2 v[92:93], v83, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[16:17], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[18:19], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[12:13], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[14:15], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
buffer_store_dwordx2 v[100:101], v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[28:29], v[vgprValuC+104:vgprValuC+104+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[30:31], v[vgprValuC+106:vgprValuC+106+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+104:vgprValuC+104+1], v[24:25], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[vgprValuC+106:vgprValuC+106+1], v[26:27], v[vgprValuC+106:vgprValuC+106+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+104], v[vgprValuC+104]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+105], v[vgprValuC+105]   // convert C to fp16
v_pack_b32_f16 v104, v[vgprValuC+104], v[vgprValuC+105] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+106], v[vgprValuC+106]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+107], v[vgprValuC+107]   // convert C to fp16
v_pack_b32_f16 v105, v[vgprValuC+106], v[vgprValuC+107] // Pack with neighbor
buffer_store_dwordx2 v[104:105], v97, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+112:vgprValuC+112+1], v[44:45], v[vgprValuC+112:vgprValuC+112+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+114:vgprValuC+114+1], v[46:47], v[vgprValuC+114:vgprValuC+114+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_pk_add_f32 v[vgprValuC+112:vgprValuC+112+1], v[40:41], v[vgprValuC+112:vgprValuC+112+1] // C += bias
v_pk_add_f32 v[vgprValuC+114:vgprValuC+114+1], v[42:43], v[vgprValuC+114:vgprValuC+114+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+112], v[vgprValuC+112]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+113], v[vgprValuC+113]   // convert C to fp16
v_pack_b32_f16 v112, v[vgprValuC+112], v[vgprValuC+113] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+114], v[vgprValuC+114]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+115], v[vgprValuC+115]   // convert C to fp16
v_pack_b32_f16 v113, v[vgprValuC+114], v[vgprValuC+115] // Pack with neighbor
buffer_store_dwordx2 v[112:113], v108, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+120:vgprValuC+120+1], v[60:61], v[vgprValuC+120:vgprValuC+120+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+122:vgprValuC+122+1], v[62:63], v[vgprValuC+122:vgprValuC+122+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_pk_add_f32 v[vgprValuC+120:vgprValuC+120+1], v[56:57], v[vgprValuC+120:vgprValuC+120+1] // C += bias
v_pk_add_f32 v[vgprValuC+122:vgprValuC+122+1], v[58:59], v[vgprValuC+122:vgprValuC+122+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+120], v[vgprValuC+120]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+121], v[vgprValuC+121]   // convert C to fp16
v_pack_b32_f16 v120, v[vgprValuC+120], v[vgprValuC+121] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+122], v[vgprValuC+122]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+123], v[vgprValuC+123]   // convert C to fp16
v_pack_b32_f16 v121, v[vgprValuC+122], v[vgprValuC+123] // Pack with neighbor
buffer_store_dwordx2 v[120:121], v111, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+128:vgprValuC+128+1], v[16:17], v[vgprValuC+128:vgprValuC+128+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+130:vgprValuC+130+1], v[18:19], v[vgprValuC+130:vgprValuC+130+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+128:vgprValuC+128+1], v[12:13], v[vgprValuC+128:vgprValuC+128+1] // C += bias
v_pk_add_f32 v[vgprValuC+130:vgprValuC+130+1], v[14:15], v[vgprValuC+130:vgprValuC+130+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+128], v[vgprValuC+128]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+129], v[vgprValuC+129]   // convert C to fp16
v_pack_b32_f16 v128, v[vgprValuC+128], v[vgprValuC+129] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+130], v[vgprValuC+130]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+131], v[vgprValuC+131]   // convert C to fp16
v_pack_b32_f16 v129, v[vgprValuC+130], v[vgprValuC+131] // Pack with neighbor
buffer_store_dwordx2 v[128:129], v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+132:vgprValuC+132+1], v[28:29], v[vgprValuC+132:vgprValuC+132+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+134:vgprValuC+134+1], v[30:31], v[vgprValuC+134:vgprValuC+134+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+132:vgprValuC+132+1], v[24:25], v[vgprValuC+132:vgprValuC+132+1] // C += bias
v_pk_add_f32 v[vgprValuC+134:vgprValuC+134+1], v[26:27], v[vgprValuC+134:vgprValuC+134+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+132], v[vgprValuC+132]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+133], v[vgprValuC+133]   // convert C to fp16
v_pack_b32_f16 v132, v[vgprValuC+132], v[vgprValuC+133] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+134], v[vgprValuC+134]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+135], v[vgprValuC+135]   // convert C to fp16
v_pack_b32_f16 v133, v[vgprValuC+134], v[vgprValuC+135] // Pack with neighbor
buffer_store_dwordx2 v[132:133], v125, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #4 (d1,d0,vc1,vc0) = */
/*    (1,0,6,0:vw4); (1,1,6,0:vw4); (1,2,6,0:vw4); (1,3,6,0:vw4); (1,0,7,0:vw4); (1,1,7,0:vw4); (1,2,7,0:vw4); (1,3,7,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v90, BufferOOB
/* (d1,vc1,d0,vc0)=(1,6,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v0, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b128 v[12:15], v7 offset:0                 // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b128 v[16:19], v8 offset:0                 // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v90, v6, s[56:57]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v4, s52
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
ds_read_b128 v[24:27], v10 offset:0                // load Bias
v_add_u32 v11, 1024, v10                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[28:31], v11 offset:0                // load scaleAlpha
v_add_lshl_u32 v9, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v90, v9, s[56:57]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v37, v4, s52
v_lshlrev_b32 v37, 0x2, v37                        // Bias address scaled by BPE
ds_read_b128 v[40:43], v37 offset:0                // load Bias
v_add_u32 v38, 1024, v37                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[44:47], v38 offset:0                // load scaleAlpha
v_add_lshl_u32 v36, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v90, v36, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v4, s52
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
ds_read_b128 v[56:59], v52 offset:0                // load Bias
v_add_u32 v53, 1024, v52                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[60:63], v53 offset:0                // load scaleAlpha
v_add_lshl_u32 v39, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v39, v90, v39, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v55, v0, s52
v_lshlrev_b32 v55, 0x2, v55                        // Bias address scaled by BPE
v_add_u32 v68, 1024, v55                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v54, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v90, v54, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v4, s52
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
v_add_u32 v71, 1024, v70                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v69, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v90, v69, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v81, v4, s52
v_lshlrev_b32 v81, 0x2, v81                        // Bias address scaled by BPE
v_add_u32 v82, 1024, v81                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v80, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v80, v90, v80, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v88, v4, s52
v_lshlrev_b32 v88, 0x2, v88                        // Bias address scaled by BPE
v_add_u32 v89, 1024, v88                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v83, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v90, v83, s[56:57]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc224         // copy acc to vreg[224]
v_accvgpr_read_b32 v[vgprValuC+21], acc225         // copy acc to vreg[225]
v_accvgpr_read_b32 v[vgprValuC+22], acc226         // copy acc to vreg[226]
v_accvgpr_read_b32 v[vgprValuC+23], acc227         // copy acc to vreg[227]
v_accvgpr_read_b32 v[vgprValuC+32], acc228         // copy acc to vreg[228]
v_accvgpr_read_b32 v[vgprValuC+33], acc229         // copy acc to vreg[229]
v_accvgpr_read_b32 v[vgprValuC+34], acc230         // copy acc to vreg[230]
v_accvgpr_read_b32 v[vgprValuC+35], acc231         // copy acc to vreg[231]
v_accvgpr_read_b32 v[vgprValuC+48], acc232         // copy acc to vreg[232]
v_accvgpr_read_b32 v[vgprValuC+49], acc233         // copy acc to vreg[233]
v_accvgpr_read_b32 v[vgprValuC+50], acc234         // copy acc to vreg[234]
v_accvgpr_read_b32 v[vgprValuC+51], acc235         // copy acc to vreg[235]
v_accvgpr_read_b32 v[vgprValuC+64], acc236         // copy acc to vreg[236]
v_accvgpr_read_b32 v[vgprValuC+65], acc237         // copy acc to vreg[237]
v_accvgpr_read_b32 v[vgprValuC+66], acc238         // copy acc to vreg[238]
v_accvgpr_read_b32 v[vgprValuC+67], acc239         // copy acc to vreg[239]
v_accvgpr_read_b32 v[vgprValuC+72], acc240         // copy acc to vreg[240]
v_accvgpr_read_b32 v[vgprValuC+73], acc241         // copy acc to vreg[241]
v_accvgpr_read_b32 v[vgprValuC+74], acc242         // copy acc to vreg[242]
v_accvgpr_read_b32 v[vgprValuC+75], acc243         // copy acc to vreg[243]
v_accvgpr_read_b32 v[vgprValuC+76], acc244         // copy acc to vreg[244]
v_accvgpr_read_b32 v[vgprValuC+77], acc245         // copy acc to vreg[245]
v_accvgpr_read_b32 v[vgprValuC+78], acc246         // copy acc to vreg[246]
v_accvgpr_read_b32 v[vgprValuC+79], acc247         // copy acc to vreg[247]
v_accvgpr_read_b32 v[vgprValuC+84], acc248         // copy acc to vreg[248]
v_accvgpr_read_b32 v[vgprValuC+85], acc249         // copy acc to vreg[249]
v_accvgpr_read_b32 v[vgprValuC+86], acc250         // copy acc to vreg[250]
v_accvgpr_read_b32 v[vgprValuC+87], acc251         // copy acc to vreg[251]
v_accvgpr_read_b32 v[vgprValuC+92], acc252         // copy acc to vreg[252]
v_accvgpr_read_b32 v[vgprValuC+93], acc253         // copy acc to vreg[253]
v_accvgpr_read_b32 v[vgprValuC+94], acc254         // copy acc to vreg[254]
v_accvgpr_read_b32 v[vgprValuC+95], acc255         // copy acc to vreg[255]

/* rC *= alpha batchElements=[(1, 0, 6, 0), (1, 1, 6, 0), (1, 2, 6, 0), (1, 3, 6, 0), (1, 0, 7, 0), (1, 1, 7, 0), (1, 2, 7, 0), (1, 3, 7, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[28:29], v[vgprValuC+32:vgprValuC+32+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[30:31], v[vgprValuC+34:vgprValuC+34+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[44:45], v[vgprValuC+48:vgprValuC+48+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[46:47], v[vgprValuC+50:vgprValuC+50+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[40:41], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[42:43], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[60:61], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[62:63], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[56:57], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[58:59], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v39, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[16:17], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[18:19], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[12:13], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[14:15], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[28:29], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(28)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[30:31], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(28)(2)
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[24:25], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[26:27], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
buffer_store_dwordx2 v[76:77], v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[44:45], v[vgprValuC+84:vgprValuC+84+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[46:47], v[vgprValuC+86:vgprValuC+86+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_pk_add_f32 v[vgprValuC+84:vgprValuC+84+1], v[40:41], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[vgprValuC+86:vgprValuC+86+1], v[42:43], v[vgprValuC+86:vgprValuC+86+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+84], v[vgprValuC+84]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+85], v[vgprValuC+85]     // convert C to fp16
v_pack_b32_f16 v84, v[vgprValuC+84], v[vgprValuC+85] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+86], v[vgprValuC+86]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+87], v[vgprValuC+87]     // convert C to fp16
v_pack_b32_f16 v85, v[vgprValuC+86], v[vgprValuC+87] // Pack with neighbor
buffer_store_dwordx2 v[84:85], v80, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[60:61], v[vgprValuC+92:vgprValuC+92+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[62:63], v[vgprValuC+94:vgprValuC+94+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_pk_add_f32 v[vgprValuC+92:vgprValuC+92+1], v[56:57], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[vgprValuC+94:vgprValuC+94+1], v[58:59], v[vgprValuC+94:vgprValuC+94+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+92], v[vgprValuC+92]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+93], v[vgprValuC+93]     // convert C to fp16
v_pack_b32_f16 v92, v[vgprValuC+92], v[vgprValuC+93] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+94], v[vgprValuC+94]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+95], v[vgprValuC+95]     // convert C to fp16
v_pack_b32_f16 v93, v[vgprValuC+94], v[vgprValuC+95] // Pack with neighbor
buffer_store_dwordx2 v[92:93], v83, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_2                            // jump to end
label_GW_B0_E1_M_1:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=40 */
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw1); (0,0,0,1:vw1); (0,0,0,2:vw1); (0,0,0,3:vw1); (0,1,0,0:vw1); (0,1,0,1:vw1); (0,1,0,2:vw1); (0,1,0,3:vw1); (0,2,0,0:vw1); (0,2,0,1:vw1); (0,2,0,2:vw1); (0,2,0,3:vw1); (0,3,0,0:vw1); (0,3,0,1:vw1); (0,3,0,2:vw1); (0,3,0,3:vw1); (0,0,1,0:vw1); (0,0,1,1:vw1); (0,0,1,2:vw1); (0,0,1,3:vw1); (0,1,1,0:vw1); (0,1,1,1:vw1); (0,1,1,2:vw1); (0,1,1,3:vw1); (0,2,1,0:vw1); (0,2,1,1:vw1); (0,2,1,2:vw1); (0,2,1,3:vw1); (0,3,1,0:vw1); (0,3,1,1:vw1); (0,3,1,2:vw1); (0,3,1,3:vw1); (0,0,2,0:vw1); (0,0,2,1:vw1); (0,0,2,2:vw1); (0,0,2,3:vw1); (0,1,2,0:vw1); (0,1,2,1:vw1); (0,1,2,2:vw1); (0,1,2,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v199, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v0, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for LDS write
s_barrier                                          // LDS write barrier
ds_read_b32 v9, v7 offset:0                        // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v10, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v199, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v13, v4, s52
v_lshlrev_b32 v13, 0x2, v13                        // Bias address scaled by BPE
ds_read_b32 v15, v13 offset:0                      // load Bias
v_add_u32 v14, 1024, v13                           // add ScaleAlphaVec offset (3)
ds_read_b32 v16, v14 offset:0                      // load scaleAlpha
v_add_lshl_u32 v12, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v199, v12, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v19, v4, s52
v_lshlrev_b32 v19, 0x2, v19                        // Bias address scaled by BPE
ds_read_b32 v21, v19 offset:0                      // load Bias
v_add_u32 v20, 1024, v19                           // add ScaleAlphaVec offset (3)
ds_read_b32 v22, v20 offset:0                      // load scaleAlpha
v_add_lshl_u32 v18, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v199, v18, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v25, v4, s52
v_lshlrev_b32 v25, 0x2, v25                        // Bias address scaled by BPE
ds_read_b32 v27, v25 offset:0                      // load Bias
v_add_u32 v26, 1024, v25                           // add ScaleAlphaVec offset (3)
ds_read_b32 v28, v26 offset:0                      // load scaleAlpha
v_add_lshl_u32 v24, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v199, v24, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v31, v4, s52
v_lshlrev_b32 v31, 0x2, v31                        // Bias address scaled by BPE
ds_read_b32 v33, v31 offset:0                      // load Bias
v_add_u32 v32, 1024, v31                           // add ScaleAlphaVec offset (3)
ds_read_b32 v34, v32 offset:0                      // load scaleAlpha
v_add_lshl_u32 v30, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v199, v30, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v37, v4, s52
v_lshlrev_b32 v37, 0x2, v37                        // Bias address scaled by BPE
ds_read_b32 v39, v37 offset:0                      // load Bias
v_add_u32 v38, 1024, v37                           // add ScaleAlphaVec offset (3)
ds_read_b32 v40, v38 offset:0                      // load scaleAlpha
v_add_lshl_u32 v36, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v199, v36, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v43, v4, s52
v_lshlrev_b32 v43, 0x2, v43                        // Bias address scaled by BPE
ds_read_b32 v45, v43 offset:0                      // load Bias
v_add_u32 v44, 1024, v43                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v44 offset:0                      // load scaleAlpha
v_add_lshl_u32 v42, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v199, v42, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v4, s52
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
ds_read_b32 v51, v49 offset:0                      // load Bias
v_add_u32 v50, 1024, v49                           // add ScaleAlphaVec offset (3)
ds_read_b32 v52, v50 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v199, v48, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v55, v4, s52
v_lshlrev_b32 v55, 0x2, v55                        // Bias address scaled by BPE
ds_read_b32 v57, v55 offset:0                      // load Bias
v_add_u32 v56, 1024, v55                           // add ScaleAlphaVec offset (3)
ds_read_b32 v58, v56 offset:0                      // load scaleAlpha
v_add_lshl_u32 v54, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v199, v54, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v61, v4, s52
v_lshlrev_b32 v61, 0x2, v61                        // Bias address scaled by BPE
ds_read_b32 v63, v61 offset:0                      // load Bias
v_add_u32 v62, 1024, v61                           // add ScaleAlphaVec offset (3)
ds_read_b32 v64, v62 offset:0                      // load scaleAlpha
v_add_lshl_u32 v60, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v60, v199, v60, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s52
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
ds_read_b32 v69, v67 offset:0                      // load Bias
v_add_u32 v68, 1024, v67                           // add ScaleAlphaVec offset (3)
ds_read_b32 v70, v68 offset:0                      // load scaleAlpha
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v199, v66, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v73, v4, s52
v_lshlrev_b32 v73, 0x2, v73                        // Bias address scaled by BPE
ds_read_b32 v75, v73 offset:0                      // load Bias
v_add_u32 v74, 1024, v73                           // add ScaleAlphaVec offset (3)
ds_read_b32 v76, v74 offset:0                      // load scaleAlpha
v_add_lshl_u32 v72, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v72, v199, v72, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v79, v4, s52
v_lshlrev_b32 v79, 0x2, v79                        // Bias address scaled by BPE
ds_read_b32 v81, v79 offset:0                      // load Bias
v_add_u32 v80, 1024, v79                           // add ScaleAlphaVec offset (3)
ds_read_b32 v82, v80 offset:0                      // load scaleAlpha
v_add_lshl_u32 v78, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v78, v199, v78, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v85, v4, s52
v_lshlrev_b32 v85, 0x2, v85                        // Bias address scaled by BPE
ds_read_b32 v87, v85 offset:0                      // load Bias
v_add_u32 v86, 1024, v85                           // add ScaleAlphaVec offset (3)
ds_read_b32 v88, v86 offset:0                      // load scaleAlpha
v_add_lshl_u32 v84, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v199, v84, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v4, s52
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
ds_read_b32 v93, v91 offset:0                      // load Bias
v_add_u32 v92, 1024, v91                           // add ScaleAlphaVec offset (3)
ds_read_b32 v94, v92 offset:0                      // load scaleAlpha
v_add_lshl_u32 v90, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v199, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s52
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
ds_read_b32 v99, v97 offset:0                      // load Bias
v_add_u32 v98, 1024, v97                           // add ScaleAlphaVec offset (3)
ds_read_b32 v100, v98 offset:0                     // load scaleAlpha
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v199, v96, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v103, v0, s52
v_lshlrev_b32 v103, 0x2, v103                      // Bias address scaled by BPE
v_add_u32 v104, 1024, v103                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v102, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v102, v199, v102, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v107, v4, s52
v_lshlrev_b32 v107, 0x2, v107                      // Bias address scaled by BPE
v_add_u32 v108, 1024, v107                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v106, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v199, v106, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v111, v4, s52
v_lshlrev_b32 v111, 0x2, v111                      // Bias address scaled by BPE
v_add_u32 v112, 1024, v111                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v110, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v110, v199, v110, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v115, v4, s52
v_lshlrev_b32 v115, 0x2, v115                      // Bias address scaled by BPE
v_add_u32 v116, 1024, v115                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v114, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v114, v199, v114, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v4, s52
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v120, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v199, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v123, v4, s52
v_lshlrev_b32 v123, 0x2, v123                      // Bias address scaled by BPE
v_add_u32 v124, 1024, v123                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v122, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v122, v199, v122, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v127, v4, s52
v_lshlrev_b32 v127, 0x2, v127                      // Bias address scaled by BPE
v_add_u32 v128, 1024, v127                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v126, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v126, v199, v126, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v131, v4, s52
v_lshlrev_b32 v131, 0x2, v131                      // Bias address scaled by BPE
v_add_u32 v132, 1024, v131                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v130, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v130, v199, v130, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v135, v4, s52
v_lshlrev_b32 v135, 0x2, v135                      // Bias address scaled by BPE
v_add_u32 v136, 1024, v135                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v134, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v134, v199, v134, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v140, v4, s52
v_lshlrev_b32 v140, 0x2, v140                      // Bias address scaled by BPE
v_add_u32 v141, 1024, v140                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v138, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v199, v138, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v144, v4, s52
v_lshlrev_b32 v144, 0x2, v144                      // Bias address scaled by BPE
v_add_u32 v145, 1024, v144                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v143, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v143, v199, v143, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v148, v4, s52
v_lshlrev_b32 v148, 0x2, v148                      // Bias address scaled by BPE
v_add_u32 v149, 1024, v148                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v147, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v147, v199, v147, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v152, v4, s52
v_lshlrev_b32 v152, 0x2, v152                      // Bias address scaled by BPE
v_add_u32 v153, 1024, v152                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v151, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v151, v199, v151, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v156, v4, s52
v_lshlrev_b32 v156, 0x2, v156                      // Bias address scaled by BPE
v_add_u32 v157, 1024, v156                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v155, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v155, v199, v155, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v160, v4, s52
v_lshlrev_b32 v160, 0x2, v160                      // Bias address scaled by BPE
v_add_u32 v161, 1024, v160                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v159, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v199, v159, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v164, v4, s52
v_lshlrev_b32 v164, 0x2, v164                      // Bias address scaled by BPE
v_add_u32 v165, 1024, v164                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v163, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v163, v199, v163, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v168, v0, s52
v_lshlrev_b32 v168, 0x2, v168                      // Bias address scaled by BPE
v_add_u32 v169, 1024, v168                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v167, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v167, v199, v167, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v172, v4, s52
v_lshlrev_b32 v172, 0x2, v172                      // Bias address scaled by BPE
v_add_u32 v173, 1024, v172                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v171, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v171, v199, v171, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v176, v4, s52
v_lshlrev_b32 v176, 0x2, v176                      // Bias address scaled by BPE
v_add_u32 v177, 1024, v176                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v175, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v175, v199, v175, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v180, v4, s52
v_lshlrev_b32 v180, 0x2, v180                      // Bias address scaled by BPE
v_add_u32 v181, 1024, v180                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v179, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v199, v179, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v184, v4, s52
v_lshlrev_b32 v184, 0x2, v184                      // Bias address scaled by BPE
v_add_u32 v185, 1024, v184                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v183, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v183, v199, v183, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v188, v4, s52
v_lshlrev_b32 v188, 0x2, v188                      // Bias address scaled by BPE
v_add_u32 v189, 1024, v188                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v187, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v187, v199, v187, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v192, v4, s52
v_lshlrev_b32 v192, 0x2, v192                      // Bias address scaled by BPE
v_add_u32 v193, 1024, v192                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v191, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v191, v199, v191, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v196, v4, s52
v_lshlrev_b32 v196, 0x2, v196                      // Bias address scaled by BPE
v_add_u32 v197, 1024, v196                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v195, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v195, v199, v195, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+17], acc1           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+23], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+29], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+35], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+41], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+47], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+53], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+59], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+65], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+71], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+77], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+83], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+89], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+95], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+101], acc15         // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+105], acc16         // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+109], acc17         // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+113], acc18         // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+117], acc19         // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+121], acc20         // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+125], acc21         // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+129], acc22         // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+133], acc23         // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+137], acc24         // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+142], acc25         // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+146], acc26         // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+150], acc27         // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+154], acc28         // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+158], acc29         // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+162], acc30         // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+166], acc31         // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+170], acc32         // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+174], acc33         // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+178], acc34         // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+182], acc35         // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+186], acc36         // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+190], acc37         // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+194], acc38         // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+198], acc39         // copy acc to vreg[39]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 0, 1), (0, 0, 0, 2), (0, 0, 0, 3), (0, 1, 0, 0), (0, 1, 0, 1), (0, 1, 0, 2), (0, 1, 0, 3), (0, 2, 0, 0), (0, 2, 0, 1), (0, 2, 0, 2), (0, 2, 0, 3), (0, 3, 0, 0), (0, 3, 0, 1), (0, 3, 0, 2), (0, 3, 0, 3), (0, 0, 1, 0), (0, 0, 1, 1), (0, 0, 1, 2), (0, 0, 1, 3), (0, 1, 1, 0), (0, 1, 1, 1), (0, 1, 1, 2), (0, 1, 1, 3), (0, 2, 1, 0), (0, 2, 1, 1), (0, 2, 1, 2), (0, 2, 1, 3), (0, 3, 1, 0), (0, 3, 1, 1), (0, 3, 1, 2), (0, 3, 1, 3), (0, 0, 2, 0), (0, 0, 2, 1), (0, 0, 2, 2), (0, 0, 2, 3), (0, 1, 2, 0), (0, 1, 2, 1), (0, 1, 2, 2), (0, 1, 2, 3)] */
v_mul_f32 v[vgprValuC+11], s[sgprAlpha], v[vgprValuC+11] // *= alpha
v_mul_f32 v[vgprValuC+17], s[sgprAlpha], v[vgprValuC+17] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+121], s[sgprAlpha], v[vgprValuC+121] // *= alpha
v_mul_f32 v[vgprValuC+125], s[sgprAlpha], v[vgprValuC+125] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+133], s[sgprAlpha], v[vgprValuC+133] // *= alpha
v_mul_f32 v[vgprValuC+137], s[sgprAlpha], v[vgprValuC+137] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+146], s[sgprAlpha], v[vgprValuC+146] // *= alpha
v_mul_f32 v[vgprValuC+150], s[sgprAlpha], v[vgprValuC+150] // *= alpha
v_mul_f32 v[vgprValuC+154], s[sgprAlpha], v[vgprValuC+154] // *= alpha
v_mul_f32 v[vgprValuC+158], s[sgprAlpha], v[vgprValuC+158] // *= alpha
v_mul_f32 v[vgprValuC+162], s[sgprAlpha], v[vgprValuC+162] // *= alpha
v_mul_f32 v[vgprValuC+166], s[sgprAlpha], v[vgprValuC+166] // *= alpha
v_mul_f32 v[vgprValuC+170], s[sgprAlpha], v[vgprValuC+170] // *= alpha
v_mul_f32 v[vgprValuC+174], s[sgprAlpha], v[vgprValuC+174] // *= alpha
v_mul_f32 v[vgprValuC+178], s[sgprAlpha], v[vgprValuC+178] // *= alpha
v_mul_f32 v[vgprValuC+182], s[sgprAlpha], v[vgprValuC+182] // *= alpha
v_mul_f32 v[vgprValuC+186], s[sgprAlpha], v[vgprValuC+186] // *= alpha
v_mul_f32 v[vgprValuC+190], s[sgprAlpha], v[vgprValuC+190] // *= alpha
v_mul_f32 v[vgprValuC+194], s[sgprAlpha], v[vgprValuC+194] // *= alpha
v_mul_f32 v[vgprValuC+198], s[sgprAlpha], v[vgprValuC+198] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+11], v10, v[vgprValuC+11]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+11], v9, v[vgprValuC+11]     // C += bias
v_cvt_f16_f32 v11, v[vgprValuC+11]                 // convert C to fp16
buffer_store_short v11, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+17], v16, v[vgprValuC+17]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+17], v15, v[vgprValuC+17]    // C += bias
v_cvt_f16_f32 v17, v[vgprValuC+17]                 // convert C to fp16
buffer_store_short v17, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+23], v22, v[vgprValuC+23]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+23], v21, v[vgprValuC+23]    // C += bias
v_cvt_f16_f32 v23, v[vgprValuC+23]                 // convert C to fp16
buffer_store_short v23, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+29], v28, v[vgprValuC+29]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+29], v27, v[vgprValuC+29]    // C += bias
v_cvt_f16_f32 v29, v[vgprValuC+29]                 // convert C to fp16
buffer_store_short v29, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+35], v34, v[vgprValuC+35]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+35], v33, v[vgprValuC+35]    // C += bias
v_cvt_f16_f32 v35, v[vgprValuC+35]                 // convert C to fp16
buffer_store_short v35, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+41], v40, v[vgprValuC+41]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+41], v39, v[vgprValuC+41]    // C += bias
v_cvt_f16_f32 v41, v[vgprValuC+41]                 // convert C to fp16
buffer_store_short v41, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+53], v52, v[vgprValuC+53]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+53], v51, v[vgprValuC+53]    // C += bias
v_cvt_f16_f32 v53, v[vgprValuC+53]                 // convert C to fp16
buffer_store_short v53, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+59], v58, v[vgprValuC+59]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+59], v57, v[vgprValuC+59]    // C += bias
v_cvt_f16_f32 v59, v[vgprValuC+59]                 // convert C to fp16
buffer_store_short v59, v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+65], v64, v[vgprValuC+65]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+65], v63, v[vgprValuC+65]    // C += bias
v_cvt_f16_f32 v65, v[vgprValuC+65]                 // convert C to fp16
buffer_store_short v65, v60, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+71], v70, v[vgprValuC+71]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+71], v69, v[vgprValuC+71]    // C += bias
v_cvt_f16_f32 v71, v[vgprValuC+71]                 // convert C to fp16
buffer_store_short v71, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+77], v76, v[vgprValuC+77]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+77], v75, v[vgprValuC+77]    // C += bias
v_cvt_f16_f32 v77, v[vgprValuC+77]                 // convert C to fp16
buffer_store_short v77, v72, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+83], v82, v[vgprValuC+83]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+83], v81, v[vgprValuC+83]    // C += bias
v_cvt_f16_f32 v83, v[vgprValuC+83]                 // convert C to fp16
buffer_store_short v83, v78, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+89], v88, v[vgprValuC+89]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+89], v87, v[vgprValuC+89]    // C += bias
v_cvt_f16_f32 v89, v[vgprValuC+89]                 // convert C to fp16
buffer_store_short v89, v84, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+95], v94, v[vgprValuC+95]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+95], v93, v[vgprValuC+95]    // C += bias
v_cvt_f16_f32 v95, v[vgprValuC+95]                 // convert C to fp16
buffer_store_short v95, v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+101], v100, v[vgprValuC+101] // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+101], v99, v[vgprValuC+101]  // C += bias
v_cvt_f16_f32 v101, v[vgprValuC+101]               // convert C to fp16
buffer_store_short v101, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+105], v10, v[vgprValuC+105]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+105], v9, v[vgprValuC+105]   // C += bias
v_cvt_f16_f32 v105, v[vgprValuC+105]               // convert C to fp16
buffer_store_short v105, v102, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+109], v16, v[vgprValuC+109]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+109], v15, v[vgprValuC+109]  // C += bias
v_cvt_f16_f32 v109, v[vgprValuC+109]               // convert C to fp16
buffer_store_short v109, v106, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+113], v22, v[vgprValuC+113]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+113], v21, v[vgprValuC+113]  // C += bias
v_cvt_f16_f32 v113, v[vgprValuC+113]               // convert C to fp16
buffer_store_short v113, v110, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+117], v28, v[vgprValuC+117]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+117], v27, v[vgprValuC+117]  // C += bias
v_cvt_f16_f32 v117, v[vgprValuC+117]               // convert C to fp16
buffer_store_short v117, v114, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+121], v34, v[vgprValuC+121]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+121], v33, v[vgprValuC+121]  // C += bias
v_cvt_f16_f32 v121, v[vgprValuC+121]               // convert C to fp16
buffer_store_short v121, v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+125], v40, v[vgprValuC+125]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+125], v39, v[vgprValuC+125]  // C += bias
v_cvt_f16_f32 v125, v[vgprValuC+125]               // convert C to fp16
buffer_store_short v125, v122, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+129], v46, v[vgprValuC+129]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+129], v45, v[vgprValuC+129]  // C += bias
v_cvt_f16_f32 v129, v[vgprValuC+129]               // convert C to fp16
buffer_store_short v129, v126, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+133], v52, v[vgprValuC+133]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+133], v51, v[vgprValuC+133]  // C += bias
v_cvt_f16_f32 v133, v[vgprValuC+133]               // convert C to fp16
buffer_store_short v133, v130, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+137], v58, v[vgprValuC+137]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+137], v57, v[vgprValuC+137]  // C += bias
v_cvt_f16_f32 v137, v[vgprValuC+137]               // convert C to fp16
buffer_store_short v137, v134, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+142], v64, v[vgprValuC+142]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+142], v63, v[vgprValuC+142]  // C += bias
v_cvt_f16_f32 v142, v[vgprValuC+142]               // convert C to fp16
buffer_store_short v142, v138, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+146], v70, v[vgprValuC+146]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+146], v69, v[vgprValuC+146]  // C += bias
v_cvt_f16_f32 v146, v[vgprValuC+146]               // convert C to fp16
buffer_store_short v146, v143, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+150], v76, v[vgprValuC+150]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+150], v75, v[vgprValuC+150]  // C += bias
v_cvt_f16_f32 v150, v[vgprValuC+150]               // convert C to fp16
buffer_store_short v150, v147, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+154], v82, v[vgprValuC+154]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+154], v81, v[vgprValuC+154]  // C += bias
v_cvt_f16_f32 v154, v[vgprValuC+154]               // convert C to fp16
buffer_store_short v154, v151, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+158], v88, v[vgprValuC+158]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+158], v87, v[vgprValuC+158]  // C += bias
v_cvt_f16_f32 v158, v[vgprValuC+158]               // convert C to fp16
buffer_store_short v158, v155, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+162], v94, v[vgprValuC+162]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+162], v93, v[vgprValuC+162]  // C += bias
v_cvt_f16_f32 v162, v[vgprValuC+162]               // convert C to fp16
buffer_store_short v162, v159, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+166], v100, v[vgprValuC+166] // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+166], v99, v[vgprValuC+166]  // C += bias
v_cvt_f16_f32 v166, v[vgprValuC+166]               // convert C to fp16
buffer_store_short v166, v163, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+170], v10, v[vgprValuC+170]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+170], v9, v[vgprValuC+170]   // C += bias
v_cvt_f16_f32 v170, v[vgprValuC+170]               // convert C to fp16
buffer_store_short v170, v167, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+174], v16, v[vgprValuC+174]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+174], v15, v[vgprValuC+174]  // C += bias
v_cvt_f16_f32 v174, v[vgprValuC+174]               // convert C to fp16
buffer_store_short v174, v171, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+178], v22, v[vgprValuC+178]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+178], v21, v[vgprValuC+178]  // C += bias
v_cvt_f16_f32 v178, v[vgprValuC+178]               // convert C to fp16
buffer_store_short v178, v175, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+182], v28, v[vgprValuC+182]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+182], v27, v[vgprValuC+182]  // C += bias
v_cvt_f16_f32 v182, v[vgprValuC+182]               // convert C to fp16
buffer_store_short v182, v179, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+186], v34, v[vgprValuC+186]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+186], v33, v[vgprValuC+186]  // C += bias
v_cvt_f16_f32 v186, v[vgprValuC+186]               // convert C to fp16
buffer_store_short v186, v183, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+190], v40, v[vgprValuC+190]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+190], v39, v[vgprValuC+190]  // C += bias
v_cvt_f16_f32 v190, v[vgprValuC+190]               // convert C to fp16
buffer_store_short v190, v187, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+194], v46, v[vgprValuC+194]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+194], v45, v[vgprValuC+194]  // C += bias
v_cvt_f16_f32 v194, v[vgprValuC+194]               // convert C to fp16
buffer_store_short v194, v191, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+198], v52, v[vgprValuC+198]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+198], v51, v[vgprValuC+198]  // C += bias
v_cvt_f16_f32 v198, v[vgprValuC+198]               // convert C to fp16
buffer_store_short v198, v195, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,2,2,0:vw1); (0,2,2,1:vw1); (0,2,2,2:vw1); (0,2,2,3:vw1); (0,3,2,0:vw1); (0,3,2,1:vw1); (0,3,2,2:vw1); (0,3,2,3:vw1); (0,0,3,0:vw1); (0,0,3,1:vw1); (0,0,3,2:vw1); (0,0,3,3:vw1); (0,1,3,0:vw1); (0,1,3,1:vw1); (0,1,3,2:vw1); (0,1,3,3:vw1); (0,2,3,0:vw1); (0,2,3,1:vw1); (0,2,3,2:vw1); (0,2,3,3:vw1); (0,3,3,0:vw1); (0,3,3,1:vw1); (0,3,3,2:vw1); (0,3,3,3:vw1); (0,0,4,0:vw1); (0,0,4,1:vw1); (0,0,4,2:vw1); (0,0,4,3:vw1); (0,1,4,0:vw1); (0,1,4,1:vw1); (0,1,4,2:vw1); (0,1,4,3:vw1); (0,2,4,0:vw1); (0,2,4,1:vw1); (0,2,4,2:vw1); (0,2,4,3:vw1); (0,3,4,0:vw1); (0,3,4,1:vw1); (0,3,4,2:vw1); (0,3,4,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v199, BufferOOB
/* (d1,vc1,d0,vc0)=(0,2,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v4, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b32 v9, v7 offset:0                        // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v10, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v199, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v13, v4, s52
v_lshlrev_b32 v13, 0x2, v13                        // Bias address scaled by BPE
ds_read_b32 v15, v13 offset:0                      // load Bias
v_add_u32 v14, 1024, v13                           // add ScaleAlphaVec offset (3)
ds_read_b32 v16, v14 offset:0                      // load scaleAlpha
v_add_lshl_u32 v12, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v199, v12, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v19, v4, s52
v_lshlrev_b32 v19, 0x2, v19                        // Bias address scaled by BPE
ds_read_b32 v21, v19 offset:0                      // load Bias
v_add_u32 v20, 1024, v19                           // add ScaleAlphaVec offset (3)
ds_read_b32 v22, v20 offset:0                      // load scaleAlpha
v_add_lshl_u32 v18, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v199, v18, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v25, v4, s52
v_lshlrev_b32 v25, 0x2, v25                        // Bias address scaled by BPE
ds_read_b32 v27, v25 offset:0                      // load Bias
v_add_u32 v26, 1024, v25                           // add ScaleAlphaVec offset (3)
ds_read_b32 v28, v26 offset:0                      // load scaleAlpha
v_add_lshl_u32 v24, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v199, v24, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v31, v4, s52
v_lshlrev_b32 v31, 0x2, v31                        // Bias address scaled by BPE
ds_read_b32 v33, v31 offset:0                      // load Bias
v_add_u32 v32, 1024, v31                           // add ScaleAlphaVec offset (3)
ds_read_b32 v34, v32 offset:0                      // load scaleAlpha
v_add_lshl_u32 v30, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v199, v30, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v37, v4, s52
v_lshlrev_b32 v37, 0x2, v37                        // Bias address scaled by BPE
ds_read_b32 v39, v37 offset:0                      // load Bias
v_add_u32 v38, 1024, v37                           // add ScaleAlphaVec offset (3)
ds_read_b32 v40, v38 offset:0                      // load scaleAlpha
v_add_lshl_u32 v36, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v199, v36, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v43, v4, s52
v_lshlrev_b32 v43, 0x2, v43                        // Bias address scaled by BPE
ds_read_b32 v45, v43 offset:0                      // load Bias
v_add_u32 v44, 1024, v43                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v44 offset:0                      // load scaleAlpha
v_add_lshl_u32 v42, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v199, v42, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v4, s52
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
ds_read_b32 v51, v49 offset:0                      // load Bias
v_add_u32 v50, 1024, v49                           // add ScaleAlphaVec offset (3)
ds_read_b32 v52, v50 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v199, v48, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v55, v0, s52
v_lshlrev_b32 v55, 0x2, v55                        // Bias address scaled by BPE
ds_read_b32 v57, v55 offset:0                      // load Bias
v_add_u32 v56, 1024, v55                           // add ScaleAlphaVec offset (3)
ds_read_b32 v58, v56 offset:0                      // load scaleAlpha
v_add_lshl_u32 v54, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v199, v54, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v61, v4, s52
v_lshlrev_b32 v61, 0x2, v61                        // Bias address scaled by BPE
ds_read_b32 v63, v61 offset:0                      // load Bias
v_add_u32 v62, 1024, v61                           // add ScaleAlphaVec offset (3)
ds_read_b32 v64, v62 offset:0                      // load scaleAlpha
v_add_lshl_u32 v60, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v60, v199, v60, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s52
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
ds_read_b32 v69, v67 offset:0                      // load Bias
v_add_u32 v68, 1024, v67                           // add ScaleAlphaVec offset (3)
ds_read_b32 v70, v68 offset:0                      // load scaleAlpha
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v199, v66, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v73, v4, s52
v_lshlrev_b32 v73, 0x2, v73                        // Bias address scaled by BPE
ds_read_b32 v75, v73 offset:0                      // load Bias
v_add_u32 v74, 1024, v73                           // add ScaleAlphaVec offset (3)
ds_read_b32 v76, v74 offset:0                      // load scaleAlpha
v_add_lshl_u32 v72, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v72, v199, v72, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v79, v4, s52
v_lshlrev_b32 v79, 0x2, v79                        // Bias address scaled by BPE
ds_read_b32 v81, v79 offset:0                      // load Bias
v_add_u32 v80, 1024, v79                           // add ScaleAlphaVec offset (3)
ds_read_b32 v82, v80 offset:0                      // load scaleAlpha
v_add_lshl_u32 v78, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v78, v199, v78, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v85, v4, s52
v_lshlrev_b32 v85, 0x2, v85                        // Bias address scaled by BPE
ds_read_b32 v87, v85 offset:0                      // load Bias
v_add_u32 v86, 1024, v85                           // add ScaleAlphaVec offset (3)
ds_read_b32 v88, v86 offset:0                      // load scaleAlpha
v_add_lshl_u32 v84, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v199, v84, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v4, s52
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
ds_read_b32 v93, v91 offset:0                      // load Bias
v_add_u32 v92, 1024, v91                           // add ScaleAlphaVec offset (3)
ds_read_b32 v94, v92 offset:0                      // load scaleAlpha
v_add_lshl_u32 v90, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v199, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s52
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
ds_read_b32 v99, v97 offset:0                      // load Bias
v_add_u32 v98, 1024, v97                           // add ScaleAlphaVec offset (3)
ds_read_b32 v100, v98 offset:0                     // load scaleAlpha
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v199, v96, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v103, v4, s52
v_lshlrev_b32 v103, 0x2, v103                      // Bias address scaled by BPE
v_add_u32 v104, 1024, v103                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v102, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v102, v199, v102, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v107, v4, s52
v_lshlrev_b32 v107, 0x2, v107                      // Bias address scaled by BPE
v_add_u32 v108, 1024, v107                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v106, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v199, v106, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v111, v4, s52
v_lshlrev_b32 v111, 0x2, v111                      // Bias address scaled by BPE
v_add_u32 v112, 1024, v111                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v110, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v110, v199, v110, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v115, v4, s52
v_lshlrev_b32 v115, 0x2, v115                      // Bias address scaled by BPE
v_add_u32 v116, 1024, v115                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v114, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v114, v199, v114, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v4, s52
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v120, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v199, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v123, v4, s52
v_lshlrev_b32 v123, 0x2, v123                      // Bias address scaled by BPE
v_add_u32 v124, 1024, v123                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v122, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v122, v199, v122, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v127, v4, s52
v_lshlrev_b32 v127, 0x2, v127                      // Bias address scaled by BPE
v_add_u32 v128, 1024, v127                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v126, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v126, v199, v126, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v131, v4, s52
v_lshlrev_b32 v131, 0x2, v131                      // Bias address scaled by BPE
v_add_u32 v132, 1024, v131                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v130, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v130, v199, v130, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v135, v0, s52
v_lshlrev_b32 v135, 0x2, v135                      // Bias address scaled by BPE
v_add_u32 v136, 1024, v135                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v134, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v134, v199, v134, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v140, v4, s52
v_lshlrev_b32 v140, 0x2, v140                      // Bias address scaled by BPE
v_add_u32 v141, 1024, v140                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v138, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v199, v138, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v144, v4, s52
v_lshlrev_b32 v144, 0x2, v144                      // Bias address scaled by BPE
v_add_u32 v145, 1024, v144                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v143, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v143, v199, v143, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v148, v4, s52
v_lshlrev_b32 v148, 0x2, v148                      // Bias address scaled by BPE
v_add_u32 v149, 1024, v148                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v147, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v147, v199, v147, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v152, v4, s52
v_lshlrev_b32 v152, 0x2, v152                      // Bias address scaled by BPE
v_add_u32 v153, 1024, v152                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v151, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v151, v199, v151, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v156, v4, s52
v_lshlrev_b32 v156, 0x2, v156                      // Bias address scaled by BPE
v_add_u32 v157, 1024, v156                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v155, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v155, v199, v155, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v160, v4, s52
v_lshlrev_b32 v160, 0x2, v160                      // Bias address scaled by BPE
v_add_u32 v161, 1024, v160                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v159, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v199, v159, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v164, v4, s52
v_lshlrev_b32 v164, 0x2, v164                      // Bias address scaled by BPE
v_add_u32 v165, 1024, v164                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v163, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v163, v199, v163, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v168, v4, s52
v_lshlrev_b32 v168, 0x2, v168                      // Bias address scaled by BPE
v_add_u32 v169, 1024, v168                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v167, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v167, v199, v167, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v172, v4, s52
v_lshlrev_b32 v172, 0x2, v172                      // Bias address scaled by BPE
v_add_u32 v173, 1024, v172                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v171, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v171, v199, v171, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v176, v4, s52
v_lshlrev_b32 v176, 0x2, v176                      // Bias address scaled by BPE
v_add_u32 v177, 1024, v176                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v175, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v175, v199, v175, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v180, v4, s52
v_lshlrev_b32 v180, 0x2, v180                      // Bias address scaled by BPE
v_add_u32 v181, 1024, v180                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v179, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v199, v179, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v184, v4, s52
v_lshlrev_b32 v184, 0x2, v184                      // Bias address scaled by BPE
v_add_u32 v185, 1024, v184                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v183, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v183, v199, v183, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v188, v4, s52
v_lshlrev_b32 v188, 0x2, v188                      // Bias address scaled by BPE
v_add_u32 v189, 1024, v188                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v187, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v187, v199, v187, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v192, v4, s52
v_lshlrev_b32 v192, 0x2, v192                      // Bias address scaled by BPE
v_add_u32 v193, 1024, v192                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v191, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v191, v199, v191, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v196, v4, s52
v_lshlrev_b32 v196, 0x2, v196                      // Bias address scaled by BPE
v_add_u32 v197, 1024, v196                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v195, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v195, v199, v195, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc40          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+17], acc41          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+23], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+29], acc43          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+35], acc44          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+41], acc45          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+47], acc46          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+53], acc47          // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+59], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+65], acc49          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+71], acc50          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+77], acc51          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+83], acc52          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+89], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+95], acc54          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+101], acc55         // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+105], acc56         // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+109], acc57         // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+113], acc58         // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+117], acc59         // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+121], acc60         // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+125], acc61         // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+129], acc62         // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+133], acc63         // copy acc to vreg[63]
v_accvgpr_read_b32 v[vgprValuC+137], acc64         // copy acc to vreg[64]
v_accvgpr_read_b32 v[vgprValuC+142], acc65         // copy acc to vreg[65]
v_accvgpr_read_b32 v[vgprValuC+146], acc66         // copy acc to vreg[66]
v_accvgpr_read_b32 v[vgprValuC+150], acc67         // copy acc to vreg[67]
v_accvgpr_read_b32 v[vgprValuC+154], acc68         // copy acc to vreg[68]
v_accvgpr_read_b32 v[vgprValuC+158], acc69         // copy acc to vreg[69]
v_accvgpr_read_b32 v[vgprValuC+162], acc70         // copy acc to vreg[70]
v_accvgpr_read_b32 v[vgprValuC+166], acc71         // copy acc to vreg[71]
v_accvgpr_read_b32 v[vgprValuC+170], acc72         // copy acc to vreg[72]
v_accvgpr_read_b32 v[vgprValuC+174], acc73         // copy acc to vreg[73]
v_accvgpr_read_b32 v[vgprValuC+178], acc74         // copy acc to vreg[74]
v_accvgpr_read_b32 v[vgprValuC+182], acc75         // copy acc to vreg[75]
v_accvgpr_read_b32 v[vgprValuC+186], acc76         // copy acc to vreg[76]
v_accvgpr_read_b32 v[vgprValuC+190], acc77         // copy acc to vreg[77]
v_accvgpr_read_b32 v[vgprValuC+194], acc78         // copy acc to vreg[78]
v_accvgpr_read_b32 v[vgprValuC+198], acc79         // copy acc to vreg[79]

/* rC *= alpha batchElements=[(0, 2, 2, 0), (0, 2, 2, 1), (0, 2, 2, 2), (0, 2, 2, 3), (0, 3, 2, 0), (0, 3, 2, 1), (0, 3, 2, 2), (0, 3, 2, 3), (0, 0, 3, 0), (0, 0, 3, 1), (0, 0, 3, 2), (0, 0, 3, 3), (0, 1, 3, 0), (0, 1, 3, 1), (0, 1, 3, 2), (0, 1, 3, 3), (0, 2, 3, 0), (0, 2, 3, 1), (0, 2, 3, 2), (0, 2, 3, 3), (0, 3, 3, 0), (0, 3, 3, 1), (0, 3, 3, 2), (0, 3, 3, 3), (0, 0, 4, 0), (0, 0, 4, 1), (0, 0, 4, 2), (0, 0, 4, 3), (0, 1, 4, 0), (0, 1, 4, 1), (0, 1, 4, 2), (0, 1, 4, 3), (0, 2, 4, 0), (0, 2, 4, 1), (0, 2, 4, 2), (0, 2, 4, 3), (0, 3, 4, 0), (0, 3, 4, 1), (0, 3, 4, 2), (0, 3, 4, 3)] */
v_mul_f32 v[vgprValuC+11], s[sgprAlpha], v[vgprValuC+11] // *= alpha
v_mul_f32 v[vgprValuC+17], s[sgprAlpha], v[vgprValuC+17] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+121], s[sgprAlpha], v[vgprValuC+121] // *= alpha
v_mul_f32 v[vgprValuC+125], s[sgprAlpha], v[vgprValuC+125] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+133], s[sgprAlpha], v[vgprValuC+133] // *= alpha
v_mul_f32 v[vgprValuC+137], s[sgprAlpha], v[vgprValuC+137] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+146], s[sgprAlpha], v[vgprValuC+146] // *= alpha
v_mul_f32 v[vgprValuC+150], s[sgprAlpha], v[vgprValuC+150] // *= alpha
v_mul_f32 v[vgprValuC+154], s[sgprAlpha], v[vgprValuC+154] // *= alpha
v_mul_f32 v[vgprValuC+158], s[sgprAlpha], v[vgprValuC+158] // *= alpha
v_mul_f32 v[vgprValuC+162], s[sgprAlpha], v[vgprValuC+162] // *= alpha
v_mul_f32 v[vgprValuC+166], s[sgprAlpha], v[vgprValuC+166] // *= alpha
v_mul_f32 v[vgprValuC+170], s[sgprAlpha], v[vgprValuC+170] // *= alpha
v_mul_f32 v[vgprValuC+174], s[sgprAlpha], v[vgprValuC+174] // *= alpha
v_mul_f32 v[vgprValuC+178], s[sgprAlpha], v[vgprValuC+178] // *= alpha
v_mul_f32 v[vgprValuC+182], s[sgprAlpha], v[vgprValuC+182] // *= alpha
v_mul_f32 v[vgprValuC+186], s[sgprAlpha], v[vgprValuC+186] // *= alpha
v_mul_f32 v[vgprValuC+190], s[sgprAlpha], v[vgprValuC+190] // *= alpha
v_mul_f32 v[vgprValuC+194], s[sgprAlpha], v[vgprValuC+194] // *= alpha
v_mul_f32 v[vgprValuC+198], s[sgprAlpha], v[vgprValuC+198] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+11], v10, v[vgprValuC+11]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+11], v9, v[vgprValuC+11]     // C += bias
v_cvt_f16_f32 v11, v[vgprValuC+11]                 // convert C to fp16
buffer_store_short v11, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+17], v16, v[vgprValuC+17]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+17], v15, v[vgprValuC+17]    // C += bias
v_cvt_f16_f32 v17, v[vgprValuC+17]                 // convert C to fp16
buffer_store_short v17, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+23], v22, v[vgprValuC+23]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+23], v21, v[vgprValuC+23]    // C += bias
v_cvt_f16_f32 v23, v[vgprValuC+23]                 // convert C to fp16
buffer_store_short v23, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+29], v28, v[vgprValuC+29]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+29], v27, v[vgprValuC+29]    // C += bias
v_cvt_f16_f32 v29, v[vgprValuC+29]                 // convert C to fp16
buffer_store_short v29, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+35], v34, v[vgprValuC+35]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+35], v33, v[vgprValuC+35]    // C += bias
v_cvt_f16_f32 v35, v[vgprValuC+35]                 // convert C to fp16
buffer_store_short v35, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+41], v40, v[vgprValuC+41]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+41], v39, v[vgprValuC+41]    // C += bias
v_cvt_f16_f32 v41, v[vgprValuC+41]                 // convert C to fp16
buffer_store_short v41, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+53], v52, v[vgprValuC+53]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+53], v51, v[vgprValuC+53]    // C += bias
v_cvt_f16_f32 v53, v[vgprValuC+53]                 // convert C to fp16
buffer_store_short v53, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+59], v58, v[vgprValuC+59]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+59], v57, v[vgprValuC+59]    // C += bias
v_cvt_f16_f32 v59, v[vgprValuC+59]                 // convert C to fp16
buffer_store_short v59, v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+65], v64, v[vgprValuC+65]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+65], v63, v[vgprValuC+65]    // C += bias
v_cvt_f16_f32 v65, v[vgprValuC+65]                 // convert C to fp16
buffer_store_short v65, v60, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+71], v70, v[vgprValuC+71]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+71], v69, v[vgprValuC+71]    // C += bias
v_cvt_f16_f32 v71, v[vgprValuC+71]                 // convert C to fp16
buffer_store_short v71, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+77], v76, v[vgprValuC+77]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+77], v75, v[vgprValuC+77]    // C += bias
v_cvt_f16_f32 v77, v[vgprValuC+77]                 // convert C to fp16
buffer_store_short v77, v72, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+83], v82, v[vgprValuC+83]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+83], v81, v[vgprValuC+83]    // C += bias
v_cvt_f16_f32 v83, v[vgprValuC+83]                 // convert C to fp16
buffer_store_short v83, v78, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+89], v88, v[vgprValuC+89]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+89], v87, v[vgprValuC+89]    // C += bias
v_cvt_f16_f32 v89, v[vgprValuC+89]                 // convert C to fp16
buffer_store_short v89, v84, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+95], v94, v[vgprValuC+95]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+95], v93, v[vgprValuC+95]    // C += bias
v_cvt_f16_f32 v95, v[vgprValuC+95]                 // convert C to fp16
buffer_store_short v95, v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+101], v100, v[vgprValuC+101] // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+101], v99, v[vgprValuC+101]  // C += bias
v_cvt_f16_f32 v101, v[vgprValuC+101]               // convert C to fp16
buffer_store_short v101, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+105], v10, v[vgprValuC+105]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+105], v9, v[vgprValuC+105]   // C += bias
v_cvt_f16_f32 v105, v[vgprValuC+105]               // convert C to fp16
buffer_store_short v105, v102, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+109], v16, v[vgprValuC+109]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+109], v15, v[vgprValuC+109]  // C += bias
v_cvt_f16_f32 v109, v[vgprValuC+109]               // convert C to fp16
buffer_store_short v109, v106, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+113], v22, v[vgprValuC+113]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+113], v21, v[vgprValuC+113]  // C += bias
v_cvt_f16_f32 v113, v[vgprValuC+113]               // convert C to fp16
buffer_store_short v113, v110, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+117], v28, v[vgprValuC+117]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+117], v27, v[vgprValuC+117]  // C += bias
v_cvt_f16_f32 v117, v[vgprValuC+117]               // convert C to fp16
buffer_store_short v117, v114, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+121], v34, v[vgprValuC+121]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+121], v33, v[vgprValuC+121]  // C += bias
v_cvt_f16_f32 v121, v[vgprValuC+121]               // convert C to fp16
buffer_store_short v121, v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+125], v40, v[vgprValuC+125]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+125], v39, v[vgprValuC+125]  // C += bias
v_cvt_f16_f32 v125, v[vgprValuC+125]               // convert C to fp16
buffer_store_short v125, v122, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+129], v46, v[vgprValuC+129]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+129], v45, v[vgprValuC+129]  // C += bias
v_cvt_f16_f32 v129, v[vgprValuC+129]               // convert C to fp16
buffer_store_short v129, v126, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+133], v52, v[vgprValuC+133]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+133], v51, v[vgprValuC+133]  // C += bias
v_cvt_f16_f32 v133, v[vgprValuC+133]               // convert C to fp16
buffer_store_short v133, v130, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+137], v58, v[vgprValuC+137]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+137], v57, v[vgprValuC+137]  // C += bias
v_cvt_f16_f32 v137, v[vgprValuC+137]               // convert C to fp16
buffer_store_short v137, v134, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+142], v64, v[vgprValuC+142]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+142], v63, v[vgprValuC+142]  // C += bias
v_cvt_f16_f32 v142, v[vgprValuC+142]               // convert C to fp16
buffer_store_short v142, v138, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+146], v70, v[vgprValuC+146]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+146], v69, v[vgprValuC+146]  // C += bias
v_cvt_f16_f32 v146, v[vgprValuC+146]               // convert C to fp16
buffer_store_short v146, v143, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+150], v76, v[vgprValuC+150]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+150], v75, v[vgprValuC+150]  // C += bias
v_cvt_f16_f32 v150, v[vgprValuC+150]               // convert C to fp16
buffer_store_short v150, v147, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+154], v82, v[vgprValuC+154]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+154], v81, v[vgprValuC+154]  // C += bias
v_cvt_f16_f32 v154, v[vgprValuC+154]               // convert C to fp16
buffer_store_short v154, v151, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+158], v88, v[vgprValuC+158]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+158], v87, v[vgprValuC+158]  // C += bias
v_cvt_f16_f32 v158, v[vgprValuC+158]               // convert C to fp16
buffer_store_short v158, v155, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+162], v94, v[vgprValuC+162]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+162], v93, v[vgprValuC+162]  // C += bias
v_cvt_f16_f32 v162, v[vgprValuC+162]               // convert C to fp16
buffer_store_short v162, v159, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+166], v100, v[vgprValuC+166] // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+166], v99, v[vgprValuC+166]  // C += bias
v_cvt_f16_f32 v166, v[vgprValuC+166]               // convert C to fp16
buffer_store_short v166, v163, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+170], v10, v[vgprValuC+170]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+170], v9, v[vgprValuC+170]   // C += bias
v_cvt_f16_f32 v170, v[vgprValuC+170]               // convert C to fp16
buffer_store_short v170, v167, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+174], v16, v[vgprValuC+174]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+174], v15, v[vgprValuC+174]  // C += bias
v_cvt_f16_f32 v174, v[vgprValuC+174]               // convert C to fp16
buffer_store_short v174, v171, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+178], v22, v[vgprValuC+178]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+178], v21, v[vgprValuC+178]  // C += bias
v_cvt_f16_f32 v178, v[vgprValuC+178]               // convert C to fp16
buffer_store_short v178, v175, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+182], v28, v[vgprValuC+182]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+182], v27, v[vgprValuC+182]  // C += bias
v_cvt_f16_f32 v182, v[vgprValuC+182]               // convert C to fp16
buffer_store_short v182, v179, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+186], v34, v[vgprValuC+186]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+186], v33, v[vgprValuC+186]  // C += bias
v_cvt_f16_f32 v186, v[vgprValuC+186]               // convert C to fp16
buffer_store_short v186, v183, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+190], v40, v[vgprValuC+190]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+190], v39, v[vgprValuC+190]  // C += bias
v_cvt_f16_f32 v190, v[vgprValuC+190]               // convert C to fp16
buffer_store_short v190, v187, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+194], v46, v[vgprValuC+194]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+194], v45, v[vgprValuC+194]  // C += bias
v_cvt_f16_f32 v194, v[vgprValuC+194]               // convert C to fp16
buffer_store_short v194, v191, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+198], v52, v[vgprValuC+198]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+198], v51, v[vgprValuC+198]  // C += bias
v_cvt_f16_f32 v198, v[vgprValuC+198]               // convert C to fp16
buffer_store_short v198, v195, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #2 (d1,d0,vc1,vc0) = */
/*    (0,0,5,0:vw1); (0,0,5,1:vw1); (0,0,5,2:vw1); (0,0,5,3:vw1); (0,1,5,0:vw1); (0,1,5,1:vw1); (0,1,5,2:vw1); (0,1,5,3:vw1); (0,2,5,0:vw1); (0,2,5,1:vw1); (0,2,5,2:vw1); (0,2,5,3:vw1); (0,3,5,0:vw1); (0,3,5,1:vw1); (0,3,5,2:vw1); (0,3,5,3:vw1); (0,0,6,0:vw1); (0,0,6,1:vw1); (0,0,6,2:vw1); (0,0,6,3:vw1); (0,1,6,0:vw1); (0,1,6,1:vw1); (0,1,6,2:vw1); (0,1,6,3:vw1); (0,2,6,0:vw1); (0,2,6,1:vw1); (0,2,6,2:vw1); (0,2,6,3:vw1); (0,3,6,0:vw1); (0,3,6,1:vw1); (0,3,6,2:vw1); (0,3,6,3:vw1); (0,0,7,0:vw1); (0,0,7,1:vw1); (0,0,7,2:vw1); (0,0,7,3:vw1); (0,1,7,0:vw1); (0,1,7,1:vw1); (0,1,7,2:vw1); (0,1,7,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v199, BufferOOB
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v0, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b32 v9, v7 offset:0                        // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v10, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v199, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v13, v4, s52
v_lshlrev_b32 v13, 0x2, v13                        // Bias address scaled by BPE
ds_read_b32 v15, v13 offset:0                      // load Bias
v_add_u32 v14, 1024, v13                           // add ScaleAlphaVec offset (3)
ds_read_b32 v16, v14 offset:0                      // load scaleAlpha
v_add_lshl_u32 v12, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v199, v12, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v19, v4, s52
v_lshlrev_b32 v19, 0x2, v19                        // Bias address scaled by BPE
ds_read_b32 v21, v19 offset:0                      // load Bias
v_add_u32 v20, 1024, v19                           // add ScaleAlphaVec offset (3)
ds_read_b32 v22, v20 offset:0                      // load scaleAlpha
v_add_lshl_u32 v18, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v199, v18, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v25, v4, s52
v_lshlrev_b32 v25, 0x2, v25                        // Bias address scaled by BPE
ds_read_b32 v27, v25 offset:0                      // load Bias
v_add_u32 v26, 1024, v25                           // add ScaleAlphaVec offset (3)
ds_read_b32 v28, v26 offset:0                      // load scaleAlpha
v_add_lshl_u32 v24, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v199, v24, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v31, v4, s52
v_lshlrev_b32 v31, 0x2, v31                        // Bias address scaled by BPE
ds_read_b32 v33, v31 offset:0                      // load Bias
v_add_u32 v32, 1024, v31                           // add ScaleAlphaVec offset (3)
ds_read_b32 v34, v32 offset:0                      // load scaleAlpha
v_add_lshl_u32 v30, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v199, v30, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v37, v4, s52
v_lshlrev_b32 v37, 0x2, v37                        // Bias address scaled by BPE
ds_read_b32 v39, v37 offset:0                      // load Bias
v_add_u32 v38, 1024, v37                           // add ScaleAlphaVec offset (3)
ds_read_b32 v40, v38 offset:0                      // load scaleAlpha
v_add_lshl_u32 v36, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v199, v36, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v43, v4, s52
v_lshlrev_b32 v43, 0x2, v43                        // Bias address scaled by BPE
ds_read_b32 v45, v43 offset:0                      // load Bias
v_add_u32 v44, 1024, v43                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v44 offset:0                      // load scaleAlpha
v_add_lshl_u32 v42, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v199, v42, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v4, s52
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
ds_read_b32 v51, v49 offset:0                      // load Bias
v_add_u32 v50, 1024, v49                           // add ScaleAlphaVec offset (3)
ds_read_b32 v52, v50 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v199, v48, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v55, v4, s52
v_lshlrev_b32 v55, 0x2, v55                        // Bias address scaled by BPE
ds_read_b32 v57, v55 offset:0                      // load Bias
v_add_u32 v56, 1024, v55                           // add ScaleAlphaVec offset (3)
ds_read_b32 v58, v56 offset:0                      // load scaleAlpha
v_add_lshl_u32 v54, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v199, v54, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v61, v4, s52
v_lshlrev_b32 v61, 0x2, v61                        // Bias address scaled by BPE
ds_read_b32 v63, v61 offset:0                      // load Bias
v_add_u32 v62, 1024, v61                           // add ScaleAlphaVec offset (3)
ds_read_b32 v64, v62 offset:0                      // load scaleAlpha
v_add_lshl_u32 v60, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v60, v199, v60, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s52
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
ds_read_b32 v69, v67 offset:0                      // load Bias
v_add_u32 v68, 1024, v67                           // add ScaleAlphaVec offset (3)
ds_read_b32 v70, v68 offset:0                      // load scaleAlpha
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v199, v66, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v73, v4, s52
v_lshlrev_b32 v73, 0x2, v73                        // Bias address scaled by BPE
ds_read_b32 v75, v73 offset:0                      // load Bias
v_add_u32 v74, 1024, v73                           // add ScaleAlphaVec offset (3)
ds_read_b32 v76, v74 offset:0                      // load scaleAlpha
v_add_lshl_u32 v72, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v72, v199, v72, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v79, v4, s52
v_lshlrev_b32 v79, 0x2, v79                        // Bias address scaled by BPE
ds_read_b32 v81, v79 offset:0                      // load Bias
v_add_u32 v80, 1024, v79                           // add ScaleAlphaVec offset (3)
ds_read_b32 v82, v80 offset:0                      // load scaleAlpha
v_add_lshl_u32 v78, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v78, v199, v78, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v85, v4, s52
v_lshlrev_b32 v85, 0x2, v85                        // Bias address scaled by BPE
ds_read_b32 v87, v85 offset:0                      // load Bias
v_add_u32 v86, 1024, v85                           // add ScaleAlphaVec offset (3)
ds_read_b32 v88, v86 offset:0                      // load scaleAlpha
v_add_lshl_u32 v84, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v199, v84, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v4, s52
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
ds_read_b32 v93, v91 offset:0                      // load Bias
v_add_u32 v92, 1024, v91                           // add ScaleAlphaVec offset (3)
ds_read_b32 v94, v92 offset:0                      // load scaleAlpha
v_add_lshl_u32 v90, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v199, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s52
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
ds_read_b32 v99, v97 offset:0                      // load Bias
v_add_u32 v98, 1024, v97                           // add ScaleAlphaVec offset (3)
ds_read_b32 v100, v98 offset:0                     // load scaleAlpha
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v199, v96, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v103, v0, s52
v_lshlrev_b32 v103, 0x2, v103                      // Bias address scaled by BPE
v_add_u32 v104, 1024, v103                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v102, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v102, v199, v102, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v107, v4, s52
v_lshlrev_b32 v107, 0x2, v107                      // Bias address scaled by BPE
v_add_u32 v108, 1024, v107                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v106, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v199, v106, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v111, v4, s52
v_lshlrev_b32 v111, 0x2, v111                      // Bias address scaled by BPE
v_add_u32 v112, 1024, v111                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v110, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v110, v199, v110, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v115, v4, s52
v_lshlrev_b32 v115, 0x2, v115                      // Bias address scaled by BPE
v_add_u32 v116, 1024, v115                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v114, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v114, v199, v114, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v4, s52
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v120, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v199, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v123, v4, s52
v_lshlrev_b32 v123, 0x2, v123                      // Bias address scaled by BPE
v_add_u32 v124, 1024, v123                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v122, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v122, v199, v122, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v127, v4, s52
v_lshlrev_b32 v127, 0x2, v127                      // Bias address scaled by BPE
v_add_u32 v128, 1024, v127                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v126, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v126, v199, v126, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v131, v4, s52
v_lshlrev_b32 v131, 0x2, v131                      // Bias address scaled by BPE
v_add_u32 v132, 1024, v131                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v130, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v130, v199, v130, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v135, v4, s52
v_lshlrev_b32 v135, 0x2, v135                      // Bias address scaled by BPE
v_add_u32 v136, 1024, v135                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v134, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v134, v199, v134, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v140, v4, s52
v_lshlrev_b32 v140, 0x2, v140                      // Bias address scaled by BPE
v_add_u32 v141, 1024, v140                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v138, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v199, v138, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v144, v4, s52
v_lshlrev_b32 v144, 0x2, v144                      // Bias address scaled by BPE
v_add_u32 v145, 1024, v144                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v143, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v143, v199, v143, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v148, v4, s52
v_lshlrev_b32 v148, 0x2, v148                      // Bias address scaled by BPE
v_add_u32 v149, 1024, v148                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v147, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v147, v199, v147, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v152, v4, s52
v_lshlrev_b32 v152, 0x2, v152                      // Bias address scaled by BPE
v_add_u32 v153, 1024, v152                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v151, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v151, v199, v151, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v156, v4, s52
v_lshlrev_b32 v156, 0x2, v156                      // Bias address scaled by BPE
v_add_u32 v157, 1024, v156                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v155, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v155, v199, v155, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v160, v4, s52
v_lshlrev_b32 v160, 0x2, v160                      // Bias address scaled by BPE
v_add_u32 v161, 1024, v160                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v159, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v199, v159, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v164, v4, s52
v_lshlrev_b32 v164, 0x2, v164                      // Bias address scaled by BPE
v_add_u32 v165, 1024, v164                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v163, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v163, v199, v163, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v168, v0, s52
v_lshlrev_b32 v168, 0x2, v168                      // Bias address scaled by BPE
v_add_u32 v169, 1024, v168                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v167, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v167, v199, v167, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v172, v4, s52
v_lshlrev_b32 v172, 0x2, v172                      // Bias address scaled by BPE
v_add_u32 v173, 1024, v172                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v171, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v171, v199, v171, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v176, v4, s52
v_lshlrev_b32 v176, 0x2, v176                      // Bias address scaled by BPE
v_add_u32 v177, 1024, v176                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v175, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v175, v199, v175, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v180, v4, s52
v_lshlrev_b32 v180, 0x2, v180                      // Bias address scaled by BPE
v_add_u32 v181, 1024, v180                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v179, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v199, v179, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v184, v4, s52
v_lshlrev_b32 v184, 0x2, v184                      // Bias address scaled by BPE
v_add_u32 v185, 1024, v184                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v183, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v183, v199, v183, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v188, v4, s52
v_lshlrev_b32 v188, 0x2, v188                      // Bias address scaled by BPE
v_add_u32 v189, 1024, v188                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v187, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v187, v199, v187, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v192, v4, s52
v_lshlrev_b32 v192, 0x2, v192                      // Bias address scaled by BPE
v_add_u32 v193, 1024, v192                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v191, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v191, v199, v191, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v196, v4, s52
v_lshlrev_b32 v196, 0x2, v196                      // Bias address scaled by BPE
v_add_u32 v197, 1024, v196                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v195, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v195, v199, v195, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc80          // copy acc to vreg[80]
v_accvgpr_read_b32 v[vgprValuC+17], acc81          // copy acc to vreg[81]
v_accvgpr_read_b32 v[vgprValuC+23], acc82          // copy acc to vreg[82]
v_accvgpr_read_b32 v[vgprValuC+29], acc83          // copy acc to vreg[83]
v_accvgpr_read_b32 v[vgprValuC+35], acc84          // copy acc to vreg[84]
v_accvgpr_read_b32 v[vgprValuC+41], acc85          // copy acc to vreg[85]
v_accvgpr_read_b32 v[vgprValuC+47], acc86          // copy acc to vreg[86]
v_accvgpr_read_b32 v[vgprValuC+53], acc87          // copy acc to vreg[87]
v_accvgpr_read_b32 v[vgprValuC+59], acc88          // copy acc to vreg[88]
v_accvgpr_read_b32 v[vgprValuC+65], acc89          // copy acc to vreg[89]
v_accvgpr_read_b32 v[vgprValuC+71], acc90          // copy acc to vreg[90]
v_accvgpr_read_b32 v[vgprValuC+77], acc91          // copy acc to vreg[91]
v_accvgpr_read_b32 v[vgprValuC+83], acc92          // copy acc to vreg[92]
v_accvgpr_read_b32 v[vgprValuC+89], acc93          // copy acc to vreg[93]
v_accvgpr_read_b32 v[vgprValuC+95], acc94          // copy acc to vreg[94]
v_accvgpr_read_b32 v[vgprValuC+101], acc95         // copy acc to vreg[95]
v_accvgpr_read_b32 v[vgprValuC+105], acc96         // copy acc to vreg[96]
v_accvgpr_read_b32 v[vgprValuC+109], acc97         // copy acc to vreg[97]
v_accvgpr_read_b32 v[vgprValuC+113], acc98         // copy acc to vreg[98]
v_accvgpr_read_b32 v[vgprValuC+117], acc99         // copy acc to vreg[99]
v_accvgpr_read_b32 v[vgprValuC+121], acc100        // copy acc to vreg[100]
v_accvgpr_read_b32 v[vgprValuC+125], acc101        // copy acc to vreg[101]
v_accvgpr_read_b32 v[vgprValuC+129], acc102        // copy acc to vreg[102]
v_accvgpr_read_b32 v[vgprValuC+133], acc103        // copy acc to vreg[103]
v_accvgpr_read_b32 v[vgprValuC+137], acc104        // copy acc to vreg[104]
v_accvgpr_read_b32 v[vgprValuC+142], acc105        // copy acc to vreg[105]
v_accvgpr_read_b32 v[vgprValuC+146], acc106        // copy acc to vreg[106]
v_accvgpr_read_b32 v[vgprValuC+150], acc107        // copy acc to vreg[107]
v_accvgpr_read_b32 v[vgprValuC+154], acc108        // copy acc to vreg[108]
v_accvgpr_read_b32 v[vgprValuC+158], acc109        // copy acc to vreg[109]
v_accvgpr_read_b32 v[vgprValuC+162], acc110        // copy acc to vreg[110]
v_accvgpr_read_b32 v[vgprValuC+166], acc111        // copy acc to vreg[111]
v_accvgpr_read_b32 v[vgprValuC+170], acc112        // copy acc to vreg[112]
v_accvgpr_read_b32 v[vgprValuC+174], acc113        // copy acc to vreg[113]
v_accvgpr_read_b32 v[vgprValuC+178], acc114        // copy acc to vreg[114]
v_accvgpr_read_b32 v[vgprValuC+182], acc115        // copy acc to vreg[115]
v_accvgpr_read_b32 v[vgprValuC+186], acc116        // copy acc to vreg[116]
v_accvgpr_read_b32 v[vgprValuC+190], acc117        // copy acc to vreg[117]
v_accvgpr_read_b32 v[vgprValuC+194], acc118        // copy acc to vreg[118]
v_accvgpr_read_b32 v[vgprValuC+198], acc119        // copy acc to vreg[119]

/* rC *= alpha batchElements=[(0, 0, 5, 0), (0, 0, 5, 1), (0, 0, 5, 2), (0, 0, 5, 3), (0, 1, 5, 0), (0, 1, 5, 1), (0, 1, 5, 2), (0, 1, 5, 3), (0, 2, 5, 0), (0, 2, 5, 1), (0, 2, 5, 2), (0, 2, 5, 3), (0, 3, 5, 0), (0, 3, 5, 1), (0, 3, 5, 2), (0, 3, 5, 3), (0, 0, 6, 0), (0, 0, 6, 1), (0, 0, 6, 2), (0, 0, 6, 3), (0, 1, 6, 0), (0, 1, 6, 1), (0, 1, 6, 2), (0, 1, 6, 3), (0, 2, 6, 0), (0, 2, 6, 1), (0, 2, 6, 2), (0, 2, 6, 3), (0, 3, 6, 0), (0, 3, 6, 1), (0, 3, 6, 2), (0, 3, 6, 3), (0, 0, 7, 0), (0, 0, 7, 1), (0, 0, 7, 2), (0, 0, 7, 3), (0, 1, 7, 0), (0, 1, 7, 1), (0, 1, 7, 2), (0, 1, 7, 3)] */
v_mul_f32 v[vgprValuC+11], s[sgprAlpha], v[vgprValuC+11] // *= alpha
v_mul_f32 v[vgprValuC+17], s[sgprAlpha], v[vgprValuC+17] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+121], s[sgprAlpha], v[vgprValuC+121] // *= alpha
v_mul_f32 v[vgprValuC+125], s[sgprAlpha], v[vgprValuC+125] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+133], s[sgprAlpha], v[vgprValuC+133] // *= alpha
v_mul_f32 v[vgprValuC+137], s[sgprAlpha], v[vgprValuC+137] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+146], s[sgprAlpha], v[vgprValuC+146] // *= alpha
v_mul_f32 v[vgprValuC+150], s[sgprAlpha], v[vgprValuC+150] // *= alpha
v_mul_f32 v[vgprValuC+154], s[sgprAlpha], v[vgprValuC+154] // *= alpha
v_mul_f32 v[vgprValuC+158], s[sgprAlpha], v[vgprValuC+158] // *= alpha
v_mul_f32 v[vgprValuC+162], s[sgprAlpha], v[vgprValuC+162] // *= alpha
v_mul_f32 v[vgprValuC+166], s[sgprAlpha], v[vgprValuC+166] // *= alpha
v_mul_f32 v[vgprValuC+170], s[sgprAlpha], v[vgprValuC+170] // *= alpha
v_mul_f32 v[vgprValuC+174], s[sgprAlpha], v[vgprValuC+174] // *= alpha
v_mul_f32 v[vgprValuC+178], s[sgprAlpha], v[vgprValuC+178] // *= alpha
v_mul_f32 v[vgprValuC+182], s[sgprAlpha], v[vgprValuC+182] // *= alpha
v_mul_f32 v[vgprValuC+186], s[sgprAlpha], v[vgprValuC+186] // *= alpha
v_mul_f32 v[vgprValuC+190], s[sgprAlpha], v[vgprValuC+190] // *= alpha
v_mul_f32 v[vgprValuC+194], s[sgprAlpha], v[vgprValuC+194] // *= alpha
v_mul_f32 v[vgprValuC+198], s[sgprAlpha], v[vgprValuC+198] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+11], v10, v[vgprValuC+11]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+11], v9, v[vgprValuC+11]     // C += bias
v_cvt_f16_f32 v11, v[vgprValuC+11]                 // convert C to fp16
buffer_store_short v11, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+17], v16, v[vgprValuC+17]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+17], v15, v[vgprValuC+17]    // C += bias
v_cvt_f16_f32 v17, v[vgprValuC+17]                 // convert C to fp16
buffer_store_short v17, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+23], v22, v[vgprValuC+23]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+23], v21, v[vgprValuC+23]    // C += bias
v_cvt_f16_f32 v23, v[vgprValuC+23]                 // convert C to fp16
buffer_store_short v23, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+29], v28, v[vgprValuC+29]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+29], v27, v[vgprValuC+29]    // C += bias
v_cvt_f16_f32 v29, v[vgprValuC+29]                 // convert C to fp16
buffer_store_short v29, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+35], v34, v[vgprValuC+35]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+35], v33, v[vgprValuC+35]    // C += bias
v_cvt_f16_f32 v35, v[vgprValuC+35]                 // convert C to fp16
buffer_store_short v35, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+41], v40, v[vgprValuC+41]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+41], v39, v[vgprValuC+41]    // C += bias
v_cvt_f16_f32 v41, v[vgprValuC+41]                 // convert C to fp16
buffer_store_short v41, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+53], v52, v[vgprValuC+53]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+53], v51, v[vgprValuC+53]    // C += bias
v_cvt_f16_f32 v53, v[vgprValuC+53]                 // convert C to fp16
buffer_store_short v53, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+59], v58, v[vgprValuC+59]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+59], v57, v[vgprValuC+59]    // C += bias
v_cvt_f16_f32 v59, v[vgprValuC+59]                 // convert C to fp16
buffer_store_short v59, v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+65], v64, v[vgprValuC+65]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+65], v63, v[vgprValuC+65]    // C += bias
v_cvt_f16_f32 v65, v[vgprValuC+65]                 // convert C to fp16
buffer_store_short v65, v60, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+71], v70, v[vgprValuC+71]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+71], v69, v[vgprValuC+71]    // C += bias
v_cvt_f16_f32 v71, v[vgprValuC+71]                 // convert C to fp16
buffer_store_short v71, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+77], v76, v[vgprValuC+77]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+77], v75, v[vgprValuC+77]    // C += bias
v_cvt_f16_f32 v77, v[vgprValuC+77]                 // convert C to fp16
buffer_store_short v77, v72, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+83], v82, v[vgprValuC+83]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+83], v81, v[vgprValuC+83]    // C += bias
v_cvt_f16_f32 v83, v[vgprValuC+83]                 // convert C to fp16
buffer_store_short v83, v78, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+89], v88, v[vgprValuC+89]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+89], v87, v[vgprValuC+89]    // C += bias
v_cvt_f16_f32 v89, v[vgprValuC+89]                 // convert C to fp16
buffer_store_short v89, v84, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+95], v94, v[vgprValuC+95]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+95], v93, v[vgprValuC+95]    // C += bias
v_cvt_f16_f32 v95, v[vgprValuC+95]                 // convert C to fp16
buffer_store_short v95, v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+101], v100, v[vgprValuC+101] // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+101], v99, v[vgprValuC+101]  // C += bias
v_cvt_f16_f32 v101, v[vgprValuC+101]               // convert C to fp16
buffer_store_short v101, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+105], v10, v[vgprValuC+105]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+105], v9, v[vgprValuC+105]   // C += bias
v_cvt_f16_f32 v105, v[vgprValuC+105]               // convert C to fp16
buffer_store_short v105, v102, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+109], v16, v[vgprValuC+109]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+109], v15, v[vgprValuC+109]  // C += bias
v_cvt_f16_f32 v109, v[vgprValuC+109]               // convert C to fp16
buffer_store_short v109, v106, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+113], v22, v[vgprValuC+113]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+113], v21, v[vgprValuC+113]  // C += bias
v_cvt_f16_f32 v113, v[vgprValuC+113]               // convert C to fp16
buffer_store_short v113, v110, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+117], v28, v[vgprValuC+117]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+117], v27, v[vgprValuC+117]  // C += bias
v_cvt_f16_f32 v117, v[vgprValuC+117]               // convert C to fp16
buffer_store_short v117, v114, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+121], v34, v[vgprValuC+121]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+121], v33, v[vgprValuC+121]  // C += bias
v_cvt_f16_f32 v121, v[vgprValuC+121]               // convert C to fp16
buffer_store_short v121, v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+125], v40, v[vgprValuC+125]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+125], v39, v[vgprValuC+125]  // C += bias
v_cvt_f16_f32 v125, v[vgprValuC+125]               // convert C to fp16
buffer_store_short v125, v122, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+129], v46, v[vgprValuC+129]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+129], v45, v[vgprValuC+129]  // C += bias
v_cvt_f16_f32 v129, v[vgprValuC+129]               // convert C to fp16
buffer_store_short v129, v126, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+133], v52, v[vgprValuC+133]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+133], v51, v[vgprValuC+133]  // C += bias
v_cvt_f16_f32 v133, v[vgprValuC+133]               // convert C to fp16
buffer_store_short v133, v130, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+137], v58, v[vgprValuC+137]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+137], v57, v[vgprValuC+137]  // C += bias
v_cvt_f16_f32 v137, v[vgprValuC+137]               // convert C to fp16
buffer_store_short v137, v134, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+142], v64, v[vgprValuC+142]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+142], v63, v[vgprValuC+142]  // C += bias
v_cvt_f16_f32 v142, v[vgprValuC+142]               // convert C to fp16
buffer_store_short v142, v138, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+146], v70, v[vgprValuC+146]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+146], v69, v[vgprValuC+146]  // C += bias
v_cvt_f16_f32 v146, v[vgprValuC+146]               // convert C to fp16
buffer_store_short v146, v143, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+150], v76, v[vgprValuC+150]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+150], v75, v[vgprValuC+150]  // C += bias
v_cvt_f16_f32 v150, v[vgprValuC+150]               // convert C to fp16
buffer_store_short v150, v147, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+154], v82, v[vgprValuC+154]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+154], v81, v[vgprValuC+154]  // C += bias
v_cvt_f16_f32 v154, v[vgprValuC+154]               // convert C to fp16
buffer_store_short v154, v151, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+158], v88, v[vgprValuC+158]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+158], v87, v[vgprValuC+158]  // C += bias
v_cvt_f16_f32 v158, v[vgprValuC+158]               // convert C to fp16
buffer_store_short v158, v155, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+162], v94, v[vgprValuC+162]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+162], v93, v[vgprValuC+162]  // C += bias
v_cvt_f16_f32 v162, v[vgprValuC+162]               // convert C to fp16
buffer_store_short v162, v159, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+166], v100, v[vgprValuC+166] // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+166], v99, v[vgprValuC+166]  // C += bias
v_cvt_f16_f32 v166, v[vgprValuC+166]               // convert C to fp16
buffer_store_short v166, v163, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+170], v10, v[vgprValuC+170]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+170], v9, v[vgprValuC+170]   // C += bias
v_cvt_f16_f32 v170, v[vgprValuC+170]               // convert C to fp16
buffer_store_short v170, v167, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+174], v16, v[vgprValuC+174]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+174], v15, v[vgprValuC+174]  // C += bias
v_cvt_f16_f32 v174, v[vgprValuC+174]               // convert C to fp16
buffer_store_short v174, v171, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+178], v22, v[vgprValuC+178]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+178], v21, v[vgprValuC+178]  // C += bias
v_cvt_f16_f32 v178, v[vgprValuC+178]               // convert C to fp16
buffer_store_short v178, v175, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+182], v28, v[vgprValuC+182]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+182], v27, v[vgprValuC+182]  // C += bias
v_cvt_f16_f32 v182, v[vgprValuC+182]               // convert C to fp16
buffer_store_short v182, v179, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+186], v34, v[vgprValuC+186]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+186], v33, v[vgprValuC+186]  // C += bias
v_cvt_f16_f32 v186, v[vgprValuC+186]               // convert C to fp16
buffer_store_short v186, v183, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+190], v40, v[vgprValuC+190]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+190], v39, v[vgprValuC+190]  // C += bias
v_cvt_f16_f32 v190, v[vgprValuC+190]               // convert C to fp16
buffer_store_short v190, v187, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+194], v46, v[vgprValuC+194]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+194], v45, v[vgprValuC+194]  // C += bias
v_cvt_f16_f32 v194, v[vgprValuC+194]               // convert C to fp16
buffer_store_short v194, v191, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+198], v52, v[vgprValuC+198]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+198], v51, v[vgprValuC+198]  // C += bias
v_cvt_f16_f32 v198, v[vgprValuC+198]               // convert C to fp16
buffer_store_short v198, v195, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #3 (d1,d0,vc1,vc0) = */
/*    (0,2,7,0:vw1); (0,2,7,1:vw1); (0,2,7,2:vw1); (0,2,7,3:vw1); (0,3,7,0:vw1); (0,3,7,1:vw1); (0,3,7,2:vw1); (0,3,7,3:vw1); (1,0,0,0:vw1); (1,0,0,1:vw1); (1,0,0,2:vw1); (1,0,0,3:vw1); (1,1,0,0:vw1); (1,1,0,1:vw1); (1,1,0,2:vw1); (1,1,0,3:vw1); (1,2,0,0:vw1); (1,2,0,1:vw1); (1,2,0,2:vw1); (1,2,0,3:vw1); (1,3,0,0:vw1); (1,3,0,1:vw1); (1,3,0,2:vw1); (1,3,0,3:vw1); (1,0,1,0:vw1); (1,0,1,1:vw1); (1,0,1,2:vw1); (1,0,1,3:vw1); (1,1,1,0:vw1); (1,1,1,1:vw1); (1,1,1,2:vw1); (1,1,1,3:vw1); (1,2,1,0:vw1); (1,2,1,1:vw1); (1,2,1,2:vw1); (1,2,1,3:vw1); (1,3,1,0:vw1); (1,3,1,1:vw1); (1,3,1,2:vw1); (1,3,1,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v199, BufferOOB
/* (d1,vc1,d0,vc0)=(0,7,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v4, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b32 v9, v7 offset:0                        // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v10, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v199, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v13, v4, s52
v_lshlrev_b32 v13, 0x2, v13                        // Bias address scaled by BPE
ds_read_b32 v15, v13 offset:0                      // load Bias
v_add_u32 v14, 1024, v13                           // add ScaleAlphaVec offset (3)
ds_read_b32 v16, v14 offset:0                      // load scaleAlpha
v_add_lshl_u32 v12, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v199, v12, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v19, v4, s52
v_lshlrev_b32 v19, 0x2, v19                        // Bias address scaled by BPE
ds_read_b32 v21, v19 offset:0                      // load Bias
v_add_u32 v20, 1024, v19                           // add ScaleAlphaVec offset (3)
ds_read_b32 v22, v20 offset:0                      // load scaleAlpha
v_add_lshl_u32 v18, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v199, v18, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v25, v4, s52
v_lshlrev_b32 v25, 0x2, v25                        // Bias address scaled by BPE
ds_read_b32 v27, v25 offset:0                      // load Bias
v_add_u32 v26, 1024, v25                           // add ScaleAlphaVec offset (3)
ds_read_b32 v28, v26 offset:0                      // load scaleAlpha
v_add_lshl_u32 v24, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v199, v24, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v31, v4, s52
v_lshlrev_b32 v31, 0x2, v31                        // Bias address scaled by BPE
ds_read_b32 v33, v31 offset:0                      // load Bias
v_add_u32 v32, 1024, v31                           // add ScaleAlphaVec offset (3)
ds_read_b32 v34, v32 offset:0                      // load scaleAlpha
v_add_lshl_u32 v30, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v199, v30, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v37, v4, s52
v_lshlrev_b32 v37, 0x2, v37                        // Bias address scaled by BPE
ds_read_b32 v39, v37 offset:0                      // load Bias
v_add_u32 v38, 1024, v37                           // add ScaleAlphaVec offset (3)
ds_read_b32 v40, v38 offset:0                      // load scaleAlpha
v_add_lshl_u32 v36, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v199, v36, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v43, v4, s52
v_lshlrev_b32 v43, 0x2, v43                        // Bias address scaled by BPE
ds_read_b32 v45, v43 offset:0                      // load Bias
v_add_u32 v44, 1024, v43                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v44 offset:0                      // load scaleAlpha
v_add_lshl_u32 v42, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v199, v42, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v4, s52
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
ds_read_b32 v51, v49 offset:0                      // load Bias
v_add_u32 v50, 1024, v49                           // add ScaleAlphaVec offset (3)
ds_read_b32 v52, v50 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v199, v48, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,0) */
s_mov_b32 s52, 121                                 // rowInc d1=0 vc1=0
v_add_co_u32 v1, vcc, v1, s52                      // coord1.2: coord1 += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s52, s[sgprStrideC1J], 121               // scale stride
v_add_i32 v2, v2, s52                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s52, s[sgprStrideD1J], 121               // scale stride
v_add_i32 v3, v3, s52                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v55, v0, s52
v_lshlrev_b32 v55, 0x2, v55                        // Bias address scaled by BPE
ds_read_b32 v57, v55 offset:0                      // load Bias
v_add_u32 v56, 1024, v55                           // add ScaleAlphaVec offset (3)
ds_read_b32 v58, v56 offset:0                      // load scaleAlpha
v_add_lshl_u32 v54, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v199, v54, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v61, v4, s52
v_lshlrev_b32 v61, 0x2, v61                        // Bias address scaled by BPE
ds_read_b32 v63, v61 offset:0                      // load Bias
v_add_u32 v62, 1024, v61                           // add ScaleAlphaVec offset (3)
ds_read_b32 v64, v62 offset:0                      // load scaleAlpha
v_add_lshl_u32 v60, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v60, v199, v60, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s52
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
ds_read_b32 v69, v67 offset:0                      // load Bias
v_add_u32 v68, 1024, v67                           // add ScaleAlphaVec offset (3)
ds_read_b32 v70, v68 offset:0                      // load scaleAlpha
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v199, v66, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v73, v4, s52
v_lshlrev_b32 v73, 0x2, v73                        // Bias address scaled by BPE
ds_read_b32 v75, v73 offset:0                      // load Bias
v_add_u32 v74, 1024, v73                           // add ScaleAlphaVec offset (3)
ds_read_b32 v76, v74 offset:0                      // load scaleAlpha
v_add_lshl_u32 v72, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v72, v199, v72, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v79, v4, s52
v_lshlrev_b32 v79, 0x2, v79                        // Bias address scaled by BPE
ds_read_b32 v81, v79 offset:0                      // load Bias
v_add_u32 v80, 1024, v79                           // add ScaleAlphaVec offset (3)
ds_read_b32 v82, v80 offset:0                      // load scaleAlpha
v_add_lshl_u32 v78, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v78, v199, v78, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v85, v4, s52
v_lshlrev_b32 v85, 0x2, v85                        // Bias address scaled by BPE
ds_read_b32 v87, v85 offset:0                      // load Bias
v_add_u32 v86, 1024, v85                           // add ScaleAlphaVec offset (3)
ds_read_b32 v88, v86 offset:0                      // load scaleAlpha
v_add_lshl_u32 v84, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v199, v84, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v4, s52
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
ds_read_b32 v93, v91 offset:0                      // load Bias
v_add_u32 v92, 1024, v91                           // add ScaleAlphaVec offset (3)
ds_read_b32 v94, v92 offset:0                      // load scaleAlpha
v_add_lshl_u32 v90, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v199, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s52
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
ds_read_b32 v99, v97 offset:0                      // load Bias
v_add_u32 v98, 1024, v97                           // add ScaleAlphaVec offset (3)
ds_read_b32 v100, v98 offset:0                     // load scaleAlpha
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v199, v96, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v103, v4, s52
v_lshlrev_b32 v103, 0x2, v103                      // Bias address scaled by BPE
v_add_u32 v104, 1024, v103                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v102, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v102, v199, v102, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v107, v4, s52
v_lshlrev_b32 v107, 0x2, v107                      // Bias address scaled by BPE
v_add_u32 v108, 1024, v107                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v106, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v199, v106, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v111, v4, s52
v_lshlrev_b32 v111, 0x2, v111                      // Bias address scaled by BPE
v_add_u32 v112, 1024, v111                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v110, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v110, v199, v110, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v115, v4, s52
v_lshlrev_b32 v115, 0x2, v115                      // Bias address scaled by BPE
v_add_u32 v116, 1024, v115                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v114, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v114, v199, v114, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v4, s52
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v120, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v199, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v123, v4, s52
v_lshlrev_b32 v123, 0x2, v123                      // Bias address scaled by BPE
v_add_u32 v124, 1024, v123                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v122, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v122, v199, v122, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v127, v4, s52
v_lshlrev_b32 v127, 0x2, v127                      // Bias address scaled by BPE
v_add_u32 v128, 1024, v127                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v126, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v126, v199, v126, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v131, v4, s52
v_lshlrev_b32 v131, 0x2, v131                      // Bias address scaled by BPE
v_add_u32 v132, 1024, v131                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v130, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v130, v199, v130, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v135, v0, s52
v_lshlrev_b32 v135, 0x2, v135                      // Bias address scaled by BPE
v_add_u32 v136, 1024, v135                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v134, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v134, v199, v134, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v140, v4, s52
v_lshlrev_b32 v140, 0x2, v140                      // Bias address scaled by BPE
v_add_u32 v141, 1024, v140                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v138, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v199, v138, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v144, v4, s52
v_lshlrev_b32 v144, 0x2, v144                      // Bias address scaled by BPE
v_add_u32 v145, 1024, v144                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v143, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v143, v199, v143, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v148, v4, s52
v_lshlrev_b32 v148, 0x2, v148                      // Bias address scaled by BPE
v_add_u32 v149, 1024, v148                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v147, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v147, v199, v147, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v152, v4, s52
v_lshlrev_b32 v152, 0x2, v152                      // Bias address scaled by BPE
v_add_u32 v153, 1024, v152                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v151, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v151, v199, v151, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v156, v4, s52
v_lshlrev_b32 v156, 0x2, v156                      // Bias address scaled by BPE
v_add_u32 v157, 1024, v156                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v155, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v155, v199, v155, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v160, v4, s52
v_lshlrev_b32 v160, 0x2, v160                      // Bias address scaled by BPE
v_add_u32 v161, 1024, v160                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v159, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v199, v159, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v164, v4, s52
v_lshlrev_b32 v164, 0x2, v164                      // Bias address scaled by BPE
v_add_u32 v165, 1024, v164                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v163, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v163, v199, v163, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v168, v4, s52
v_lshlrev_b32 v168, 0x2, v168                      // Bias address scaled by BPE
v_add_u32 v169, 1024, v168                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v167, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v167, v199, v167, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v172, v4, s52
v_lshlrev_b32 v172, 0x2, v172                      // Bias address scaled by BPE
v_add_u32 v173, 1024, v172                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v171, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v171, v199, v171, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v176, v4, s52
v_lshlrev_b32 v176, 0x2, v176                      // Bias address scaled by BPE
v_add_u32 v177, 1024, v176                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v175, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v175, v199, v175, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v180, v4, s52
v_lshlrev_b32 v180, 0x2, v180                      // Bias address scaled by BPE
v_add_u32 v181, 1024, v180                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v179, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v199, v179, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v184, v4, s52
v_lshlrev_b32 v184, 0x2, v184                      // Bias address scaled by BPE
v_add_u32 v185, 1024, v184                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v183, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v183, v199, v183, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v188, v4, s52
v_lshlrev_b32 v188, 0x2, v188                      // Bias address scaled by BPE
v_add_u32 v189, 1024, v188                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v187, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v187, v199, v187, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v192, v4, s52
v_lshlrev_b32 v192, 0x2, v192                      // Bias address scaled by BPE
v_add_u32 v193, 1024, v192                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v191, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v191, v199, v191, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v196, v4, s52
v_lshlrev_b32 v196, 0x2, v196                      // Bias address scaled by BPE
v_add_u32 v197, 1024, v196                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v195, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v195, v199, v195, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc120         // copy acc to vreg[120]
v_accvgpr_read_b32 v[vgprValuC+17], acc121         // copy acc to vreg[121]
v_accvgpr_read_b32 v[vgprValuC+23], acc122         // copy acc to vreg[122]
v_accvgpr_read_b32 v[vgprValuC+29], acc123         // copy acc to vreg[123]
v_accvgpr_read_b32 v[vgprValuC+35], acc124         // copy acc to vreg[124]
v_accvgpr_read_b32 v[vgprValuC+41], acc125         // copy acc to vreg[125]
v_accvgpr_read_b32 v[vgprValuC+47], acc126         // copy acc to vreg[126]
v_accvgpr_read_b32 v[vgprValuC+53], acc127         // copy acc to vreg[127]
v_accvgpr_read_b32 v[vgprValuC+59], acc128         // copy acc to vreg[128]
v_accvgpr_read_b32 v[vgprValuC+65], acc129         // copy acc to vreg[129]
v_accvgpr_read_b32 v[vgprValuC+71], acc130         // copy acc to vreg[130]
v_accvgpr_read_b32 v[vgprValuC+77], acc131         // copy acc to vreg[131]
v_accvgpr_read_b32 v[vgprValuC+83], acc132         // copy acc to vreg[132]
v_accvgpr_read_b32 v[vgprValuC+89], acc133         // copy acc to vreg[133]
v_accvgpr_read_b32 v[vgprValuC+95], acc134         // copy acc to vreg[134]
v_accvgpr_read_b32 v[vgprValuC+101], acc135        // copy acc to vreg[135]
v_accvgpr_read_b32 v[vgprValuC+105], acc136        // copy acc to vreg[136]
v_accvgpr_read_b32 v[vgprValuC+109], acc137        // copy acc to vreg[137]
v_accvgpr_read_b32 v[vgprValuC+113], acc138        // copy acc to vreg[138]
v_accvgpr_read_b32 v[vgprValuC+117], acc139        // copy acc to vreg[139]
v_accvgpr_read_b32 v[vgprValuC+121], acc140        // copy acc to vreg[140]
v_accvgpr_read_b32 v[vgprValuC+125], acc141        // copy acc to vreg[141]
v_accvgpr_read_b32 v[vgprValuC+129], acc142        // copy acc to vreg[142]
v_accvgpr_read_b32 v[vgprValuC+133], acc143        // copy acc to vreg[143]
v_accvgpr_read_b32 v[vgprValuC+137], acc144        // copy acc to vreg[144]
v_accvgpr_read_b32 v[vgprValuC+142], acc145        // copy acc to vreg[145]
v_accvgpr_read_b32 v[vgprValuC+146], acc146        // copy acc to vreg[146]
v_accvgpr_read_b32 v[vgprValuC+150], acc147        // copy acc to vreg[147]
v_accvgpr_read_b32 v[vgprValuC+154], acc148        // copy acc to vreg[148]
v_accvgpr_read_b32 v[vgprValuC+158], acc149        // copy acc to vreg[149]
v_accvgpr_read_b32 v[vgprValuC+162], acc150        // copy acc to vreg[150]
v_accvgpr_read_b32 v[vgprValuC+166], acc151        // copy acc to vreg[151]
v_accvgpr_read_b32 v[vgprValuC+170], acc152        // copy acc to vreg[152]
v_accvgpr_read_b32 v[vgprValuC+174], acc153        // copy acc to vreg[153]
v_accvgpr_read_b32 v[vgprValuC+178], acc154        // copy acc to vreg[154]
v_accvgpr_read_b32 v[vgprValuC+182], acc155        // copy acc to vreg[155]
v_accvgpr_read_b32 v[vgprValuC+186], acc156        // copy acc to vreg[156]
v_accvgpr_read_b32 v[vgprValuC+190], acc157        // copy acc to vreg[157]
v_accvgpr_read_b32 v[vgprValuC+194], acc158        // copy acc to vreg[158]
v_accvgpr_read_b32 v[vgprValuC+198], acc159        // copy acc to vreg[159]

/* rC *= alpha batchElements=[(0, 2, 7, 0), (0, 2, 7, 1), (0, 2, 7, 2), (0, 2, 7, 3), (0, 3, 7, 0), (0, 3, 7, 1), (0, 3, 7, 2), (0, 3, 7, 3), (1, 0, 0, 0), (1, 0, 0, 1), (1, 0, 0, 2), (1, 0, 0, 3), (1, 1, 0, 0), (1, 1, 0, 1), (1, 1, 0, 2), (1, 1, 0, 3), (1, 2, 0, 0), (1, 2, 0, 1), (1, 2, 0, 2), (1, 2, 0, 3), (1, 3, 0, 0), (1, 3, 0, 1), (1, 3, 0, 2), (1, 3, 0, 3), (1, 0, 1, 0), (1, 0, 1, 1), (1, 0, 1, 2), (1, 0, 1, 3), (1, 1, 1, 0), (1, 1, 1, 1), (1, 1, 1, 2), (1, 1, 1, 3), (1, 2, 1, 0), (1, 2, 1, 1), (1, 2, 1, 2), (1, 2, 1, 3), (1, 3, 1, 0), (1, 3, 1, 1), (1, 3, 1, 2), (1, 3, 1, 3)] */
v_mul_f32 v[vgprValuC+11], s[sgprAlpha], v[vgprValuC+11] // *= alpha
v_mul_f32 v[vgprValuC+17], s[sgprAlpha], v[vgprValuC+17] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+121], s[sgprAlpha], v[vgprValuC+121] // *= alpha
v_mul_f32 v[vgprValuC+125], s[sgprAlpha], v[vgprValuC+125] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+133], s[sgprAlpha], v[vgprValuC+133] // *= alpha
v_mul_f32 v[vgprValuC+137], s[sgprAlpha], v[vgprValuC+137] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+146], s[sgprAlpha], v[vgprValuC+146] // *= alpha
v_mul_f32 v[vgprValuC+150], s[sgprAlpha], v[vgprValuC+150] // *= alpha
v_mul_f32 v[vgprValuC+154], s[sgprAlpha], v[vgprValuC+154] // *= alpha
v_mul_f32 v[vgprValuC+158], s[sgprAlpha], v[vgprValuC+158] // *= alpha
v_mul_f32 v[vgprValuC+162], s[sgprAlpha], v[vgprValuC+162] // *= alpha
v_mul_f32 v[vgprValuC+166], s[sgprAlpha], v[vgprValuC+166] // *= alpha
v_mul_f32 v[vgprValuC+170], s[sgprAlpha], v[vgprValuC+170] // *= alpha
v_mul_f32 v[vgprValuC+174], s[sgprAlpha], v[vgprValuC+174] // *= alpha
v_mul_f32 v[vgprValuC+178], s[sgprAlpha], v[vgprValuC+178] // *= alpha
v_mul_f32 v[vgprValuC+182], s[sgprAlpha], v[vgprValuC+182] // *= alpha
v_mul_f32 v[vgprValuC+186], s[sgprAlpha], v[vgprValuC+186] // *= alpha
v_mul_f32 v[vgprValuC+190], s[sgprAlpha], v[vgprValuC+190] // *= alpha
v_mul_f32 v[vgprValuC+194], s[sgprAlpha], v[vgprValuC+194] // *= alpha
v_mul_f32 v[vgprValuC+198], s[sgprAlpha], v[vgprValuC+198] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+11], v10, v[vgprValuC+11]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+11], v9, v[vgprValuC+11]     // C += bias
v_cvt_f16_f32 v11, v[vgprValuC+11]                 // convert C to fp16
buffer_store_short v11, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+17], v16, v[vgprValuC+17]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+17], v15, v[vgprValuC+17]    // C += bias
v_cvt_f16_f32 v17, v[vgprValuC+17]                 // convert C to fp16
buffer_store_short v17, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+23], v22, v[vgprValuC+23]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+23], v21, v[vgprValuC+23]    // C += bias
v_cvt_f16_f32 v23, v[vgprValuC+23]                 // convert C to fp16
buffer_store_short v23, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+29], v28, v[vgprValuC+29]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+29], v27, v[vgprValuC+29]    // C += bias
v_cvt_f16_f32 v29, v[vgprValuC+29]                 // convert C to fp16
buffer_store_short v29, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+35], v34, v[vgprValuC+35]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+35], v33, v[vgprValuC+35]    // C += bias
v_cvt_f16_f32 v35, v[vgprValuC+35]                 // convert C to fp16
buffer_store_short v35, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+41], v40, v[vgprValuC+41]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+41], v39, v[vgprValuC+41]    // C += bias
v_cvt_f16_f32 v41, v[vgprValuC+41]                 // convert C to fp16
buffer_store_short v41, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+53], v52, v[vgprValuC+53]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+53], v51, v[vgprValuC+53]    // C += bias
v_cvt_f16_f32 v53, v[vgprValuC+53]                 // convert C to fp16
buffer_store_short v53, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+59], v58, v[vgprValuC+59]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+59], v57, v[vgprValuC+59]    // C += bias
v_cvt_f16_f32 v59, v[vgprValuC+59]                 // convert C to fp16
buffer_store_short v59, v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+65], v64, v[vgprValuC+65]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+65], v63, v[vgprValuC+65]    // C += bias
v_cvt_f16_f32 v65, v[vgprValuC+65]                 // convert C to fp16
buffer_store_short v65, v60, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+71], v70, v[vgprValuC+71]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+71], v69, v[vgprValuC+71]    // C += bias
v_cvt_f16_f32 v71, v[vgprValuC+71]                 // convert C to fp16
buffer_store_short v71, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+77], v76, v[vgprValuC+77]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+77], v75, v[vgprValuC+77]    // C += bias
v_cvt_f16_f32 v77, v[vgprValuC+77]                 // convert C to fp16
buffer_store_short v77, v72, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+83], v82, v[vgprValuC+83]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+83], v81, v[vgprValuC+83]    // C += bias
v_cvt_f16_f32 v83, v[vgprValuC+83]                 // convert C to fp16
buffer_store_short v83, v78, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+89], v88, v[vgprValuC+89]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+89], v87, v[vgprValuC+89]    // C += bias
v_cvt_f16_f32 v89, v[vgprValuC+89]                 // convert C to fp16
buffer_store_short v89, v84, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+95], v94, v[vgprValuC+95]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+95], v93, v[vgprValuC+95]    // C += bias
v_cvt_f16_f32 v95, v[vgprValuC+95]                 // convert C to fp16
buffer_store_short v95, v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+101], v100, v[vgprValuC+101] // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+101], v99, v[vgprValuC+101]  // C += bias
v_cvt_f16_f32 v101, v[vgprValuC+101]               // convert C to fp16
buffer_store_short v101, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+105], v10, v[vgprValuC+105]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+105], v9, v[vgprValuC+105]   // C += bias
v_cvt_f16_f32 v105, v[vgprValuC+105]               // convert C to fp16
buffer_store_short v105, v102, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+109], v16, v[vgprValuC+109]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+109], v15, v[vgprValuC+109]  // C += bias
v_cvt_f16_f32 v109, v[vgprValuC+109]               // convert C to fp16
buffer_store_short v109, v106, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+113], v22, v[vgprValuC+113]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+113], v21, v[vgprValuC+113]  // C += bias
v_cvt_f16_f32 v113, v[vgprValuC+113]               // convert C to fp16
buffer_store_short v113, v110, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+117], v28, v[vgprValuC+117]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+117], v27, v[vgprValuC+117]  // C += bias
v_cvt_f16_f32 v117, v[vgprValuC+117]               // convert C to fp16
buffer_store_short v117, v114, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+121], v34, v[vgprValuC+121]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+121], v33, v[vgprValuC+121]  // C += bias
v_cvt_f16_f32 v121, v[vgprValuC+121]               // convert C to fp16
buffer_store_short v121, v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+125], v40, v[vgprValuC+125]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+125], v39, v[vgprValuC+125]  // C += bias
v_cvt_f16_f32 v125, v[vgprValuC+125]               // convert C to fp16
buffer_store_short v125, v122, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+129], v46, v[vgprValuC+129]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+129], v45, v[vgprValuC+129]  // C += bias
v_cvt_f16_f32 v129, v[vgprValuC+129]               // convert C to fp16
buffer_store_short v129, v126, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+133], v52, v[vgprValuC+133]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+133], v51, v[vgprValuC+133]  // C += bias
v_cvt_f16_f32 v133, v[vgprValuC+133]               // convert C to fp16
buffer_store_short v133, v130, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+137], v58, v[vgprValuC+137]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+137], v57, v[vgprValuC+137]  // C += bias
v_cvt_f16_f32 v137, v[vgprValuC+137]               // convert C to fp16
buffer_store_short v137, v134, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+142], v64, v[vgprValuC+142]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+142], v63, v[vgprValuC+142]  // C += bias
v_cvt_f16_f32 v142, v[vgprValuC+142]               // convert C to fp16
buffer_store_short v142, v138, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+146], v70, v[vgprValuC+146]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+146], v69, v[vgprValuC+146]  // C += bias
v_cvt_f16_f32 v146, v[vgprValuC+146]               // convert C to fp16
buffer_store_short v146, v143, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+150], v76, v[vgprValuC+150]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+150], v75, v[vgprValuC+150]  // C += bias
v_cvt_f16_f32 v150, v[vgprValuC+150]               // convert C to fp16
buffer_store_short v150, v147, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+154], v82, v[vgprValuC+154]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+154], v81, v[vgprValuC+154]  // C += bias
v_cvt_f16_f32 v154, v[vgprValuC+154]               // convert C to fp16
buffer_store_short v154, v151, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+158], v88, v[vgprValuC+158]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+158], v87, v[vgprValuC+158]  // C += bias
v_cvt_f16_f32 v158, v[vgprValuC+158]               // convert C to fp16
buffer_store_short v158, v155, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+162], v94, v[vgprValuC+162]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+162], v93, v[vgprValuC+162]  // C += bias
v_cvt_f16_f32 v162, v[vgprValuC+162]               // convert C to fp16
buffer_store_short v162, v159, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+166], v100, v[vgprValuC+166] // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+166], v99, v[vgprValuC+166]  // C += bias
v_cvt_f16_f32 v166, v[vgprValuC+166]               // convert C to fp16
buffer_store_short v166, v163, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+170], v10, v[vgprValuC+170]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+170], v9, v[vgprValuC+170]   // C += bias
v_cvt_f16_f32 v170, v[vgprValuC+170]               // convert C to fp16
buffer_store_short v170, v167, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+174], v16, v[vgprValuC+174]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+174], v15, v[vgprValuC+174]  // C += bias
v_cvt_f16_f32 v174, v[vgprValuC+174]               // convert C to fp16
buffer_store_short v174, v171, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+178], v22, v[vgprValuC+178]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+178], v21, v[vgprValuC+178]  // C += bias
v_cvt_f16_f32 v178, v[vgprValuC+178]               // convert C to fp16
buffer_store_short v178, v175, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+182], v28, v[vgprValuC+182]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+182], v27, v[vgprValuC+182]  // C += bias
v_cvt_f16_f32 v182, v[vgprValuC+182]               // convert C to fp16
buffer_store_short v182, v179, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+186], v34, v[vgprValuC+186]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+186], v33, v[vgprValuC+186]  // C += bias
v_cvt_f16_f32 v186, v[vgprValuC+186]               // convert C to fp16
buffer_store_short v186, v183, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+190], v40, v[vgprValuC+190]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+190], v39, v[vgprValuC+190]  // C += bias
v_cvt_f16_f32 v190, v[vgprValuC+190]               // convert C to fp16
buffer_store_short v190, v187, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+194], v46, v[vgprValuC+194]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+194], v45, v[vgprValuC+194]  // C += bias
v_cvt_f16_f32 v194, v[vgprValuC+194]               // convert C to fp16
buffer_store_short v194, v191, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+198], v52, v[vgprValuC+198]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+198], v51, v[vgprValuC+198]  // C += bias
v_cvt_f16_f32 v198, v[vgprValuC+198]               // convert C to fp16
buffer_store_short v198, v195, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #4 (d1,d0,vc1,vc0) = */
/*    (1,0,2,0:vw1); (1,0,2,1:vw1); (1,0,2,2:vw1); (1,0,2,3:vw1); (1,1,2,0:vw1); (1,1,2,1:vw1); (1,1,2,2:vw1); (1,1,2,3:vw1); (1,2,2,0:vw1); (1,2,2,1:vw1); (1,2,2,2:vw1); (1,2,2,3:vw1); (1,3,2,0:vw1); (1,3,2,1:vw1); (1,3,2,2:vw1); (1,3,2,3:vw1); (1,0,3,0:vw1); (1,0,3,1:vw1); (1,0,3,2:vw1); (1,0,3,3:vw1); (1,1,3,0:vw1); (1,1,3,1:vw1); (1,1,3,2:vw1); (1,1,3,3:vw1); (1,2,3,0:vw1); (1,2,3,1:vw1); (1,2,3,2:vw1); (1,2,3,3:vw1); (1,3,3,0:vw1); (1,3,3,1:vw1); (1,3,3,2:vw1); (1,3,3,3:vw1); (1,0,4,0:vw1); (1,0,4,1:vw1); (1,0,4,2:vw1); (1,0,4,3:vw1); (1,1,4,0:vw1); (1,1,4,1:vw1); (1,1,4,2:vw1); (1,1,4,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v199, BufferOOB
/* (d1,vc1,d0,vc0)=(1,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v0, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b32 v9, v7 offset:0                        // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v10, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v199, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v13, v4, s52
v_lshlrev_b32 v13, 0x2, v13                        // Bias address scaled by BPE
ds_read_b32 v15, v13 offset:0                      // load Bias
v_add_u32 v14, 1024, v13                           // add ScaleAlphaVec offset (3)
ds_read_b32 v16, v14 offset:0                      // load scaleAlpha
v_add_lshl_u32 v12, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v199, v12, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v19, v4, s52
v_lshlrev_b32 v19, 0x2, v19                        // Bias address scaled by BPE
ds_read_b32 v21, v19 offset:0                      // load Bias
v_add_u32 v20, 1024, v19                           // add ScaleAlphaVec offset (3)
ds_read_b32 v22, v20 offset:0                      // load scaleAlpha
v_add_lshl_u32 v18, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v199, v18, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v25, v4, s52
v_lshlrev_b32 v25, 0x2, v25                        // Bias address scaled by BPE
ds_read_b32 v27, v25 offset:0                      // load Bias
v_add_u32 v26, 1024, v25                           // add ScaleAlphaVec offset (3)
ds_read_b32 v28, v26 offset:0                      // load scaleAlpha
v_add_lshl_u32 v24, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v199, v24, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v31, v4, s52
v_lshlrev_b32 v31, 0x2, v31                        // Bias address scaled by BPE
ds_read_b32 v33, v31 offset:0                      // load Bias
v_add_u32 v32, 1024, v31                           // add ScaleAlphaVec offset (3)
ds_read_b32 v34, v32 offset:0                      // load scaleAlpha
v_add_lshl_u32 v30, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v199, v30, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v37, v4, s52
v_lshlrev_b32 v37, 0x2, v37                        // Bias address scaled by BPE
ds_read_b32 v39, v37 offset:0                      // load Bias
v_add_u32 v38, 1024, v37                           // add ScaleAlphaVec offset (3)
ds_read_b32 v40, v38 offset:0                      // load scaleAlpha
v_add_lshl_u32 v36, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v199, v36, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v43, v4, s52
v_lshlrev_b32 v43, 0x2, v43                        // Bias address scaled by BPE
ds_read_b32 v45, v43 offset:0                      // load Bias
v_add_u32 v44, 1024, v43                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v44 offset:0                      // load scaleAlpha
v_add_lshl_u32 v42, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v199, v42, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v4, s52
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
ds_read_b32 v51, v49 offset:0                      // load Bias
v_add_u32 v50, 1024, v49                           // add ScaleAlphaVec offset (3)
ds_read_b32 v52, v50 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v199, v48, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v55, v4, s52
v_lshlrev_b32 v55, 0x2, v55                        // Bias address scaled by BPE
ds_read_b32 v57, v55 offset:0                      // load Bias
v_add_u32 v56, 1024, v55                           // add ScaleAlphaVec offset (3)
ds_read_b32 v58, v56 offset:0                      // load scaleAlpha
v_add_lshl_u32 v54, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v199, v54, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v61, v4, s52
v_lshlrev_b32 v61, 0x2, v61                        // Bias address scaled by BPE
ds_read_b32 v63, v61 offset:0                      // load Bias
v_add_u32 v62, 1024, v61                           // add ScaleAlphaVec offset (3)
ds_read_b32 v64, v62 offset:0                      // load scaleAlpha
v_add_lshl_u32 v60, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v60, v199, v60, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s52
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
ds_read_b32 v69, v67 offset:0                      // load Bias
v_add_u32 v68, 1024, v67                           // add ScaleAlphaVec offset (3)
ds_read_b32 v70, v68 offset:0                      // load scaleAlpha
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v199, v66, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v73, v4, s52
v_lshlrev_b32 v73, 0x2, v73                        // Bias address scaled by BPE
ds_read_b32 v75, v73 offset:0                      // load Bias
v_add_u32 v74, 1024, v73                           // add ScaleAlphaVec offset (3)
ds_read_b32 v76, v74 offset:0                      // load scaleAlpha
v_add_lshl_u32 v72, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v72, v199, v72, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v79, v4, s52
v_lshlrev_b32 v79, 0x2, v79                        // Bias address scaled by BPE
ds_read_b32 v81, v79 offset:0                      // load Bias
v_add_u32 v80, 1024, v79                           // add ScaleAlphaVec offset (3)
ds_read_b32 v82, v80 offset:0                      // load scaleAlpha
v_add_lshl_u32 v78, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v78, v199, v78, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v85, v4, s52
v_lshlrev_b32 v85, 0x2, v85                        // Bias address scaled by BPE
ds_read_b32 v87, v85 offset:0                      // load Bias
v_add_u32 v86, 1024, v85                           // add ScaleAlphaVec offset (3)
ds_read_b32 v88, v86 offset:0                      // load scaleAlpha
v_add_lshl_u32 v84, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v199, v84, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v4, s52
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
ds_read_b32 v93, v91 offset:0                      // load Bias
v_add_u32 v92, 1024, v91                           // add ScaleAlphaVec offset (3)
ds_read_b32 v94, v92 offset:0                      // load scaleAlpha
v_add_lshl_u32 v90, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v199, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s52
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
ds_read_b32 v99, v97 offset:0                      // load Bias
v_add_u32 v98, 1024, v97                           // add ScaleAlphaVec offset (3)
ds_read_b32 v100, v98 offset:0                     // load scaleAlpha
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v199, v96, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v103, v0, s52
v_lshlrev_b32 v103, 0x2, v103                      // Bias address scaled by BPE
v_add_u32 v104, 1024, v103                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v102, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v102, v199, v102, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v107, v4, s52
v_lshlrev_b32 v107, 0x2, v107                      // Bias address scaled by BPE
v_add_u32 v108, 1024, v107                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v106, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v199, v106, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v111, v4, s52
v_lshlrev_b32 v111, 0x2, v111                      // Bias address scaled by BPE
v_add_u32 v112, 1024, v111                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v110, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v110, v199, v110, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v115, v4, s52
v_lshlrev_b32 v115, 0x2, v115                      // Bias address scaled by BPE
v_add_u32 v116, 1024, v115                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v114, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v114, v199, v114, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v4, s52
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v120, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v199, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v123, v4, s52
v_lshlrev_b32 v123, 0x2, v123                      // Bias address scaled by BPE
v_add_u32 v124, 1024, v123                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v122, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v122, v199, v122, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v127, v4, s52
v_lshlrev_b32 v127, 0x2, v127                      // Bias address scaled by BPE
v_add_u32 v128, 1024, v127                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v126, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v126, v199, v126, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v131, v4, s52
v_lshlrev_b32 v131, 0x2, v131                      // Bias address scaled by BPE
v_add_u32 v132, 1024, v131                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v130, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v130, v199, v130, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v135, v4, s52
v_lshlrev_b32 v135, 0x2, v135                      // Bias address scaled by BPE
v_add_u32 v136, 1024, v135                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v134, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v134, v199, v134, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v140, v4, s52
v_lshlrev_b32 v140, 0x2, v140                      // Bias address scaled by BPE
v_add_u32 v141, 1024, v140                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v138, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v199, v138, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v144, v4, s52
v_lshlrev_b32 v144, 0x2, v144                      // Bias address scaled by BPE
v_add_u32 v145, 1024, v144                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v143, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v143, v199, v143, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v148, v4, s52
v_lshlrev_b32 v148, 0x2, v148                      // Bias address scaled by BPE
v_add_u32 v149, 1024, v148                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v147, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v147, v199, v147, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v152, v4, s52
v_lshlrev_b32 v152, 0x2, v152                      // Bias address scaled by BPE
v_add_u32 v153, 1024, v152                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v151, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v151, v199, v151, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v156, v4, s52
v_lshlrev_b32 v156, 0x2, v156                      // Bias address scaled by BPE
v_add_u32 v157, 1024, v156                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v155, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v155, v199, v155, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v160, v4, s52
v_lshlrev_b32 v160, 0x2, v160                      // Bias address scaled by BPE
v_add_u32 v161, 1024, v160                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v159, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v199, v159, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v164, v4, s52
v_lshlrev_b32 v164, 0x2, v164                      // Bias address scaled by BPE
v_add_u32 v165, 1024, v164                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v163, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v163, v199, v163, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v168, v0, s52
v_lshlrev_b32 v168, 0x2, v168                      // Bias address scaled by BPE
v_add_u32 v169, 1024, v168                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v167, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v167, v199, v167, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v172, v4, s52
v_lshlrev_b32 v172, 0x2, v172                      // Bias address scaled by BPE
v_add_u32 v173, 1024, v172                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v171, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v171, v199, v171, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v176, v4, s52
v_lshlrev_b32 v176, 0x2, v176                      // Bias address scaled by BPE
v_add_u32 v177, 1024, v176                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v175, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v175, v199, v175, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v180, v4, s52
v_lshlrev_b32 v180, 0x2, v180                      // Bias address scaled by BPE
v_add_u32 v181, 1024, v180                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v179, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v199, v179, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v184, v4, s52
v_lshlrev_b32 v184, 0x2, v184                      // Bias address scaled by BPE
v_add_u32 v185, 1024, v184                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v183, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v183, v199, v183, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v188, v4, s52
v_lshlrev_b32 v188, 0x2, v188                      // Bias address scaled by BPE
v_add_u32 v189, 1024, v188                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v187, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v187, v199, v187, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v192, v4, s52
v_lshlrev_b32 v192, 0x2, v192                      // Bias address scaled by BPE
v_add_u32 v193, 1024, v192                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v191, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v191, v199, v191, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v196, v4, s52
v_lshlrev_b32 v196, 0x2, v196                      // Bias address scaled by BPE
v_add_u32 v197, 1024, v196                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v195, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v195, v199, v195, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc160         // copy acc to vreg[160]
v_accvgpr_read_b32 v[vgprValuC+17], acc161         // copy acc to vreg[161]
v_accvgpr_read_b32 v[vgprValuC+23], acc162         // copy acc to vreg[162]
v_accvgpr_read_b32 v[vgprValuC+29], acc163         // copy acc to vreg[163]
v_accvgpr_read_b32 v[vgprValuC+35], acc164         // copy acc to vreg[164]
v_accvgpr_read_b32 v[vgprValuC+41], acc165         // copy acc to vreg[165]
v_accvgpr_read_b32 v[vgprValuC+47], acc166         // copy acc to vreg[166]
v_accvgpr_read_b32 v[vgprValuC+53], acc167         // copy acc to vreg[167]
v_accvgpr_read_b32 v[vgprValuC+59], acc168         // copy acc to vreg[168]
v_accvgpr_read_b32 v[vgprValuC+65], acc169         // copy acc to vreg[169]
v_accvgpr_read_b32 v[vgprValuC+71], acc170         // copy acc to vreg[170]
v_accvgpr_read_b32 v[vgprValuC+77], acc171         // copy acc to vreg[171]
v_accvgpr_read_b32 v[vgprValuC+83], acc172         // copy acc to vreg[172]
v_accvgpr_read_b32 v[vgprValuC+89], acc173         // copy acc to vreg[173]
v_accvgpr_read_b32 v[vgprValuC+95], acc174         // copy acc to vreg[174]
v_accvgpr_read_b32 v[vgprValuC+101], acc175        // copy acc to vreg[175]
v_accvgpr_read_b32 v[vgprValuC+105], acc176        // copy acc to vreg[176]
v_accvgpr_read_b32 v[vgprValuC+109], acc177        // copy acc to vreg[177]
v_accvgpr_read_b32 v[vgprValuC+113], acc178        // copy acc to vreg[178]
v_accvgpr_read_b32 v[vgprValuC+117], acc179        // copy acc to vreg[179]
v_accvgpr_read_b32 v[vgprValuC+121], acc180        // copy acc to vreg[180]
v_accvgpr_read_b32 v[vgprValuC+125], acc181        // copy acc to vreg[181]
v_accvgpr_read_b32 v[vgprValuC+129], acc182        // copy acc to vreg[182]
v_accvgpr_read_b32 v[vgprValuC+133], acc183        // copy acc to vreg[183]
v_accvgpr_read_b32 v[vgprValuC+137], acc184        // copy acc to vreg[184]
v_accvgpr_read_b32 v[vgprValuC+142], acc185        // copy acc to vreg[185]
v_accvgpr_read_b32 v[vgprValuC+146], acc186        // copy acc to vreg[186]
v_accvgpr_read_b32 v[vgprValuC+150], acc187        // copy acc to vreg[187]
v_accvgpr_read_b32 v[vgprValuC+154], acc188        // copy acc to vreg[188]
v_accvgpr_read_b32 v[vgprValuC+158], acc189        // copy acc to vreg[189]
v_accvgpr_read_b32 v[vgprValuC+162], acc190        // copy acc to vreg[190]
v_accvgpr_read_b32 v[vgprValuC+166], acc191        // copy acc to vreg[191]
v_accvgpr_read_b32 v[vgprValuC+170], acc192        // copy acc to vreg[192]
v_accvgpr_read_b32 v[vgprValuC+174], acc193        // copy acc to vreg[193]
v_accvgpr_read_b32 v[vgprValuC+178], acc194        // copy acc to vreg[194]
v_accvgpr_read_b32 v[vgprValuC+182], acc195        // copy acc to vreg[195]
v_accvgpr_read_b32 v[vgprValuC+186], acc196        // copy acc to vreg[196]
v_accvgpr_read_b32 v[vgprValuC+190], acc197        // copy acc to vreg[197]
v_accvgpr_read_b32 v[vgprValuC+194], acc198        // copy acc to vreg[198]
v_accvgpr_read_b32 v[vgprValuC+198], acc199        // copy acc to vreg[199]

/* rC *= alpha batchElements=[(1, 0, 2, 0), (1, 0, 2, 1), (1, 0, 2, 2), (1, 0, 2, 3), (1, 1, 2, 0), (1, 1, 2, 1), (1, 1, 2, 2), (1, 1, 2, 3), (1, 2, 2, 0), (1, 2, 2, 1), (1, 2, 2, 2), (1, 2, 2, 3), (1, 3, 2, 0), (1, 3, 2, 1), (1, 3, 2, 2), (1, 3, 2, 3), (1, 0, 3, 0), (1, 0, 3, 1), (1, 0, 3, 2), (1, 0, 3, 3), (1, 1, 3, 0), (1, 1, 3, 1), (1, 1, 3, 2), (1, 1, 3, 3), (1, 2, 3, 0), (1, 2, 3, 1), (1, 2, 3, 2), (1, 2, 3, 3), (1, 3, 3, 0), (1, 3, 3, 1), (1, 3, 3, 2), (1, 3, 3, 3), (1, 0, 4, 0), (1, 0, 4, 1), (1, 0, 4, 2), (1, 0, 4, 3), (1, 1, 4, 0), (1, 1, 4, 1), (1, 1, 4, 2), (1, 1, 4, 3)] */
v_mul_f32 v[vgprValuC+11], s[sgprAlpha], v[vgprValuC+11] // *= alpha
v_mul_f32 v[vgprValuC+17], s[sgprAlpha], v[vgprValuC+17] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+121], s[sgprAlpha], v[vgprValuC+121] // *= alpha
v_mul_f32 v[vgprValuC+125], s[sgprAlpha], v[vgprValuC+125] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+133], s[sgprAlpha], v[vgprValuC+133] // *= alpha
v_mul_f32 v[vgprValuC+137], s[sgprAlpha], v[vgprValuC+137] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+146], s[sgprAlpha], v[vgprValuC+146] // *= alpha
v_mul_f32 v[vgprValuC+150], s[sgprAlpha], v[vgprValuC+150] // *= alpha
v_mul_f32 v[vgprValuC+154], s[sgprAlpha], v[vgprValuC+154] // *= alpha
v_mul_f32 v[vgprValuC+158], s[sgprAlpha], v[vgprValuC+158] // *= alpha
v_mul_f32 v[vgprValuC+162], s[sgprAlpha], v[vgprValuC+162] // *= alpha
v_mul_f32 v[vgprValuC+166], s[sgprAlpha], v[vgprValuC+166] // *= alpha
v_mul_f32 v[vgprValuC+170], s[sgprAlpha], v[vgprValuC+170] // *= alpha
v_mul_f32 v[vgprValuC+174], s[sgprAlpha], v[vgprValuC+174] // *= alpha
v_mul_f32 v[vgprValuC+178], s[sgprAlpha], v[vgprValuC+178] // *= alpha
v_mul_f32 v[vgprValuC+182], s[sgprAlpha], v[vgprValuC+182] // *= alpha
v_mul_f32 v[vgprValuC+186], s[sgprAlpha], v[vgprValuC+186] // *= alpha
v_mul_f32 v[vgprValuC+190], s[sgprAlpha], v[vgprValuC+190] // *= alpha
v_mul_f32 v[vgprValuC+194], s[sgprAlpha], v[vgprValuC+194] // *= alpha
v_mul_f32 v[vgprValuC+198], s[sgprAlpha], v[vgprValuC+198] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+11], v10, v[vgprValuC+11]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+11], v9, v[vgprValuC+11]     // C += bias
v_cvt_f16_f32 v11, v[vgprValuC+11]                 // convert C to fp16
buffer_store_short v11, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+17], v16, v[vgprValuC+17]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+17], v15, v[vgprValuC+17]    // C += bias
v_cvt_f16_f32 v17, v[vgprValuC+17]                 // convert C to fp16
buffer_store_short v17, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+23], v22, v[vgprValuC+23]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+23], v21, v[vgprValuC+23]    // C += bias
v_cvt_f16_f32 v23, v[vgprValuC+23]                 // convert C to fp16
buffer_store_short v23, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+29], v28, v[vgprValuC+29]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+29], v27, v[vgprValuC+29]    // C += bias
v_cvt_f16_f32 v29, v[vgprValuC+29]                 // convert C to fp16
buffer_store_short v29, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+35], v34, v[vgprValuC+35]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+35], v33, v[vgprValuC+35]    // C += bias
v_cvt_f16_f32 v35, v[vgprValuC+35]                 // convert C to fp16
buffer_store_short v35, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+41], v40, v[vgprValuC+41]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+41], v39, v[vgprValuC+41]    // C += bias
v_cvt_f16_f32 v41, v[vgprValuC+41]                 // convert C to fp16
buffer_store_short v41, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+53], v52, v[vgprValuC+53]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+53], v51, v[vgprValuC+53]    // C += bias
v_cvt_f16_f32 v53, v[vgprValuC+53]                 // convert C to fp16
buffer_store_short v53, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+59], v58, v[vgprValuC+59]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+59], v57, v[vgprValuC+59]    // C += bias
v_cvt_f16_f32 v59, v[vgprValuC+59]                 // convert C to fp16
buffer_store_short v59, v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+65], v64, v[vgprValuC+65]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+65], v63, v[vgprValuC+65]    // C += bias
v_cvt_f16_f32 v65, v[vgprValuC+65]                 // convert C to fp16
buffer_store_short v65, v60, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+71], v70, v[vgprValuC+71]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+71], v69, v[vgprValuC+71]    // C += bias
v_cvt_f16_f32 v71, v[vgprValuC+71]                 // convert C to fp16
buffer_store_short v71, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+77], v76, v[vgprValuC+77]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+77], v75, v[vgprValuC+77]    // C += bias
v_cvt_f16_f32 v77, v[vgprValuC+77]                 // convert C to fp16
buffer_store_short v77, v72, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+83], v82, v[vgprValuC+83]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+83], v81, v[vgprValuC+83]    // C += bias
v_cvt_f16_f32 v83, v[vgprValuC+83]                 // convert C to fp16
buffer_store_short v83, v78, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+89], v88, v[vgprValuC+89]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+89], v87, v[vgprValuC+89]    // C += bias
v_cvt_f16_f32 v89, v[vgprValuC+89]                 // convert C to fp16
buffer_store_short v89, v84, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+95], v94, v[vgprValuC+95]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+95], v93, v[vgprValuC+95]    // C += bias
v_cvt_f16_f32 v95, v[vgprValuC+95]                 // convert C to fp16
buffer_store_short v95, v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+101], v100, v[vgprValuC+101] // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+101], v99, v[vgprValuC+101]  // C += bias
v_cvt_f16_f32 v101, v[vgprValuC+101]               // convert C to fp16
buffer_store_short v101, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+105], v10, v[vgprValuC+105]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+105], v9, v[vgprValuC+105]   // C += bias
v_cvt_f16_f32 v105, v[vgprValuC+105]               // convert C to fp16
buffer_store_short v105, v102, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+109], v16, v[vgprValuC+109]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+109], v15, v[vgprValuC+109]  // C += bias
v_cvt_f16_f32 v109, v[vgprValuC+109]               // convert C to fp16
buffer_store_short v109, v106, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+113], v22, v[vgprValuC+113]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+113], v21, v[vgprValuC+113]  // C += bias
v_cvt_f16_f32 v113, v[vgprValuC+113]               // convert C to fp16
buffer_store_short v113, v110, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+117], v28, v[vgprValuC+117]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+117], v27, v[vgprValuC+117]  // C += bias
v_cvt_f16_f32 v117, v[vgprValuC+117]               // convert C to fp16
buffer_store_short v117, v114, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+121], v34, v[vgprValuC+121]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+121], v33, v[vgprValuC+121]  // C += bias
v_cvt_f16_f32 v121, v[vgprValuC+121]               // convert C to fp16
buffer_store_short v121, v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+125], v40, v[vgprValuC+125]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+125], v39, v[vgprValuC+125]  // C += bias
v_cvt_f16_f32 v125, v[vgprValuC+125]               // convert C to fp16
buffer_store_short v125, v122, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+129], v46, v[vgprValuC+129]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+129], v45, v[vgprValuC+129]  // C += bias
v_cvt_f16_f32 v129, v[vgprValuC+129]               // convert C to fp16
buffer_store_short v129, v126, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+133], v52, v[vgprValuC+133]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+133], v51, v[vgprValuC+133]  // C += bias
v_cvt_f16_f32 v133, v[vgprValuC+133]               // convert C to fp16
buffer_store_short v133, v130, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+137], v58, v[vgprValuC+137]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+137], v57, v[vgprValuC+137]  // C += bias
v_cvt_f16_f32 v137, v[vgprValuC+137]               // convert C to fp16
buffer_store_short v137, v134, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+142], v64, v[vgprValuC+142]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+142], v63, v[vgprValuC+142]  // C += bias
v_cvt_f16_f32 v142, v[vgprValuC+142]               // convert C to fp16
buffer_store_short v142, v138, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+146], v70, v[vgprValuC+146]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+146], v69, v[vgprValuC+146]  // C += bias
v_cvt_f16_f32 v146, v[vgprValuC+146]               // convert C to fp16
buffer_store_short v146, v143, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+150], v76, v[vgprValuC+150]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+150], v75, v[vgprValuC+150]  // C += bias
v_cvt_f16_f32 v150, v[vgprValuC+150]               // convert C to fp16
buffer_store_short v150, v147, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+154], v82, v[vgprValuC+154]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+154], v81, v[vgprValuC+154]  // C += bias
v_cvt_f16_f32 v154, v[vgprValuC+154]               // convert C to fp16
buffer_store_short v154, v151, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+158], v88, v[vgprValuC+158]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+158], v87, v[vgprValuC+158]  // C += bias
v_cvt_f16_f32 v158, v[vgprValuC+158]               // convert C to fp16
buffer_store_short v158, v155, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+162], v94, v[vgprValuC+162]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+162], v93, v[vgprValuC+162]  // C += bias
v_cvt_f16_f32 v162, v[vgprValuC+162]               // convert C to fp16
buffer_store_short v162, v159, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+166], v100, v[vgprValuC+166] // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+166], v99, v[vgprValuC+166]  // C += bias
v_cvt_f16_f32 v166, v[vgprValuC+166]               // convert C to fp16
buffer_store_short v166, v163, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+170], v10, v[vgprValuC+170]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+170], v9, v[vgprValuC+170]   // C += bias
v_cvt_f16_f32 v170, v[vgprValuC+170]               // convert C to fp16
buffer_store_short v170, v167, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+174], v16, v[vgprValuC+174]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+174], v15, v[vgprValuC+174]  // C += bias
v_cvt_f16_f32 v174, v[vgprValuC+174]               // convert C to fp16
buffer_store_short v174, v171, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+178], v22, v[vgprValuC+178]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+178], v21, v[vgprValuC+178]  // C += bias
v_cvt_f16_f32 v178, v[vgprValuC+178]               // convert C to fp16
buffer_store_short v178, v175, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+182], v28, v[vgprValuC+182]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+182], v27, v[vgprValuC+182]  // C += bias
v_cvt_f16_f32 v182, v[vgprValuC+182]               // convert C to fp16
buffer_store_short v182, v179, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+186], v34, v[vgprValuC+186]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+186], v33, v[vgprValuC+186]  // C += bias
v_cvt_f16_f32 v186, v[vgprValuC+186]               // convert C to fp16
buffer_store_short v186, v183, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+190], v40, v[vgprValuC+190]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+190], v39, v[vgprValuC+190]  // C += bias
v_cvt_f16_f32 v190, v[vgprValuC+190]               // convert C to fp16
buffer_store_short v190, v187, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+194], v46, v[vgprValuC+194]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+194], v45, v[vgprValuC+194]  // C += bias
v_cvt_f16_f32 v194, v[vgprValuC+194]               // convert C to fp16
buffer_store_short v194, v191, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+198], v52, v[vgprValuC+198]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+198], v51, v[vgprValuC+198]  // C += bias
v_cvt_f16_f32 v198, v[vgprValuC+198]               // convert C to fp16
buffer_store_short v198, v195, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #5 (d1,d0,vc1,vc0) = */
/*    (1,2,4,0:vw1); (1,2,4,1:vw1); (1,2,4,2:vw1); (1,2,4,3:vw1); (1,3,4,0:vw1); (1,3,4,1:vw1); (1,3,4,2:vw1); (1,3,4,3:vw1); (1,0,5,0:vw1); (1,0,5,1:vw1); (1,0,5,2:vw1); (1,0,5,3:vw1); (1,1,5,0:vw1); (1,1,5,1:vw1); (1,1,5,2:vw1); (1,1,5,3:vw1); (1,2,5,0:vw1); (1,2,5,1:vw1); (1,2,5,2:vw1); (1,2,5,3:vw1); (1,3,5,0:vw1); (1,3,5,1:vw1); (1,3,5,2:vw1); (1,3,5,3:vw1); (1,0,6,0:vw1); (1,0,6,1:vw1); (1,0,6,2:vw1); (1,0,6,3:vw1); (1,1,6,0:vw1); (1,1,6,1:vw1); (1,1,6,2:vw1); (1,1,6,3:vw1); (1,2,6,0:vw1); (1,2,6,1:vw1); (1,2,6,2:vw1); (1,2,6,3:vw1); (1,3,6,0:vw1); (1,3,6,1:vw1); (1,3,6,2:vw1); (1,3,6,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v199, BufferOOB
/* (d1,vc1,d0,vc0)=(1,4,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v4, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b32 v9, v7 offset:0                        // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v10, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v199, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v13, v4, s52
v_lshlrev_b32 v13, 0x2, v13                        // Bias address scaled by BPE
ds_read_b32 v15, v13 offset:0                      // load Bias
v_add_u32 v14, 1024, v13                           // add ScaleAlphaVec offset (3)
ds_read_b32 v16, v14 offset:0                      // load scaleAlpha
v_add_lshl_u32 v12, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v199, v12, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v19, v4, s52
v_lshlrev_b32 v19, 0x2, v19                        // Bias address scaled by BPE
ds_read_b32 v21, v19 offset:0                      // load Bias
v_add_u32 v20, 1024, v19                           // add ScaleAlphaVec offset (3)
ds_read_b32 v22, v20 offset:0                      // load scaleAlpha
v_add_lshl_u32 v18, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v199, v18, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v25, v4, s52
v_lshlrev_b32 v25, 0x2, v25                        // Bias address scaled by BPE
ds_read_b32 v27, v25 offset:0                      // load Bias
v_add_u32 v26, 1024, v25                           // add ScaleAlphaVec offset (3)
ds_read_b32 v28, v26 offset:0                      // load scaleAlpha
v_add_lshl_u32 v24, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v199, v24, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v31, v4, s52
v_lshlrev_b32 v31, 0x2, v31                        // Bias address scaled by BPE
ds_read_b32 v33, v31 offset:0                      // load Bias
v_add_u32 v32, 1024, v31                           // add ScaleAlphaVec offset (3)
ds_read_b32 v34, v32 offset:0                      // load scaleAlpha
v_add_lshl_u32 v30, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v199, v30, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v37, v4, s52
v_lshlrev_b32 v37, 0x2, v37                        // Bias address scaled by BPE
ds_read_b32 v39, v37 offset:0                      // load Bias
v_add_u32 v38, 1024, v37                           // add ScaleAlphaVec offset (3)
ds_read_b32 v40, v38 offset:0                      // load scaleAlpha
v_add_lshl_u32 v36, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v199, v36, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v43, v4, s52
v_lshlrev_b32 v43, 0x2, v43                        // Bias address scaled by BPE
ds_read_b32 v45, v43 offset:0                      // load Bias
v_add_u32 v44, 1024, v43                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v44 offset:0                      // load scaleAlpha
v_add_lshl_u32 v42, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v199, v42, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v4, s52
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
ds_read_b32 v51, v49 offset:0                      // load Bias
v_add_u32 v50, 1024, v49                           // add ScaleAlphaVec offset (3)
ds_read_b32 v52, v50 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v199, v48, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v55, v0, s52
v_lshlrev_b32 v55, 0x2, v55                        // Bias address scaled by BPE
ds_read_b32 v57, v55 offset:0                      // load Bias
v_add_u32 v56, 1024, v55                           // add ScaleAlphaVec offset (3)
ds_read_b32 v58, v56 offset:0                      // load scaleAlpha
v_add_lshl_u32 v54, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v199, v54, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v61, v4, s52
v_lshlrev_b32 v61, 0x2, v61                        // Bias address scaled by BPE
ds_read_b32 v63, v61 offset:0                      // load Bias
v_add_u32 v62, 1024, v61                           // add ScaleAlphaVec offset (3)
ds_read_b32 v64, v62 offset:0                      // load scaleAlpha
v_add_lshl_u32 v60, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v60, v199, v60, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s52
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
ds_read_b32 v69, v67 offset:0                      // load Bias
v_add_u32 v68, 1024, v67                           // add ScaleAlphaVec offset (3)
ds_read_b32 v70, v68 offset:0                      // load scaleAlpha
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v199, v66, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v73, v4, s52
v_lshlrev_b32 v73, 0x2, v73                        // Bias address scaled by BPE
ds_read_b32 v75, v73 offset:0                      // load Bias
v_add_u32 v74, 1024, v73                           // add ScaleAlphaVec offset (3)
ds_read_b32 v76, v74 offset:0                      // load scaleAlpha
v_add_lshl_u32 v72, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v72, v199, v72, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v79, v4, s52
v_lshlrev_b32 v79, 0x2, v79                        // Bias address scaled by BPE
ds_read_b32 v81, v79 offset:0                      // load Bias
v_add_u32 v80, 1024, v79                           // add ScaleAlphaVec offset (3)
ds_read_b32 v82, v80 offset:0                      // load scaleAlpha
v_add_lshl_u32 v78, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v78, v199, v78, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v85, v4, s52
v_lshlrev_b32 v85, 0x2, v85                        // Bias address scaled by BPE
ds_read_b32 v87, v85 offset:0                      // load Bias
v_add_u32 v86, 1024, v85                           // add ScaleAlphaVec offset (3)
ds_read_b32 v88, v86 offset:0                      // load scaleAlpha
v_add_lshl_u32 v84, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v199, v84, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v4, s52
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
ds_read_b32 v93, v91 offset:0                      // load Bias
v_add_u32 v92, 1024, v91                           // add ScaleAlphaVec offset (3)
ds_read_b32 v94, v92 offset:0                      // load scaleAlpha
v_add_lshl_u32 v90, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v199, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s52
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
ds_read_b32 v99, v97 offset:0                      // load Bias
v_add_u32 v98, 1024, v97                           // add ScaleAlphaVec offset (3)
ds_read_b32 v100, v98 offset:0                     // load scaleAlpha
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v199, v96, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v103, v4, s52
v_lshlrev_b32 v103, 0x2, v103                      // Bias address scaled by BPE
v_add_u32 v104, 1024, v103                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v102, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v102, v199, v102, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v107, v4, s52
v_lshlrev_b32 v107, 0x2, v107                      // Bias address scaled by BPE
v_add_u32 v108, 1024, v107                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v106, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v199, v106, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v111, v4, s52
v_lshlrev_b32 v111, 0x2, v111                      // Bias address scaled by BPE
v_add_u32 v112, 1024, v111                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v110, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v110, v199, v110, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v115, v4, s52
v_lshlrev_b32 v115, 0x2, v115                      // Bias address scaled by BPE
v_add_u32 v116, 1024, v115                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v114, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v114, v199, v114, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v4, s52
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v120, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v199, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v123, v4, s52
v_lshlrev_b32 v123, 0x2, v123                      // Bias address scaled by BPE
v_add_u32 v124, 1024, v123                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v122, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v122, v199, v122, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v127, v4, s52
v_lshlrev_b32 v127, 0x2, v127                      // Bias address scaled by BPE
v_add_u32 v128, 1024, v127                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v126, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v126, v199, v126, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v131, v4, s52
v_lshlrev_b32 v131, 0x2, v131                      // Bias address scaled by BPE
v_add_u32 v132, 1024, v131                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v130, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v130, v199, v130, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v135, v0, s52
v_lshlrev_b32 v135, 0x2, v135                      // Bias address scaled by BPE
v_add_u32 v136, 1024, v135                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v134, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v134, v199, v134, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v140, v4, s52
v_lshlrev_b32 v140, 0x2, v140                      // Bias address scaled by BPE
v_add_u32 v141, 1024, v140                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v138, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v199, v138, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v144, v4, s52
v_lshlrev_b32 v144, 0x2, v144                      // Bias address scaled by BPE
v_add_u32 v145, 1024, v144                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v143, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v143, v199, v143, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v148, v4, s52
v_lshlrev_b32 v148, 0x2, v148                      // Bias address scaled by BPE
v_add_u32 v149, 1024, v148                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v147, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v147, v199, v147, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v152, v4, s52
v_lshlrev_b32 v152, 0x2, v152                      // Bias address scaled by BPE
v_add_u32 v153, 1024, v152                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v151, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v151, v199, v151, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v156, v4, s52
v_lshlrev_b32 v156, 0x2, v156                      // Bias address scaled by BPE
v_add_u32 v157, 1024, v156                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v155, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v155, v199, v155, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v160, v4, s52
v_lshlrev_b32 v160, 0x2, v160                      // Bias address scaled by BPE
v_add_u32 v161, 1024, v160                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v159, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v199, v159, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v164, v4, s52
v_lshlrev_b32 v164, 0x2, v164                      // Bias address scaled by BPE
v_add_u32 v165, 1024, v164                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v163, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v163, v199, v163, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v168, v4, s52
v_lshlrev_b32 v168, 0x2, v168                      // Bias address scaled by BPE
v_add_u32 v169, 1024, v168                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v167, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v167, v199, v167, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v172, v4, s52
v_lshlrev_b32 v172, 0x2, v172                      // Bias address scaled by BPE
v_add_u32 v173, 1024, v172                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v171, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v171, v199, v171, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v176, v4, s52
v_lshlrev_b32 v176, 0x2, v176                      // Bias address scaled by BPE
v_add_u32 v177, 1024, v176                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v175, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v175, v199, v175, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v180, v4, s52
v_lshlrev_b32 v180, 0x2, v180                      // Bias address scaled by BPE
v_add_u32 v181, 1024, v180                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v179, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v199, v179, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v184, v4, s52
v_lshlrev_b32 v184, 0x2, v184                      // Bias address scaled by BPE
v_add_u32 v185, 1024, v184                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v183, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v183, v199, v183, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v188, v4, s52
v_lshlrev_b32 v188, 0x2, v188                      // Bias address scaled by BPE
v_add_u32 v189, 1024, v188                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v187, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v187, v199, v187, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v192, v4, s52
v_lshlrev_b32 v192, 0x2, v192                      // Bias address scaled by BPE
v_add_u32 v193, 1024, v192                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v191, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v191, v199, v191, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v196, v4, s52
v_lshlrev_b32 v196, 0x2, v196                      // Bias address scaled by BPE
v_add_u32 v197, 1024, v196                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v195, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v195, v199, v195, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc200         // copy acc to vreg[200]
v_accvgpr_read_b32 v[vgprValuC+17], acc201         // copy acc to vreg[201]
v_accvgpr_read_b32 v[vgprValuC+23], acc202         // copy acc to vreg[202]
v_accvgpr_read_b32 v[vgprValuC+29], acc203         // copy acc to vreg[203]
v_accvgpr_read_b32 v[vgprValuC+35], acc204         // copy acc to vreg[204]
v_accvgpr_read_b32 v[vgprValuC+41], acc205         // copy acc to vreg[205]
v_accvgpr_read_b32 v[vgprValuC+47], acc206         // copy acc to vreg[206]
v_accvgpr_read_b32 v[vgprValuC+53], acc207         // copy acc to vreg[207]
v_accvgpr_read_b32 v[vgprValuC+59], acc208         // copy acc to vreg[208]
v_accvgpr_read_b32 v[vgprValuC+65], acc209         // copy acc to vreg[209]
v_accvgpr_read_b32 v[vgprValuC+71], acc210         // copy acc to vreg[210]
v_accvgpr_read_b32 v[vgprValuC+77], acc211         // copy acc to vreg[211]
v_accvgpr_read_b32 v[vgprValuC+83], acc212         // copy acc to vreg[212]
v_accvgpr_read_b32 v[vgprValuC+89], acc213         // copy acc to vreg[213]
v_accvgpr_read_b32 v[vgprValuC+95], acc214         // copy acc to vreg[214]
v_accvgpr_read_b32 v[vgprValuC+101], acc215        // copy acc to vreg[215]
v_accvgpr_read_b32 v[vgprValuC+105], acc216        // copy acc to vreg[216]
v_accvgpr_read_b32 v[vgprValuC+109], acc217        // copy acc to vreg[217]
v_accvgpr_read_b32 v[vgprValuC+113], acc218        // copy acc to vreg[218]
v_accvgpr_read_b32 v[vgprValuC+117], acc219        // copy acc to vreg[219]
v_accvgpr_read_b32 v[vgprValuC+121], acc220        // copy acc to vreg[220]
v_accvgpr_read_b32 v[vgprValuC+125], acc221        // copy acc to vreg[221]
v_accvgpr_read_b32 v[vgprValuC+129], acc222        // copy acc to vreg[222]
v_accvgpr_read_b32 v[vgprValuC+133], acc223        // copy acc to vreg[223]
v_accvgpr_read_b32 v[vgprValuC+137], acc224        // copy acc to vreg[224]
v_accvgpr_read_b32 v[vgprValuC+142], acc225        // copy acc to vreg[225]
v_accvgpr_read_b32 v[vgprValuC+146], acc226        // copy acc to vreg[226]
v_accvgpr_read_b32 v[vgprValuC+150], acc227        // copy acc to vreg[227]
v_accvgpr_read_b32 v[vgprValuC+154], acc228        // copy acc to vreg[228]
v_accvgpr_read_b32 v[vgprValuC+158], acc229        // copy acc to vreg[229]
v_accvgpr_read_b32 v[vgprValuC+162], acc230        // copy acc to vreg[230]
v_accvgpr_read_b32 v[vgprValuC+166], acc231        // copy acc to vreg[231]
v_accvgpr_read_b32 v[vgprValuC+170], acc232        // copy acc to vreg[232]
v_accvgpr_read_b32 v[vgprValuC+174], acc233        // copy acc to vreg[233]
v_accvgpr_read_b32 v[vgprValuC+178], acc234        // copy acc to vreg[234]
v_accvgpr_read_b32 v[vgprValuC+182], acc235        // copy acc to vreg[235]
v_accvgpr_read_b32 v[vgprValuC+186], acc236        // copy acc to vreg[236]
v_accvgpr_read_b32 v[vgprValuC+190], acc237        // copy acc to vreg[237]
v_accvgpr_read_b32 v[vgprValuC+194], acc238        // copy acc to vreg[238]
v_accvgpr_read_b32 v[vgprValuC+198], acc239        // copy acc to vreg[239]

/* rC *= alpha batchElements=[(1, 2, 4, 0), (1, 2, 4, 1), (1, 2, 4, 2), (1, 2, 4, 3), (1, 3, 4, 0), (1, 3, 4, 1), (1, 3, 4, 2), (1, 3, 4, 3), (1, 0, 5, 0), (1, 0, 5, 1), (1, 0, 5, 2), (1, 0, 5, 3), (1, 1, 5, 0), (1, 1, 5, 1), (1, 1, 5, 2), (1, 1, 5, 3), (1, 2, 5, 0), (1, 2, 5, 1), (1, 2, 5, 2), (1, 2, 5, 3), (1, 3, 5, 0), (1, 3, 5, 1), (1, 3, 5, 2), (1, 3, 5, 3), (1, 0, 6, 0), (1, 0, 6, 1), (1, 0, 6, 2), (1, 0, 6, 3), (1, 1, 6, 0), (1, 1, 6, 1), (1, 1, 6, 2), (1, 1, 6, 3), (1, 2, 6, 0), (1, 2, 6, 1), (1, 2, 6, 2), (1, 2, 6, 3), (1, 3, 6, 0), (1, 3, 6, 1), (1, 3, 6, 2), (1, 3, 6, 3)] */
v_mul_f32 v[vgprValuC+11], s[sgprAlpha], v[vgprValuC+11] // *= alpha
v_mul_f32 v[vgprValuC+17], s[sgprAlpha], v[vgprValuC+17] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+121], s[sgprAlpha], v[vgprValuC+121] // *= alpha
v_mul_f32 v[vgprValuC+125], s[sgprAlpha], v[vgprValuC+125] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+133], s[sgprAlpha], v[vgprValuC+133] // *= alpha
v_mul_f32 v[vgprValuC+137], s[sgprAlpha], v[vgprValuC+137] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+146], s[sgprAlpha], v[vgprValuC+146] // *= alpha
v_mul_f32 v[vgprValuC+150], s[sgprAlpha], v[vgprValuC+150] // *= alpha
v_mul_f32 v[vgprValuC+154], s[sgprAlpha], v[vgprValuC+154] // *= alpha
v_mul_f32 v[vgprValuC+158], s[sgprAlpha], v[vgprValuC+158] // *= alpha
v_mul_f32 v[vgprValuC+162], s[sgprAlpha], v[vgprValuC+162] // *= alpha
v_mul_f32 v[vgprValuC+166], s[sgprAlpha], v[vgprValuC+166] // *= alpha
v_mul_f32 v[vgprValuC+170], s[sgprAlpha], v[vgprValuC+170] // *= alpha
v_mul_f32 v[vgprValuC+174], s[sgprAlpha], v[vgprValuC+174] // *= alpha
v_mul_f32 v[vgprValuC+178], s[sgprAlpha], v[vgprValuC+178] // *= alpha
v_mul_f32 v[vgprValuC+182], s[sgprAlpha], v[vgprValuC+182] // *= alpha
v_mul_f32 v[vgprValuC+186], s[sgprAlpha], v[vgprValuC+186] // *= alpha
v_mul_f32 v[vgprValuC+190], s[sgprAlpha], v[vgprValuC+190] // *= alpha
v_mul_f32 v[vgprValuC+194], s[sgprAlpha], v[vgprValuC+194] // *= alpha
v_mul_f32 v[vgprValuC+198], s[sgprAlpha], v[vgprValuC+198] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+11], v10, v[vgprValuC+11]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+11], v9, v[vgprValuC+11]     // C += bias
v_cvt_f16_f32 v11, v[vgprValuC+11]                 // convert C to fp16
buffer_store_short v11, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+17], v16, v[vgprValuC+17]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+17], v15, v[vgprValuC+17]    // C += bias
v_cvt_f16_f32 v17, v[vgprValuC+17]                 // convert C to fp16
buffer_store_short v17, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+23], v22, v[vgprValuC+23]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+23], v21, v[vgprValuC+23]    // C += bias
v_cvt_f16_f32 v23, v[vgprValuC+23]                 // convert C to fp16
buffer_store_short v23, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+29], v28, v[vgprValuC+29]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+29], v27, v[vgprValuC+29]    // C += bias
v_cvt_f16_f32 v29, v[vgprValuC+29]                 // convert C to fp16
buffer_store_short v29, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+35], v34, v[vgprValuC+35]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+35], v33, v[vgprValuC+35]    // C += bias
v_cvt_f16_f32 v35, v[vgprValuC+35]                 // convert C to fp16
buffer_store_short v35, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+41], v40, v[vgprValuC+41]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+41], v39, v[vgprValuC+41]    // C += bias
v_cvt_f16_f32 v41, v[vgprValuC+41]                 // convert C to fp16
buffer_store_short v41, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+53], v52, v[vgprValuC+53]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+53], v51, v[vgprValuC+53]    // C += bias
v_cvt_f16_f32 v53, v[vgprValuC+53]                 // convert C to fp16
buffer_store_short v53, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+59], v58, v[vgprValuC+59]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+59], v57, v[vgprValuC+59]    // C += bias
v_cvt_f16_f32 v59, v[vgprValuC+59]                 // convert C to fp16
buffer_store_short v59, v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+65], v64, v[vgprValuC+65]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+65], v63, v[vgprValuC+65]    // C += bias
v_cvt_f16_f32 v65, v[vgprValuC+65]                 // convert C to fp16
buffer_store_short v65, v60, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+71], v70, v[vgprValuC+71]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+71], v69, v[vgprValuC+71]    // C += bias
v_cvt_f16_f32 v71, v[vgprValuC+71]                 // convert C to fp16
buffer_store_short v71, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+77], v76, v[vgprValuC+77]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+77], v75, v[vgprValuC+77]    // C += bias
v_cvt_f16_f32 v77, v[vgprValuC+77]                 // convert C to fp16
buffer_store_short v77, v72, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+83], v82, v[vgprValuC+83]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+83], v81, v[vgprValuC+83]    // C += bias
v_cvt_f16_f32 v83, v[vgprValuC+83]                 // convert C to fp16
buffer_store_short v83, v78, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+89], v88, v[vgprValuC+89]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+89], v87, v[vgprValuC+89]    // C += bias
v_cvt_f16_f32 v89, v[vgprValuC+89]                 // convert C to fp16
buffer_store_short v89, v84, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+95], v94, v[vgprValuC+95]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+95], v93, v[vgprValuC+95]    // C += bias
v_cvt_f16_f32 v95, v[vgprValuC+95]                 // convert C to fp16
buffer_store_short v95, v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+101], v100, v[vgprValuC+101] // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+101], v99, v[vgprValuC+101]  // C += bias
v_cvt_f16_f32 v101, v[vgprValuC+101]               // convert C to fp16
buffer_store_short v101, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+105], v10, v[vgprValuC+105]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+105], v9, v[vgprValuC+105]   // C += bias
v_cvt_f16_f32 v105, v[vgprValuC+105]               // convert C to fp16
buffer_store_short v105, v102, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+109], v16, v[vgprValuC+109]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+109], v15, v[vgprValuC+109]  // C += bias
v_cvt_f16_f32 v109, v[vgprValuC+109]               // convert C to fp16
buffer_store_short v109, v106, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+113], v22, v[vgprValuC+113]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+113], v21, v[vgprValuC+113]  // C += bias
v_cvt_f16_f32 v113, v[vgprValuC+113]               // convert C to fp16
buffer_store_short v113, v110, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+117], v28, v[vgprValuC+117]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+117], v27, v[vgprValuC+117]  // C += bias
v_cvt_f16_f32 v117, v[vgprValuC+117]               // convert C to fp16
buffer_store_short v117, v114, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+121], v34, v[vgprValuC+121]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+121], v33, v[vgprValuC+121]  // C += bias
v_cvt_f16_f32 v121, v[vgprValuC+121]               // convert C to fp16
buffer_store_short v121, v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+125], v40, v[vgprValuC+125]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+125], v39, v[vgprValuC+125]  // C += bias
v_cvt_f16_f32 v125, v[vgprValuC+125]               // convert C to fp16
buffer_store_short v125, v122, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+129], v46, v[vgprValuC+129]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+129], v45, v[vgprValuC+129]  // C += bias
v_cvt_f16_f32 v129, v[vgprValuC+129]               // convert C to fp16
buffer_store_short v129, v126, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+133], v52, v[vgprValuC+133]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+133], v51, v[vgprValuC+133]  // C += bias
v_cvt_f16_f32 v133, v[vgprValuC+133]               // convert C to fp16
buffer_store_short v133, v130, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+137], v58, v[vgprValuC+137]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+137], v57, v[vgprValuC+137]  // C += bias
v_cvt_f16_f32 v137, v[vgprValuC+137]               // convert C to fp16
buffer_store_short v137, v134, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+142], v64, v[vgprValuC+142]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+142], v63, v[vgprValuC+142]  // C += bias
v_cvt_f16_f32 v142, v[vgprValuC+142]               // convert C to fp16
buffer_store_short v142, v138, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+146], v70, v[vgprValuC+146]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+146], v69, v[vgprValuC+146]  // C += bias
v_cvt_f16_f32 v146, v[vgprValuC+146]               // convert C to fp16
buffer_store_short v146, v143, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+150], v76, v[vgprValuC+150]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+150], v75, v[vgprValuC+150]  // C += bias
v_cvt_f16_f32 v150, v[vgprValuC+150]               // convert C to fp16
buffer_store_short v150, v147, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+154], v82, v[vgprValuC+154]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+154], v81, v[vgprValuC+154]  // C += bias
v_cvt_f16_f32 v154, v[vgprValuC+154]               // convert C to fp16
buffer_store_short v154, v151, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+158], v88, v[vgprValuC+158]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+158], v87, v[vgprValuC+158]  // C += bias
v_cvt_f16_f32 v158, v[vgprValuC+158]               // convert C to fp16
buffer_store_short v158, v155, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+162], v94, v[vgprValuC+162]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+162], v93, v[vgprValuC+162]  // C += bias
v_cvt_f16_f32 v162, v[vgprValuC+162]               // convert C to fp16
buffer_store_short v162, v159, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+166], v100, v[vgprValuC+166] // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+166], v99, v[vgprValuC+166]  // C += bias
v_cvt_f16_f32 v166, v[vgprValuC+166]               // convert C to fp16
buffer_store_short v166, v163, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+170], v10, v[vgprValuC+170]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+170], v9, v[vgprValuC+170]   // C += bias
v_cvt_f16_f32 v170, v[vgprValuC+170]               // convert C to fp16
buffer_store_short v170, v167, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+174], v16, v[vgprValuC+174]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+174], v15, v[vgprValuC+174]  // C += bias
v_cvt_f16_f32 v174, v[vgprValuC+174]               // convert C to fp16
buffer_store_short v174, v171, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+178], v22, v[vgprValuC+178]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+178], v21, v[vgprValuC+178]  // C += bias
v_cvt_f16_f32 v178, v[vgprValuC+178]               // convert C to fp16
buffer_store_short v178, v175, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+182], v28, v[vgprValuC+182]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+182], v27, v[vgprValuC+182]  // C += bias
v_cvt_f16_f32 v182, v[vgprValuC+182]               // convert C to fp16
buffer_store_short v182, v179, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+186], v34, v[vgprValuC+186]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+186], v33, v[vgprValuC+186]  // C += bias
v_cvt_f16_f32 v186, v[vgprValuC+186]               // convert C to fp16
buffer_store_short v186, v183, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+190], v40, v[vgprValuC+190]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+190], v39, v[vgprValuC+190]  // C += bias
v_cvt_f16_f32 v190, v[vgprValuC+190]               // convert C to fp16
buffer_store_short v190, v187, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+194], v46, v[vgprValuC+194]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+194], v45, v[vgprValuC+194]  // C += bias
v_cvt_f16_f32 v194, v[vgprValuC+194]               // convert C to fp16
buffer_store_short v194, v191, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+198], v52, v[vgprValuC+198]  // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+198], v51, v[vgprValuC+198]  // C += bias
v_cvt_f16_f32 v198, v[vgprValuC+198]               // convert C to fp16
buffer_store_short v198, v195, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Edge Batch #6 (d1,d0,vc1,vc0) = */
/*    (1,0,7,0:vw1); (1,0,7,1:vw1); (1,0,7,2:vw1); (1,0,7,3:vw1); (1,1,7,0:vw1); (1,1,7,1:vw1); (1,1,7,2:vw1); (1,1,7,3:vw1); (1,2,7,0:vw1); (1,2,7,1:vw1); (1,2,7,2:vw1); (1,2,7,3:vw1); (1,3,7,0:vw1); (1,3,7,1:vw1); (1,3,7,2:vw1); (1,3,7,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v102, BufferOOB
/* (d1,vc1,d0,vc0)=(1,7,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v0, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b32 v9, v7 offset:0                        // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v10, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v102, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v13, v4, s52
v_lshlrev_b32 v13, 0x2, v13                        // Bias address scaled by BPE
ds_read_b32 v15, v13 offset:0                      // load Bias
v_add_u32 v14, 1024, v13                           // add ScaleAlphaVec offset (3)
ds_read_b32 v16, v14 offset:0                      // load scaleAlpha
v_add_lshl_u32 v12, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v102, v12, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v19, v4, s52
v_lshlrev_b32 v19, 0x2, v19                        // Bias address scaled by BPE
ds_read_b32 v21, v19 offset:0                      // load Bias
v_add_u32 v20, 1024, v19                           // add ScaleAlphaVec offset (3)
ds_read_b32 v22, v20 offset:0                      // load scaleAlpha
v_add_lshl_u32 v18, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v102, v18, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v25, v4, s52
v_lshlrev_b32 v25, 0x2, v25                        // Bias address scaled by BPE
ds_read_b32 v27, v25 offset:0                      // load Bias
v_add_u32 v26, 1024, v25                           // add ScaleAlphaVec offset (3)
ds_read_b32 v28, v26 offset:0                      // load scaleAlpha
v_add_lshl_u32 v24, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v102, v24, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v31, v4, s52
v_lshlrev_b32 v31, 0x2, v31                        // Bias address scaled by BPE
ds_read_b32 v33, v31 offset:0                      // load Bias
v_add_u32 v32, 1024, v31                           // add ScaleAlphaVec offset (3)
ds_read_b32 v34, v32 offset:0                      // load scaleAlpha
v_add_lshl_u32 v30, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v102, v30, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v37, v4, s52
v_lshlrev_b32 v37, 0x2, v37                        // Bias address scaled by BPE
ds_read_b32 v39, v37 offset:0                      // load Bias
v_add_u32 v38, 1024, v37                           // add ScaleAlphaVec offset (3)
ds_read_b32 v40, v38 offset:0                      // load scaleAlpha
v_add_lshl_u32 v36, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v102, v36, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v43, v4, s52
v_lshlrev_b32 v43, 0x2, v43                        // Bias address scaled by BPE
ds_read_b32 v45, v43 offset:0                      // load Bias
v_add_u32 v44, 1024, v43                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v44 offset:0                      // load scaleAlpha
v_add_lshl_u32 v42, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v102, v42, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v4, s52
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
ds_read_b32 v51, v49 offset:0                      // load Bias
v_add_u32 v50, 1024, v49                           // add ScaleAlphaVec offset (3)
ds_read_b32 v52, v50 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v102, v48, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v55, v4, s52
v_lshlrev_b32 v55, 0x2, v55                        // Bias address scaled by BPE
ds_read_b32 v57, v55 offset:0                      // load Bias
v_add_u32 v56, 1024, v55                           // add ScaleAlphaVec offset (3)
ds_read_b32 v58, v56 offset:0                      // load scaleAlpha
v_add_lshl_u32 v54, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v54, v102, v54, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v61, v4, s52
v_lshlrev_b32 v61, 0x2, v61                        // Bias address scaled by BPE
ds_read_b32 v63, v61 offset:0                      // load Bias
v_add_u32 v62, 1024, v61                           // add ScaleAlphaVec offset (3)
ds_read_b32 v64, v62 offset:0                      // load scaleAlpha
v_add_lshl_u32 v60, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v60, v102, v60, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s52
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
ds_read_b32 v69, v67 offset:0                      // load Bias
v_add_u32 v68, 1024, v67                           // add ScaleAlphaVec offset (3)
ds_read_b32 v70, v68 offset:0                      // load scaleAlpha
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v102, v66, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v73, v4, s52
v_lshlrev_b32 v73, 0x2, v73                        // Bias address scaled by BPE
ds_read_b32 v75, v73 offset:0                      // load Bias
v_add_u32 v74, 1024, v73                           // add ScaleAlphaVec offset (3)
ds_read_b32 v76, v74 offset:0                      // load scaleAlpha
v_add_lshl_u32 v72, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v72, v102, v72, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v79, v4, s52
v_lshlrev_b32 v79, 0x2, v79                        // Bias address scaled by BPE
ds_read_b32 v81, v79 offset:0                      // load Bias
v_add_u32 v80, 1024, v79                           // add ScaleAlphaVec offset (3)
ds_read_b32 v82, v80 offset:0                      // load scaleAlpha
v_add_lshl_u32 v78, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v78, v102, v78, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v85, v4, s52
v_lshlrev_b32 v85, 0x2, v85                        // Bias address scaled by BPE
ds_read_b32 v87, v85 offset:0                      // load Bias
v_add_u32 v86, 1024, v85                           // add ScaleAlphaVec offset (3)
ds_read_b32 v88, v86 offset:0                      // load scaleAlpha
v_add_lshl_u32 v84, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v102, v84, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v4, s52
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
ds_read_b32 v93, v91 offset:0                      // load Bias
v_add_u32 v92, 1024, v91                           // add ScaleAlphaVec offset (3)
ds_read_b32 v94, v92 offset:0                      // load scaleAlpha
v_add_lshl_u32 v90, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v102, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s52
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
ds_read_b32 v99, v97 offset:0                      // load Bias
v_add_u32 v98, 1024, v97                           // add ScaleAlphaVec offset (3)
ds_read_b32 v100, v98 offset:0                     // load scaleAlpha
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v102, v96, s[56:57]             // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc240         // copy acc to vreg[240]
v_accvgpr_read_b32 v[vgprValuC+17], acc241         // copy acc to vreg[241]
v_accvgpr_read_b32 v[vgprValuC+23], acc242         // copy acc to vreg[242]
v_accvgpr_read_b32 v[vgprValuC+29], acc243         // copy acc to vreg[243]
v_accvgpr_read_b32 v[vgprValuC+35], acc244         // copy acc to vreg[244]
v_accvgpr_read_b32 v[vgprValuC+41], acc245         // copy acc to vreg[245]
v_accvgpr_read_b32 v[vgprValuC+47], acc246         // copy acc to vreg[246]
v_accvgpr_read_b32 v[vgprValuC+53], acc247         // copy acc to vreg[247]
v_accvgpr_read_b32 v[vgprValuC+59], acc248         // copy acc to vreg[248]
v_accvgpr_read_b32 v[vgprValuC+65], acc249         // copy acc to vreg[249]
v_accvgpr_read_b32 v[vgprValuC+71], acc250         // copy acc to vreg[250]
v_accvgpr_read_b32 v[vgprValuC+77], acc251         // copy acc to vreg[251]
v_accvgpr_read_b32 v[vgprValuC+83], acc252         // copy acc to vreg[252]
v_accvgpr_read_b32 v[vgprValuC+89], acc253         // copy acc to vreg[253]
v_accvgpr_read_b32 v[vgprValuC+95], acc254         // copy acc to vreg[254]
v_accvgpr_read_b32 v[vgprValuC+101], acc255        // copy acc to vreg[255]

/* rC *= alpha batchElements=[(1, 0, 7, 0), (1, 0, 7, 1), (1, 0, 7, 2), (1, 0, 7, 3), (1, 1, 7, 0), (1, 1, 7, 1), (1, 1, 7, 2), (1, 1, 7, 3), (1, 2, 7, 0), (1, 2, 7, 1), (1, 2, 7, 2), (1, 2, 7, 3), (1, 3, 7, 0), (1, 3, 7, 1), (1, 3, 7, 2), (1, 3, 7, 3)] */
v_mul_f32 v[vgprValuC+11], s[sgprAlpha], v[vgprValuC+11] // *= alpha
v_mul_f32 v[vgprValuC+17], s[sgprAlpha], v[vgprValuC+17] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+11], v10, v[vgprValuC+11]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+11], v9, v[vgprValuC+11]     // C += bias
v_cvt_f16_f32 v11, v[vgprValuC+11]                 // convert C to fp16
buffer_store_short v11, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+17], v16, v[vgprValuC+17]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+17], v15, v[vgprValuC+17]    // C += bias
v_cvt_f16_f32 v17, v[vgprValuC+17]                 // convert C to fp16
buffer_store_short v17, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+23], v22, v[vgprValuC+23]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+23], v21, v[vgprValuC+23]    // C += bias
v_cvt_f16_f32 v23, v[vgprValuC+23]                 // convert C to fp16
buffer_store_short v23, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+29], v28, v[vgprValuC+29]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+29], v27, v[vgprValuC+29]    // C += bias
v_cvt_f16_f32 v29, v[vgprValuC+29]                 // convert C to fp16
buffer_store_short v29, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+35], v34, v[vgprValuC+35]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+35], v33, v[vgprValuC+35]    // C += bias
v_cvt_f16_f32 v35, v[vgprValuC+35]                 // convert C to fp16
buffer_store_short v35, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+41], v40, v[vgprValuC+41]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+41], v39, v[vgprValuC+41]    // C += bias
v_cvt_f16_f32 v41, v[vgprValuC+41]                 // convert C to fp16
buffer_store_short v41, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+53], v52, v[vgprValuC+53]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+53], v51, v[vgprValuC+53]    // C += bias
v_cvt_f16_f32 v53, v[vgprValuC+53]                 // convert C to fp16
buffer_store_short v53, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+59], v58, v[vgprValuC+59]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+59], v57, v[vgprValuC+59]    // C += bias
v_cvt_f16_f32 v59, v[vgprValuC+59]                 // convert C to fp16
buffer_store_short v59, v54, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+65], v64, v[vgprValuC+65]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+65], v63, v[vgprValuC+65]    // C += bias
v_cvt_f16_f32 v65, v[vgprValuC+65]                 // convert C to fp16
buffer_store_short v65, v60, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+71], v70, v[vgprValuC+71]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+71], v69, v[vgprValuC+71]    // C += bias
v_cvt_f16_f32 v71, v[vgprValuC+71]                 // convert C to fp16
buffer_store_short v71, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+77], v76, v[vgprValuC+77]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+77], v75, v[vgprValuC+77]    // C += bias
v_cvt_f16_f32 v77, v[vgprValuC+77]                 // convert C to fp16
buffer_store_short v77, v72, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+83], v82, v[vgprValuC+83]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+83], v81, v[vgprValuC+83]    // C += bias
v_cvt_f16_f32 v83, v[vgprValuC+83]                 // convert C to fp16
buffer_store_short v83, v78, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+89], v88, v[vgprValuC+89]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+89], v87, v[vgprValuC+89]    // C += bias
v_cvt_f16_f32 v89, v[vgprValuC+89]                 // convert C to fp16
buffer_store_short v89, v84, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+95], v94, v[vgprValuC+95]    // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+95], v93, v[vgprValuC+95]    // C += bias
v_cvt_f16_f32 v95, v[vgprValuC+95]                 // convert C to fp16
buffer_store_short v95, v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+101], v100, v[vgprValuC+101] // *= ScaleAlphaVecVMul
v_add_f32 v[vgprValuC+101], v99, v[vgprValuC+101]  // C += bias
v_cvt_f16_f32 v101, v[vgprValuC+101]               // convert C to fp16
buffer_store_short v101, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_2                            // jump to end
label_GW_Beta_2:
s_and_b32 s40, 255, s[sgprSizeI]                   // s40 = s[sgprSizeI] % 256
s_add_u32 s41, -0x1, s[sgprNumWorkGroups0]
s_cmp_ge_u32 s[sgprWorkGroup0], s41                // wg0 >= nwg0-1 ?
s_cselect_b32 s40, s40, 0                          // set rMT0
s_cmpk_gt_u32 s40, 0x0                             // rMT0 > 0
s_cbranch_scc1 label_GW_B1_E1_M                    // jump if edges required
s_and_b32 s40, 255, s[sgprSizeJ]                   // s40 = s[sgprSizeJ] % 256
s_add_u32 s41, -0x1, s[sgprNumWorkGroups1]
s_cmp_ge_u32 s[sgprWorkGroup1], s41                // wg1 >= nwg1-1
s_cselect_b32 s40, s40, 0                          // set rMT1
s_cmpk_gt_u32 s40, 0x0                             // rMT1 > 0
s_cbranch_scc1 label_GW_B1_E1_N                    // jump if edges required
label_GW_B1_E0:

/* edge=0, allocate 2 sgpr. perBatchTmpS=2 perBatchMaskS=0 perElementMaskS=0 elementsPerBatch=16 */
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,1,0,0:vw4); (0,2,0,0:vw4); (0,3,0,0:vw4); (0,0,1,0:vw4); (0,1,1,0:vw4); (0,2,1,0:vw4); (0,3,1,0:vw4); (0,0,2,0:vw4); (0,1,2,0:vw4); (0,2,2,0:vw4); (0,3,2,0:vw4); (0,0,3,0:vw4); (0,1,3,0:vw4); (0,2,3,0:vw4); (0,3,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v7, v2, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[10:11], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s12, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s12
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for LDS write
s_barrier                                          // LDS write barrier
ds_read_b128 v[12:15], v8 offset:0                 // load Bias
v_add_u32 v9, 1024, v8                             // add ScaleAlphaVec offset (1)
ds_read_b128 v[16:19], v9 offset:0                 // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
buffer_load_dwordx2 v[24:25], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
ds_read_b128 v[28:31], v8 offset:256               // load Bias
ds_read_b128 v[32:35], v9 offset:256               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,0,2,0) */
buffer_load_dwordx2 v[26:27], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:256 // load C
ds_read_b128 v[40:43], v8 offset:512               // load Bias
ds_read_b128 v[44:47], v9 offset:512               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,0,3,0) */
buffer_load_dwordx2 v[52:53], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:384 // load C
ds_read_b128 v[56:59], v8 offset:768               // load Bias
ds_read_b128 v[60:63], v9 offset:768               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[54:55], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
buffer_load_dwordx2 v[72:73], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
/* (d1,vc1,d0,vc0)=(0,1,2,0) */
buffer_load_dwordx2 v[74:75], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:256 // load C
/* (d1,vc1,d0,vc0)=(0,1,3,0) */
buffer_load_dwordx2 v[84:85], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:384 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[86:87], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
buffer_load_dwordx2 v[96:97], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
/* (d1,vc1,d0,vc0)=(0,2,2,0) */
buffer_load_dwordx2 v[98:99], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:256 // load C
/* (d1,vc1,d0,vc0)=(0,2,3,0) */
buffer_load_dwordx2 v[108:109], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:384 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[110:111], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
buffer_load_dwordx2 v[120:121], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
/* (d1,vc1,d0,vc0)=(0,3,2,0) */
buffer_load_dwordx2 v[122:123], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:256 // load C
/* (d1,vc1,d0,vc0)=(0,3,3,0) */
buffer_load_dwordx2 v[132:133], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:384 // load C
v_add_lshl_u32 v6, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc1           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+36], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+37], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+38], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+39], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+48], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+49], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+50], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+51], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+64], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+65], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+66], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+67], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+68], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+69], acc17          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+70], acc18          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+71], acc19          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+76], acc20          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+77], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+78], acc22          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+79], acc23          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+80], acc24          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+81], acc25          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+82], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+83], acc27          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+88], acc28          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+89], acc29          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+90], acc30          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+91], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+92], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+93], acc33          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+94], acc34          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+95], acc35          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+100], acc36         // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+101], acc37         // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+102], acc38         // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+103], acc39         // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+104], acc40         // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+105], acc41         // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+106], acc42         // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+107], acc43         // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+112], acc44         // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+113], acc45         // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+114], acc46         // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+115], acc47         // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+116], acc48         // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+117], acc49         // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+118], acc50         // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+119], acc51         // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+124], acc52         // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+125], acc53         // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+126], acc54         // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+127], acc55         // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+128], acc56         // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+129], acc57         // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+130], acc58         // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+131], acc59         // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+140], acc60         // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+141], acc61         // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+142], acc62         // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+143], acc63         // copy acc to vreg[63]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 1, 0, 0), (0, 2, 0, 0), (0, 3, 0, 0), (0, 0, 1, 0), (0, 1, 1, 0), (0, 2, 1, 0), (0, 3, 1, 0), (0, 0, 2, 0), (0, 1, 2, 0), (0, 2, 2, 0), (0, 3, 2, 0), (0, 0, 3, 0), (0, 1, 3, 0), (0, 2, 3, 0), (0, 3, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+104], s[sgprAlpha], v[vgprValuC+104] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+107], s[sgprAlpha], v[vgprValuC+107] // *= alpha
v_mul_f32 v[vgprValuC+112], s[sgprAlpha], v[vgprValuC+112] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+114], s[sgprAlpha], v[vgprValuC+114] // *= alpha
v_mul_f32 v[vgprValuC+115], s[sgprAlpha], v[vgprValuC+115] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+118], s[sgprAlpha], v[vgprValuC+118] // *= alpha
v_mul_f32 v[vgprValuC+119], s[sgprAlpha], v[vgprValuC+119] // *= alpha
v_mul_f32 v[vgprValuC+124], s[sgprAlpha], v[vgprValuC+124] // *= alpha
v_mul_f32 v[vgprValuC+125], s[sgprAlpha], v[vgprValuC+125] // *= alpha
v_mul_f32 v[vgprValuC+126], s[sgprAlpha], v[vgprValuC+126] // *= alpha
v_mul_f32 v[vgprValuC+127], s[sgprAlpha], v[vgprValuC+127] // *= alpha
v_mul_f32 v[vgprValuC+128], s[sgprAlpha], v[vgprValuC+128] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+130], s[sgprAlpha], v[vgprValuC+130] // *= alpha
v_mul_f32 v[vgprValuC+131], s[sgprAlpha], v[vgprValuC+131] // *= alpha
v_mul_f32 v[vgprValuC+140], s[sgprAlpha], v[vgprValuC+140] // *= alpha
v_mul_f32 v[vgprValuC+141], s[sgprAlpha], v[vgprValuC+141] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+143], s[sgprAlpha], v[vgprValuC+143] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(6), vmcnt(15)                    // vmcnt(15) = 16 - 1 (beta) lgkmcnt(6) = 8 - 1 (bias) - 1 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+20], s[sgprBeta], v10, v[vgprValuC+20] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+21], s[sgprBeta], v10, v[vgprValuC+21] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+22], s[sgprBeta], v11, v[vgprValuC+22] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+23], s[sgprBeta], v11, v[vgprValuC+23] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt lgkmcnt(4), vmcnt(15)                    // vmcnt(14) = 16 - 2 (beta) lgkmcnt(4) = 8 - 2 (bias) - 2 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[32:33], v[vgprValuC+36:vgprValuC+36+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[34:35], v[vgprValuC+38:vgprValuC+38+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v24, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v24, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v25, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v25, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[28:29], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[30:31], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D

s_waitcnt lgkmcnt(2), vmcnt(15)                    // vmcnt(13) = 16 - 3 (beta) lgkmcnt(2) = 8 - 3 (bias) - 3 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[44:45], v[vgprValuC+48:vgprValuC+48+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[46:47], v[vgprValuC+50:vgprValuC+50+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v26, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v26, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v27, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v27, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[40:41], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[42:43], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt lgkmcnt(0), vmcnt(15)                    // vmcnt(12) = 16 - 4 (beta) lgkmcnt(0) = 8 - 4 (bias) - 4 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[60:61], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[62:63], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_fma_mix_f32 v[vgprValuC+64], s[sgprBeta], v52, v[vgprValuC+64] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+65], s[sgprBeta], v52, v[vgprValuC+65] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+66], s[sgprBeta], v53, v[vgprValuC+66] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+67], s[sgprBeta], v53, v[vgprValuC+67] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[56:57], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[58:59], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D

s_waitcnt vmcnt(15)                                // vmcnt(11) = 16 - 5 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[16:17], v[vgprValuC+68:vgprValuC+68+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[18:19], v[vgprValuC+70:vgprValuC+70+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+68], s[sgprBeta], v54, v[vgprValuC+68] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+69], s[sgprBeta], v54, v[vgprValuC+69] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+70], s[sgprBeta], v55, v[vgprValuC+70] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+71], s[sgprBeta], v55, v[vgprValuC+71] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+68:vgprValuC+68+1], v[12:13], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[vgprValuC+70:vgprValuC+70+1], v[14:15], v[vgprValuC+70:vgprValuC+70+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+68], v[vgprValuC+68]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+69], v[vgprValuC+69]     // convert C to fp16
v_pack_b32_f16 v68, v[vgprValuC+68], v[vgprValuC+69] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+70], v[vgprValuC+70]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+71], v[vgprValuC+71]     // convert C to fp16
v_pack_b32_f16 v69, v[vgprValuC+70], v[vgprValuC+71] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[68:69], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(10) = 16 - 6 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[32:33], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[34:35], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+76], s[sgprBeta], v72, v[vgprValuC+76] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+77], s[sgprBeta], v72, v[vgprValuC+77] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+78], s[sgprBeta], v73, v[vgprValuC+78] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+79], s[sgprBeta], v73, v[vgprValuC+79] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[28:29], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[30:31], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
buffer_store_dwordx2 v[76:77], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D

s_waitcnt vmcnt(15)                                // vmcnt(9) = 16 - 7 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[44:45], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[46:47], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_fma_mix_f32 v[vgprValuC+80], s[sgprBeta], v74, v[vgprValuC+80] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+81], s[sgprBeta], v74, v[vgprValuC+81] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+82], s[sgprBeta], v75, v[vgprValuC+82] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+83], s[sgprBeta], v75, v[vgprValuC+83] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[40:41], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[42:43], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt vmcnt(15)                                // vmcnt(8) = 16 - 8 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[60:61], v[vgprValuC+88:vgprValuC+88+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[62:63], v[vgprValuC+90:vgprValuC+90+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_fma_mix_f32 v[vgprValuC+88], s[sgprBeta], v84, v[vgprValuC+88] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+89], s[sgprBeta], v84, v[vgprValuC+89] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+90], s[sgprBeta], v85, v[vgprValuC+90] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+91], s[sgprBeta], v85, v[vgprValuC+91] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+88:vgprValuC+88+1], v[56:57], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[vgprValuC+90:vgprValuC+90+1], v[58:59], v[vgprValuC+90:vgprValuC+90+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+88], v[vgprValuC+88]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+89], v[vgprValuC+89]     // convert C to fp16
v_pack_b32_f16 v88, v[vgprValuC+88], v[vgprValuC+89] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+90], v[vgprValuC+90]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+91], v[vgprValuC+91]     // convert C to fp16
v_pack_b32_f16 v89, v[vgprValuC+90], v[vgprValuC+91] // Pack with neighbor
buffer_store_dwordx2 v[88:89], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D

s_waitcnt vmcnt(15)                                // vmcnt(7) = 16 - 9 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[16:17], v[vgprValuC+92:vgprValuC+92+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[18:19], v[vgprValuC+94:vgprValuC+94+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+92], s[sgprBeta], v86, v[vgprValuC+92] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+93], s[sgprBeta], v86, v[vgprValuC+93] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+94], s[sgprBeta], v87, v[vgprValuC+94] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+95], s[sgprBeta], v87, v[vgprValuC+95] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+92:vgprValuC+92+1], v[12:13], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[vgprValuC+94:vgprValuC+94+1], v[14:15], v[vgprValuC+94:vgprValuC+94+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+92], v[vgprValuC+92]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+93], v[vgprValuC+93]     // convert C to fp16
v_pack_b32_f16 v92, v[vgprValuC+92], v[vgprValuC+93] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+94], v[vgprValuC+94]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+95], v[vgprValuC+95]     // convert C to fp16
v_pack_b32_f16 v93, v[vgprValuC+94], v[vgprValuC+95] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[92:93], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(6) = 16 - 10 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[32:33], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[34:35], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+100], s[sgprBeta], v96, v[vgprValuC+100] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+101], s[sgprBeta], v96, v[vgprValuC+101] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+102], s[sgprBeta], v97, v[vgprValuC+102] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+103], s[sgprBeta], v97, v[vgprValuC+103] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[28:29], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[30:31], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
buffer_store_dwordx2 v[100:101], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D

s_waitcnt vmcnt(15)                                // vmcnt(5) = 16 - 11 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[44:45], v[vgprValuC+104:vgprValuC+104+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[46:47], v[vgprValuC+106:vgprValuC+106+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_fma_mix_f32 v[vgprValuC+104], s[sgprBeta], v98, v[vgprValuC+104] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+105], s[sgprBeta], v98, v[vgprValuC+105] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+106], s[sgprBeta], v99, v[vgprValuC+106] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+107], s[sgprBeta], v99, v[vgprValuC+107] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+104:vgprValuC+104+1], v[40:41], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[vgprValuC+106:vgprValuC+106+1], v[42:43], v[vgprValuC+106:vgprValuC+106+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+104], v[vgprValuC+104]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+105], v[vgprValuC+105]   // convert C to fp16
v_pack_b32_f16 v104, v[vgprValuC+104], v[vgprValuC+105] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+106], v[vgprValuC+106]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+107], v[vgprValuC+107]   // convert C to fp16
v_pack_b32_f16 v105, v[vgprValuC+106], v[vgprValuC+107] // Pack with neighbor
buffer_store_dwordx2 v[104:105], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt vmcnt(15)                                // vmcnt(4) = 16 - 12 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+112:vgprValuC+112+1], v[60:61], v[vgprValuC+112:vgprValuC+112+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+114:vgprValuC+114+1], v[62:63], v[vgprValuC+114:vgprValuC+114+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_fma_mix_f32 v[vgprValuC+112], s[sgprBeta], v108, v[vgprValuC+112] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+113], s[sgprBeta], v108, v[vgprValuC+113] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+114], s[sgprBeta], v109, v[vgprValuC+114] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+115], s[sgprBeta], v109, v[vgprValuC+115] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+112:vgprValuC+112+1], v[56:57], v[vgprValuC+112:vgprValuC+112+1] // C += bias
v_pk_add_f32 v[vgprValuC+114:vgprValuC+114+1], v[58:59], v[vgprValuC+114:vgprValuC+114+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+112], v[vgprValuC+112]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+113], v[vgprValuC+113]   // convert C to fp16
v_pack_b32_f16 v112, v[vgprValuC+112], v[vgprValuC+113] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+114], v[vgprValuC+114]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+115], v[vgprValuC+115]   // convert C to fp16
v_pack_b32_f16 v113, v[vgprValuC+114], v[vgprValuC+115] // Pack with neighbor
buffer_store_dwordx2 v[112:113], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D

s_waitcnt vmcnt(15)                                // vmcnt(3) = 16 - 13 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+116:vgprValuC+116+1], v[16:17], v[vgprValuC+116:vgprValuC+116+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+118:vgprValuC+118+1], v[18:19], v[vgprValuC+118:vgprValuC+118+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+116], s[sgprBeta], v110, v[vgprValuC+116] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v110, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+118], s[sgprBeta], v111, v[vgprValuC+118] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+119], s[sgprBeta], v111, v[vgprValuC+119] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+116:vgprValuC+116+1], v[12:13], v[vgprValuC+116:vgprValuC+116+1] // C += bias
v_pk_add_f32 v[vgprValuC+118:vgprValuC+118+1], v[14:15], v[vgprValuC+118:vgprValuC+118+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+116], v[vgprValuC+116]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+117], v[vgprValuC+117]   // convert C to fp16
v_pack_b32_f16 v116, v[vgprValuC+116], v[vgprValuC+117] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+118], v[vgprValuC+118]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+119], v[vgprValuC+119]   // convert C to fp16
v_pack_b32_f16 v117, v[vgprValuC+118], v[vgprValuC+119] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[116:117], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(2) = 16 - 14 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+124:vgprValuC+124+1], v[32:33], v[vgprValuC+124:vgprValuC+124+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+126:vgprValuC+126+1], v[34:35], v[vgprValuC+126:vgprValuC+126+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+124], s[sgprBeta], v120, v[vgprValuC+124] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+125], s[sgprBeta], v120, v[vgprValuC+125] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+126], s[sgprBeta], v121, v[vgprValuC+126] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+127], s[sgprBeta], v121, v[vgprValuC+127] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+124:vgprValuC+124+1], v[28:29], v[vgprValuC+124:vgprValuC+124+1] // C += bias
v_pk_add_f32 v[vgprValuC+126:vgprValuC+126+1], v[30:31], v[vgprValuC+126:vgprValuC+126+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+124], v[vgprValuC+124]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+125], v[vgprValuC+125]   // convert C to fp16
v_pack_b32_f16 v124, v[vgprValuC+124], v[vgprValuC+125] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+126], v[vgprValuC+126]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+127], v[vgprValuC+127]   // convert C to fp16
v_pack_b32_f16 v125, v[vgprValuC+126], v[vgprValuC+127] // Pack with neighbor
buffer_store_dwordx2 v[124:125], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D

s_waitcnt vmcnt(15)                                // vmcnt(1) = 16 - 15 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+128:vgprValuC+128+1], v[44:45], v[vgprValuC+128:vgprValuC+128+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+130:vgprValuC+130+1], v[46:47], v[vgprValuC+130:vgprValuC+130+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_fma_mix_f32 v[vgprValuC+128], s[sgprBeta], v122, v[vgprValuC+128] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+129], s[sgprBeta], v122, v[vgprValuC+129] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+130], s[sgprBeta], v123, v[vgprValuC+130] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+131], s[sgprBeta], v123, v[vgprValuC+131] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+128:vgprValuC+128+1], v[40:41], v[vgprValuC+128:vgprValuC+128+1] // C += bias
v_pk_add_f32 v[vgprValuC+130:vgprValuC+130+1], v[42:43], v[vgprValuC+130:vgprValuC+130+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+128], v[vgprValuC+128]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+129], v[vgprValuC+129]   // convert C to fp16
v_pack_b32_f16 v128, v[vgprValuC+128], v[vgprValuC+129] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+130], v[vgprValuC+130]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+131], v[vgprValuC+131]   // convert C to fp16
v_pack_b32_f16 v129, v[vgprValuC+130], v[vgprValuC+131] // Pack with neighbor
buffer_store_dwordx2 v[128:129], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt vmcnt(15)                                // vmcnt(0) = 16 - 16 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+140:vgprValuC+140+1], v[60:61], v[vgprValuC+140:vgprValuC+140+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+142:vgprValuC+142+1], v[62:63], v[vgprValuC+142:vgprValuC+142+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_fma_mix_f32 v[vgprValuC+140], s[sgprBeta], v132, v[vgprValuC+140] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+141], s[sgprBeta], v132, v[vgprValuC+141] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+142], s[sgprBeta], v133, v[vgprValuC+142] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+143], s[sgprBeta], v133, v[vgprValuC+143] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+140:vgprValuC+140+1], v[56:57], v[vgprValuC+140:vgprValuC+140+1] // C += bias
v_pk_add_f32 v[vgprValuC+142:vgprValuC+142+1], v[58:59], v[vgprValuC+142:vgprValuC+142+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+140], v[vgprValuC+140]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+141], v[vgprValuC+141]   // convert C to fp16
v_pack_b32_f16 v140, v[vgprValuC+140], v[vgprValuC+141] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+142], v[vgprValuC+142]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+143], v[vgprValuC+143]   // convert C to fp16
v_pack_b32_f16 v141, v[vgprValuC+142], v[vgprValuC+143] // Pack with neighbor
buffer_store_dwordx2 v[140:141], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Beta Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,0,4,0:vw4); (0,1,4,0:vw4); (0,2,4,0:vw4); (0,3,4,0:vw4); (0,0,5,0:vw4); (0,1,5,0:vw4); (0,2,5,0:vw4); (0,3,5,0:vw4); (0,0,6,0:vw4); (0,1,6,0:vw4); (0,2,6,0:vw4); (0,3,6,0:vw4); (0,0,7,0:vw4); (0,1,7,0:vw4); (0,2,7,0:vw4); (0,3,7,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[10:11], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
ds_read_b128 v[12:15], v8 offset:0                 // load Bias
ds_read_b128 v[16:19], v9 offset:0                 // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
buffer_load_dwordx2 v[24:25], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
ds_read_b128 v[28:31], v8 offset:256               // load Bias
ds_read_b128 v[32:35], v9 offset:256               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,4,2,0) */
buffer_load_dwordx2 v[26:27], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:256 // load C
ds_read_b128 v[40:43], v8 offset:512               // load Bias
ds_read_b128 v[44:47], v9 offset:512               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,4,3,0) */
buffer_load_dwordx2 v[52:53], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:384 // load C
ds_read_b128 v[56:59], v8 offset:768               // load Bias
ds_read_b128 v[60:63], v9 offset:768               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[54:55], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
buffer_load_dwordx2 v[72:73], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
/* (d1,vc1,d0,vc0)=(0,5,2,0) */
buffer_load_dwordx2 v[74:75], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:256 // load C
/* (d1,vc1,d0,vc0)=(0,5,3,0) */
buffer_load_dwordx2 v[84:85], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:384 // load C
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[86:87], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
buffer_load_dwordx2 v[96:97], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
/* (d1,vc1,d0,vc0)=(0,6,2,0) */
buffer_load_dwordx2 v[98:99], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:256 // load C
/* (d1,vc1,d0,vc0)=(0,6,3,0) */
buffer_load_dwordx2 v[108:109], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:384 // load C
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[110:111], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
buffer_load_dwordx2 v[120:121], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
/* (d1,vc1,d0,vc0)=(0,7,2,0) */
buffer_load_dwordx2 v[122:123], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:256 // load C
/* (d1,vc1,d0,vc0)=(0,7,3,0) */
buffer_load_dwordx2 v[132:133], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:384 // load C
v_accvgpr_read_b32 v[vgprValuC+20], acc64          // copy acc to vreg[64]
v_accvgpr_read_b32 v[vgprValuC+21], acc65          // copy acc to vreg[65]
v_accvgpr_read_b32 v[vgprValuC+22], acc66          // copy acc to vreg[66]
v_accvgpr_read_b32 v[vgprValuC+23], acc67          // copy acc to vreg[67]
v_accvgpr_read_b32 v[vgprValuC+36], acc68          // copy acc to vreg[68]
v_accvgpr_read_b32 v[vgprValuC+37], acc69          // copy acc to vreg[69]
v_accvgpr_read_b32 v[vgprValuC+38], acc70          // copy acc to vreg[70]
v_accvgpr_read_b32 v[vgprValuC+39], acc71          // copy acc to vreg[71]
v_accvgpr_read_b32 v[vgprValuC+48], acc72          // copy acc to vreg[72]
v_accvgpr_read_b32 v[vgprValuC+49], acc73          // copy acc to vreg[73]
v_accvgpr_read_b32 v[vgprValuC+50], acc74          // copy acc to vreg[74]
v_accvgpr_read_b32 v[vgprValuC+51], acc75          // copy acc to vreg[75]
v_accvgpr_read_b32 v[vgprValuC+64], acc76          // copy acc to vreg[76]
v_accvgpr_read_b32 v[vgprValuC+65], acc77          // copy acc to vreg[77]
v_accvgpr_read_b32 v[vgprValuC+66], acc78          // copy acc to vreg[78]
v_accvgpr_read_b32 v[vgprValuC+67], acc79          // copy acc to vreg[79]
v_accvgpr_read_b32 v[vgprValuC+68], acc80          // copy acc to vreg[80]
v_accvgpr_read_b32 v[vgprValuC+69], acc81          // copy acc to vreg[81]
v_accvgpr_read_b32 v[vgprValuC+70], acc82          // copy acc to vreg[82]
v_accvgpr_read_b32 v[vgprValuC+71], acc83          // copy acc to vreg[83]
v_accvgpr_read_b32 v[vgprValuC+76], acc84          // copy acc to vreg[84]
v_accvgpr_read_b32 v[vgprValuC+77], acc85          // copy acc to vreg[85]
v_accvgpr_read_b32 v[vgprValuC+78], acc86          // copy acc to vreg[86]
v_accvgpr_read_b32 v[vgprValuC+79], acc87          // copy acc to vreg[87]
v_accvgpr_read_b32 v[vgprValuC+80], acc88          // copy acc to vreg[88]
v_accvgpr_read_b32 v[vgprValuC+81], acc89          // copy acc to vreg[89]
v_accvgpr_read_b32 v[vgprValuC+82], acc90          // copy acc to vreg[90]
v_accvgpr_read_b32 v[vgprValuC+83], acc91          // copy acc to vreg[91]
v_accvgpr_read_b32 v[vgprValuC+88], acc92          // copy acc to vreg[92]
v_accvgpr_read_b32 v[vgprValuC+89], acc93          // copy acc to vreg[93]
v_accvgpr_read_b32 v[vgprValuC+90], acc94          // copy acc to vreg[94]
v_accvgpr_read_b32 v[vgprValuC+91], acc95          // copy acc to vreg[95]
v_accvgpr_read_b32 v[vgprValuC+92], acc96          // copy acc to vreg[96]
v_accvgpr_read_b32 v[vgprValuC+93], acc97          // copy acc to vreg[97]
v_accvgpr_read_b32 v[vgprValuC+94], acc98          // copy acc to vreg[98]
v_accvgpr_read_b32 v[vgprValuC+95], acc99          // copy acc to vreg[99]
v_accvgpr_read_b32 v[vgprValuC+100], acc100        // copy acc to vreg[100]
v_accvgpr_read_b32 v[vgprValuC+101], acc101        // copy acc to vreg[101]
v_accvgpr_read_b32 v[vgprValuC+102], acc102        // copy acc to vreg[102]
v_accvgpr_read_b32 v[vgprValuC+103], acc103        // copy acc to vreg[103]
v_accvgpr_read_b32 v[vgprValuC+104], acc104        // copy acc to vreg[104]
v_accvgpr_read_b32 v[vgprValuC+105], acc105        // copy acc to vreg[105]
v_accvgpr_read_b32 v[vgprValuC+106], acc106        // copy acc to vreg[106]
v_accvgpr_read_b32 v[vgprValuC+107], acc107        // copy acc to vreg[107]
v_accvgpr_read_b32 v[vgprValuC+112], acc108        // copy acc to vreg[108]
v_accvgpr_read_b32 v[vgprValuC+113], acc109        // copy acc to vreg[109]
v_accvgpr_read_b32 v[vgprValuC+114], acc110        // copy acc to vreg[110]
v_accvgpr_read_b32 v[vgprValuC+115], acc111        // copy acc to vreg[111]
v_accvgpr_read_b32 v[vgprValuC+116], acc112        // copy acc to vreg[112]
v_accvgpr_read_b32 v[vgprValuC+117], acc113        // copy acc to vreg[113]
v_accvgpr_read_b32 v[vgprValuC+118], acc114        // copy acc to vreg[114]
v_accvgpr_read_b32 v[vgprValuC+119], acc115        // copy acc to vreg[115]
v_accvgpr_read_b32 v[vgprValuC+124], acc116        // copy acc to vreg[116]
v_accvgpr_read_b32 v[vgprValuC+125], acc117        // copy acc to vreg[117]
v_accvgpr_read_b32 v[vgprValuC+126], acc118        // copy acc to vreg[118]
v_accvgpr_read_b32 v[vgprValuC+127], acc119        // copy acc to vreg[119]
v_accvgpr_read_b32 v[vgprValuC+128], acc120        // copy acc to vreg[120]
v_accvgpr_read_b32 v[vgprValuC+129], acc121        // copy acc to vreg[121]
v_accvgpr_read_b32 v[vgprValuC+130], acc122        // copy acc to vreg[122]
v_accvgpr_read_b32 v[vgprValuC+131], acc123        // copy acc to vreg[123]
v_accvgpr_read_b32 v[vgprValuC+140], acc124        // copy acc to vreg[124]
v_accvgpr_read_b32 v[vgprValuC+141], acc125        // copy acc to vreg[125]
v_accvgpr_read_b32 v[vgprValuC+142], acc126        // copy acc to vreg[126]
v_accvgpr_read_b32 v[vgprValuC+143], acc127        // copy acc to vreg[127]

/* rC *= alpha batchElements=[(0, 0, 4, 0), (0, 1, 4, 0), (0, 2, 4, 0), (0, 3, 4, 0), (0, 0, 5, 0), (0, 1, 5, 0), (0, 2, 5, 0), (0, 3, 5, 0), (0, 0, 6, 0), (0, 1, 6, 0), (0, 2, 6, 0), (0, 3, 6, 0), (0, 0, 7, 0), (0, 1, 7, 0), (0, 2, 7, 0), (0, 3, 7, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+104], s[sgprAlpha], v[vgprValuC+104] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+107], s[sgprAlpha], v[vgprValuC+107] // *= alpha
v_mul_f32 v[vgprValuC+112], s[sgprAlpha], v[vgprValuC+112] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+114], s[sgprAlpha], v[vgprValuC+114] // *= alpha
v_mul_f32 v[vgprValuC+115], s[sgprAlpha], v[vgprValuC+115] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+118], s[sgprAlpha], v[vgprValuC+118] // *= alpha
v_mul_f32 v[vgprValuC+119], s[sgprAlpha], v[vgprValuC+119] // *= alpha
v_mul_f32 v[vgprValuC+124], s[sgprAlpha], v[vgprValuC+124] // *= alpha
v_mul_f32 v[vgprValuC+125], s[sgprAlpha], v[vgprValuC+125] // *= alpha
v_mul_f32 v[vgprValuC+126], s[sgprAlpha], v[vgprValuC+126] // *= alpha
v_mul_f32 v[vgprValuC+127], s[sgprAlpha], v[vgprValuC+127] // *= alpha
v_mul_f32 v[vgprValuC+128], s[sgprAlpha], v[vgprValuC+128] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+130], s[sgprAlpha], v[vgprValuC+130] // *= alpha
v_mul_f32 v[vgprValuC+131], s[sgprAlpha], v[vgprValuC+131] // *= alpha
v_mul_f32 v[vgprValuC+140], s[sgprAlpha], v[vgprValuC+140] // *= alpha
v_mul_f32 v[vgprValuC+141], s[sgprAlpha], v[vgprValuC+141] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+143], s[sgprAlpha], v[vgprValuC+143] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(6), vmcnt(15)                    // vmcnt(15) = 16 - 1 (beta) lgkmcnt(6) = 8 - 1 (bias) - 1 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+20], s[sgprBeta], v10, v[vgprValuC+20] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+21], s[sgprBeta], v10, v[vgprValuC+21] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+22], s[sgprBeta], v11, v[vgprValuC+22] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+23], s[sgprBeta], v11, v[vgprValuC+23] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt lgkmcnt(4), vmcnt(15)                    // vmcnt(14) = 16 - 2 (beta) lgkmcnt(4) = 8 - 2 (bias) - 2 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[32:33], v[vgprValuC+36:vgprValuC+36+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[34:35], v[vgprValuC+38:vgprValuC+38+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v24, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v24, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v25, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v25, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[28:29], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[30:31], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D

s_waitcnt lgkmcnt(2), vmcnt(15)                    // vmcnt(13) = 16 - 3 (beta) lgkmcnt(2) = 8 - 3 (bias) - 3 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[44:45], v[vgprValuC+48:vgprValuC+48+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[46:47], v[vgprValuC+50:vgprValuC+50+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v26, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v26, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v27, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v27, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[40:41], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[42:43], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt lgkmcnt(0), vmcnt(15)                    // vmcnt(12) = 16 - 4 (beta) lgkmcnt(0) = 8 - 4 (bias) - 4 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[60:61], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[62:63], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_fma_mix_f32 v[vgprValuC+64], s[sgprBeta], v52, v[vgprValuC+64] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+65], s[sgprBeta], v52, v[vgprValuC+65] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+66], s[sgprBeta], v53, v[vgprValuC+66] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+67], s[sgprBeta], v53, v[vgprValuC+67] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[56:57], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[58:59], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D

s_waitcnt vmcnt(15)                                // vmcnt(11) = 16 - 5 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[16:17], v[vgprValuC+68:vgprValuC+68+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[18:19], v[vgprValuC+70:vgprValuC+70+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+68], s[sgprBeta], v54, v[vgprValuC+68] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+69], s[sgprBeta], v54, v[vgprValuC+69] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+70], s[sgprBeta], v55, v[vgprValuC+70] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+71], s[sgprBeta], v55, v[vgprValuC+71] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+68:vgprValuC+68+1], v[12:13], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[vgprValuC+70:vgprValuC+70+1], v[14:15], v[vgprValuC+70:vgprValuC+70+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+68], v[vgprValuC+68]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+69], v[vgprValuC+69]     // convert C to fp16
v_pack_b32_f16 v68, v[vgprValuC+68], v[vgprValuC+69] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+70], v[vgprValuC+70]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+71], v[vgprValuC+71]     // convert C to fp16
v_pack_b32_f16 v69, v[vgprValuC+70], v[vgprValuC+71] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[68:69], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(10) = 16 - 6 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[32:33], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[34:35], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+76], s[sgprBeta], v72, v[vgprValuC+76] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+77], s[sgprBeta], v72, v[vgprValuC+77] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+78], s[sgprBeta], v73, v[vgprValuC+78] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+79], s[sgprBeta], v73, v[vgprValuC+79] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[28:29], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[30:31], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
buffer_store_dwordx2 v[76:77], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D

s_waitcnt vmcnt(15)                                // vmcnt(9) = 16 - 7 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[44:45], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[46:47], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_fma_mix_f32 v[vgprValuC+80], s[sgprBeta], v74, v[vgprValuC+80] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+81], s[sgprBeta], v74, v[vgprValuC+81] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+82], s[sgprBeta], v75, v[vgprValuC+82] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+83], s[sgprBeta], v75, v[vgprValuC+83] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[40:41], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[42:43], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt vmcnt(15)                                // vmcnt(8) = 16 - 8 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[60:61], v[vgprValuC+88:vgprValuC+88+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[62:63], v[vgprValuC+90:vgprValuC+90+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_fma_mix_f32 v[vgprValuC+88], s[sgprBeta], v84, v[vgprValuC+88] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+89], s[sgprBeta], v84, v[vgprValuC+89] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+90], s[sgprBeta], v85, v[vgprValuC+90] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+91], s[sgprBeta], v85, v[vgprValuC+91] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+88:vgprValuC+88+1], v[56:57], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[vgprValuC+90:vgprValuC+90+1], v[58:59], v[vgprValuC+90:vgprValuC+90+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+88], v[vgprValuC+88]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+89], v[vgprValuC+89]     // convert C to fp16
v_pack_b32_f16 v88, v[vgprValuC+88], v[vgprValuC+89] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+90], v[vgprValuC+90]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+91], v[vgprValuC+91]     // convert C to fp16
v_pack_b32_f16 v89, v[vgprValuC+90], v[vgprValuC+91] // Pack with neighbor
buffer_store_dwordx2 v[88:89], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D

s_waitcnt vmcnt(15)                                // vmcnt(7) = 16 - 9 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[16:17], v[vgprValuC+92:vgprValuC+92+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[18:19], v[vgprValuC+94:vgprValuC+94+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+92], s[sgprBeta], v86, v[vgprValuC+92] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+93], s[sgprBeta], v86, v[vgprValuC+93] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+94], s[sgprBeta], v87, v[vgprValuC+94] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+95], s[sgprBeta], v87, v[vgprValuC+95] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+92:vgprValuC+92+1], v[12:13], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[vgprValuC+94:vgprValuC+94+1], v[14:15], v[vgprValuC+94:vgprValuC+94+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+92], v[vgprValuC+92]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+93], v[vgprValuC+93]     // convert C to fp16
v_pack_b32_f16 v92, v[vgprValuC+92], v[vgprValuC+93] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+94], v[vgprValuC+94]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+95], v[vgprValuC+95]     // convert C to fp16
v_pack_b32_f16 v93, v[vgprValuC+94], v[vgprValuC+95] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[92:93], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(6) = 16 - 10 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[32:33], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[34:35], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+100], s[sgprBeta], v96, v[vgprValuC+100] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+101], s[sgprBeta], v96, v[vgprValuC+101] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+102], s[sgprBeta], v97, v[vgprValuC+102] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+103], s[sgprBeta], v97, v[vgprValuC+103] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[28:29], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[30:31], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
buffer_store_dwordx2 v[100:101], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D

s_waitcnt vmcnt(15)                                // vmcnt(5) = 16 - 11 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[44:45], v[vgprValuC+104:vgprValuC+104+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[46:47], v[vgprValuC+106:vgprValuC+106+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_fma_mix_f32 v[vgprValuC+104], s[sgprBeta], v98, v[vgprValuC+104] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+105], s[sgprBeta], v98, v[vgprValuC+105] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+106], s[sgprBeta], v99, v[vgprValuC+106] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+107], s[sgprBeta], v99, v[vgprValuC+107] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+104:vgprValuC+104+1], v[40:41], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[vgprValuC+106:vgprValuC+106+1], v[42:43], v[vgprValuC+106:vgprValuC+106+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+104], v[vgprValuC+104]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+105], v[vgprValuC+105]   // convert C to fp16
v_pack_b32_f16 v104, v[vgprValuC+104], v[vgprValuC+105] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+106], v[vgprValuC+106]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+107], v[vgprValuC+107]   // convert C to fp16
v_pack_b32_f16 v105, v[vgprValuC+106], v[vgprValuC+107] // Pack with neighbor
buffer_store_dwordx2 v[104:105], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt vmcnt(15)                                // vmcnt(4) = 16 - 12 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+112:vgprValuC+112+1], v[60:61], v[vgprValuC+112:vgprValuC+112+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+114:vgprValuC+114+1], v[62:63], v[vgprValuC+114:vgprValuC+114+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_fma_mix_f32 v[vgprValuC+112], s[sgprBeta], v108, v[vgprValuC+112] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+113], s[sgprBeta], v108, v[vgprValuC+113] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+114], s[sgprBeta], v109, v[vgprValuC+114] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+115], s[sgprBeta], v109, v[vgprValuC+115] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+112:vgprValuC+112+1], v[56:57], v[vgprValuC+112:vgprValuC+112+1] // C += bias
v_pk_add_f32 v[vgprValuC+114:vgprValuC+114+1], v[58:59], v[vgprValuC+114:vgprValuC+114+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+112], v[vgprValuC+112]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+113], v[vgprValuC+113]   // convert C to fp16
v_pack_b32_f16 v112, v[vgprValuC+112], v[vgprValuC+113] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+114], v[vgprValuC+114]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+115], v[vgprValuC+115]   // convert C to fp16
v_pack_b32_f16 v113, v[vgprValuC+114], v[vgprValuC+115] // Pack with neighbor
buffer_store_dwordx2 v[112:113], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D

s_waitcnt vmcnt(15)                                // vmcnt(3) = 16 - 13 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+116:vgprValuC+116+1], v[16:17], v[vgprValuC+116:vgprValuC+116+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+118:vgprValuC+118+1], v[18:19], v[vgprValuC+118:vgprValuC+118+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+116], s[sgprBeta], v110, v[vgprValuC+116] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v110, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+118], s[sgprBeta], v111, v[vgprValuC+118] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+119], s[sgprBeta], v111, v[vgprValuC+119] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+116:vgprValuC+116+1], v[12:13], v[vgprValuC+116:vgprValuC+116+1] // C += bias
v_pk_add_f32 v[vgprValuC+118:vgprValuC+118+1], v[14:15], v[vgprValuC+118:vgprValuC+118+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+116], v[vgprValuC+116]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+117], v[vgprValuC+117]   // convert C to fp16
v_pack_b32_f16 v116, v[vgprValuC+116], v[vgprValuC+117] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+118], v[vgprValuC+118]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+119], v[vgprValuC+119]   // convert C to fp16
v_pack_b32_f16 v117, v[vgprValuC+118], v[vgprValuC+119] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[116:117], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(2) = 16 - 14 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+124:vgprValuC+124+1], v[32:33], v[vgprValuC+124:vgprValuC+124+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+126:vgprValuC+126+1], v[34:35], v[vgprValuC+126:vgprValuC+126+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+124], s[sgprBeta], v120, v[vgprValuC+124] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+125], s[sgprBeta], v120, v[vgprValuC+125] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+126], s[sgprBeta], v121, v[vgprValuC+126] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+127], s[sgprBeta], v121, v[vgprValuC+127] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+124:vgprValuC+124+1], v[28:29], v[vgprValuC+124:vgprValuC+124+1] // C += bias
v_pk_add_f32 v[vgprValuC+126:vgprValuC+126+1], v[30:31], v[vgprValuC+126:vgprValuC+126+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+124], v[vgprValuC+124]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+125], v[vgprValuC+125]   // convert C to fp16
v_pack_b32_f16 v124, v[vgprValuC+124], v[vgprValuC+125] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+126], v[vgprValuC+126]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+127], v[vgprValuC+127]   // convert C to fp16
v_pack_b32_f16 v125, v[vgprValuC+126], v[vgprValuC+127] // Pack with neighbor
buffer_store_dwordx2 v[124:125], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D

s_waitcnt vmcnt(15)                                // vmcnt(1) = 16 - 15 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+128:vgprValuC+128+1], v[44:45], v[vgprValuC+128:vgprValuC+128+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+130:vgprValuC+130+1], v[46:47], v[vgprValuC+130:vgprValuC+130+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_fma_mix_f32 v[vgprValuC+128], s[sgprBeta], v122, v[vgprValuC+128] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+129], s[sgprBeta], v122, v[vgprValuC+129] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+130], s[sgprBeta], v123, v[vgprValuC+130] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+131], s[sgprBeta], v123, v[vgprValuC+131] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+128:vgprValuC+128+1], v[40:41], v[vgprValuC+128:vgprValuC+128+1] // C += bias
v_pk_add_f32 v[vgprValuC+130:vgprValuC+130+1], v[42:43], v[vgprValuC+130:vgprValuC+130+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+128], v[vgprValuC+128]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+129], v[vgprValuC+129]   // convert C to fp16
v_pack_b32_f16 v128, v[vgprValuC+128], v[vgprValuC+129] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+130], v[vgprValuC+130]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+131], v[vgprValuC+131]   // convert C to fp16
v_pack_b32_f16 v129, v[vgprValuC+130], v[vgprValuC+131] // Pack with neighbor
buffer_store_dwordx2 v[128:129], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt vmcnt(15)                                // vmcnt(0) = 16 - 16 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+140:vgprValuC+140+1], v[60:61], v[vgprValuC+140:vgprValuC+140+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+142:vgprValuC+142+1], v[62:63], v[vgprValuC+142:vgprValuC+142+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_fma_mix_f32 v[vgprValuC+140], s[sgprBeta], v132, v[vgprValuC+140] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+141], s[sgprBeta], v132, v[vgprValuC+141] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+142], s[sgprBeta], v133, v[vgprValuC+142] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+143], s[sgprBeta], v133, v[vgprValuC+143] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+140:vgprValuC+140+1], v[56:57], v[vgprValuC+140:vgprValuC+140+1] // C += bias
v_pk_add_f32 v[vgprValuC+142:vgprValuC+142+1], v[58:59], v[vgprValuC+142:vgprValuC+142+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+140], v[vgprValuC+140]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+141], v[vgprValuC+141]   // convert C to fp16
v_pack_b32_f16 v140, v[vgprValuC+140], v[vgprValuC+141] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+142], v[vgprValuC+142]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+143], v[vgprValuC+143]   // convert C to fp16
v_pack_b32_f16 v141, v[vgprValuC+142], v[vgprValuC+143] // Pack with neighbor
buffer_store_dwordx2 v[140:141], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Beta Batch #2 (d1,d0,vc1,vc0) = */
/*    (1,0,0,0:vw4); (1,1,0,0:vw4); (1,2,0,0:vw4); (1,3,0,0:vw4); (1,0,1,0:vw4); (1,1,1,0:vw4); (1,2,1,0:vw4); (1,3,1,0:vw4); (1,0,2,0:vw4); (1,1,2,0:vw4); (1,2,2,0:vw4); (1,3,2,0:vw4); (1,0,3,0:vw4); (1,1,3,0:vw4); (1,2,3,0:vw4); (1,3,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(1,0,0,0) */
s_mul_i32 s12, s[sgprStrideC1J], 242               // scale StrideC *= numRows(121) * bpe
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[10:11], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
ds_read_b128 v[12:15], v8 offset:0                 // load Bias
ds_read_b128 v[16:19], v9 offset:0                 // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,0,1,0) */
buffer_load_dwordx2 v[24:25], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
ds_read_b128 v[28:31], v8 offset:256               // load Bias
ds_read_b128 v[32:35], v9 offset:256               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,0,2,0) */
buffer_load_dwordx2 v[26:27], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:256 // load C
ds_read_b128 v[40:43], v8 offset:512               // load Bias
ds_read_b128 v[44:47], v9 offset:512               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,0,3,0) */
buffer_load_dwordx2 v[52:53], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:384 // load C
ds_read_b128 v[56:59], v8 offset:768               // load Bias
ds_read_b128 v[60:63], v9 offset:768               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,1,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[54:55], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(1,1,1,0) */
buffer_load_dwordx2 v[72:73], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
/* (d1,vc1,d0,vc0)=(1,1,2,0) */
buffer_load_dwordx2 v[74:75], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:256 // load C
/* (d1,vc1,d0,vc0)=(1,1,3,0) */
buffer_load_dwordx2 v[84:85], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:384 // load C
/* (d1,vc1,d0,vc0)=(1,2,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[86:87], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(1,2,1,0) */
buffer_load_dwordx2 v[96:97], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
/* (d1,vc1,d0,vc0)=(1,2,2,0) */
buffer_load_dwordx2 v[98:99], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:256 // load C
/* (d1,vc1,d0,vc0)=(1,2,3,0) */
buffer_load_dwordx2 v[108:109], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:384 // load C
/* (d1,vc1,d0,vc0)=(1,3,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[110:111], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(1,3,1,0) */
buffer_load_dwordx2 v[120:121], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
/* (d1,vc1,d0,vc0)=(1,3,2,0) */
buffer_load_dwordx2 v[122:123], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:256 // load C
/* (d1,vc1,d0,vc0)=(1,3,3,0) */
buffer_load_dwordx2 v[132:133], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:384 // load C
v_accvgpr_read_b32 v[vgprValuC+20], acc128         // copy acc to vreg[128]
v_accvgpr_read_b32 v[vgprValuC+21], acc129         // copy acc to vreg[129]
v_accvgpr_read_b32 v[vgprValuC+22], acc130         // copy acc to vreg[130]
v_accvgpr_read_b32 v[vgprValuC+23], acc131         // copy acc to vreg[131]
v_accvgpr_read_b32 v[vgprValuC+36], acc132         // copy acc to vreg[132]
v_accvgpr_read_b32 v[vgprValuC+37], acc133         // copy acc to vreg[133]
v_accvgpr_read_b32 v[vgprValuC+38], acc134         // copy acc to vreg[134]
v_accvgpr_read_b32 v[vgprValuC+39], acc135         // copy acc to vreg[135]
v_accvgpr_read_b32 v[vgprValuC+48], acc136         // copy acc to vreg[136]
v_accvgpr_read_b32 v[vgprValuC+49], acc137         // copy acc to vreg[137]
v_accvgpr_read_b32 v[vgprValuC+50], acc138         // copy acc to vreg[138]
v_accvgpr_read_b32 v[vgprValuC+51], acc139         // copy acc to vreg[139]
v_accvgpr_read_b32 v[vgprValuC+64], acc140         // copy acc to vreg[140]
v_accvgpr_read_b32 v[vgprValuC+65], acc141         // copy acc to vreg[141]
v_accvgpr_read_b32 v[vgprValuC+66], acc142         // copy acc to vreg[142]
v_accvgpr_read_b32 v[vgprValuC+67], acc143         // copy acc to vreg[143]
v_accvgpr_read_b32 v[vgprValuC+68], acc144         // copy acc to vreg[144]
v_accvgpr_read_b32 v[vgprValuC+69], acc145         // copy acc to vreg[145]
v_accvgpr_read_b32 v[vgprValuC+70], acc146         // copy acc to vreg[146]
v_accvgpr_read_b32 v[vgprValuC+71], acc147         // copy acc to vreg[147]
v_accvgpr_read_b32 v[vgprValuC+76], acc148         // copy acc to vreg[148]
v_accvgpr_read_b32 v[vgprValuC+77], acc149         // copy acc to vreg[149]
v_accvgpr_read_b32 v[vgprValuC+78], acc150         // copy acc to vreg[150]
v_accvgpr_read_b32 v[vgprValuC+79], acc151         // copy acc to vreg[151]
v_accvgpr_read_b32 v[vgprValuC+80], acc152         // copy acc to vreg[152]
v_accvgpr_read_b32 v[vgprValuC+81], acc153         // copy acc to vreg[153]
v_accvgpr_read_b32 v[vgprValuC+82], acc154         // copy acc to vreg[154]
v_accvgpr_read_b32 v[vgprValuC+83], acc155         // copy acc to vreg[155]
v_accvgpr_read_b32 v[vgprValuC+88], acc156         // copy acc to vreg[156]
v_accvgpr_read_b32 v[vgprValuC+89], acc157         // copy acc to vreg[157]
v_accvgpr_read_b32 v[vgprValuC+90], acc158         // copy acc to vreg[158]
v_accvgpr_read_b32 v[vgprValuC+91], acc159         // copy acc to vreg[159]
v_accvgpr_read_b32 v[vgprValuC+92], acc160         // copy acc to vreg[160]
v_accvgpr_read_b32 v[vgprValuC+93], acc161         // copy acc to vreg[161]
v_accvgpr_read_b32 v[vgprValuC+94], acc162         // copy acc to vreg[162]
v_accvgpr_read_b32 v[vgprValuC+95], acc163         // copy acc to vreg[163]
v_accvgpr_read_b32 v[vgprValuC+100], acc164        // copy acc to vreg[164]
v_accvgpr_read_b32 v[vgprValuC+101], acc165        // copy acc to vreg[165]
v_accvgpr_read_b32 v[vgprValuC+102], acc166        // copy acc to vreg[166]
v_accvgpr_read_b32 v[vgprValuC+103], acc167        // copy acc to vreg[167]
v_accvgpr_read_b32 v[vgprValuC+104], acc168        // copy acc to vreg[168]
v_accvgpr_read_b32 v[vgprValuC+105], acc169        // copy acc to vreg[169]
v_accvgpr_read_b32 v[vgprValuC+106], acc170        // copy acc to vreg[170]
v_accvgpr_read_b32 v[vgprValuC+107], acc171        // copy acc to vreg[171]
v_accvgpr_read_b32 v[vgprValuC+112], acc172        // copy acc to vreg[172]
v_accvgpr_read_b32 v[vgprValuC+113], acc173        // copy acc to vreg[173]
v_accvgpr_read_b32 v[vgprValuC+114], acc174        // copy acc to vreg[174]
v_accvgpr_read_b32 v[vgprValuC+115], acc175        // copy acc to vreg[175]
v_accvgpr_read_b32 v[vgprValuC+116], acc176        // copy acc to vreg[176]
v_accvgpr_read_b32 v[vgprValuC+117], acc177        // copy acc to vreg[177]
v_accvgpr_read_b32 v[vgprValuC+118], acc178        // copy acc to vreg[178]
v_accvgpr_read_b32 v[vgprValuC+119], acc179        // copy acc to vreg[179]
v_accvgpr_read_b32 v[vgprValuC+124], acc180        // copy acc to vreg[180]
v_accvgpr_read_b32 v[vgprValuC+125], acc181        // copy acc to vreg[181]
v_accvgpr_read_b32 v[vgprValuC+126], acc182        // copy acc to vreg[182]
v_accvgpr_read_b32 v[vgprValuC+127], acc183        // copy acc to vreg[183]
v_accvgpr_read_b32 v[vgprValuC+128], acc184        // copy acc to vreg[184]
v_accvgpr_read_b32 v[vgprValuC+129], acc185        // copy acc to vreg[185]
v_accvgpr_read_b32 v[vgprValuC+130], acc186        // copy acc to vreg[186]
v_accvgpr_read_b32 v[vgprValuC+131], acc187        // copy acc to vreg[187]
v_accvgpr_read_b32 v[vgprValuC+140], acc188        // copy acc to vreg[188]
v_accvgpr_read_b32 v[vgprValuC+141], acc189        // copy acc to vreg[189]
v_accvgpr_read_b32 v[vgprValuC+142], acc190        // copy acc to vreg[190]
v_accvgpr_read_b32 v[vgprValuC+143], acc191        // copy acc to vreg[191]

/* rC *= alpha batchElements=[(1, 0, 0, 0), (1, 1, 0, 0), (1, 2, 0, 0), (1, 3, 0, 0), (1, 0, 1, 0), (1, 1, 1, 0), (1, 2, 1, 0), (1, 3, 1, 0), (1, 0, 2, 0), (1, 1, 2, 0), (1, 2, 2, 0), (1, 3, 2, 0), (1, 0, 3, 0), (1, 1, 3, 0), (1, 2, 3, 0), (1, 3, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+104], s[sgprAlpha], v[vgprValuC+104] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+107], s[sgprAlpha], v[vgprValuC+107] // *= alpha
v_mul_f32 v[vgprValuC+112], s[sgprAlpha], v[vgprValuC+112] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+114], s[sgprAlpha], v[vgprValuC+114] // *= alpha
v_mul_f32 v[vgprValuC+115], s[sgprAlpha], v[vgprValuC+115] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+118], s[sgprAlpha], v[vgprValuC+118] // *= alpha
v_mul_f32 v[vgprValuC+119], s[sgprAlpha], v[vgprValuC+119] // *= alpha
v_mul_f32 v[vgprValuC+124], s[sgprAlpha], v[vgprValuC+124] // *= alpha
v_mul_f32 v[vgprValuC+125], s[sgprAlpha], v[vgprValuC+125] // *= alpha
v_mul_f32 v[vgprValuC+126], s[sgprAlpha], v[vgprValuC+126] // *= alpha
v_mul_f32 v[vgprValuC+127], s[sgprAlpha], v[vgprValuC+127] // *= alpha
v_mul_f32 v[vgprValuC+128], s[sgprAlpha], v[vgprValuC+128] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+130], s[sgprAlpha], v[vgprValuC+130] // *= alpha
v_mul_f32 v[vgprValuC+131], s[sgprAlpha], v[vgprValuC+131] // *= alpha
v_mul_f32 v[vgprValuC+140], s[sgprAlpha], v[vgprValuC+140] // *= alpha
v_mul_f32 v[vgprValuC+141], s[sgprAlpha], v[vgprValuC+141] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+143], s[sgprAlpha], v[vgprValuC+143] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(6), vmcnt(15)                    // vmcnt(15) = 16 - 1 (beta) lgkmcnt(6) = 8 - 1 (bias) - 1 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+20], s[sgprBeta], v10, v[vgprValuC+20] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+21], s[sgprBeta], v10, v[vgprValuC+21] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+22], s[sgprBeta], v11, v[vgprValuC+22] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+23], s[sgprBeta], v11, v[vgprValuC+23] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
s_mul_i32 s12, s[sgprStrideD1J], 242               // scale StrideD *= numRows(121) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt lgkmcnt(4), vmcnt(15)                    // vmcnt(14) = 16 - 2 (beta) lgkmcnt(4) = 8 - 2 (bias) - 2 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[32:33], v[vgprValuC+36:vgprValuC+36+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[34:35], v[vgprValuC+38:vgprValuC+38+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v24, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v24, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v25, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v25, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[28:29], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[30:31], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D

s_waitcnt lgkmcnt(2), vmcnt(15)                    // vmcnt(13) = 16 - 3 (beta) lgkmcnt(2) = 8 - 3 (bias) - 3 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[44:45], v[vgprValuC+48:vgprValuC+48+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[46:47], v[vgprValuC+50:vgprValuC+50+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v26, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v26, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v27, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v27, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[40:41], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[42:43], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt lgkmcnt(0), vmcnt(15)                    // vmcnt(12) = 16 - 4 (beta) lgkmcnt(0) = 8 - 4 (bias) - 4 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[60:61], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[62:63], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_fma_mix_f32 v[vgprValuC+64], s[sgprBeta], v52, v[vgprValuC+64] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+65], s[sgprBeta], v52, v[vgprValuC+65] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+66], s[sgprBeta], v53, v[vgprValuC+66] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+67], s[sgprBeta], v53, v[vgprValuC+67] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[56:57], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[58:59], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D

s_waitcnt vmcnt(15)                                // vmcnt(11) = 16 - 5 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[16:17], v[vgprValuC+68:vgprValuC+68+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[18:19], v[vgprValuC+70:vgprValuC+70+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+68], s[sgprBeta], v54, v[vgprValuC+68] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+69], s[sgprBeta], v54, v[vgprValuC+69] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+70], s[sgprBeta], v55, v[vgprValuC+70] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+71], s[sgprBeta], v55, v[vgprValuC+71] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+68:vgprValuC+68+1], v[12:13], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[vgprValuC+70:vgprValuC+70+1], v[14:15], v[vgprValuC+70:vgprValuC+70+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+68], v[vgprValuC+68]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+69], v[vgprValuC+69]     // convert C to fp16
v_pack_b32_f16 v68, v[vgprValuC+68], v[vgprValuC+69] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+70], v[vgprValuC+70]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+71], v[vgprValuC+71]     // convert C to fp16
v_pack_b32_f16 v69, v[vgprValuC+70], v[vgprValuC+71] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[68:69], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(10) = 16 - 6 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[32:33], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[34:35], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+76], s[sgprBeta], v72, v[vgprValuC+76] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+77], s[sgprBeta], v72, v[vgprValuC+77] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+78], s[sgprBeta], v73, v[vgprValuC+78] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+79], s[sgprBeta], v73, v[vgprValuC+79] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[28:29], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[30:31], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
buffer_store_dwordx2 v[76:77], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D

s_waitcnt vmcnt(15)                                // vmcnt(9) = 16 - 7 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[44:45], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[46:47], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_fma_mix_f32 v[vgprValuC+80], s[sgprBeta], v74, v[vgprValuC+80] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+81], s[sgprBeta], v74, v[vgprValuC+81] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+82], s[sgprBeta], v75, v[vgprValuC+82] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+83], s[sgprBeta], v75, v[vgprValuC+83] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[40:41], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[42:43], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt vmcnt(15)                                // vmcnt(8) = 16 - 8 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[60:61], v[vgprValuC+88:vgprValuC+88+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[62:63], v[vgprValuC+90:vgprValuC+90+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_fma_mix_f32 v[vgprValuC+88], s[sgprBeta], v84, v[vgprValuC+88] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+89], s[sgprBeta], v84, v[vgprValuC+89] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+90], s[sgprBeta], v85, v[vgprValuC+90] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+91], s[sgprBeta], v85, v[vgprValuC+91] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+88:vgprValuC+88+1], v[56:57], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[vgprValuC+90:vgprValuC+90+1], v[58:59], v[vgprValuC+90:vgprValuC+90+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+88], v[vgprValuC+88]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+89], v[vgprValuC+89]     // convert C to fp16
v_pack_b32_f16 v88, v[vgprValuC+88], v[vgprValuC+89] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+90], v[vgprValuC+90]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+91], v[vgprValuC+91]     // convert C to fp16
v_pack_b32_f16 v89, v[vgprValuC+90], v[vgprValuC+91] // Pack with neighbor
buffer_store_dwordx2 v[88:89], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D

s_waitcnt vmcnt(15)                                // vmcnt(7) = 16 - 9 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[16:17], v[vgprValuC+92:vgprValuC+92+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[18:19], v[vgprValuC+94:vgprValuC+94+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+92], s[sgprBeta], v86, v[vgprValuC+92] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+93], s[sgprBeta], v86, v[vgprValuC+93] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+94], s[sgprBeta], v87, v[vgprValuC+94] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+95], s[sgprBeta], v87, v[vgprValuC+95] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+92:vgprValuC+92+1], v[12:13], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[vgprValuC+94:vgprValuC+94+1], v[14:15], v[vgprValuC+94:vgprValuC+94+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+92], v[vgprValuC+92]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+93], v[vgprValuC+93]     // convert C to fp16
v_pack_b32_f16 v92, v[vgprValuC+92], v[vgprValuC+93] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+94], v[vgprValuC+94]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+95], v[vgprValuC+95]     // convert C to fp16
v_pack_b32_f16 v93, v[vgprValuC+94], v[vgprValuC+95] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[92:93], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(6) = 16 - 10 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[32:33], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[34:35], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+100], s[sgprBeta], v96, v[vgprValuC+100] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+101], s[sgprBeta], v96, v[vgprValuC+101] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+102], s[sgprBeta], v97, v[vgprValuC+102] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+103], s[sgprBeta], v97, v[vgprValuC+103] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[28:29], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[30:31], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
buffer_store_dwordx2 v[100:101], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D

s_waitcnt vmcnt(15)                                // vmcnt(5) = 16 - 11 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[44:45], v[vgprValuC+104:vgprValuC+104+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[46:47], v[vgprValuC+106:vgprValuC+106+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_fma_mix_f32 v[vgprValuC+104], s[sgprBeta], v98, v[vgprValuC+104] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+105], s[sgprBeta], v98, v[vgprValuC+105] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+106], s[sgprBeta], v99, v[vgprValuC+106] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+107], s[sgprBeta], v99, v[vgprValuC+107] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+104:vgprValuC+104+1], v[40:41], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[vgprValuC+106:vgprValuC+106+1], v[42:43], v[vgprValuC+106:vgprValuC+106+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+104], v[vgprValuC+104]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+105], v[vgprValuC+105]   // convert C to fp16
v_pack_b32_f16 v104, v[vgprValuC+104], v[vgprValuC+105] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+106], v[vgprValuC+106]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+107], v[vgprValuC+107]   // convert C to fp16
v_pack_b32_f16 v105, v[vgprValuC+106], v[vgprValuC+107] // Pack with neighbor
buffer_store_dwordx2 v[104:105], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt vmcnt(15)                                // vmcnt(4) = 16 - 12 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+112:vgprValuC+112+1], v[60:61], v[vgprValuC+112:vgprValuC+112+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+114:vgprValuC+114+1], v[62:63], v[vgprValuC+114:vgprValuC+114+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_fma_mix_f32 v[vgprValuC+112], s[sgprBeta], v108, v[vgprValuC+112] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+113], s[sgprBeta], v108, v[vgprValuC+113] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+114], s[sgprBeta], v109, v[vgprValuC+114] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+115], s[sgprBeta], v109, v[vgprValuC+115] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+112:vgprValuC+112+1], v[56:57], v[vgprValuC+112:vgprValuC+112+1] // C += bias
v_pk_add_f32 v[vgprValuC+114:vgprValuC+114+1], v[58:59], v[vgprValuC+114:vgprValuC+114+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+112], v[vgprValuC+112]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+113], v[vgprValuC+113]   // convert C to fp16
v_pack_b32_f16 v112, v[vgprValuC+112], v[vgprValuC+113] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+114], v[vgprValuC+114]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+115], v[vgprValuC+115]   // convert C to fp16
v_pack_b32_f16 v113, v[vgprValuC+114], v[vgprValuC+115] // Pack with neighbor
buffer_store_dwordx2 v[112:113], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D

s_waitcnt vmcnt(15)                                // vmcnt(3) = 16 - 13 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+116:vgprValuC+116+1], v[16:17], v[vgprValuC+116:vgprValuC+116+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+118:vgprValuC+118+1], v[18:19], v[vgprValuC+118:vgprValuC+118+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+116], s[sgprBeta], v110, v[vgprValuC+116] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v110, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+118], s[sgprBeta], v111, v[vgprValuC+118] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+119], s[sgprBeta], v111, v[vgprValuC+119] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+116:vgprValuC+116+1], v[12:13], v[vgprValuC+116:vgprValuC+116+1] // C += bias
v_pk_add_f32 v[vgprValuC+118:vgprValuC+118+1], v[14:15], v[vgprValuC+118:vgprValuC+118+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+116], v[vgprValuC+116]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+117], v[vgprValuC+117]   // convert C to fp16
v_pack_b32_f16 v116, v[vgprValuC+116], v[vgprValuC+117] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+118], v[vgprValuC+118]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+119], v[vgprValuC+119]   // convert C to fp16
v_pack_b32_f16 v117, v[vgprValuC+118], v[vgprValuC+119] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[116:117], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(2) = 16 - 14 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+124:vgprValuC+124+1], v[32:33], v[vgprValuC+124:vgprValuC+124+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+126:vgprValuC+126+1], v[34:35], v[vgprValuC+126:vgprValuC+126+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+124], s[sgprBeta], v120, v[vgprValuC+124] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+125], s[sgprBeta], v120, v[vgprValuC+125] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+126], s[sgprBeta], v121, v[vgprValuC+126] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+127], s[sgprBeta], v121, v[vgprValuC+127] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+124:vgprValuC+124+1], v[28:29], v[vgprValuC+124:vgprValuC+124+1] // C += bias
v_pk_add_f32 v[vgprValuC+126:vgprValuC+126+1], v[30:31], v[vgprValuC+126:vgprValuC+126+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+124], v[vgprValuC+124]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+125], v[vgprValuC+125]   // convert C to fp16
v_pack_b32_f16 v124, v[vgprValuC+124], v[vgprValuC+125] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+126], v[vgprValuC+126]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+127], v[vgprValuC+127]   // convert C to fp16
v_pack_b32_f16 v125, v[vgprValuC+126], v[vgprValuC+127] // Pack with neighbor
buffer_store_dwordx2 v[124:125], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D

s_waitcnt vmcnt(15)                                // vmcnt(1) = 16 - 15 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+128:vgprValuC+128+1], v[44:45], v[vgprValuC+128:vgprValuC+128+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+130:vgprValuC+130+1], v[46:47], v[vgprValuC+130:vgprValuC+130+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_fma_mix_f32 v[vgprValuC+128], s[sgprBeta], v122, v[vgprValuC+128] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+129], s[sgprBeta], v122, v[vgprValuC+129] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+130], s[sgprBeta], v123, v[vgprValuC+130] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+131], s[sgprBeta], v123, v[vgprValuC+131] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+128:vgprValuC+128+1], v[40:41], v[vgprValuC+128:vgprValuC+128+1] // C += bias
v_pk_add_f32 v[vgprValuC+130:vgprValuC+130+1], v[42:43], v[vgprValuC+130:vgprValuC+130+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+128], v[vgprValuC+128]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+129], v[vgprValuC+129]   // convert C to fp16
v_pack_b32_f16 v128, v[vgprValuC+128], v[vgprValuC+129] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+130], v[vgprValuC+130]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+131], v[vgprValuC+131]   // convert C to fp16
v_pack_b32_f16 v129, v[vgprValuC+130], v[vgprValuC+131] // Pack with neighbor
buffer_store_dwordx2 v[128:129], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt vmcnt(15)                                // vmcnt(0) = 16 - 16 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+140:vgprValuC+140+1], v[60:61], v[vgprValuC+140:vgprValuC+140+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+142:vgprValuC+142+1], v[62:63], v[vgprValuC+142:vgprValuC+142+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_fma_mix_f32 v[vgprValuC+140], s[sgprBeta], v132, v[vgprValuC+140] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+141], s[sgprBeta], v132, v[vgprValuC+141] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+142], s[sgprBeta], v133, v[vgprValuC+142] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+143], s[sgprBeta], v133, v[vgprValuC+143] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+140:vgprValuC+140+1], v[56:57], v[vgprValuC+140:vgprValuC+140+1] // C += bias
v_pk_add_f32 v[vgprValuC+142:vgprValuC+142+1], v[58:59], v[vgprValuC+142:vgprValuC+142+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+140], v[vgprValuC+140]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+141], v[vgprValuC+141]   // convert C to fp16
v_pack_b32_f16 v140, v[vgprValuC+140], v[vgprValuC+141] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+142], v[vgprValuC+142]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+143], v[vgprValuC+143]   // convert C to fp16
v_pack_b32_f16 v141, v[vgprValuC+142], v[vgprValuC+143] // Pack with neighbor
buffer_store_dwordx2 v[140:141], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 factorDim=0 */

/******************************************/
/* Global Write Beta Batch #3 (d1,d0,vc1,vc0) = */
/*    (1,0,4,0:vw4); (1,1,4,0:vw4); (1,2,4,0:vw4); (1,3,4,0:vw4); (1,0,5,0:vw4); (1,1,5,0:vw4); (1,2,5,0:vw4); (1,3,5,0:vw4); (1,0,6,0:vw4); (1,1,6,0:vw4); (1,2,6,0:vw4); (1,3,6,0:vw4); (1,0,7,0:vw4); (1,1,7,0:vw4); (1,2,7,0:vw4); (1,3,7,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(1,4,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[10:11], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
ds_read_b128 v[12:15], v8 offset:0                 // load Bias
ds_read_b128 v[16:19], v9 offset:0                 // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,4,1,0) */
buffer_load_dwordx2 v[24:25], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
ds_read_b128 v[28:31], v8 offset:256               // load Bias
ds_read_b128 v[32:35], v9 offset:256               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,4,2,0) */
buffer_load_dwordx2 v[26:27], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:256 // load C
ds_read_b128 v[40:43], v8 offset:512               // load Bias
ds_read_b128 v[44:47], v9 offset:512               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,4,3,0) */
buffer_load_dwordx2 v[52:53], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:384 // load C
ds_read_b128 v[56:59], v8 offset:768               // load Bias
ds_read_b128 v[60:63], v9 offset:768               // load scaleAlpha
/* (d1,vc1,d0,vc0)=(1,5,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[54:55], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(1,5,1,0) */
buffer_load_dwordx2 v[72:73], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
/* (d1,vc1,d0,vc0)=(1,5,2,0) */
buffer_load_dwordx2 v[74:75], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:256 // load C
/* (d1,vc1,d0,vc0)=(1,5,3,0) */
buffer_load_dwordx2 v[84:85], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:384 // load C
/* (d1,vc1,d0,vc0)=(1,6,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[86:87], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(1,6,1,0) */
buffer_load_dwordx2 v[96:97], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
/* (d1,vc1,d0,vc0)=(1,6,2,0) */
buffer_load_dwordx2 v[98:99], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:256 // load C
/* (d1,vc1,d0,vc0)=(1,6,3,0) */
buffer_load_dwordx2 v[108:109], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:384 // load C
/* (d1,vc1,d0,vc0)=(1,7,0,0) */
s_lshl_b32 s12, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[110:111], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(1,7,1,0) */
buffer_load_dwordx2 v[120:121], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:128 // load C
/* (d1,vc1,d0,vc0)=(1,7,2,0) */
buffer_load_dwordx2 v[122:123], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:256 // load C
/* (d1,vc1,d0,vc0)=(1,7,3,0) */
buffer_load_dwordx2 v[132:133], v7, s[sgprSrdC:sgprSrdC+3], 0 offen offset:384 // load C
v_accvgpr_read_b32 v[vgprValuC+20], acc192         // copy acc to vreg[192]
v_accvgpr_read_b32 v[vgprValuC+21], acc193         // copy acc to vreg[193]
v_accvgpr_read_b32 v[vgprValuC+22], acc194         // copy acc to vreg[194]
v_accvgpr_read_b32 v[vgprValuC+23], acc195         // copy acc to vreg[195]
v_accvgpr_read_b32 v[vgprValuC+36], acc196         // copy acc to vreg[196]
v_accvgpr_read_b32 v[vgprValuC+37], acc197         // copy acc to vreg[197]
v_accvgpr_read_b32 v[vgprValuC+38], acc198         // copy acc to vreg[198]
v_accvgpr_read_b32 v[vgprValuC+39], acc199         // copy acc to vreg[199]
v_accvgpr_read_b32 v[vgprValuC+48], acc200         // copy acc to vreg[200]
v_accvgpr_read_b32 v[vgprValuC+49], acc201         // copy acc to vreg[201]
v_accvgpr_read_b32 v[vgprValuC+50], acc202         // copy acc to vreg[202]
v_accvgpr_read_b32 v[vgprValuC+51], acc203         // copy acc to vreg[203]
v_accvgpr_read_b32 v[vgprValuC+64], acc204         // copy acc to vreg[204]
v_accvgpr_read_b32 v[vgprValuC+65], acc205         // copy acc to vreg[205]
v_accvgpr_read_b32 v[vgprValuC+66], acc206         // copy acc to vreg[206]
v_accvgpr_read_b32 v[vgprValuC+67], acc207         // copy acc to vreg[207]
v_accvgpr_read_b32 v[vgprValuC+68], acc208         // copy acc to vreg[208]
v_accvgpr_read_b32 v[vgprValuC+69], acc209         // copy acc to vreg[209]
v_accvgpr_read_b32 v[vgprValuC+70], acc210         // copy acc to vreg[210]
v_accvgpr_read_b32 v[vgprValuC+71], acc211         // copy acc to vreg[211]
v_accvgpr_read_b32 v[vgprValuC+76], acc212         // copy acc to vreg[212]
v_accvgpr_read_b32 v[vgprValuC+77], acc213         // copy acc to vreg[213]
v_accvgpr_read_b32 v[vgprValuC+78], acc214         // copy acc to vreg[214]
v_accvgpr_read_b32 v[vgprValuC+79], acc215         // copy acc to vreg[215]
v_accvgpr_read_b32 v[vgprValuC+80], acc216         // copy acc to vreg[216]
v_accvgpr_read_b32 v[vgprValuC+81], acc217         // copy acc to vreg[217]
v_accvgpr_read_b32 v[vgprValuC+82], acc218         // copy acc to vreg[218]
v_accvgpr_read_b32 v[vgprValuC+83], acc219         // copy acc to vreg[219]
v_accvgpr_read_b32 v[vgprValuC+88], acc220         // copy acc to vreg[220]
v_accvgpr_read_b32 v[vgprValuC+89], acc221         // copy acc to vreg[221]
v_accvgpr_read_b32 v[vgprValuC+90], acc222         // copy acc to vreg[222]
v_accvgpr_read_b32 v[vgprValuC+91], acc223         // copy acc to vreg[223]
v_accvgpr_read_b32 v[vgprValuC+92], acc224         // copy acc to vreg[224]
v_accvgpr_read_b32 v[vgprValuC+93], acc225         // copy acc to vreg[225]
v_accvgpr_read_b32 v[vgprValuC+94], acc226         // copy acc to vreg[226]
v_accvgpr_read_b32 v[vgprValuC+95], acc227         // copy acc to vreg[227]
v_accvgpr_read_b32 v[vgprValuC+100], acc228        // copy acc to vreg[228]
v_accvgpr_read_b32 v[vgprValuC+101], acc229        // copy acc to vreg[229]
v_accvgpr_read_b32 v[vgprValuC+102], acc230        // copy acc to vreg[230]
v_accvgpr_read_b32 v[vgprValuC+103], acc231        // copy acc to vreg[231]
v_accvgpr_read_b32 v[vgprValuC+104], acc232        // copy acc to vreg[232]
v_accvgpr_read_b32 v[vgprValuC+105], acc233        // copy acc to vreg[233]
v_accvgpr_read_b32 v[vgprValuC+106], acc234        // copy acc to vreg[234]
v_accvgpr_read_b32 v[vgprValuC+107], acc235        // copy acc to vreg[235]
v_accvgpr_read_b32 v[vgprValuC+112], acc236        // copy acc to vreg[236]
v_accvgpr_read_b32 v[vgprValuC+113], acc237        // copy acc to vreg[237]
v_accvgpr_read_b32 v[vgprValuC+114], acc238        // copy acc to vreg[238]
v_accvgpr_read_b32 v[vgprValuC+115], acc239        // copy acc to vreg[239]
v_accvgpr_read_b32 v[vgprValuC+116], acc240        // copy acc to vreg[240]
v_accvgpr_read_b32 v[vgprValuC+117], acc241        // copy acc to vreg[241]
v_accvgpr_read_b32 v[vgprValuC+118], acc242        // copy acc to vreg[242]
v_accvgpr_read_b32 v[vgprValuC+119], acc243        // copy acc to vreg[243]
v_accvgpr_read_b32 v[vgprValuC+124], acc244        // copy acc to vreg[244]
v_accvgpr_read_b32 v[vgprValuC+125], acc245        // copy acc to vreg[245]
v_accvgpr_read_b32 v[vgprValuC+126], acc246        // copy acc to vreg[246]
v_accvgpr_read_b32 v[vgprValuC+127], acc247        // copy acc to vreg[247]
v_accvgpr_read_b32 v[vgprValuC+128], acc248        // copy acc to vreg[248]
v_accvgpr_read_b32 v[vgprValuC+129], acc249        // copy acc to vreg[249]
v_accvgpr_read_b32 v[vgprValuC+130], acc250        // copy acc to vreg[250]
v_accvgpr_read_b32 v[vgprValuC+131], acc251        // copy acc to vreg[251]
v_accvgpr_read_b32 v[vgprValuC+140], acc252        // copy acc to vreg[252]
v_accvgpr_read_b32 v[vgprValuC+141], acc253        // copy acc to vreg[253]
v_accvgpr_read_b32 v[vgprValuC+142], acc254        // copy acc to vreg[254]
v_accvgpr_read_b32 v[vgprValuC+143], acc255        // copy acc to vreg[255]

/* rC *= alpha batchElements=[(1, 0, 4, 0), (1, 1, 4, 0), (1, 2, 4, 0), (1, 3, 4, 0), (1, 0, 5, 0), (1, 1, 5, 0), (1, 2, 5, 0), (1, 3, 5, 0), (1, 0, 6, 0), (1, 1, 6, 0), (1, 2, 6, 0), (1, 3, 6, 0), (1, 0, 7, 0), (1, 1, 7, 0), (1, 2, 7, 0), (1, 3, 7, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+104], s[sgprAlpha], v[vgprValuC+104] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+107], s[sgprAlpha], v[vgprValuC+107] // *= alpha
v_mul_f32 v[vgprValuC+112], s[sgprAlpha], v[vgprValuC+112] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+114], s[sgprAlpha], v[vgprValuC+114] // *= alpha
v_mul_f32 v[vgprValuC+115], s[sgprAlpha], v[vgprValuC+115] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+118], s[sgprAlpha], v[vgprValuC+118] // *= alpha
v_mul_f32 v[vgprValuC+119], s[sgprAlpha], v[vgprValuC+119] // *= alpha
v_mul_f32 v[vgprValuC+124], s[sgprAlpha], v[vgprValuC+124] // *= alpha
v_mul_f32 v[vgprValuC+125], s[sgprAlpha], v[vgprValuC+125] // *= alpha
v_mul_f32 v[vgprValuC+126], s[sgprAlpha], v[vgprValuC+126] // *= alpha
v_mul_f32 v[vgprValuC+127], s[sgprAlpha], v[vgprValuC+127] // *= alpha
v_mul_f32 v[vgprValuC+128], s[sgprAlpha], v[vgprValuC+128] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+130], s[sgprAlpha], v[vgprValuC+130] // *= alpha
v_mul_f32 v[vgprValuC+131], s[sgprAlpha], v[vgprValuC+131] // *= alpha
v_mul_f32 v[vgprValuC+140], s[sgprAlpha], v[vgprValuC+140] // *= alpha
v_mul_f32 v[vgprValuC+141], s[sgprAlpha], v[vgprValuC+141] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+143], s[sgprAlpha], v[vgprValuC+143] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(6), vmcnt(15)                    // vmcnt(15) = 16 - 1 (beta) lgkmcnt(6) = 8 - 1 (bias) - 1 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+20], s[sgprBeta], v10, v[vgprValuC+20] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+21], s[sgprBeta], v10, v[vgprValuC+21] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+22], s[sgprBeta], v11, v[vgprValuC+22] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+23], s[sgprBeta], v11, v[vgprValuC+23] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt lgkmcnt(4), vmcnt(15)                    // vmcnt(14) = 16 - 2 (beta) lgkmcnt(4) = 8 - 2 (bias) - 2 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[32:33], v[vgprValuC+36:vgprValuC+36+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[34:35], v[vgprValuC+38:vgprValuC+38+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v24, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v24, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v25, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v25, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[28:29], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[30:31], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D

s_waitcnt lgkmcnt(2), vmcnt(15)                    // vmcnt(13) = 16 - 3 (beta) lgkmcnt(2) = 8 - 3 (bias) - 3 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[44:45], v[vgprValuC+48:vgprValuC+48+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[46:47], v[vgprValuC+50:vgprValuC+50+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v26, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v26, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v27, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v27, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[40:41], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[42:43], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt lgkmcnt(0), vmcnt(15)                    // vmcnt(12) = 16 - 4 (beta) lgkmcnt(0) = 8 - 4 (bias) - 4 (scaleAlphaVec) (interleaved)
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[60:61], v[vgprValuC+64:vgprValuC+64+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[62:63], v[vgprValuC+66:vgprValuC+66+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_fma_mix_f32 v[vgprValuC+64], s[sgprBeta], v52, v[vgprValuC+64] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+65], s[sgprBeta], v52, v[vgprValuC+65] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+66], s[sgprBeta], v53, v[vgprValuC+66] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+67], s[sgprBeta], v53, v[vgprValuC+67] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+64:vgprValuC+64+1], v[56:57], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[vgprValuC+66:vgprValuC+66+1], v[58:59], v[vgprValuC+66:vgprValuC+66+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+64], v[vgprValuC+64]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+65], v[vgprValuC+65]     // convert C to fp16
v_pack_b32_f16 v64, v[vgprValuC+64], v[vgprValuC+65] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+66], v[vgprValuC+66]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+67], v[vgprValuC+67]     // convert C to fp16
v_pack_b32_f16 v65, v[vgprValuC+66], v[vgprValuC+67] // Pack with neighbor
buffer_store_dwordx2 v[64:65], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D

s_waitcnt vmcnt(15)                                // vmcnt(11) = 16 - 5 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[16:17], v[vgprValuC+68:vgprValuC+68+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[18:19], v[vgprValuC+70:vgprValuC+70+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+68], s[sgprBeta], v54, v[vgprValuC+68] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+69], s[sgprBeta], v54, v[vgprValuC+69] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+70], s[sgprBeta], v55, v[vgprValuC+70] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+71], s[sgprBeta], v55, v[vgprValuC+71] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+68:vgprValuC+68+1], v[12:13], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[vgprValuC+70:vgprValuC+70+1], v[14:15], v[vgprValuC+70:vgprValuC+70+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+68], v[vgprValuC+68]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+69], v[vgprValuC+69]     // convert C to fp16
v_pack_b32_f16 v68, v[vgprValuC+68], v[vgprValuC+69] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+70], v[vgprValuC+70]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+71], v[vgprValuC+71]     // convert C to fp16
v_pack_b32_f16 v69, v[vgprValuC+70], v[vgprValuC+71] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[68:69], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(10) = 16 - 6 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[32:33], v[vgprValuC+76:vgprValuC+76+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[34:35], v[vgprValuC+78:vgprValuC+78+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+76], s[sgprBeta], v72, v[vgprValuC+76] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+77], s[sgprBeta], v72, v[vgprValuC+77] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+78], s[sgprBeta], v73, v[vgprValuC+78] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+79], s[sgprBeta], v73, v[vgprValuC+79] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+76:vgprValuC+76+1], v[28:29], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[vgprValuC+78:vgprValuC+78+1], v[30:31], v[vgprValuC+78:vgprValuC+78+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+76], v[vgprValuC+76]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+77], v[vgprValuC+77]     // convert C to fp16
v_pack_b32_f16 v76, v[vgprValuC+76], v[vgprValuC+77] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+78], v[vgprValuC+78]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+79], v[vgprValuC+79]     // convert C to fp16
v_pack_b32_f16 v77, v[vgprValuC+78], v[vgprValuC+79] // Pack with neighbor
buffer_store_dwordx2 v[76:77], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D

s_waitcnt vmcnt(15)                                // vmcnt(9) = 16 - 7 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[44:45], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[46:47], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_fma_mix_f32 v[vgprValuC+80], s[sgprBeta], v74, v[vgprValuC+80] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+81], s[sgprBeta], v74, v[vgprValuC+81] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+82], s[sgprBeta], v75, v[vgprValuC+82] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+83], s[sgprBeta], v75, v[vgprValuC+83] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[40:41], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[42:43], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt vmcnt(15)                                // vmcnt(8) = 16 - 8 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[60:61], v[vgprValuC+88:vgprValuC+88+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[62:63], v[vgprValuC+90:vgprValuC+90+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_fma_mix_f32 v[vgprValuC+88], s[sgprBeta], v84, v[vgprValuC+88] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+89], s[sgprBeta], v84, v[vgprValuC+89] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+90], s[sgprBeta], v85, v[vgprValuC+90] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+91], s[sgprBeta], v85, v[vgprValuC+91] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+88:vgprValuC+88+1], v[56:57], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[vgprValuC+90:vgprValuC+90+1], v[58:59], v[vgprValuC+90:vgprValuC+90+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+88], v[vgprValuC+88]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+89], v[vgprValuC+89]     // convert C to fp16
v_pack_b32_f16 v88, v[vgprValuC+88], v[vgprValuC+89] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+90], v[vgprValuC+90]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+91], v[vgprValuC+91]     // convert C to fp16
v_pack_b32_f16 v89, v[vgprValuC+90], v[vgprValuC+91] // Pack with neighbor
buffer_store_dwordx2 v[88:89], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D

s_waitcnt vmcnt(15)                                // vmcnt(7) = 16 - 9 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[16:17], v[vgprValuC+92:vgprValuC+92+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[18:19], v[vgprValuC+94:vgprValuC+94+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+92], s[sgprBeta], v86, v[vgprValuC+92] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+93], s[sgprBeta], v86, v[vgprValuC+93] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+94], s[sgprBeta], v87, v[vgprValuC+94] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+95], s[sgprBeta], v87, v[vgprValuC+95] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+92:vgprValuC+92+1], v[12:13], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[vgprValuC+94:vgprValuC+94+1], v[14:15], v[vgprValuC+94:vgprValuC+94+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+92], v[vgprValuC+92]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+93], v[vgprValuC+93]     // convert C to fp16
v_pack_b32_f16 v92, v[vgprValuC+92], v[vgprValuC+93] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+94], v[vgprValuC+94]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+95], v[vgprValuC+95]     // convert C to fp16
v_pack_b32_f16 v93, v[vgprValuC+94], v[vgprValuC+95] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[92:93], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(6) = 16 - 10 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[32:33], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[34:35], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+100], s[sgprBeta], v96, v[vgprValuC+100] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+101], s[sgprBeta], v96, v[vgprValuC+101] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+102], s[sgprBeta], v97, v[vgprValuC+102] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+103], s[sgprBeta], v97, v[vgprValuC+103] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[28:29], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[30:31], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
buffer_store_dwordx2 v[100:101], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D

s_waitcnt vmcnt(15)                                // vmcnt(5) = 16 - 11 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[44:45], v[vgprValuC+104:vgprValuC+104+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[46:47], v[vgprValuC+106:vgprValuC+106+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_fma_mix_f32 v[vgprValuC+104], s[sgprBeta], v98, v[vgprValuC+104] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+105], s[sgprBeta], v98, v[vgprValuC+105] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+106], s[sgprBeta], v99, v[vgprValuC+106] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+107], s[sgprBeta], v99, v[vgprValuC+107] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+104:vgprValuC+104+1], v[40:41], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[vgprValuC+106:vgprValuC+106+1], v[42:43], v[vgprValuC+106:vgprValuC+106+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+104], v[vgprValuC+104]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+105], v[vgprValuC+105]   // convert C to fp16
v_pack_b32_f16 v104, v[vgprValuC+104], v[vgprValuC+105] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+106], v[vgprValuC+106]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+107], v[vgprValuC+107]   // convert C to fp16
v_pack_b32_f16 v105, v[vgprValuC+106], v[vgprValuC+107] // Pack with neighbor
buffer_store_dwordx2 v[104:105], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt vmcnt(15)                                // vmcnt(4) = 16 - 12 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+112:vgprValuC+112+1], v[60:61], v[vgprValuC+112:vgprValuC+112+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+114:vgprValuC+114+1], v[62:63], v[vgprValuC+114:vgprValuC+114+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_fma_mix_f32 v[vgprValuC+112], s[sgprBeta], v108, v[vgprValuC+112] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+113], s[sgprBeta], v108, v[vgprValuC+113] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+114], s[sgprBeta], v109, v[vgprValuC+114] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+115], s[sgprBeta], v109, v[vgprValuC+115] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+112:vgprValuC+112+1], v[56:57], v[vgprValuC+112:vgprValuC+112+1] // C += bias
v_pk_add_f32 v[vgprValuC+114:vgprValuC+114+1], v[58:59], v[vgprValuC+114:vgprValuC+114+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+112], v[vgprValuC+112]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+113], v[vgprValuC+113]   // convert C to fp16
v_pack_b32_f16 v112, v[vgprValuC+112], v[vgprValuC+113] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+114], v[vgprValuC+114]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+115], v[vgprValuC+115]   // convert C to fp16
v_pack_b32_f16 v113, v[vgprValuC+114], v[vgprValuC+115] // Pack with neighbor
buffer_store_dwordx2 v[112:113], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D

s_waitcnt vmcnt(15)                                // vmcnt(3) = 16 - 13 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+116:vgprValuC+116+1], v[16:17], v[vgprValuC+116:vgprValuC+116+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+118:vgprValuC+118+1], v[18:19], v[vgprValuC+118:vgprValuC+118+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+116], s[sgprBeta], v110, v[vgprValuC+116] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v110, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+118], s[sgprBeta], v111, v[vgprValuC+118] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+119], s[sgprBeta], v111, v[vgprValuC+119] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+116:vgprValuC+116+1], v[12:13], v[vgprValuC+116:vgprValuC+116+1] // C += bias
v_pk_add_f32 v[vgprValuC+118:vgprValuC+118+1], v[14:15], v[vgprValuC+118:vgprValuC+118+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+116], v[vgprValuC+116]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+117], v[vgprValuC+117]   // convert C to fp16
v_pack_b32_f16 v116, v[vgprValuC+116], v[vgprValuC+117] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+118], v[vgprValuC+118]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+119], v[vgprValuC+119]   // convert C to fp16
v_pack_b32_f16 v117, v[vgprValuC+118], v[vgprValuC+119] // Pack with neighbor
s_lshl_b32 s12, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[116:117], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(2) = 16 - 14 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+124:vgprValuC+124+1], v[32:33], v[vgprValuC+124:vgprValuC+124+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+126:vgprValuC+126+1], v[34:35], v[vgprValuC+126:vgprValuC+126+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+124], s[sgprBeta], v120, v[vgprValuC+124] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+125], s[sgprBeta], v120, v[vgprValuC+125] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+126], s[sgprBeta], v121, v[vgprValuC+126] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+127], s[sgprBeta], v121, v[vgprValuC+127] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+124:vgprValuC+124+1], v[28:29], v[vgprValuC+124:vgprValuC+124+1] // C += bias
v_pk_add_f32 v[vgprValuC+126:vgprValuC+126+1], v[30:31], v[vgprValuC+126:vgprValuC+126+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+124], v[vgprValuC+124]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+125], v[vgprValuC+125]   // convert C to fp16
v_pack_b32_f16 v124, v[vgprValuC+124], v[vgprValuC+125] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+126], v[vgprValuC+126]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+127], v[vgprValuC+127]   // convert C to fp16
v_pack_b32_f16 v125, v[vgprValuC+126], v[vgprValuC+127] // Pack with neighbor
buffer_store_dwordx2 v[124:125], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:128 // store D

s_waitcnt vmcnt(15)                                // vmcnt(1) = 16 - 15 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+128:vgprValuC+128+1], v[44:45], v[vgprValuC+128:vgprValuC+128+1] // *= ScaleAlphaVecVMulPK(44)(0)
v_pk_mul_f32 v[vgprValuC+130:vgprValuC+130+1], v[46:47], v[vgprValuC+130:vgprValuC+130+1] // *= ScaleAlphaVecVMulPK(44)(2)
v_fma_mix_f32 v[vgprValuC+128], s[sgprBeta], v122, v[vgprValuC+128] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+129], s[sgprBeta], v122, v[vgprValuC+129] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+130], s[sgprBeta], v123, v[vgprValuC+130] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+131], s[sgprBeta], v123, v[vgprValuC+131] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+128:vgprValuC+128+1], v[40:41], v[vgprValuC+128:vgprValuC+128+1] // C += bias
v_pk_add_f32 v[vgprValuC+130:vgprValuC+130+1], v[42:43], v[vgprValuC+130:vgprValuC+130+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+128], v[vgprValuC+128]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+129], v[vgprValuC+129]   // convert C to fp16
v_pack_b32_f16 v128, v[vgprValuC+128], v[vgprValuC+129] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+130], v[vgprValuC+130]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+131], v[vgprValuC+131]   // convert C to fp16
v_pack_b32_f16 v129, v[vgprValuC+130], v[vgprValuC+131] // Pack with neighbor
buffer_store_dwordx2 v[128:129], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:256 // store D

s_waitcnt vmcnt(15)                                // vmcnt(0) = 16 - 16 (beta) (interleaved)
v_pk_mul_f32 v[vgprValuC+140:vgprValuC+140+1], v[60:61], v[vgprValuC+140:vgprValuC+140+1] // *= ScaleAlphaVecVMulPK(60)(0)
v_pk_mul_f32 v[vgprValuC+142:vgprValuC+142+1], v[62:63], v[vgprValuC+142:vgprValuC+142+1] // *= ScaleAlphaVecVMulPK(60)(2)
v_fma_mix_f32 v[vgprValuC+140], s[sgprBeta], v132, v[vgprValuC+140] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+141], s[sgprBeta], v132, v[vgprValuC+141] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+142], s[sgprBeta], v133, v[vgprValuC+142] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+143], s[sgprBeta], v133, v[vgprValuC+143] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+140:vgprValuC+140+1], v[56:57], v[vgprValuC+140:vgprValuC+140+1] // C += bias
v_pk_add_f32 v[vgprValuC+142:vgprValuC+142+1], v[58:59], v[vgprValuC+142:vgprValuC+142+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+140], v[vgprValuC+140]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+141], v[vgprValuC+141]   // convert C to fp16
v_pack_b32_f16 v140, v[vgprValuC+140], v[vgprValuC+141] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+142], v[vgprValuC+142]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+143], v[vgprValuC+143]   // convert C to fp16
v_pack_b32_f16 v141, v[vgprValuC+142], v[vgprValuC+143] // Pack with neighbor
buffer_store_dwordx2 v[140:141], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:384 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_2                            // jump to end
label_GW_B1_E1_N:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=12 */
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,1,0,0:vw4); (0,2,0,0:vw4); (0,3,0,0:vw4); (0,0,1,0:vw4); (0,1,1,0:vw4); (0,2,1,0:vw4); (0,3,1,0:vw4); (0,0,2,0:vw4); (0,1,2,0:vw4); (0,2,2,0:vw4); (0,3,2,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v138, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v138, v6, s[56:57]               // LDC clip if OOB. offset
buffer_load_dwordx2 v[10:11], v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v0, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for LDS write
s_barrier                                          // LDS write barrier
ds_read_b128 v[12:15], v7 offset:0                 // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b128 v[16:19], v8 offset:0                 // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v138, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v9, v2, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v138, v9, s[56:57]               // LDC clip if OOB. offset
buffer_load_dwordx2 v[26:27], v9, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v4, s52
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
ds_read_b128 v[28:31], v24 offset:0                // load Bias
v_add_u32 v25, 1024, v24                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[32:35], v25 offset:0                // load scaleAlpha
v_add_lshl_u32 v9, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v138, v9, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v40, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v138, v40, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[44:45], v40, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v41, v4, s52
v_lshlrev_b32 v41, 0x2, v41                        // Bias address scaled by BPE
ds_read_b128 v[48:51], v41 offset:0                // load Bias
v_add_u32 v42, 1024, v41                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[52:55], v42 offset:0                // load scaleAlpha
v_add_lshl_u32 v40, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v138, v40, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v43, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v138, v43, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[60:61], v43, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s52
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
ds_read_b128 v[64:67], v46 offset:0                // load Bias
v_add_u32 v47, 1024, v46                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[68:71], v47 offset:0                // load scaleAlpha
v_add_lshl_u32 v43, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v138, v43, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v62, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v138, v62, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[78:79], v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v0, s52
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
v_add_u32 v76, 1024, v63                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v62, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v138, v62, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v77, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v138, v77, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[86:87], v77, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v84, v4, s52
v_lshlrev_b32 v84, 0x2, v84                        // Bias address scaled by BPE
v_add_u32 v85, 1024, v84                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v138, v77, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v92, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v92, v138, v92, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[96:97], v92, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v93, v4, s52
v_lshlrev_b32 v93, 0x2, v93                        // Bias address scaled by BPE
v_add_u32 v94, 1024, v93                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v92, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v92, v138, v92, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v95, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v95, v138, v95, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[104:105], v95, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v98, v4, s52
v_lshlrev_b32 v98, 0x2, v98                        // Bias address scaled by BPE
v_add_u32 v99, 1024, v98                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v95, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v95, v138, v95, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v106, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v138, v106, s[56:57]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[114:115], v106, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v107, v0, s52
v_lshlrev_b32 v107, 0x2, v107                      // Bias address scaled by BPE
v_add_u32 v112, 1024, v107                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v106, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v138, v106, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v113, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v113, v138, v113, s[56:57]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[122:123], v113, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v120, v4, s52
v_lshlrev_b32 v120, 0x2, v120                      // Bias address scaled by BPE
v_add_u32 v121, 1024, v120                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v113, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v113, v138, v113, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v128, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v138, v128, s[56:57]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[132:133], v128, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v129, v4, s52
v_lshlrev_b32 v129, 0x2, v129                      // Bias address scaled by BPE
v_add_u32 v130, 1024, v129                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v128, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v138, v128, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v131, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v131, v138, v131, s[56:57]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[136:137], v131, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v134, v4, s52
v_lshlrev_b32 v134, 0x2, v134                      // Bias address scaled by BPE
v_add_u32 v135, 1024, v134                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v131, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v131, v138, v131, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc1           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+36], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+37], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+38], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+39], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+56], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+57], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+58], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+59], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+72], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+73], acc13          // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+74], acc14          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+75], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+80], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+81], acc17          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+82], acc18          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+83], acc19          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+88], acc20          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+89], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+90], acc22          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+91], acc23          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+100], acc24         // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+101], acc25         // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+102], acc26         // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+103], acc27         // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+108], acc28         // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+109], acc29         // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+110], acc30         // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+111], acc31         // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+116], acc32         // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+117], acc33         // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+118], acc34         // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+119], acc35         // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+124], acc36         // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+125], acc37         // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+126], acc38         // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+127], acc39         // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+140], acc40         // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+141], acc41         // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+142], acc42         // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+143], acc43         // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+144], acc44         // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+145], acc45         // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+146], acc46         // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+147], acc47         // copy acc to vreg[47]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 1, 0, 0), (0, 2, 0, 0), (0, 3, 0, 0), (0, 0, 1, 0), (0, 1, 1, 0), (0, 2, 1, 0), (0, 3, 1, 0), (0, 0, 2, 0), (0, 1, 2, 0), (0, 2, 2, 0), (0, 3, 2, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+108], s[sgprAlpha], v[vgprValuC+108] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+118], s[sgprAlpha], v[vgprValuC+118] // *= alpha
v_mul_f32 v[vgprValuC+119], s[sgprAlpha], v[vgprValuC+119] // *= alpha
v_mul_f32 v[vgprValuC+124], s[sgprAlpha], v[vgprValuC+124] // *= alpha
v_mul_f32 v[vgprValuC+125], s[sgprAlpha], v[vgprValuC+125] // *= alpha
v_mul_f32 v[vgprValuC+126], s[sgprAlpha], v[vgprValuC+126] // *= alpha
v_mul_f32 v[vgprValuC+127], s[sgprAlpha], v[vgprValuC+127] // *= alpha
v_mul_f32 v[vgprValuC+140], s[sgprAlpha], v[vgprValuC+140] // *= alpha
v_mul_f32 v[vgprValuC+141], s[sgprAlpha], v[vgprValuC+141] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+143], s[sgprAlpha], v[vgprValuC+143] // *= alpha
v_mul_f32 v[vgprValuC+144], s[sgprAlpha], v[vgprValuC+144] // *= alpha
v_mul_f32 v[vgprValuC+145], s[sgprAlpha], v[vgprValuC+145] // *= alpha
v_mul_f32 v[vgprValuC+146], s[sgprAlpha], v[vgprValuC+146] // *= alpha
v_mul_f32 v[vgprValuC+147], s[sgprAlpha], v[vgprValuC+147] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+20], s[sgprBeta], v10, v[vgprValuC+20] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+21], s[sgprBeta], v10, v[vgprValuC+21] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+22], s[sgprBeta], v11, v[vgprValuC+22] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+23], s[sgprBeta], v11, v[vgprValuC+23] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[32:33], v[vgprValuC+36:vgprValuC+36+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[34:35], v[vgprValuC+38:vgprValuC+38+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v26, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v26, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v27, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v27, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[28:29], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[30:31], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[52:53], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[54:55], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_fma_mix_f32 v[vgprValuC+56], s[sgprBeta], v44, v[vgprValuC+56] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+57], s[sgprBeta], v44, v[vgprValuC+57] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+58], s[sgprBeta], v45, v[vgprValuC+58] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+59], s[sgprBeta], v45, v[vgprValuC+59] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[48:49], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[50:51], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[68:69], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(68)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[70:71], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(68)(2)
v_fma_mix_f32 v[vgprValuC+72], s[sgprBeta], v60, v[vgprValuC+72] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+73], s[sgprBeta], v60, v[vgprValuC+73] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+74], s[sgprBeta], v61, v[vgprValuC+74] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v61, v[vgprValuC+75] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[64:65], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[66:67], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v43, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[16:17], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[18:19], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+80], s[sgprBeta], v78, v[vgprValuC+80] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+81], s[sgprBeta], v78, v[vgprValuC+81] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+82], s[sgprBeta], v79, v[vgprValuC+82] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+83], s[sgprBeta], v79, v[vgprValuC+83] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[12:13], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[14:15], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[32:33], v[vgprValuC+88:vgprValuC+88+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[34:35], v[vgprValuC+90:vgprValuC+90+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+88], s[sgprBeta], v86, v[vgprValuC+88] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+89], s[sgprBeta], v86, v[vgprValuC+89] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+90], s[sgprBeta], v87, v[vgprValuC+90] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+91], s[sgprBeta], v87, v[vgprValuC+91] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+88:vgprValuC+88+1], v[28:29], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[vgprValuC+90:vgprValuC+90+1], v[30:31], v[vgprValuC+90:vgprValuC+90+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+88], v[vgprValuC+88]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+89], v[vgprValuC+89]     // convert C to fp16
v_pack_b32_f16 v88, v[vgprValuC+88], v[vgprValuC+89] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+90], v[vgprValuC+90]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+91], v[vgprValuC+91]     // convert C to fp16
v_pack_b32_f16 v89, v[vgprValuC+90], v[vgprValuC+91] // Pack with neighbor
buffer_store_dwordx2 v[88:89], v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[52:53], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[54:55], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_fma_mix_f32 v[vgprValuC+100], s[sgprBeta], v96, v[vgprValuC+100] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+101], s[sgprBeta], v96, v[vgprValuC+101] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+102], s[sgprBeta], v97, v[vgprValuC+102] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+103], s[sgprBeta], v97, v[vgprValuC+103] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[48:49], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[50:51], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
buffer_store_dwordx2 v[100:101], v92, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[68:69], v[vgprValuC+108:vgprValuC+108+1] // *= ScaleAlphaVecVMulPK(68)(0)
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[70:71], v[vgprValuC+110:vgprValuC+110+1] // *= ScaleAlphaVecVMulPK(68)(2)
v_fma_mix_f32 v[vgprValuC+108], s[sgprBeta], v104, v[vgprValuC+108] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+109], s[sgprBeta], v104, v[vgprValuC+109] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+110], s[sgprBeta], v105, v[vgprValuC+110] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+111], s[sgprBeta], v105, v[vgprValuC+111] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+108:vgprValuC+108+1], v[64:65], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[vgprValuC+110:vgprValuC+110+1], v[66:67], v[vgprValuC+110:vgprValuC+110+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+108], v[vgprValuC+108]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+109], v[vgprValuC+109]   // convert C to fp16
v_pack_b32_f16 v108, v[vgprValuC+108], v[vgprValuC+109] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+110], v[vgprValuC+110]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+111], v[vgprValuC+111]   // convert C to fp16
v_pack_b32_f16 v109, v[vgprValuC+110], v[vgprValuC+111] // Pack with neighbor
buffer_store_dwordx2 v[108:109], v95, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+116:vgprValuC+116+1], v[16:17], v[vgprValuC+116:vgprValuC+116+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+118:vgprValuC+118+1], v[18:19], v[vgprValuC+118:vgprValuC+118+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+116], s[sgprBeta], v114, v[vgprValuC+116] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v114, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+118], s[sgprBeta], v115, v[vgprValuC+118] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+119], s[sgprBeta], v115, v[vgprValuC+119] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+116:vgprValuC+116+1], v[12:13], v[vgprValuC+116:vgprValuC+116+1] // C += bias
v_pk_add_f32 v[vgprValuC+118:vgprValuC+118+1], v[14:15], v[vgprValuC+118:vgprValuC+118+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+116], v[vgprValuC+116]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+117], v[vgprValuC+117]   // convert C to fp16
v_pack_b32_f16 v116, v[vgprValuC+116], v[vgprValuC+117] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+118], v[vgprValuC+118]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+119], v[vgprValuC+119]   // convert C to fp16
v_pack_b32_f16 v117, v[vgprValuC+118], v[vgprValuC+119] // Pack with neighbor
buffer_store_dwordx2 v[116:117], v106, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+124:vgprValuC+124+1], v[32:33], v[vgprValuC+124:vgprValuC+124+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+126:vgprValuC+126+1], v[34:35], v[vgprValuC+126:vgprValuC+126+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+124], s[sgprBeta], v122, v[vgprValuC+124] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+125], s[sgprBeta], v122, v[vgprValuC+125] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+126], s[sgprBeta], v123, v[vgprValuC+126] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+127], s[sgprBeta], v123, v[vgprValuC+127] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+124:vgprValuC+124+1], v[28:29], v[vgprValuC+124:vgprValuC+124+1] // C += bias
v_pk_add_f32 v[vgprValuC+126:vgprValuC+126+1], v[30:31], v[vgprValuC+126:vgprValuC+126+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+124], v[vgprValuC+124]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+125], v[vgprValuC+125]   // convert C to fp16
v_pack_b32_f16 v124, v[vgprValuC+124], v[vgprValuC+125] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+126], v[vgprValuC+126]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+127], v[vgprValuC+127]   // convert C to fp16
v_pack_b32_f16 v125, v[vgprValuC+126], v[vgprValuC+127] // Pack with neighbor
buffer_store_dwordx2 v[124:125], v113, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+140:vgprValuC+140+1], v[52:53], v[vgprValuC+140:vgprValuC+140+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+142:vgprValuC+142+1], v[54:55], v[vgprValuC+142:vgprValuC+142+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_fma_mix_f32 v[vgprValuC+140], s[sgprBeta], v132, v[vgprValuC+140] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+141], s[sgprBeta], v132, v[vgprValuC+141] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+142], s[sgprBeta], v133, v[vgprValuC+142] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+143], s[sgprBeta], v133, v[vgprValuC+143] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+140:vgprValuC+140+1], v[48:49], v[vgprValuC+140:vgprValuC+140+1] // C += bias
v_pk_add_f32 v[vgprValuC+142:vgprValuC+142+1], v[50:51], v[vgprValuC+142:vgprValuC+142+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+140], v[vgprValuC+140]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+141], v[vgprValuC+141]   // convert C to fp16
v_pack_b32_f16 v140, v[vgprValuC+140], v[vgprValuC+141] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+142], v[vgprValuC+142]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+143], v[vgprValuC+143]   // convert C to fp16
v_pack_b32_f16 v141, v[vgprValuC+142], v[vgprValuC+143] // Pack with neighbor
buffer_store_dwordx2 v[140:141], v128, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+144:vgprValuC+144+1], v[68:69], v[vgprValuC+144:vgprValuC+144+1] // *= ScaleAlphaVecVMulPK(68)(0)
v_pk_mul_f32 v[vgprValuC+146:vgprValuC+146+1], v[70:71], v[vgprValuC+146:vgprValuC+146+1] // *= ScaleAlphaVecVMulPK(68)(2)
v_fma_mix_f32 v[vgprValuC+144], s[sgprBeta], v136, v[vgprValuC+144] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+145], s[sgprBeta], v136, v[vgprValuC+145] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+146], s[sgprBeta], v137, v[vgprValuC+146] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+147], s[sgprBeta], v137, v[vgprValuC+147] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+144:vgprValuC+144+1], v[64:65], v[vgprValuC+144:vgprValuC+144+1] // C += bias
v_pk_add_f32 v[vgprValuC+146:vgprValuC+146+1], v[66:67], v[vgprValuC+146:vgprValuC+146+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+144], v[vgprValuC+144]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+145], v[vgprValuC+145]   // convert C to fp16
v_pack_b32_f16 v144, v[vgprValuC+144], v[vgprValuC+145] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+146], v[vgprValuC+146]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+147], v[vgprValuC+147]   // convert C to fp16
v_pack_b32_f16 v145, v[vgprValuC+146], v[vgprValuC+147] // Pack with neighbor
buffer_store_dwordx2 v[144:145], v131, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,0,3,0:vw4); (0,1,3,0:vw4); (0,2,3,0:vw4); (0,3,3,0:vw4); (0,0,4,0:vw4); (0,1,4,0:vw4); (0,2,4,0:vw4); (0,3,4,0:vw4); (0,0,5,0:vw4); (0,1,5,0:vw4); (0,2,5,0:vw4); (0,3,5,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v138, BufferOOB
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v138, v6, s[56:57]               // LDC clip if OOB. offset
buffer_load_dwordx2 v[10:11], v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v0, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b128 v[12:15], v7 offset:0                 // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b128 v[16:19], v8 offset:0                 // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v138, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v9, v2, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v138, v9, s[56:57]               // LDC clip if OOB. offset
buffer_load_dwordx2 v[26:27], v9, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v4, s52
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
ds_read_b128 v[28:31], v24 offset:0                // load Bias
v_add_u32 v25, 1024, v24                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[32:35], v25 offset:0                // load scaleAlpha
v_add_lshl_u32 v9, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v138, v9, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v40, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v138, v40, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[44:45], v40, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v41, v4, s52
v_lshlrev_b32 v41, 0x2, v41                        // Bias address scaled by BPE
ds_read_b128 v[48:51], v41 offset:0                // load Bias
v_add_u32 v42, 1024, v41                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[52:55], v42 offset:0                // load scaleAlpha
v_add_lshl_u32 v40, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v138, v40, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v43, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v138, v43, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[60:61], v43, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s52
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
ds_read_b128 v[64:67], v46 offset:0                // load Bias
v_add_u32 v47, 1024, v46                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[68:71], v47 offset:0                // load scaleAlpha
v_add_lshl_u32 v43, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v138, v43, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v62, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v138, v62, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[78:79], v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v0, s52
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
v_add_u32 v76, 1024, v63                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v62, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v138, v62, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v77, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v138, v77, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[86:87], v77, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v84, v4, s52
v_lshlrev_b32 v84, 0x2, v84                        // Bias address scaled by BPE
v_add_u32 v85, 1024, v84                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v138, v77, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v92, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v92, v138, v92, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[96:97], v92, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v93, v4, s52
v_lshlrev_b32 v93, 0x2, v93                        // Bias address scaled by BPE
v_add_u32 v94, 1024, v93                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v92, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v92, v138, v92, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v95, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v95, v138, v95, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[104:105], v95, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v98, v4, s52
v_lshlrev_b32 v98, 0x2, v98                        // Bias address scaled by BPE
v_add_u32 v99, 1024, v98                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v95, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v95, v138, v95, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v106, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v138, v106, s[56:57]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[114:115], v106, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v107, v0, s52
v_lshlrev_b32 v107, 0x2, v107                      // Bias address scaled by BPE
v_add_u32 v112, 1024, v107                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v106, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v138, v106, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v113, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v113, v138, v113, s[56:57]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[122:123], v113, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v120, v4, s52
v_lshlrev_b32 v120, 0x2, v120                      // Bias address scaled by BPE
v_add_u32 v121, 1024, v120                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v113, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v113, v138, v113, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v128, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v138, v128, s[56:57]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[132:133], v128, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v129, v4, s52
v_lshlrev_b32 v129, 0x2, v129                      // Bias address scaled by BPE
v_add_u32 v130, 1024, v129                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v128, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v138, v128, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v131, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v131, v138, v131, s[56:57]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[136:137], v131, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v134, v4, s52
v_lshlrev_b32 v134, 0x2, v134                      // Bias address scaled by BPE
v_add_u32 v135, 1024, v134                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v131, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v131, v138, v131, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+21], acc49          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+22], acc50          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+23], acc51          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+36], acc52          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+37], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+38], acc54          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+39], acc55          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+56], acc56          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+57], acc57          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+58], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+59], acc59          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+72], acc60          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+73], acc61          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+74], acc62          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+75], acc63          // copy acc to vreg[63]
v_accvgpr_read_b32 v[vgprValuC+80], acc64          // copy acc to vreg[64]
v_accvgpr_read_b32 v[vgprValuC+81], acc65          // copy acc to vreg[65]
v_accvgpr_read_b32 v[vgprValuC+82], acc66          // copy acc to vreg[66]
v_accvgpr_read_b32 v[vgprValuC+83], acc67          // copy acc to vreg[67]
v_accvgpr_read_b32 v[vgprValuC+88], acc68          // copy acc to vreg[68]
v_accvgpr_read_b32 v[vgprValuC+89], acc69          // copy acc to vreg[69]
v_accvgpr_read_b32 v[vgprValuC+90], acc70          // copy acc to vreg[70]
v_accvgpr_read_b32 v[vgprValuC+91], acc71          // copy acc to vreg[71]
v_accvgpr_read_b32 v[vgprValuC+100], acc72         // copy acc to vreg[72]
v_accvgpr_read_b32 v[vgprValuC+101], acc73         // copy acc to vreg[73]
v_accvgpr_read_b32 v[vgprValuC+102], acc74         // copy acc to vreg[74]
v_accvgpr_read_b32 v[vgprValuC+103], acc75         // copy acc to vreg[75]
v_accvgpr_read_b32 v[vgprValuC+108], acc76         // copy acc to vreg[76]
v_accvgpr_read_b32 v[vgprValuC+109], acc77         // copy acc to vreg[77]
v_accvgpr_read_b32 v[vgprValuC+110], acc78         // copy acc to vreg[78]
v_accvgpr_read_b32 v[vgprValuC+111], acc79         // copy acc to vreg[79]
v_accvgpr_read_b32 v[vgprValuC+116], acc80         // copy acc to vreg[80]
v_accvgpr_read_b32 v[vgprValuC+117], acc81         // copy acc to vreg[81]
v_accvgpr_read_b32 v[vgprValuC+118], acc82         // copy acc to vreg[82]
v_accvgpr_read_b32 v[vgprValuC+119], acc83         // copy acc to vreg[83]
v_accvgpr_read_b32 v[vgprValuC+124], acc84         // copy acc to vreg[84]
v_accvgpr_read_b32 v[vgprValuC+125], acc85         // copy acc to vreg[85]
v_accvgpr_read_b32 v[vgprValuC+126], acc86         // copy acc to vreg[86]
v_accvgpr_read_b32 v[vgprValuC+127], acc87         // copy acc to vreg[87]
v_accvgpr_read_b32 v[vgprValuC+140], acc88         // copy acc to vreg[88]
v_accvgpr_read_b32 v[vgprValuC+141], acc89         // copy acc to vreg[89]
v_accvgpr_read_b32 v[vgprValuC+142], acc90         // copy acc to vreg[90]
v_accvgpr_read_b32 v[vgprValuC+143], acc91         // copy acc to vreg[91]
v_accvgpr_read_b32 v[vgprValuC+144], acc92         // copy acc to vreg[92]
v_accvgpr_read_b32 v[vgprValuC+145], acc93         // copy acc to vreg[93]
v_accvgpr_read_b32 v[vgprValuC+146], acc94         // copy acc to vreg[94]
v_accvgpr_read_b32 v[vgprValuC+147], acc95         // copy acc to vreg[95]

/* rC *= alpha batchElements=[(0, 0, 3, 0), (0, 1, 3, 0), (0, 2, 3, 0), (0, 3, 3, 0), (0, 0, 4, 0), (0, 1, 4, 0), (0, 2, 4, 0), (0, 3, 4, 0), (0, 0, 5, 0), (0, 1, 5, 0), (0, 2, 5, 0), (0, 3, 5, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+108], s[sgprAlpha], v[vgprValuC+108] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+118], s[sgprAlpha], v[vgprValuC+118] // *= alpha
v_mul_f32 v[vgprValuC+119], s[sgprAlpha], v[vgprValuC+119] // *= alpha
v_mul_f32 v[vgprValuC+124], s[sgprAlpha], v[vgprValuC+124] // *= alpha
v_mul_f32 v[vgprValuC+125], s[sgprAlpha], v[vgprValuC+125] // *= alpha
v_mul_f32 v[vgprValuC+126], s[sgprAlpha], v[vgprValuC+126] // *= alpha
v_mul_f32 v[vgprValuC+127], s[sgprAlpha], v[vgprValuC+127] // *= alpha
v_mul_f32 v[vgprValuC+140], s[sgprAlpha], v[vgprValuC+140] // *= alpha
v_mul_f32 v[vgprValuC+141], s[sgprAlpha], v[vgprValuC+141] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+143], s[sgprAlpha], v[vgprValuC+143] // *= alpha
v_mul_f32 v[vgprValuC+144], s[sgprAlpha], v[vgprValuC+144] // *= alpha
v_mul_f32 v[vgprValuC+145], s[sgprAlpha], v[vgprValuC+145] // *= alpha
v_mul_f32 v[vgprValuC+146], s[sgprAlpha], v[vgprValuC+146] // *= alpha
v_mul_f32 v[vgprValuC+147], s[sgprAlpha], v[vgprValuC+147] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+20], s[sgprBeta], v10, v[vgprValuC+20] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+21], s[sgprBeta], v10, v[vgprValuC+21] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+22], s[sgprBeta], v11, v[vgprValuC+22] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+23], s[sgprBeta], v11, v[vgprValuC+23] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[32:33], v[vgprValuC+36:vgprValuC+36+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[34:35], v[vgprValuC+38:vgprValuC+38+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v26, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v26, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v27, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v27, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[28:29], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[30:31], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[52:53], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[54:55], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_fma_mix_f32 v[vgprValuC+56], s[sgprBeta], v44, v[vgprValuC+56] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+57], s[sgprBeta], v44, v[vgprValuC+57] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+58], s[sgprBeta], v45, v[vgprValuC+58] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+59], s[sgprBeta], v45, v[vgprValuC+59] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[48:49], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[50:51], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[68:69], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(68)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[70:71], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(68)(2)
v_fma_mix_f32 v[vgprValuC+72], s[sgprBeta], v60, v[vgprValuC+72] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+73], s[sgprBeta], v60, v[vgprValuC+73] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+74], s[sgprBeta], v61, v[vgprValuC+74] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v61, v[vgprValuC+75] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[64:65], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[66:67], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v43, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[16:17], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[18:19], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+80], s[sgprBeta], v78, v[vgprValuC+80] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+81], s[sgprBeta], v78, v[vgprValuC+81] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+82], s[sgprBeta], v79, v[vgprValuC+82] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+83], s[sgprBeta], v79, v[vgprValuC+83] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[12:13], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[14:15], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[32:33], v[vgprValuC+88:vgprValuC+88+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[34:35], v[vgprValuC+90:vgprValuC+90+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+88], s[sgprBeta], v86, v[vgprValuC+88] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+89], s[sgprBeta], v86, v[vgprValuC+89] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+90], s[sgprBeta], v87, v[vgprValuC+90] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+91], s[sgprBeta], v87, v[vgprValuC+91] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+88:vgprValuC+88+1], v[28:29], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[vgprValuC+90:vgprValuC+90+1], v[30:31], v[vgprValuC+90:vgprValuC+90+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+88], v[vgprValuC+88]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+89], v[vgprValuC+89]     // convert C to fp16
v_pack_b32_f16 v88, v[vgprValuC+88], v[vgprValuC+89] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+90], v[vgprValuC+90]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+91], v[vgprValuC+91]     // convert C to fp16
v_pack_b32_f16 v89, v[vgprValuC+90], v[vgprValuC+91] // Pack with neighbor
buffer_store_dwordx2 v[88:89], v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[52:53], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[54:55], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_fma_mix_f32 v[vgprValuC+100], s[sgprBeta], v96, v[vgprValuC+100] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+101], s[sgprBeta], v96, v[vgprValuC+101] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+102], s[sgprBeta], v97, v[vgprValuC+102] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+103], s[sgprBeta], v97, v[vgprValuC+103] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[48:49], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[50:51], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
buffer_store_dwordx2 v[100:101], v92, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[68:69], v[vgprValuC+108:vgprValuC+108+1] // *= ScaleAlphaVecVMulPK(68)(0)
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[70:71], v[vgprValuC+110:vgprValuC+110+1] // *= ScaleAlphaVecVMulPK(68)(2)
v_fma_mix_f32 v[vgprValuC+108], s[sgprBeta], v104, v[vgprValuC+108] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+109], s[sgprBeta], v104, v[vgprValuC+109] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+110], s[sgprBeta], v105, v[vgprValuC+110] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+111], s[sgprBeta], v105, v[vgprValuC+111] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+108:vgprValuC+108+1], v[64:65], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[vgprValuC+110:vgprValuC+110+1], v[66:67], v[vgprValuC+110:vgprValuC+110+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+108], v[vgprValuC+108]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+109], v[vgprValuC+109]   // convert C to fp16
v_pack_b32_f16 v108, v[vgprValuC+108], v[vgprValuC+109] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+110], v[vgprValuC+110]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+111], v[vgprValuC+111]   // convert C to fp16
v_pack_b32_f16 v109, v[vgprValuC+110], v[vgprValuC+111] // Pack with neighbor
buffer_store_dwordx2 v[108:109], v95, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+116:vgprValuC+116+1], v[16:17], v[vgprValuC+116:vgprValuC+116+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+118:vgprValuC+118+1], v[18:19], v[vgprValuC+118:vgprValuC+118+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+116], s[sgprBeta], v114, v[vgprValuC+116] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v114, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+118], s[sgprBeta], v115, v[vgprValuC+118] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+119], s[sgprBeta], v115, v[vgprValuC+119] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+116:vgprValuC+116+1], v[12:13], v[vgprValuC+116:vgprValuC+116+1] // C += bias
v_pk_add_f32 v[vgprValuC+118:vgprValuC+118+1], v[14:15], v[vgprValuC+118:vgprValuC+118+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+116], v[vgprValuC+116]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+117], v[vgprValuC+117]   // convert C to fp16
v_pack_b32_f16 v116, v[vgprValuC+116], v[vgprValuC+117] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+118], v[vgprValuC+118]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+119], v[vgprValuC+119]   // convert C to fp16
v_pack_b32_f16 v117, v[vgprValuC+118], v[vgprValuC+119] // Pack with neighbor
buffer_store_dwordx2 v[116:117], v106, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+124:vgprValuC+124+1], v[32:33], v[vgprValuC+124:vgprValuC+124+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+126:vgprValuC+126+1], v[34:35], v[vgprValuC+126:vgprValuC+126+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+124], s[sgprBeta], v122, v[vgprValuC+124] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+125], s[sgprBeta], v122, v[vgprValuC+125] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+126], s[sgprBeta], v123, v[vgprValuC+126] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+127], s[sgprBeta], v123, v[vgprValuC+127] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+124:vgprValuC+124+1], v[28:29], v[vgprValuC+124:vgprValuC+124+1] // C += bias
v_pk_add_f32 v[vgprValuC+126:vgprValuC+126+1], v[30:31], v[vgprValuC+126:vgprValuC+126+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+124], v[vgprValuC+124]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+125], v[vgprValuC+125]   // convert C to fp16
v_pack_b32_f16 v124, v[vgprValuC+124], v[vgprValuC+125] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+126], v[vgprValuC+126]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+127], v[vgprValuC+127]   // convert C to fp16
v_pack_b32_f16 v125, v[vgprValuC+126], v[vgprValuC+127] // Pack with neighbor
buffer_store_dwordx2 v[124:125], v113, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+140:vgprValuC+140+1], v[52:53], v[vgprValuC+140:vgprValuC+140+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+142:vgprValuC+142+1], v[54:55], v[vgprValuC+142:vgprValuC+142+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_fma_mix_f32 v[vgprValuC+140], s[sgprBeta], v132, v[vgprValuC+140] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+141], s[sgprBeta], v132, v[vgprValuC+141] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+142], s[sgprBeta], v133, v[vgprValuC+142] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+143], s[sgprBeta], v133, v[vgprValuC+143] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+140:vgprValuC+140+1], v[48:49], v[vgprValuC+140:vgprValuC+140+1] // C += bias
v_pk_add_f32 v[vgprValuC+142:vgprValuC+142+1], v[50:51], v[vgprValuC+142:vgprValuC+142+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+140], v[vgprValuC+140]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+141], v[vgprValuC+141]   // convert C to fp16
v_pack_b32_f16 v140, v[vgprValuC+140], v[vgprValuC+141] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+142], v[vgprValuC+142]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+143], v[vgprValuC+143]   // convert C to fp16
v_pack_b32_f16 v141, v[vgprValuC+142], v[vgprValuC+143] // Pack with neighbor
buffer_store_dwordx2 v[140:141], v128, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+144:vgprValuC+144+1], v[68:69], v[vgprValuC+144:vgprValuC+144+1] // *= ScaleAlphaVecVMulPK(68)(0)
v_pk_mul_f32 v[vgprValuC+146:vgprValuC+146+1], v[70:71], v[vgprValuC+146:vgprValuC+146+1] // *= ScaleAlphaVecVMulPK(68)(2)
v_fma_mix_f32 v[vgprValuC+144], s[sgprBeta], v136, v[vgprValuC+144] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+145], s[sgprBeta], v136, v[vgprValuC+145] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+146], s[sgprBeta], v137, v[vgprValuC+146] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+147], s[sgprBeta], v137, v[vgprValuC+147] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+144:vgprValuC+144+1], v[64:65], v[vgprValuC+144:vgprValuC+144+1] // C += bias
v_pk_add_f32 v[vgprValuC+146:vgprValuC+146+1], v[66:67], v[vgprValuC+146:vgprValuC+146+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+144], v[vgprValuC+144]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+145], v[vgprValuC+145]   // convert C to fp16
v_pack_b32_f16 v144, v[vgprValuC+144], v[vgprValuC+145] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+146], v[vgprValuC+146]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+147], v[vgprValuC+147]   // convert C to fp16
v_pack_b32_f16 v145, v[vgprValuC+146], v[vgprValuC+147] // Pack with neighbor
buffer_store_dwordx2 v[144:145], v131, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #2 (d1,d0,vc1,vc0) = */
/*    (0,0,6,0:vw4); (0,1,6,0:vw4); (0,2,6,0:vw4); (0,3,6,0:vw4); (0,0,7,0:vw4); (0,1,7,0:vw4); (0,2,7,0:vw4); (0,3,7,0:vw4); (1,0,0,0:vw4); (1,1,0,0:vw4); (1,2,0,0:vw4); (1,3,0,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v138, BufferOOB
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v138, v6, s[56:57]               // LDC clip if OOB. offset
buffer_load_dwordx2 v[10:11], v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v0, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b128 v[12:15], v7 offset:0                 // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b128 v[16:19], v8 offset:0                 // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v138, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v9, v2, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v138, v9, s[56:57]               // LDC clip if OOB. offset
buffer_load_dwordx2 v[26:27], v9, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v4, s52
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
ds_read_b128 v[28:31], v24 offset:0                // load Bias
v_add_u32 v25, 1024, v24                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[32:35], v25 offset:0                // load scaleAlpha
v_add_lshl_u32 v9, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v138, v9, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v40, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v138, v40, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[44:45], v40, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v41, v4, s52
v_lshlrev_b32 v41, 0x2, v41                        // Bias address scaled by BPE
ds_read_b128 v[48:51], v41 offset:0                // load Bias
v_add_u32 v42, 1024, v41                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[52:55], v42 offset:0                // load scaleAlpha
v_add_lshl_u32 v40, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v138, v40, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v43, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v138, v43, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[60:61], v43, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s52
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
ds_read_b128 v[64:67], v46 offset:0                // load Bias
v_add_u32 v47, 1024, v46                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[68:71], v47 offset:0                // load scaleAlpha
v_add_lshl_u32 v43, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v138, v43, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v62, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v138, v62, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[78:79], v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v0, s52
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
v_add_u32 v76, 1024, v63                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v62, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v138, v62, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v77, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v138, v77, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[86:87], v77, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v84, v4, s52
v_lshlrev_b32 v84, 0x2, v84                        // Bias address scaled by BPE
v_add_u32 v85, 1024, v84                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v138, v77, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v92, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v92, v138, v92, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[96:97], v92, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v93, v4, s52
v_lshlrev_b32 v93, 0x2, v93                        // Bias address scaled by BPE
v_add_u32 v94, 1024, v93                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v92, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v92, v138, v92, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v95, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v95, v138, v95, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[104:105], v95, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v98, v4, s52
v_lshlrev_b32 v98, 0x2, v98                        // Bias address scaled by BPE
v_add_u32 v99, 1024, v98                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v95, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v95, v138, v95, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,0) */
s_mov_b32 s52, 121                                 // rowInc d1=0 vc1=0
v_add_co_u32 v1, vcc, v1, s52                      // coord1.2: coord1 += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s52, s[sgprStrideC1J], 121               // scale stride
v_add_i32 v2, v2, s52                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s52, s[sgprStrideD1J], 121               // scale stride
v_add_i32 v3, v3, s52                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v106, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v138, v106, s[56:57]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[114:115], v106, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v107, v0, s52
v_lshlrev_b32 v107, 0x2, v107                      // Bias address scaled by BPE
v_add_u32 v112, 1024, v107                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v106, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v138, v106, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v113, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v113, v138, v113, s[56:57]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[122:123], v113, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v120, v4, s52
v_lshlrev_b32 v120, 0x2, v120                      // Bias address scaled by BPE
v_add_u32 v121, 1024, v120                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v113, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v113, v138, v113, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v128, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v138, v128, s[56:57]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[132:133], v128, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v129, v4, s52
v_lshlrev_b32 v129, 0x2, v129                      // Bias address scaled by BPE
v_add_u32 v130, 1024, v129                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v128, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v138, v128, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v131, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v131, v138, v131, s[56:57]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[136:137], v131, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v134, v4, s52
v_lshlrev_b32 v134, 0x2, v134                      // Bias address scaled by BPE
v_add_u32 v135, 1024, v134                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v131, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v131, v138, v131, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc96          // copy acc to vreg[96]
v_accvgpr_read_b32 v[vgprValuC+21], acc97          // copy acc to vreg[97]
v_accvgpr_read_b32 v[vgprValuC+22], acc98          // copy acc to vreg[98]
v_accvgpr_read_b32 v[vgprValuC+23], acc99          // copy acc to vreg[99]
v_accvgpr_read_b32 v[vgprValuC+36], acc100         // copy acc to vreg[100]
v_accvgpr_read_b32 v[vgprValuC+37], acc101         // copy acc to vreg[101]
v_accvgpr_read_b32 v[vgprValuC+38], acc102         // copy acc to vreg[102]
v_accvgpr_read_b32 v[vgprValuC+39], acc103         // copy acc to vreg[103]
v_accvgpr_read_b32 v[vgprValuC+56], acc104         // copy acc to vreg[104]
v_accvgpr_read_b32 v[vgprValuC+57], acc105         // copy acc to vreg[105]
v_accvgpr_read_b32 v[vgprValuC+58], acc106         // copy acc to vreg[106]
v_accvgpr_read_b32 v[vgprValuC+59], acc107         // copy acc to vreg[107]
v_accvgpr_read_b32 v[vgprValuC+72], acc108         // copy acc to vreg[108]
v_accvgpr_read_b32 v[vgprValuC+73], acc109         // copy acc to vreg[109]
v_accvgpr_read_b32 v[vgprValuC+74], acc110         // copy acc to vreg[110]
v_accvgpr_read_b32 v[vgprValuC+75], acc111         // copy acc to vreg[111]
v_accvgpr_read_b32 v[vgprValuC+80], acc112         // copy acc to vreg[112]
v_accvgpr_read_b32 v[vgprValuC+81], acc113         // copy acc to vreg[113]
v_accvgpr_read_b32 v[vgprValuC+82], acc114         // copy acc to vreg[114]
v_accvgpr_read_b32 v[vgprValuC+83], acc115         // copy acc to vreg[115]
v_accvgpr_read_b32 v[vgprValuC+88], acc116         // copy acc to vreg[116]
v_accvgpr_read_b32 v[vgprValuC+89], acc117         // copy acc to vreg[117]
v_accvgpr_read_b32 v[vgprValuC+90], acc118         // copy acc to vreg[118]
v_accvgpr_read_b32 v[vgprValuC+91], acc119         // copy acc to vreg[119]
v_accvgpr_read_b32 v[vgprValuC+100], acc120        // copy acc to vreg[120]
v_accvgpr_read_b32 v[vgprValuC+101], acc121        // copy acc to vreg[121]
v_accvgpr_read_b32 v[vgprValuC+102], acc122        // copy acc to vreg[122]
v_accvgpr_read_b32 v[vgprValuC+103], acc123        // copy acc to vreg[123]
v_accvgpr_read_b32 v[vgprValuC+108], acc124        // copy acc to vreg[124]
v_accvgpr_read_b32 v[vgprValuC+109], acc125        // copy acc to vreg[125]
v_accvgpr_read_b32 v[vgprValuC+110], acc126        // copy acc to vreg[126]
v_accvgpr_read_b32 v[vgprValuC+111], acc127        // copy acc to vreg[127]
v_accvgpr_read_b32 v[vgprValuC+116], acc128        // copy acc to vreg[128]
v_accvgpr_read_b32 v[vgprValuC+117], acc129        // copy acc to vreg[129]
v_accvgpr_read_b32 v[vgprValuC+118], acc130        // copy acc to vreg[130]
v_accvgpr_read_b32 v[vgprValuC+119], acc131        // copy acc to vreg[131]
v_accvgpr_read_b32 v[vgprValuC+124], acc132        // copy acc to vreg[132]
v_accvgpr_read_b32 v[vgprValuC+125], acc133        // copy acc to vreg[133]
v_accvgpr_read_b32 v[vgprValuC+126], acc134        // copy acc to vreg[134]
v_accvgpr_read_b32 v[vgprValuC+127], acc135        // copy acc to vreg[135]
v_accvgpr_read_b32 v[vgprValuC+140], acc136        // copy acc to vreg[136]
v_accvgpr_read_b32 v[vgprValuC+141], acc137        // copy acc to vreg[137]
v_accvgpr_read_b32 v[vgprValuC+142], acc138        // copy acc to vreg[138]
v_accvgpr_read_b32 v[vgprValuC+143], acc139        // copy acc to vreg[139]
v_accvgpr_read_b32 v[vgprValuC+144], acc140        // copy acc to vreg[140]
v_accvgpr_read_b32 v[vgprValuC+145], acc141        // copy acc to vreg[141]
v_accvgpr_read_b32 v[vgprValuC+146], acc142        // copy acc to vreg[142]
v_accvgpr_read_b32 v[vgprValuC+147], acc143        // copy acc to vreg[143]

/* rC *= alpha batchElements=[(0, 0, 6, 0), (0, 1, 6, 0), (0, 2, 6, 0), (0, 3, 6, 0), (0, 0, 7, 0), (0, 1, 7, 0), (0, 2, 7, 0), (0, 3, 7, 0), (1, 0, 0, 0), (1, 1, 0, 0), (1, 2, 0, 0), (1, 3, 0, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+108], s[sgprAlpha], v[vgprValuC+108] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+118], s[sgprAlpha], v[vgprValuC+118] // *= alpha
v_mul_f32 v[vgprValuC+119], s[sgprAlpha], v[vgprValuC+119] // *= alpha
v_mul_f32 v[vgprValuC+124], s[sgprAlpha], v[vgprValuC+124] // *= alpha
v_mul_f32 v[vgprValuC+125], s[sgprAlpha], v[vgprValuC+125] // *= alpha
v_mul_f32 v[vgprValuC+126], s[sgprAlpha], v[vgprValuC+126] // *= alpha
v_mul_f32 v[vgprValuC+127], s[sgprAlpha], v[vgprValuC+127] // *= alpha
v_mul_f32 v[vgprValuC+140], s[sgprAlpha], v[vgprValuC+140] // *= alpha
v_mul_f32 v[vgprValuC+141], s[sgprAlpha], v[vgprValuC+141] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+143], s[sgprAlpha], v[vgprValuC+143] // *= alpha
v_mul_f32 v[vgprValuC+144], s[sgprAlpha], v[vgprValuC+144] // *= alpha
v_mul_f32 v[vgprValuC+145], s[sgprAlpha], v[vgprValuC+145] // *= alpha
v_mul_f32 v[vgprValuC+146], s[sgprAlpha], v[vgprValuC+146] // *= alpha
v_mul_f32 v[vgprValuC+147], s[sgprAlpha], v[vgprValuC+147] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+20], s[sgprBeta], v10, v[vgprValuC+20] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+21], s[sgprBeta], v10, v[vgprValuC+21] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+22], s[sgprBeta], v11, v[vgprValuC+22] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+23], s[sgprBeta], v11, v[vgprValuC+23] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[32:33], v[vgprValuC+36:vgprValuC+36+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[34:35], v[vgprValuC+38:vgprValuC+38+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v26, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v26, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v27, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v27, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[28:29], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[30:31], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[52:53], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[54:55], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_fma_mix_f32 v[vgprValuC+56], s[sgprBeta], v44, v[vgprValuC+56] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+57], s[sgprBeta], v44, v[vgprValuC+57] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+58], s[sgprBeta], v45, v[vgprValuC+58] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+59], s[sgprBeta], v45, v[vgprValuC+59] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[48:49], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[50:51], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[68:69], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(68)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[70:71], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(68)(2)
v_fma_mix_f32 v[vgprValuC+72], s[sgprBeta], v60, v[vgprValuC+72] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+73], s[sgprBeta], v60, v[vgprValuC+73] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+74], s[sgprBeta], v61, v[vgprValuC+74] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v61, v[vgprValuC+75] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[64:65], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[66:67], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v43, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[16:17], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[18:19], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+80], s[sgprBeta], v78, v[vgprValuC+80] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+81], s[sgprBeta], v78, v[vgprValuC+81] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+82], s[sgprBeta], v79, v[vgprValuC+82] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+83], s[sgprBeta], v79, v[vgprValuC+83] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[12:13], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[14:15], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[32:33], v[vgprValuC+88:vgprValuC+88+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[34:35], v[vgprValuC+90:vgprValuC+90+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+88], s[sgprBeta], v86, v[vgprValuC+88] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+89], s[sgprBeta], v86, v[vgprValuC+89] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+90], s[sgprBeta], v87, v[vgprValuC+90] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+91], s[sgprBeta], v87, v[vgprValuC+91] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+88:vgprValuC+88+1], v[28:29], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[vgprValuC+90:vgprValuC+90+1], v[30:31], v[vgprValuC+90:vgprValuC+90+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+88], v[vgprValuC+88]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+89], v[vgprValuC+89]     // convert C to fp16
v_pack_b32_f16 v88, v[vgprValuC+88], v[vgprValuC+89] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+90], v[vgprValuC+90]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+91], v[vgprValuC+91]     // convert C to fp16
v_pack_b32_f16 v89, v[vgprValuC+90], v[vgprValuC+91] // Pack with neighbor
buffer_store_dwordx2 v[88:89], v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[52:53], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[54:55], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_fma_mix_f32 v[vgprValuC+100], s[sgprBeta], v96, v[vgprValuC+100] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+101], s[sgprBeta], v96, v[vgprValuC+101] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+102], s[sgprBeta], v97, v[vgprValuC+102] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+103], s[sgprBeta], v97, v[vgprValuC+103] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[48:49], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[50:51], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
buffer_store_dwordx2 v[100:101], v92, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[68:69], v[vgprValuC+108:vgprValuC+108+1] // *= ScaleAlphaVecVMulPK(68)(0)
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[70:71], v[vgprValuC+110:vgprValuC+110+1] // *= ScaleAlphaVecVMulPK(68)(2)
v_fma_mix_f32 v[vgprValuC+108], s[sgprBeta], v104, v[vgprValuC+108] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+109], s[sgprBeta], v104, v[vgprValuC+109] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+110], s[sgprBeta], v105, v[vgprValuC+110] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+111], s[sgprBeta], v105, v[vgprValuC+111] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+108:vgprValuC+108+1], v[64:65], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[vgprValuC+110:vgprValuC+110+1], v[66:67], v[vgprValuC+110:vgprValuC+110+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+108], v[vgprValuC+108]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+109], v[vgprValuC+109]   // convert C to fp16
v_pack_b32_f16 v108, v[vgprValuC+108], v[vgprValuC+109] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+110], v[vgprValuC+110]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+111], v[vgprValuC+111]   // convert C to fp16
v_pack_b32_f16 v109, v[vgprValuC+110], v[vgprValuC+111] // Pack with neighbor
buffer_store_dwordx2 v[108:109], v95, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+116:vgprValuC+116+1], v[16:17], v[vgprValuC+116:vgprValuC+116+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+118:vgprValuC+118+1], v[18:19], v[vgprValuC+118:vgprValuC+118+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+116], s[sgprBeta], v114, v[vgprValuC+116] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v114, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+118], s[sgprBeta], v115, v[vgprValuC+118] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+119], s[sgprBeta], v115, v[vgprValuC+119] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+116:vgprValuC+116+1], v[12:13], v[vgprValuC+116:vgprValuC+116+1] // C += bias
v_pk_add_f32 v[vgprValuC+118:vgprValuC+118+1], v[14:15], v[vgprValuC+118:vgprValuC+118+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+116], v[vgprValuC+116]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+117], v[vgprValuC+117]   // convert C to fp16
v_pack_b32_f16 v116, v[vgprValuC+116], v[vgprValuC+117] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+118], v[vgprValuC+118]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+119], v[vgprValuC+119]   // convert C to fp16
v_pack_b32_f16 v117, v[vgprValuC+118], v[vgprValuC+119] // Pack with neighbor
buffer_store_dwordx2 v[116:117], v106, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+124:vgprValuC+124+1], v[32:33], v[vgprValuC+124:vgprValuC+124+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+126:vgprValuC+126+1], v[34:35], v[vgprValuC+126:vgprValuC+126+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+124], s[sgprBeta], v122, v[vgprValuC+124] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+125], s[sgprBeta], v122, v[vgprValuC+125] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+126], s[sgprBeta], v123, v[vgprValuC+126] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+127], s[sgprBeta], v123, v[vgprValuC+127] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+124:vgprValuC+124+1], v[28:29], v[vgprValuC+124:vgprValuC+124+1] // C += bias
v_pk_add_f32 v[vgprValuC+126:vgprValuC+126+1], v[30:31], v[vgprValuC+126:vgprValuC+126+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+124], v[vgprValuC+124]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+125], v[vgprValuC+125]   // convert C to fp16
v_pack_b32_f16 v124, v[vgprValuC+124], v[vgprValuC+125] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+126], v[vgprValuC+126]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+127], v[vgprValuC+127]   // convert C to fp16
v_pack_b32_f16 v125, v[vgprValuC+126], v[vgprValuC+127] // Pack with neighbor
buffer_store_dwordx2 v[124:125], v113, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+140:vgprValuC+140+1], v[52:53], v[vgprValuC+140:vgprValuC+140+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+142:vgprValuC+142+1], v[54:55], v[vgprValuC+142:vgprValuC+142+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_fma_mix_f32 v[vgprValuC+140], s[sgprBeta], v132, v[vgprValuC+140] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+141], s[sgprBeta], v132, v[vgprValuC+141] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+142], s[sgprBeta], v133, v[vgprValuC+142] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+143], s[sgprBeta], v133, v[vgprValuC+143] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+140:vgprValuC+140+1], v[48:49], v[vgprValuC+140:vgprValuC+140+1] // C += bias
v_pk_add_f32 v[vgprValuC+142:vgprValuC+142+1], v[50:51], v[vgprValuC+142:vgprValuC+142+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+140], v[vgprValuC+140]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+141], v[vgprValuC+141]   // convert C to fp16
v_pack_b32_f16 v140, v[vgprValuC+140], v[vgprValuC+141] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+142], v[vgprValuC+142]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+143], v[vgprValuC+143]   // convert C to fp16
v_pack_b32_f16 v141, v[vgprValuC+142], v[vgprValuC+143] // Pack with neighbor
buffer_store_dwordx2 v[140:141], v128, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+144:vgprValuC+144+1], v[68:69], v[vgprValuC+144:vgprValuC+144+1] // *= ScaleAlphaVecVMulPK(68)(0)
v_pk_mul_f32 v[vgprValuC+146:vgprValuC+146+1], v[70:71], v[vgprValuC+146:vgprValuC+146+1] // *= ScaleAlphaVecVMulPK(68)(2)
v_fma_mix_f32 v[vgprValuC+144], s[sgprBeta], v136, v[vgprValuC+144] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+145], s[sgprBeta], v136, v[vgprValuC+145] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+146], s[sgprBeta], v137, v[vgprValuC+146] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+147], s[sgprBeta], v137, v[vgprValuC+147] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+144:vgprValuC+144+1], v[64:65], v[vgprValuC+144:vgprValuC+144+1] // C += bias
v_pk_add_f32 v[vgprValuC+146:vgprValuC+146+1], v[66:67], v[vgprValuC+146:vgprValuC+146+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+144], v[vgprValuC+144]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+145], v[vgprValuC+145]   // convert C to fp16
v_pack_b32_f16 v144, v[vgprValuC+144], v[vgprValuC+145] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+146], v[vgprValuC+146]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+147], v[vgprValuC+147]   // convert C to fp16
v_pack_b32_f16 v145, v[vgprValuC+146], v[vgprValuC+147] // Pack with neighbor
buffer_store_dwordx2 v[144:145], v131, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #3 (d1,d0,vc1,vc0) = */
/*    (1,0,1,0:vw4); (1,1,1,0:vw4); (1,2,1,0:vw4); (1,3,1,0:vw4); (1,0,2,0:vw4); (1,1,2,0:vw4); (1,2,2,0:vw4); (1,3,2,0:vw4); (1,0,3,0:vw4); (1,1,3,0:vw4); (1,2,3,0:vw4); (1,3,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v138, BufferOOB
/* (d1,vc1,d0,vc0)=(1,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v138, v6, s[56:57]               // LDC clip if OOB. offset
buffer_load_dwordx2 v[10:11], v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v0, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b128 v[12:15], v7 offset:0                 // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b128 v[16:19], v8 offset:0                 // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v138, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v9, v2, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v138, v9, s[56:57]               // LDC clip if OOB. offset
buffer_load_dwordx2 v[26:27], v9, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v4, s52
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
ds_read_b128 v[28:31], v24 offset:0                // load Bias
v_add_u32 v25, 1024, v24                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[32:35], v25 offset:0                // load scaleAlpha
v_add_lshl_u32 v9, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v138, v9, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v40, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v138, v40, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[44:45], v40, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v41, v4, s52
v_lshlrev_b32 v41, 0x2, v41                        // Bias address scaled by BPE
ds_read_b128 v[48:51], v41 offset:0                // load Bias
v_add_u32 v42, 1024, v41                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[52:55], v42 offset:0                // load scaleAlpha
v_add_lshl_u32 v40, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v138, v40, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v43, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v138, v43, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[60:61], v43, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s52
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
ds_read_b128 v[64:67], v46 offset:0                // load Bias
v_add_u32 v47, 1024, v46                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[68:71], v47 offset:0                // load scaleAlpha
v_add_lshl_u32 v43, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v138, v43, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v62, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v138, v62, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[78:79], v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v0, s52
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
v_add_u32 v76, 1024, v63                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v62, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v138, v62, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v77, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v138, v77, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[86:87], v77, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v84, v4, s52
v_lshlrev_b32 v84, 0x2, v84                        // Bias address scaled by BPE
v_add_u32 v85, 1024, v84                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v138, v77, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v92, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v92, v138, v92, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[96:97], v92, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v93, v4, s52
v_lshlrev_b32 v93, 0x2, v93                        // Bias address scaled by BPE
v_add_u32 v94, 1024, v93                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v92, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v92, v138, v92, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v95, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v95, v138, v95, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[104:105], v95, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v98, v4, s52
v_lshlrev_b32 v98, 0x2, v98                        // Bias address scaled by BPE
v_add_u32 v99, 1024, v98                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v95, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v95, v138, v95, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v106, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v138, v106, s[56:57]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[114:115], v106, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v107, v0, s52
v_lshlrev_b32 v107, 0x2, v107                      // Bias address scaled by BPE
v_add_u32 v112, 1024, v107                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v106, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v138, v106, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v113, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v113, v138, v113, s[56:57]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[122:123], v113, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v120, v4, s52
v_lshlrev_b32 v120, 0x2, v120                      // Bias address scaled by BPE
v_add_u32 v121, 1024, v120                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v113, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v113, v138, v113, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v128, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v138, v128, s[56:57]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[132:133], v128, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v129, v4, s52
v_lshlrev_b32 v129, 0x2, v129                      // Bias address scaled by BPE
v_add_u32 v130, 1024, v129                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v128, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v138, v128, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v131, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v131, v138, v131, s[56:57]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[136:137], v131, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v134, v4, s52
v_lshlrev_b32 v134, 0x2, v134                      // Bias address scaled by BPE
v_add_u32 v135, 1024, v134                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v131, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v131, v138, v131, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc144         // copy acc to vreg[144]
v_accvgpr_read_b32 v[vgprValuC+21], acc145         // copy acc to vreg[145]
v_accvgpr_read_b32 v[vgprValuC+22], acc146         // copy acc to vreg[146]
v_accvgpr_read_b32 v[vgprValuC+23], acc147         // copy acc to vreg[147]
v_accvgpr_read_b32 v[vgprValuC+36], acc148         // copy acc to vreg[148]
v_accvgpr_read_b32 v[vgprValuC+37], acc149         // copy acc to vreg[149]
v_accvgpr_read_b32 v[vgprValuC+38], acc150         // copy acc to vreg[150]
v_accvgpr_read_b32 v[vgprValuC+39], acc151         // copy acc to vreg[151]
v_accvgpr_read_b32 v[vgprValuC+56], acc152         // copy acc to vreg[152]
v_accvgpr_read_b32 v[vgprValuC+57], acc153         // copy acc to vreg[153]
v_accvgpr_read_b32 v[vgprValuC+58], acc154         // copy acc to vreg[154]
v_accvgpr_read_b32 v[vgprValuC+59], acc155         // copy acc to vreg[155]
v_accvgpr_read_b32 v[vgprValuC+72], acc156         // copy acc to vreg[156]
v_accvgpr_read_b32 v[vgprValuC+73], acc157         // copy acc to vreg[157]
v_accvgpr_read_b32 v[vgprValuC+74], acc158         // copy acc to vreg[158]
v_accvgpr_read_b32 v[vgprValuC+75], acc159         // copy acc to vreg[159]
v_accvgpr_read_b32 v[vgprValuC+80], acc160         // copy acc to vreg[160]
v_accvgpr_read_b32 v[vgprValuC+81], acc161         // copy acc to vreg[161]
v_accvgpr_read_b32 v[vgprValuC+82], acc162         // copy acc to vreg[162]
v_accvgpr_read_b32 v[vgprValuC+83], acc163         // copy acc to vreg[163]
v_accvgpr_read_b32 v[vgprValuC+88], acc164         // copy acc to vreg[164]
v_accvgpr_read_b32 v[vgprValuC+89], acc165         // copy acc to vreg[165]
v_accvgpr_read_b32 v[vgprValuC+90], acc166         // copy acc to vreg[166]
v_accvgpr_read_b32 v[vgprValuC+91], acc167         // copy acc to vreg[167]
v_accvgpr_read_b32 v[vgprValuC+100], acc168        // copy acc to vreg[168]
v_accvgpr_read_b32 v[vgprValuC+101], acc169        // copy acc to vreg[169]
v_accvgpr_read_b32 v[vgprValuC+102], acc170        // copy acc to vreg[170]
v_accvgpr_read_b32 v[vgprValuC+103], acc171        // copy acc to vreg[171]
v_accvgpr_read_b32 v[vgprValuC+108], acc172        // copy acc to vreg[172]
v_accvgpr_read_b32 v[vgprValuC+109], acc173        // copy acc to vreg[173]
v_accvgpr_read_b32 v[vgprValuC+110], acc174        // copy acc to vreg[174]
v_accvgpr_read_b32 v[vgprValuC+111], acc175        // copy acc to vreg[175]
v_accvgpr_read_b32 v[vgprValuC+116], acc176        // copy acc to vreg[176]
v_accvgpr_read_b32 v[vgprValuC+117], acc177        // copy acc to vreg[177]
v_accvgpr_read_b32 v[vgprValuC+118], acc178        // copy acc to vreg[178]
v_accvgpr_read_b32 v[vgprValuC+119], acc179        // copy acc to vreg[179]
v_accvgpr_read_b32 v[vgprValuC+124], acc180        // copy acc to vreg[180]
v_accvgpr_read_b32 v[vgprValuC+125], acc181        // copy acc to vreg[181]
v_accvgpr_read_b32 v[vgprValuC+126], acc182        // copy acc to vreg[182]
v_accvgpr_read_b32 v[vgprValuC+127], acc183        // copy acc to vreg[183]
v_accvgpr_read_b32 v[vgprValuC+140], acc184        // copy acc to vreg[184]
v_accvgpr_read_b32 v[vgprValuC+141], acc185        // copy acc to vreg[185]
v_accvgpr_read_b32 v[vgprValuC+142], acc186        // copy acc to vreg[186]
v_accvgpr_read_b32 v[vgprValuC+143], acc187        // copy acc to vreg[187]
v_accvgpr_read_b32 v[vgprValuC+144], acc188        // copy acc to vreg[188]
v_accvgpr_read_b32 v[vgprValuC+145], acc189        // copy acc to vreg[189]
v_accvgpr_read_b32 v[vgprValuC+146], acc190        // copy acc to vreg[190]
v_accvgpr_read_b32 v[vgprValuC+147], acc191        // copy acc to vreg[191]

/* rC *= alpha batchElements=[(1, 0, 1, 0), (1, 1, 1, 0), (1, 2, 1, 0), (1, 3, 1, 0), (1, 0, 2, 0), (1, 1, 2, 0), (1, 2, 2, 0), (1, 3, 2, 0), (1, 0, 3, 0), (1, 1, 3, 0), (1, 2, 3, 0), (1, 3, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+108], s[sgprAlpha], v[vgprValuC+108] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+118], s[sgprAlpha], v[vgprValuC+118] // *= alpha
v_mul_f32 v[vgprValuC+119], s[sgprAlpha], v[vgprValuC+119] // *= alpha
v_mul_f32 v[vgprValuC+124], s[sgprAlpha], v[vgprValuC+124] // *= alpha
v_mul_f32 v[vgprValuC+125], s[sgprAlpha], v[vgprValuC+125] // *= alpha
v_mul_f32 v[vgprValuC+126], s[sgprAlpha], v[vgprValuC+126] // *= alpha
v_mul_f32 v[vgprValuC+127], s[sgprAlpha], v[vgprValuC+127] // *= alpha
v_mul_f32 v[vgprValuC+140], s[sgprAlpha], v[vgprValuC+140] // *= alpha
v_mul_f32 v[vgprValuC+141], s[sgprAlpha], v[vgprValuC+141] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+143], s[sgprAlpha], v[vgprValuC+143] // *= alpha
v_mul_f32 v[vgprValuC+144], s[sgprAlpha], v[vgprValuC+144] // *= alpha
v_mul_f32 v[vgprValuC+145], s[sgprAlpha], v[vgprValuC+145] // *= alpha
v_mul_f32 v[vgprValuC+146], s[sgprAlpha], v[vgprValuC+146] // *= alpha
v_mul_f32 v[vgprValuC+147], s[sgprAlpha], v[vgprValuC+147] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+20], s[sgprBeta], v10, v[vgprValuC+20] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+21], s[sgprBeta], v10, v[vgprValuC+21] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+22], s[sgprBeta], v11, v[vgprValuC+22] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+23], s[sgprBeta], v11, v[vgprValuC+23] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[32:33], v[vgprValuC+36:vgprValuC+36+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[34:35], v[vgprValuC+38:vgprValuC+38+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v26, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v26, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v27, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v27, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[28:29], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[30:31], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[52:53], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[54:55], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_fma_mix_f32 v[vgprValuC+56], s[sgprBeta], v44, v[vgprValuC+56] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+57], s[sgprBeta], v44, v[vgprValuC+57] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+58], s[sgprBeta], v45, v[vgprValuC+58] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+59], s[sgprBeta], v45, v[vgprValuC+59] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[48:49], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[50:51], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[68:69], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(68)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[70:71], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(68)(2)
v_fma_mix_f32 v[vgprValuC+72], s[sgprBeta], v60, v[vgprValuC+72] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+73], s[sgprBeta], v60, v[vgprValuC+73] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+74], s[sgprBeta], v61, v[vgprValuC+74] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v61, v[vgprValuC+75] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[64:65], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[66:67], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v43, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[16:17], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[18:19], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+80], s[sgprBeta], v78, v[vgprValuC+80] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+81], s[sgprBeta], v78, v[vgprValuC+81] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+82], s[sgprBeta], v79, v[vgprValuC+82] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+83], s[sgprBeta], v79, v[vgprValuC+83] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[12:13], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[14:15], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[32:33], v[vgprValuC+88:vgprValuC+88+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[34:35], v[vgprValuC+90:vgprValuC+90+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+88], s[sgprBeta], v86, v[vgprValuC+88] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+89], s[sgprBeta], v86, v[vgprValuC+89] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+90], s[sgprBeta], v87, v[vgprValuC+90] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+91], s[sgprBeta], v87, v[vgprValuC+91] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+88:vgprValuC+88+1], v[28:29], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[vgprValuC+90:vgprValuC+90+1], v[30:31], v[vgprValuC+90:vgprValuC+90+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+88], v[vgprValuC+88]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+89], v[vgprValuC+89]     // convert C to fp16
v_pack_b32_f16 v88, v[vgprValuC+88], v[vgprValuC+89] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+90], v[vgprValuC+90]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+91], v[vgprValuC+91]     // convert C to fp16
v_pack_b32_f16 v89, v[vgprValuC+90], v[vgprValuC+91] // Pack with neighbor
buffer_store_dwordx2 v[88:89], v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[52:53], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[54:55], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_fma_mix_f32 v[vgprValuC+100], s[sgprBeta], v96, v[vgprValuC+100] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+101], s[sgprBeta], v96, v[vgprValuC+101] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+102], s[sgprBeta], v97, v[vgprValuC+102] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+103], s[sgprBeta], v97, v[vgprValuC+103] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[48:49], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[50:51], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
buffer_store_dwordx2 v[100:101], v92, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[68:69], v[vgprValuC+108:vgprValuC+108+1] // *= ScaleAlphaVecVMulPK(68)(0)
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[70:71], v[vgprValuC+110:vgprValuC+110+1] // *= ScaleAlphaVecVMulPK(68)(2)
v_fma_mix_f32 v[vgprValuC+108], s[sgprBeta], v104, v[vgprValuC+108] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+109], s[sgprBeta], v104, v[vgprValuC+109] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+110], s[sgprBeta], v105, v[vgprValuC+110] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+111], s[sgprBeta], v105, v[vgprValuC+111] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+108:vgprValuC+108+1], v[64:65], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[vgprValuC+110:vgprValuC+110+1], v[66:67], v[vgprValuC+110:vgprValuC+110+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+108], v[vgprValuC+108]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+109], v[vgprValuC+109]   // convert C to fp16
v_pack_b32_f16 v108, v[vgprValuC+108], v[vgprValuC+109] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+110], v[vgprValuC+110]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+111], v[vgprValuC+111]   // convert C to fp16
v_pack_b32_f16 v109, v[vgprValuC+110], v[vgprValuC+111] // Pack with neighbor
buffer_store_dwordx2 v[108:109], v95, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+116:vgprValuC+116+1], v[16:17], v[vgprValuC+116:vgprValuC+116+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+118:vgprValuC+118+1], v[18:19], v[vgprValuC+118:vgprValuC+118+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+116], s[sgprBeta], v114, v[vgprValuC+116] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v114, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+118], s[sgprBeta], v115, v[vgprValuC+118] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+119], s[sgprBeta], v115, v[vgprValuC+119] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+116:vgprValuC+116+1], v[12:13], v[vgprValuC+116:vgprValuC+116+1] // C += bias
v_pk_add_f32 v[vgprValuC+118:vgprValuC+118+1], v[14:15], v[vgprValuC+118:vgprValuC+118+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+116], v[vgprValuC+116]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+117], v[vgprValuC+117]   // convert C to fp16
v_pack_b32_f16 v116, v[vgprValuC+116], v[vgprValuC+117] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+118], v[vgprValuC+118]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+119], v[vgprValuC+119]   // convert C to fp16
v_pack_b32_f16 v117, v[vgprValuC+118], v[vgprValuC+119] // Pack with neighbor
buffer_store_dwordx2 v[116:117], v106, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+124:vgprValuC+124+1], v[32:33], v[vgprValuC+124:vgprValuC+124+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+126:vgprValuC+126+1], v[34:35], v[vgprValuC+126:vgprValuC+126+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+124], s[sgprBeta], v122, v[vgprValuC+124] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+125], s[sgprBeta], v122, v[vgprValuC+125] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+126], s[sgprBeta], v123, v[vgprValuC+126] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+127], s[sgprBeta], v123, v[vgprValuC+127] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+124:vgprValuC+124+1], v[28:29], v[vgprValuC+124:vgprValuC+124+1] // C += bias
v_pk_add_f32 v[vgprValuC+126:vgprValuC+126+1], v[30:31], v[vgprValuC+126:vgprValuC+126+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+124], v[vgprValuC+124]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+125], v[vgprValuC+125]   // convert C to fp16
v_pack_b32_f16 v124, v[vgprValuC+124], v[vgprValuC+125] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+126], v[vgprValuC+126]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+127], v[vgprValuC+127]   // convert C to fp16
v_pack_b32_f16 v125, v[vgprValuC+126], v[vgprValuC+127] // Pack with neighbor
buffer_store_dwordx2 v[124:125], v113, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+140:vgprValuC+140+1], v[52:53], v[vgprValuC+140:vgprValuC+140+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+142:vgprValuC+142+1], v[54:55], v[vgprValuC+142:vgprValuC+142+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_fma_mix_f32 v[vgprValuC+140], s[sgprBeta], v132, v[vgprValuC+140] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+141], s[sgprBeta], v132, v[vgprValuC+141] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+142], s[sgprBeta], v133, v[vgprValuC+142] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+143], s[sgprBeta], v133, v[vgprValuC+143] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+140:vgprValuC+140+1], v[48:49], v[vgprValuC+140:vgprValuC+140+1] // C += bias
v_pk_add_f32 v[vgprValuC+142:vgprValuC+142+1], v[50:51], v[vgprValuC+142:vgprValuC+142+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+140], v[vgprValuC+140]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+141], v[vgprValuC+141]   // convert C to fp16
v_pack_b32_f16 v140, v[vgprValuC+140], v[vgprValuC+141] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+142], v[vgprValuC+142]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+143], v[vgprValuC+143]   // convert C to fp16
v_pack_b32_f16 v141, v[vgprValuC+142], v[vgprValuC+143] // Pack with neighbor
buffer_store_dwordx2 v[140:141], v128, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+144:vgprValuC+144+1], v[68:69], v[vgprValuC+144:vgprValuC+144+1] // *= ScaleAlphaVecVMulPK(68)(0)
v_pk_mul_f32 v[vgprValuC+146:vgprValuC+146+1], v[70:71], v[vgprValuC+146:vgprValuC+146+1] // *= ScaleAlphaVecVMulPK(68)(2)
v_fma_mix_f32 v[vgprValuC+144], s[sgprBeta], v136, v[vgprValuC+144] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+145], s[sgprBeta], v136, v[vgprValuC+145] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+146], s[sgprBeta], v137, v[vgprValuC+146] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+147], s[sgprBeta], v137, v[vgprValuC+147] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+144:vgprValuC+144+1], v[64:65], v[vgprValuC+144:vgprValuC+144+1] // C += bias
v_pk_add_f32 v[vgprValuC+146:vgprValuC+146+1], v[66:67], v[vgprValuC+146:vgprValuC+146+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+144], v[vgprValuC+144]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+145], v[vgprValuC+145]   // convert C to fp16
v_pack_b32_f16 v144, v[vgprValuC+144], v[vgprValuC+145] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+146], v[vgprValuC+146]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+147], v[vgprValuC+147]   // convert C to fp16
v_pack_b32_f16 v145, v[vgprValuC+146], v[vgprValuC+147] // Pack with neighbor
buffer_store_dwordx2 v[144:145], v131, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #4 (d1,d0,vc1,vc0) = */
/*    (1,0,4,0:vw4); (1,1,4,0:vw4); (1,2,4,0:vw4); (1,3,4,0:vw4); (1,0,5,0:vw4); (1,1,5,0:vw4); (1,2,5,0:vw4); (1,3,5,0:vw4); (1,0,6,0:vw4); (1,1,6,0:vw4); (1,2,6,0:vw4); (1,3,6,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v138, BufferOOB
/* (d1,vc1,d0,vc0)=(1,4,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v138, v6, s[56:57]               // LDC clip if OOB. offset
buffer_load_dwordx2 v[10:11], v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v0, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b128 v[12:15], v7 offset:0                 // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b128 v[16:19], v8 offset:0                 // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v138, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v9, v2, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v138, v9, s[56:57]               // LDC clip if OOB. offset
buffer_load_dwordx2 v[26:27], v9, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v4, s52
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
ds_read_b128 v[28:31], v24 offset:0                // load Bias
v_add_u32 v25, 1024, v24                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[32:35], v25 offset:0                // load scaleAlpha
v_add_lshl_u32 v9, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v138, v9, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v40, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v138, v40, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[44:45], v40, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v41, v4, s52
v_lshlrev_b32 v41, 0x2, v41                        // Bias address scaled by BPE
ds_read_b128 v[48:51], v41 offset:0                // load Bias
v_add_u32 v42, 1024, v41                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[52:55], v42 offset:0                // load scaleAlpha
v_add_lshl_u32 v40, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v138, v40, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v43, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v138, v43, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[60:61], v43, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s52
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
ds_read_b128 v[64:67], v46 offset:0                // load Bias
v_add_u32 v47, 1024, v46                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[68:71], v47 offset:0                // load scaleAlpha
v_add_lshl_u32 v43, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v138, v43, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v62, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v138, v62, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[78:79], v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v0, s52
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
v_add_u32 v76, 1024, v63                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v62, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v138, v62, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v77, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v138, v77, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[86:87], v77, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v84, v4, s52
v_lshlrev_b32 v84, 0x2, v84                        // Bias address scaled by BPE
v_add_u32 v85, 1024, v84                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v138, v77, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v92, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v92, v138, v92, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[96:97], v92, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v93, v4, s52
v_lshlrev_b32 v93, 0x2, v93                        // Bias address scaled by BPE
v_add_u32 v94, 1024, v93                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v92, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v92, v138, v92, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v95, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v95, v138, v95, s[56:57]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[104:105], v95, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v98, v4, s52
v_lshlrev_b32 v98, 0x2, v98                        // Bias address scaled by BPE
v_add_u32 v99, 1024, v98                           // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v95, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v95, v138, v95, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v106, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v138, v106, s[56:57]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[114:115], v106, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v107, v0, s52
v_lshlrev_b32 v107, 0x2, v107                      // Bias address scaled by BPE
v_add_u32 v112, 1024, v107                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v106, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v106, v138, v106, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v113, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v113, v138, v113, s[56:57]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[122:123], v113, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v120, v4, s52
v_lshlrev_b32 v120, 0x2, v120                      // Bias address scaled by BPE
v_add_u32 v121, 1024, v120                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v113, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v113, v138, v113, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v128, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v138, v128, s[56:57]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[132:133], v128, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v129, v4, s52
v_lshlrev_b32 v129, 0x2, v129                      // Bias address scaled by BPE
v_add_u32 v130, 1024, v129                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v128, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v138, v128, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v131, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v131, v138, v131, s[56:57]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[136:137], v131, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v134, v4, s52
v_lshlrev_b32 v134, 0x2, v134                      // Bias address scaled by BPE
v_add_u32 v135, 1024, v134                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v131, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v131, v138, v131, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc192         // copy acc to vreg[192]
v_accvgpr_read_b32 v[vgprValuC+21], acc193         // copy acc to vreg[193]
v_accvgpr_read_b32 v[vgprValuC+22], acc194         // copy acc to vreg[194]
v_accvgpr_read_b32 v[vgprValuC+23], acc195         // copy acc to vreg[195]
v_accvgpr_read_b32 v[vgprValuC+36], acc196         // copy acc to vreg[196]
v_accvgpr_read_b32 v[vgprValuC+37], acc197         // copy acc to vreg[197]
v_accvgpr_read_b32 v[vgprValuC+38], acc198         // copy acc to vreg[198]
v_accvgpr_read_b32 v[vgprValuC+39], acc199         // copy acc to vreg[199]
v_accvgpr_read_b32 v[vgprValuC+56], acc200         // copy acc to vreg[200]
v_accvgpr_read_b32 v[vgprValuC+57], acc201         // copy acc to vreg[201]
v_accvgpr_read_b32 v[vgprValuC+58], acc202         // copy acc to vreg[202]
v_accvgpr_read_b32 v[vgprValuC+59], acc203         // copy acc to vreg[203]
v_accvgpr_read_b32 v[vgprValuC+72], acc204         // copy acc to vreg[204]
v_accvgpr_read_b32 v[vgprValuC+73], acc205         // copy acc to vreg[205]
v_accvgpr_read_b32 v[vgprValuC+74], acc206         // copy acc to vreg[206]
v_accvgpr_read_b32 v[vgprValuC+75], acc207         // copy acc to vreg[207]
v_accvgpr_read_b32 v[vgprValuC+80], acc208         // copy acc to vreg[208]
v_accvgpr_read_b32 v[vgprValuC+81], acc209         // copy acc to vreg[209]
v_accvgpr_read_b32 v[vgprValuC+82], acc210         // copy acc to vreg[210]
v_accvgpr_read_b32 v[vgprValuC+83], acc211         // copy acc to vreg[211]
v_accvgpr_read_b32 v[vgprValuC+88], acc212         // copy acc to vreg[212]
v_accvgpr_read_b32 v[vgprValuC+89], acc213         // copy acc to vreg[213]
v_accvgpr_read_b32 v[vgprValuC+90], acc214         // copy acc to vreg[214]
v_accvgpr_read_b32 v[vgprValuC+91], acc215         // copy acc to vreg[215]
v_accvgpr_read_b32 v[vgprValuC+100], acc216        // copy acc to vreg[216]
v_accvgpr_read_b32 v[vgprValuC+101], acc217        // copy acc to vreg[217]
v_accvgpr_read_b32 v[vgprValuC+102], acc218        // copy acc to vreg[218]
v_accvgpr_read_b32 v[vgprValuC+103], acc219        // copy acc to vreg[219]
v_accvgpr_read_b32 v[vgprValuC+108], acc220        // copy acc to vreg[220]
v_accvgpr_read_b32 v[vgprValuC+109], acc221        // copy acc to vreg[221]
v_accvgpr_read_b32 v[vgprValuC+110], acc222        // copy acc to vreg[222]
v_accvgpr_read_b32 v[vgprValuC+111], acc223        // copy acc to vreg[223]
v_accvgpr_read_b32 v[vgprValuC+116], acc224        // copy acc to vreg[224]
v_accvgpr_read_b32 v[vgprValuC+117], acc225        // copy acc to vreg[225]
v_accvgpr_read_b32 v[vgprValuC+118], acc226        // copy acc to vreg[226]
v_accvgpr_read_b32 v[vgprValuC+119], acc227        // copy acc to vreg[227]
v_accvgpr_read_b32 v[vgprValuC+124], acc228        // copy acc to vreg[228]
v_accvgpr_read_b32 v[vgprValuC+125], acc229        // copy acc to vreg[229]
v_accvgpr_read_b32 v[vgprValuC+126], acc230        // copy acc to vreg[230]
v_accvgpr_read_b32 v[vgprValuC+127], acc231        // copy acc to vreg[231]
v_accvgpr_read_b32 v[vgprValuC+140], acc232        // copy acc to vreg[232]
v_accvgpr_read_b32 v[vgprValuC+141], acc233        // copy acc to vreg[233]
v_accvgpr_read_b32 v[vgprValuC+142], acc234        // copy acc to vreg[234]
v_accvgpr_read_b32 v[vgprValuC+143], acc235        // copy acc to vreg[235]
v_accvgpr_read_b32 v[vgprValuC+144], acc236        // copy acc to vreg[236]
v_accvgpr_read_b32 v[vgprValuC+145], acc237        // copy acc to vreg[237]
v_accvgpr_read_b32 v[vgprValuC+146], acc238        // copy acc to vreg[238]
v_accvgpr_read_b32 v[vgprValuC+147], acc239        // copy acc to vreg[239]

/* rC *= alpha batchElements=[(1, 0, 4, 0), (1, 1, 4, 0), (1, 2, 4, 0), (1, 3, 4, 0), (1, 0, 5, 0), (1, 1, 5, 0), (1, 2, 5, 0), (1, 3, 5, 0), (1, 0, 6, 0), (1, 1, 6, 0), (1, 2, 6, 0), (1, 3, 6, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+108], s[sgprAlpha], v[vgprValuC+108] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+118], s[sgprAlpha], v[vgprValuC+118] // *= alpha
v_mul_f32 v[vgprValuC+119], s[sgprAlpha], v[vgprValuC+119] // *= alpha
v_mul_f32 v[vgprValuC+124], s[sgprAlpha], v[vgprValuC+124] // *= alpha
v_mul_f32 v[vgprValuC+125], s[sgprAlpha], v[vgprValuC+125] // *= alpha
v_mul_f32 v[vgprValuC+126], s[sgprAlpha], v[vgprValuC+126] // *= alpha
v_mul_f32 v[vgprValuC+127], s[sgprAlpha], v[vgprValuC+127] // *= alpha
v_mul_f32 v[vgprValuC+140], s[sgprAlpha], v[vgprValuC+140] // *= alpha
v_mul_f32 v[vgprValuC+141], s[sgprAlpha], v[vgprValuC+141] // *= alpha
v_mul_f32 v[vgprValuC+142], s[sgprAlpha], v[vgprValuC+142] // *= alpha
v_mul_f32 v[vgprValuC+143], s[sgprAlpha], v[vgprValuC+143] // *= alpha
v_mul_f32 v[vgprValuC+144], s[sgprAlpha], v[vgprValuC+144] // *= alpha
v_mul_f32 v[vgprValuC+145], s[sgprAlpha], v[vgprValuC+145] // *= alpha
v_mul_f32 v[vgprValuC+146], s[sgprAlpha], v[vgprValuC+146] // *= alpha
v_mul_f32 v[vgprValuC+147], s[sgprAlpha], v[vgprValuC+147] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+20], s[sgprBeta], v10, v[vgprValuC+20] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+21], s[sgprBeta], v10, v[vgprValuC+21] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+22], s[sgprBeta], v11, v[vgprValuC+22] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+23], s[sgprBeta], v11, v[vgprValuC+23] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[32:33], v[vgprValuC+36:vgprValuC+36+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[34:35], v[vgprValuC+38:vgprValuC+38+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v26, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v26, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v27, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v27, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[28:29], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[30:31], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[52:53], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[54:55], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_fma_mix_f32 v[vgprValuC+56], s[sgprBeta], v44, v[vgprValuC+56] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+57], s[sgprBeta], v44, v[vgprValuC+57] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+58], s[sgprBeta], v45, v[vgprValuC+58] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+59], s[sgprBeta], v45, v[vgprValuC+59] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[48:49], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[50:51], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[68:69], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(68)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[70:71], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(68)(2)
v_fma_mix_f32 v[vgprValuC+72], s[sgprBeta], v60, v[vgprValuC+72] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+73], s[sgprBeta], v60, v[vgprValuC+73] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+74], s[sgprBeta], v61, v[vgprValuC+74] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v61, v[vgprValuC+75] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[64:65], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[66:67], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v43, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[16:17], v[vgprValuC+80:vgprValuC+80+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[18:19], v[vgprValuC+82:vgprValuC+82+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+80], s[sgprBeta], v78, v[vgprValuC+80] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+81], s[sgprBeta], v78, v[vgprValuC+81] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+82], s[sgprBeta], v79, v[vgprValuC+82] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+83], s[sgprBeta], v79, v[vgprValuC+83] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+80:vgprValuC+80+1], v[12:13], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[vgprValuC+82:vgprValuC+82+1], v[14:15], v[vgprValuC+82:vgprValuC+82+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+80], v[vgprValuC+80]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+81], v[vgprValuC+81]     // convert C to fp16
v_pack_b32_f16 v80, v[vgprValuC+80], v[vgprValuC+81] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+82], v[vgprValuC+82]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+83], v[vgprValuC+83]     // convert C to fp16
v_pack_b32_f16 v81, v[vgprValuC+82], v[vgprValuC+83] // Pack with neighbor
buffer_store_dwordx2 v[80:81], v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[32:33], v[vgprValuC+88:vgprValuC+88+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[34:35], v[vgprValuC+90:vgprValuC+90+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+88], s[sgprBeta], v86, v[vgprValuC+88] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+89], s[sgprBeta], v86, v[vgprValuC+89] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+90], s[sgprBeta], v87, v[vgprValuC+90] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+91], s[sgprBeta], v87, v[vgprValuC+91] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+88:vgprValuC+88+1], v[28:29], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[vgprValuC+90:vgprValuC+90+1], v[30:31], v[vgprValuC+90:vgprValuC+90+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+88], v[vgprValuC+88]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+89], v[vgprValuC+89]     // convert C to fp16
v_pack_b32_f16 v88, v[vgprValuC+88], v[vgprValuC+89] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+90], v[vgprValuC+90]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+91], v[vgprValuC+91]     // convert C to fp16
v_pack_b32_f16 v89, v[vgprValuC+90], v[vgprValuC+91] // Pack with neighbor
buffer_store_dwordx2 v[88:89], v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[52:53], v[vgprValuC+100:vgprValuC+100+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[54:55], v[vgprValuC+102:vgprValuC+102+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_fma_mix_f32 v[vgprValuC+100], s[sgprBeta], v96, v[vgprValuC+100] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+101], s[sgprBeta], v96, v[vgprValuC+101] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+102], s[sgprBeta], v97, v[vgprValuC+102] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+103], s[sgprBeta], v97, v[vgprValuC+103] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+100:vgprValuC+100+1], v[48:49], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[vgprValuC+102:vgprValuC+102+1], v[50:51], v[vgprValuC+102:vgprValuC+102+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+100], v[vgprValuC+100]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+101], v[vgprValuC+101]   // convert C to fp16
v_pack_b32_f16 v100, v[vgprValuC+100], v[vgprValuC+101] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+102], v[vgprValuC+102]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+103], v[vgprValuC+103]   // convert C to fp16
v_pack_b32_f16 v101, v[vgprValuC+102], v[vgprValuC+103] // Pack with neighbor
buffer_store_dwordx2 v[100:101], v92, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[68:69], v[vgprValuC+108:vgprValuC+108+1] // *= ScaleAlphaVecVMulPK(68)(0)
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[70:71], v[vgprValuC+110:vgprValuC+110+1] // *= ScaleAlphaVecVMulPK(68)(2)
v_fma_mix_f32 v[vgprValuC+108], s[sgprBeta], v104, v[vgprValuC+108] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+109], s[sgprBeta], v104, v[vgprValuC+109] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+110], s[sgprBeta], v105, v[vgprValuC+110] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+111], s[sgprBeta], v105, v[vgprValuC+111] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+108:vgprValuC+108+1], v[64:65], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[vgprValuC+110:vgprValuC+110+1], v[66:67], v[vgprValuC+110:vgprValuC+110+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+108], v[vgprValuC+108]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+109], v[vgprValuC+109]   // convert C to fp16
v_pack_b32_f16 v108, v[vgprValuC+108], v[vgprValuC+109] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+110], v[vgprValuC+110]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+111], v[vgprValuC+111]   // convert C to fp16
v_pack_b32_f16 v109, v[vgprValuC+110], v[vgprValuC+111] // Pack with neighbor
buffer_store_dwordx2 v[108:109], v95, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+116:vgprValuC+116+1], v[16:17], v[vgprValuC+116:vgprValuC+116+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+118:vgprValuC+118+1], v[18:19], v[vgprValuC+118:vgprValuC+118+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+116], s[sgprBeta], v114, v[vgprValuC+116] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v114, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+118], s[sgprBeta], v115, v[vgprValuC+118] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+119], s[sgprBeta], v115, v[vgprValuC+119] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+116:vgprValuC+116+1], v[12:13], v[vgprValuC+116:vgprValuC+116+1] // C += bias
v_pk_add_f32 v[vgprValuC+118:vgprValuC+118+1], v[14:15], v[vgprValuC+118:vgprValuC+118+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+116], v[vgprValuC+116]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+117], v[vgprValuC+117]   // convert C to fp16
v_pack_b32_f16 v116, v[vgprValuC+116], v[vgprValuC+117] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+118], v[vgprValuC+118]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+119], v[vgprValuC+119]   // convert C to fp16
v_pack_b32_f16 v117, v[vgprValuC+118], v[vgprValuC+119] // Pack with neighbor
buffer_store_dwordx2 v[116:117], v106, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+124:vgprValuC+124+1], v[32:33], v[vgprValuC+124:vgprValuC+124+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+126:vgprValuC+126+1], v[34:35], v[vgprValuC+126:vgprValuC+126+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+124], s[sgprBeta], v122, v[vgprValuC+124] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+125], s[sgprBeta], v122, v[vgprValuC+125] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+126], s[sgprBeta], v123, v[vgprValuC+126] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+127], s[sgprBeta], v123, v[vgprValuC+127] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+124:vgprValuC+124+1], v[28:29], v[vgprValuC+124:vgprValuC+124+1] // C += bias
v_pk_add_f32 v[vgprValuC+126:vgprValuC+126+1], v[30:31], v[vgprValuC+126:vgprValuC+126+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+124], v[vgprValuC+124]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+125], v[vgprValuC+125]   // convert C to fp16
v_pack_b32_f16 v124, v[vgprValuC+124], v[vgprValuC+125] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+126], v[vgprValuC+126]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+127], v[vgprValuC+127]   // convert C to fp16
v_pack_b32_f16 v125, v[vgprValuC+126], v[vgprValuC+127] // Pack with neighbor
buffer_store_dwordx2 v[124:125], v113, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+140:vgprValuC+140+1], v[52:53], v[vgprValuC+140:vgprValuC+140+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+142:vgprValuC+142+1], v[54:55], v[vgprValuC+142:vgprValuC+142+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_fma_mix_f32 v[vgprValuC+140], s[sgprBeta], v132, v[vgprValuC+140] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+141], s[sgprBeta], v132, v[vgprValuC+141] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+142], s[sgprBeta], v133, v[vgprValuC+142] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+143], s[sgprBeta], v133, v[vgprValuC+143] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+140:vgprValuC+140+1], v[48:49], v[vgprValuC+140:vgprValuC+140+1] // C += bias
v_pk_add_f32 v[vgprValuC+142:vgprValuC+142+1], v[50:51], v[vgprValuC+142:vgprValuC+142+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+140], v[vgprValuC+140]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+141], v[vgprValuC+141]   // convert C to fp16
v_pack_b32_f16 v140, v[vgprValuC+140], v[vgprValuC+141] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+142], v[vgprValuC+142]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+143], v[vgprValuC+143]   // convert C to fp16
v_pack_b32_f16 v141, v[vgprValuC+142], v[vgprValuC+143] // Pack with neighbor
buffer_store_dwordx2 v[140:141], v128, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+144:vgprValuC+144+1], v[68:69], v[vgprValuC+144:vgprValuC+144+1] // *= ScaleAlphaVecVMulPK(68)(0)
v_pk_mul_f32 v[vgprValuC+146:vgprValuC+146+1], v[70:71], v[vgprValuC+146:vgprValuC+146+1] // *= ScaleAlphaVecVMulPK(68)(2)
v_fma_mix_f32 v[vgprValuC+144], s[sgprBeta], v136, v[vgprValuC+144] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+145], s[sgprBeta], v136, v[vgprValuC+145] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+146], s[sgprBeta], v137, v[vgprValuC+146] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+147], s[sgprBeta], v137, v[vgprValuC+147] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+144:vgprValuC+144+1], v[64:65], v[vgprValuC+144:vgprValuC+144+1] // C += bias
v_pk_add_f32 v[vgprValuC+146:vgprValuC+146+1], v[66:67], v[vgprValuC+146:vgprValuC+146+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+144], v[vgprValuC+144]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+145], v[vgprValuC+145]   // convert C to fp16
v_pack_b32_f16 v144, v[vgprValuC+144], v[vgprValuC+145] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+146], v[vgprValuC+146]   // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+147], v[vgprValuC+147]   // convert C to fp16
v_pack_b32_f16 v145, v[vgprValuC+146], v[vgprValuC+147] // Pack with neighbor
buffer_store_dwordx2 v[144:145], v131, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #5 (d1,d0,vc1,vc0) = */
/*    (1,0,7,0:vw4); (1,1,7,0:vw4); (1,2,7,0:vw4); (1,3,7,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v62, BufferOOB
/* (d1,vc1,d0,vc0)=(1,7,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v62, v6, s[56:57]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[10:11], v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v0, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b128 v[12:15], v7 offset:0                 // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b128 v[16:19], v8 offset:0                 // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v62, v6, s[56:57]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v9, v2, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v62, v9, s[56:57]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[26:27], v9, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v4, s52
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
ds_read_b128 v[28:31], v24 offset:0                // load Bias
v_add_u32 v25, 1024, v24                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[32:35], v25 offset:0                // load scaleAlpha
v_add_lshl_u32 v9, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v62, v9, s[56:57]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v40, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v62, v40, s[56:57]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[44:45], v40, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v41, v4, s52
v_lshlrev_b32 v41, 0x2, v41                        // Bias address scaled by BPE
ds_read_b128 v[48:51], v41 offset:0                // load Bias
v_add_u32 v42, 1024, v41                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[52:55], v42 offset:0                // load scaleAlpha
v_add_lshl_u32 v40, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v62, v40, s[56:57]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v43, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v62, v43, s[56:57]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[60:61], v43, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s52
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
ds_read_b128 v[64:67], v46 offset:0                // load Bias
v_add_u32 v47, 1024, v46                           // add ScaleAlphaVec offset (3)
ds_read_b128 v[68:71], v47 offset:0                // load scaleAlpha
v_add_lshl_u32 v43, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v62, v43, s[56:57]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc240         // copy acc to vreg[240]
v_accvgpr_read_b32 v[vgprValuC+21], acc241         // copy acc to vreg[241]
v_accvgpr_read_b32 v[vgprValuC+22], acc242         // copy acc to vreg[242]
v_accvgpr_read_b32 v[vgprValuC+23], acc243         // copy acc to vreg[243]
v_accvgpr_read_b32 v[vgprValuC+36], acc244         // copy acc to vreg[244]
v_accvgpr_read_b32 v[vgprValuC+37], acc245         // copy acc to vreg[245]
v_accvgpr_read_b32 v[vgprValuC+38], acc246         // copy acc to vreg[246]
v_accvgpr_read_b32 v[vgprValuC+39], acc247         // copy acc to vreg[247]
v_accvgpr_read_b32 v[vgprValuC+56], acc248         // copy acc to vreg[248]
v_accvgpr_read_b32 v[vgprValuC+57], acc249         // copy acc to vreg[249]
v_accvgpr_read_b32 v[vgprValuC+58], acc250         // copy acc to vreg[250]
v_accvgpr_read_b32 v[vgprValuC+59], acc251         // copy acc to vreg[251]
v_accvgpr_read_b32 v[vgprValuC+72], acc252         // copy acc to vreg[252]
v_accvgpr_read_b32 v[vgprValuC+73], acc253         // copy acc to vreg[253]
v_accvgpr_read_b32 v[vgprValuC+74], acc254         // copy acc to vreg[254]
v_accvgpr_read_b32 v[vgprValuC+75], acc255         // copy acc to vreg[255]

/* rC *= alpha batchElements=[(1, 0, 7, 0), (1, 1, 7, 0), (1, 2, 7, 0), (1, 3, 7, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= ScaleAlphaVecVMulPK(16)(0)
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= ScaleAlphaVecVMulPK(16)(2)
v_fma_mix_f32 v[vgprValuC+20], s[sgprBeta], v10, v[vgprValuC+20] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+21], s[sgprBeta], v10, v[vgprValuC+21] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+22], s[sgprBeta], v11, v[vgprValuC+22] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+23], s[sgprBeta], v11, v[vgprValuC+23] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[32:33], v[vgprValuC+36:vgprValuC+36+1] // *= ScaleAlphaVecVMulPK(32)(0)
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[34:35], v[vgprValuC+38:vgprValuC+38+1] // *= ScaleAlphaVecVMulPK(32)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v26, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v26, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v27, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v27, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[28:29], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[30:31], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[52:53], v[vgprValuC+56:vgprValuC+56+1] // *= ScaleAlphaVecVMulPK(52)(0)
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[54:55], v[vgprValuC+58:vgprValuC+58+1] // *= ScaleAlphaVecVMulPK(52)(2)
v_fma_mix_f32 v[vgprValuC+56], s[sgprBeta], v44, v[vgprValuC+56] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+57], s[sgprBeta], v44, v[vgprValuC+57] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+58], s[sgprBeta], v45, v[vgprValuC+58] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+59], s[sgprBeta], v45, v[vgprValuC+59] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+56:vgprValuC+56+1], v[48:49], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[vgprValuC+58:vgprValuC+58+1], v[50:51], v[vgprValuC+58:vgprValuC+58+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+56], v[vgprValuC+56]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+57], v[vgprValuC+57]     // convert C to fp16
v_pack_b32_f16 v56, v[vgprValuC+56], v[vgprValuC+57] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+58], v[vgprValuC+58]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+59], v[vgprValuC+59]     // convert C to fp16
v_pack_b32_f16 v57, v[vgprValuC+58], v[vgprValuC+59] // Pack with neighbor
buffer_store_dwordx2 v[56:57], v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[68:69], v[vgprValuC+72:vgprValuC+72+1] // *= ScaleAlphaVecVMulPK(68)(0)
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[70:71], v[vgprValuC+74:vgprValuC+74+1] // *= ScaleAlphaVecVMulPK(68)(2)
v_fma_mix_f32 v[vgprValuC+72], s[sgprBeta], v60, v[vgprValuC+72] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+73], s[sgprBeta], v60, v[vgprValuC+73] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+74], s[sgprBeta], v61, v[vgprValuC+74] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v61, v[vgprValuC+75] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+72:vgprValuC+72+1], v[64:65], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[vgprValuC+74:vgprValuC+74+1], v[66:67], v[vgprValuC+74:vgprValuC+74+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+72], v[vgprValuC+72]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+73], v[vgprValuC+73]     // convert C to fp16
v_pack_b32_f16 v72, v[vgprValuC+72], v[vgprValuC+73] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+74], v[vgprValuC+74]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+75], v[vgprValuC+75]     // convert C to fp16
v_pack_b32_f16 v73, v[vgprValuC+74], v[vgprValuC+75] // Pack with neighbor
buffer_store_dwordx2 v[72:73], v43, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_2                            // jump to end
label_GW_B1_E1_M:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=34 */
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw1); (0,0,0,1:vw1); (0,0,0,2:vw1); (0,0,0,3:vw1); (0,1,0,0:vw1); (0,1,0,1:vw1); (0,1,0,2:vw1); (0,1,0,3:vw1); (0,2,0,0:vw1); (0,2,0,1:vw1); (0,2,0,2:vw1); (0,2,0,3:vw1); (0,3,0,0:vw1); (0,3,0,1:vw1); (0,3,0,2:vw1); (0,3,0,3:vw1); (0,0,1,0:vw1); (0,0,1,1:vw1); (0,0,1,2:vw1); (0,0,1,3:vw1); (0,1,1,0:vw1); (0,1,1,1:vw1); (0,1,1,2:vw1); (0,1,1,3:vw1); (0,2,1,0:vw1); (0,2,1,1:vw1); (0,2,1,2:vw1); (0,2,1,3:vw1); (0,3,1,0:vw1); (0,3,1,1:vw1); (0,3,1,2:vw1); (0,3,1,3:vw1); (0,0,2,0:vw1); (0,0,2,1:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v209, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v209, v6, s[56:57]               // LDC clip if OOB. offset
buffer_load_short_d16 v9, v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v0, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for LDS write
s_barrier                                          // LDS write barrier
ds_read_b32 v10, v7 offset:0                       // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v11, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v209, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v13, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v209, v13, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v4, s52
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
ds_read_b32 v17, v14 offset:0                      // load Bias
v_add_u32 v15, 1024, v14                           // add ScaleAlphaVec offset (3)
ds_read_b32 v18, v15 offset:0                      // load scaleAlpha
v_add_lshl_u32 v13, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v209, v13, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v209, v20, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s52
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
ds_read_b32 v24, v21 offset:0                      // load Bias
v_add_u32 v22, 1024, v21                           // add ScaleAlphaVec offset (3)
ds_read_b32 v25, v22 offset:0                      // load scaleAlpha
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v209, v20, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v209, v27, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s52
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
ds_read_b32 v31, v28 offset:0                      // load Bias
v_add_u32 v29, 1024, v28                           // add ScaleAlphaVec offset (3)
ds_read_b32 v32, v29 offset:0                      // load scaleAlpha
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v209, v27, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v209, v34, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s52
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
ds_read_b32 v38, v35 offset:0                      // load Bias
v_add_u32 v36, 1024, v35                           // add ScaleAlphaVec offset (3)
ds_read_b32 v39, v36 offset:0                      // load scaleAlpha
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v209, v34, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v41, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v209, v41, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s52
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
ds_read_b32 v45, v42 offset:0                      // load Bias
v_add_u32 v43, 1024, v42                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v43 offset:0                      // load scaleAlpha
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v209, v41, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v48, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v209, v48, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v51, v48, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v4, s52
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
ds_read_b32 v52, v49 offset:0                      // load Bias
v_add_u32 v50, 1024, v49                           // add ScaleAlphaVec offset (3)
ds_read_b32 v53, v50 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v209, v48, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v55, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v209, v55, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v58, v55, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v56, v4, s52
v_lshlrev_b32 v56, 0x2, v56                        // Bias address scaled by BPE
ds_read_b32 v59, v56 offset:0                      // load Bias
v_add_u32 v57, 1024, v56                           // add ScaleAlphaVec offset (3)
ds_read_b32 v60, v57 offset:0                      // load scaleAlpha
v_add_lshl_u32 v55, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v209, v55, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v62, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v209, v62, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v65, v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v4, s52
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
ds_read_b32 v66, v63 offset:0                      // load Bias
v_add_u32 v64, 1024, v63                           // add ScaleAlphaVec offset (3)
ds_read_b32 v67, v64 offset:0                      // load scaleAlpha
v_add_lshl_u32 v62, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v209, v62, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v69, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v209, v69, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v72, v69, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v4, s52
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
ds_read_b32 v73, v70 offset:0                      // load Bias
v_add_u32 v71, 1024, v70                           // add ScaleAlphaVec offset (3)
ds_read_b32 v74, v71 offset:0                      // load scaleAlpha
v_add_lshl_u32 v69, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v209, v69, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v209, v76, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s52
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
ds_read_b32 v80, v77 offset:0                      // load Bias
v_add_u32 v78, 1024, v77                           // add ScaleAlphaVec offset (3)
ds_read_b32 v81, v78 offset:0                      // load scaleAlpha
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v209, v76, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v83, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v209, v83, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v86, v83, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v84, v4, s52
v_lshlrev_b32 v84, 0x2, v84                        // Bias address scaled by BPE
ds_read_b32 v87, v84 offset:0                      // load Bias
v_add_u32 v85, 1024, v84                           // add ScaleAlphaVec offset (3)
ds_read_b32 v88, v85 offset:0                      // load scaleAlpha
v_add_lshl_u32 v83, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v209, v83, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v90, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v209, v90, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v93, v90, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v4, s52
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
ds_read_b32 v94, v91 offset:0                      // load Bias
v_add_u32 v92, 1024, v91                           // add ScaleAlphaVec offset (3)
ds_read_b32 v95, v92 offset:0                      // load scaleAlpha
v_add_lshl_u32 v90, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v209, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v97, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v209, v97, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v100, v97, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v98, v4, s52
v_lshlrev_b32 v98, 0x2, v98                        // Bias address scaled by BPE
ds_read_b32 v101, v98 offset:0                     // load Bias
v_add_u32 v99, 1024, v98                           // add ScaleAlphaVec offset (3)
ds_read_b32 v102, v99 offset:0                     // load scaleAlpha
v_add_lshl_u32 v97, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v209, v97, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v104, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v104, v209, v104, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v107, v104, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v105, v4, s52
v_lshlrev_b32 v105, 0x2, v105                      // Bias address scaled by BPE
ds_read_b32 v108, v105 offset:0                    // load Bias
v_add_u32 v106, 1024, v105                         // add ScaleAlphaVec offset (3)
ds_read_b32 v109, v106 offset:0                    // load scaleAlpha
v_add_lshl_u32 v104, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v104, v209, v104, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v111, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v111, v209, v111, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v114, v111, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v112, v4, s52
v_lshlrev_b32 v112, 0x2, v112                      // Bias address scaled by BPE
ds_read_b32 v115, v112 offset:0                    // load Bias
v_add_u32 v113, 1024, v112                         // add ScaleAlphaVec offset (3)
ds_read_b32 v116, v113 offset:0                    // load scaleAlpha
v_add_lshl_u32 v111, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v111, v209, v111, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v118, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v209, v118, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v121, v118, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v0, s52
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v120, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v209, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v123, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v123, v209, v123, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v126, v123, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v124, v4, s52
v_lshlrev_b32 v124, 0x2, v124                      // Bias address scaled by BPE
v_add_u32 v125, 1024, v124                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v123, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v123, v209, v123, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v128, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v209, v128, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v131, v128, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v129, v4, s52
v_lshlrev_b32 v129, 0x2, v129                      // Bias address scaled by BPE
v_add_u32 v130, 1024, v129                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v128, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v209, v128, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v133, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v133, v209, v133, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v136, v133, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v134, v4, s52
v_lshlrev_b32 v134, 0x2, v134                      // Bias address scaled by BPE
v_add_u32 v135, 1024, v134                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v133, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v133, v209, v133, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v138, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v209, v138, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v142, v138, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v140, v4, s52
v_lshlrev_b32 v140, 0x2, v140                      // Bias address scaled by BPE
v_add_u32 v141, 1024, v140                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v138, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v209, v138, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v144, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v144, v209, v144, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v147, v144, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v145, v4, s52
v_lshlrev_b32 v145, 0x2, v145                      // Bias address scaled by BPE
v_add_u32 v146, 1024, v145                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v144, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v144, v209, v144, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v149, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v209, v149, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v152, v149, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v150, v4, s52
v_lshlrev_b32 v150, 0x2, v150                      // Bias address scaled by BPE
v_add_u32 v151, 1024, v150                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v149, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v209, v149, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v154, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v154, v209, v154, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v157, v154, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v155, v4, s52
v_lshlrev_b32 v155, 0x2, v155                      // Bias address scaled by BPE
v_add_u32 v156, 1024, v155                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v154, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v154, v209, v154, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v159, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v209, v159, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v162, v159, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v160, v4, s52
v_lshlrev_b32 v160, 0x2, v160                      // Bias address scaled by BPE
v_add_u32 v161, 1024, v160                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v159, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v209, v159, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v164, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v164, v209, v164, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v167, v164, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v165, v4, s52
v_lshlrev_b32 v165, 0x2, v165                      // Bias address scaled by BPE
v_add_u32 v166, 1024, v165                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v164, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v164, v209, v164, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v169, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v169, v209, v169, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v172, v169, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v170, v4, s52
v_lshlrev_b32 v170, 0x2, v170                      // Bias address scaled by BPE
v_add_u32 v171, 1024, v170                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v169, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v169, v209, v169, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v174, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v174, v209, v174, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v177, v174, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v175, v4, s52
v_lshlrev_b32 v175, 0x2, v175                      // Bias address scaled by BPE
v_add_u32 v176, 1024, v175                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v174, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v174, v209, v174, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v179, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v209, v179, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v182, v179, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v180, v4, s52
v_lshlrev_b32 v180, 0x2, v180                      // Bias address scaled by BPE
v_add_u32 v181, 1024, v180                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v179, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v209, v179, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v184, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v184, v209, v184, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v187, v184, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v185, v4, s52
v_lshlrev_b32 v185, 0x2, v185                      // Bias address scaled by BPE
v_add_u32 v186, 1024, v185                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v184, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v184, v209, v184, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v189, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v189, v209, v189, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v192, v189, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v190, v4, s52
v_lshlrev_b32 v190, 0x2, v190                      // Bias address scaled by BPE
v_add_u32 v191, 1024, v190                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v189, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v189, v209, v189, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v194, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v194, v209, v194, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v197, v194, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v195, v4, s52
v_lshlrev_b32 v195, 0x2, v195                      // Bias address scaled by BPE
v_add_u32 v196, 1024, v195                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v194, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v194, v209, v194, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v199, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v199, v209, v199, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v202, v199, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v200, v0, s52
v_lshlrev_b32 v200, 0x2, v200                      // Bias address scaled by BPE
v_add_u32 v201, 1024, v200                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v199, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v199, v209, v199, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v204, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v204, v209, v204, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v207, v204, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v205, v4, s52
v_lshlrev_b32 v205, 0x2, v205                      // Bias address scaled by BPE
v_add_u32 v206, 1024, v205                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v204, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v204, v209, v204, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+12], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+19], acc1           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc2           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+33], acc3           // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+40], acc4           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+47], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+54], acc6           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+61], acc7           // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+68], acc8           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+75], acc9           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+82], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+89], acc11          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+96], acc12          // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+103], acc13         // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+110], acc14         // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+117], acc15         // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+122], acc16         // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+127], acc17         // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+132], acc18         // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+137], acc19         // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+143], acc20         // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+148], acc21         // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+153], acc22         // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+158], acc23         // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+163], acc24         // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+168], acc25         // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+173], acc26         // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+178], acc27         // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+183], acc28         // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+188], acc29         // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+193], acc30         // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+198], acc31         // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+203], acc32         // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+208], acc33         // copy acc to vreg[33]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 0, 1), (0, 0, 0, 2), (0, 0, 0, 3), (0, 1, 0, 0), (0, 1, 0, 1), (0, 1, 0, 2), (0, 1, 0, 3), (0, 2, 0, 0), (0, 2, 0, 1), (0, 2, 0, 2), (0, 2, 0, 3), (0, 3, 0, 0), (0, 3, 0, 1), (0, 3, 0, 2), (0, 3, 0, 3), (0, 0, 1, 0), (0, 0, 1, 1), (0, 0, 1, 2), (0, 0, 1, 3), (0, 1, 1, 0), (0, 1, 1, 1), (0, 1, 1, 2), (0, 1, 1, 3), (0, 2, 1, 0), (0, 2, 1, 1), (0, 2, 1, 2), (0, 2, 1, 3), (0, 3, 1, 0), (0, 3, 1, 1), (0, 3, 1, 2), (0, 3, 1, 3), (0, 0, 2, 0), (0, 0, 2, 1)] */
v_mul_f32 v[vgprValuC+12], s[sgprAlpha], v[vgprValuC+12] // *= alpha
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+122], s[sgprAlpha], v[vgprValuC+122] // *= alpha
v_mul_f32 v[vgprValuC+127], s[sgprAlpha], v[vgprValuC+127] // *= alpha
v_mul_f32 v[vgprValuC+132], s[sgprAlpha], v[vgprValuC+132] // *= alpha
v_mul_f32 v[vgprValuC+137], s[sgprAlpha], v[vgprValuC+137] // *= alpha
v_mul_f32 v[vgprValuC+143], s[sgprAlpha], v[vgprValuC+143] // *= alpha
v_mul_f32 v[vgprValuC+148], s[sgprAlpha], v[vgprValuC+148] // *= alpha
v_mul_f32 v[vgprValuC+153], s[sgprAlpha], v[vgprValuC+153] // *= alpha
v_mul_f32 v[vgprValuC+158], s[sgprAlpha], v[vgprValuC+158] // *= alpha
v_mul_f32 v[vgprValuC+163], s[sgprAlpha], v[vgprValuC+163] // *= alpha
v_mul_f32 v[vgprValuC+168], s[sgprAlpha], v[vgprValuC+168] // *= alpha
v_mul_f32 v[vgprValuC+173], s[sgprAlpha], v[vgprValuC+173] // *= alpha
v_mul_f32 v[vgprValuC+178], s[sgprAlpha], v[vgprValuC+178] // *= alpha
v_mul_f32 v[vgprValuC+183], s[sgprAlpha], v[vgprValuC+183] // *= alpha
v_mul_f32 v[vgprValuC+188], s[sgprAlpha], v[vgprValuC+188] // *= alpha
v_mul_f32 v[vgprValuC+193], s[sgprAlpha], v[vgprValuC+193] // *= alpha
v_mul_f32 v[vgprValuC+198], s[sgprAlpha], v[vgprValuC+198] // *= alpha
v_mul_f32 v[vgprValuC+203], s[sgprAlpha], v[vgprValuC+203] // *= alpha
v_mul_f32 v[vgprValuC+208], s[sgprAlpha], v[vgprValuC+208] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+12], v11, v[vgprValuC+12]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+12], s[sgprBeta], v9, v[vgprValuC+12] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+12], v10, v[vgprValuC+12]    // C += bias
v_cvt_f16_f32 v12, v[vgprValuC+12]                 // convert C to fp16
buffer_store_short v12, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+19], s[sgprBeta], v16, v[vgprValuC+19] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+19], v17, v[vgprValuC+19]    // C += bias
v_cvt_f16_f32 v19, v[vgprValuC+19]                 // convert C to fp16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v23, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+26], v24, v[vgprValuC+26]    // C += bias
v_cvt_f16_f32 v26, v[vgprValuC+26]                 // convert C to fp16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v30, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+33], v31, v[vgprValuC+33]    // C += bias
v_cvt_f16_f32 v33, v[vgprValuC+33]                 // convert C to fp16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v37, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+40], v38, v[vgprValuC+40]    // C += bias
v_cvt_f16_f32 v40, v[vgprValuC+40]                 // convert C to fp16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v44, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+54], v53, v[vgprValuC+54]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v51, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+54], v52, v[vgprValuC+54]    // C += bias
v_cvt_f16_f32 v54, v[vgprValuC+54]                 // convert C to fp16
buffer_store_short v54, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+61], v60, v[vgprValuC+61]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v58, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+61], v59, v[vgprValuC+61]    // C += bias
v_cvt_f16_f32 v61, v[vgprValuC+61]                 // convert C to fp16
buffer_store_short v61, v55, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+68], v67, v[vgprValuC+68]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+68], s[sgprBeta], v65, v[vgprValuC+68] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+68], v66, v[vgprValuC+68]    // C += bias
v_cvt_f16_f32 v68, v[vgprValuC+68]                 // convert C to fp16
buffer_store_short v68, v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+75], v74, v[vgprValuC+75]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v72, v[vgprValuC+75] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+75], v73, v[vgprValuC+75]    // C += bias
v_cvt_f16_f32 v75, v[vgprValuC+75]                 // convert C to fp16
buffer_store_short v75, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+82], v81, v[vgprValuC+82]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+82], s[sgprBeta], v79, v[vgprValuC+82] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+82], v80, v[vgprValuC+82]    // C += bias
v_cvt_f16_f32 v82, v[vgprValuC+82]                 // convert C to fp16
buffer_store_short v82, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+89], v88, v[vgprValuC+89]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+89], s[sgprBeta], v86, v[vgprValuC+89] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+89], v87, v[vgprValuC+89]    // C += bias
v_cvt_f16_f32 v89, v[vgprValuC+89]                 // convert C to fp16
buffer_store_short v89, v83, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+96], v95, v[vgprValuC+96]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+96], s[sgprBeta], v93, v[vgprValuC+96] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+96], v94, v[vgprValuC+96]    // C += bias
v_cvt_f16_f32 v96, v[vgprValuC+96]                 // convert C to fp16
buffer_store_short v96, v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+103], v102, v[vgprValuC+103] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+103], s[sgprBeta], v100, v[vgprValuC+103] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+103], v101, v[vgprValuC+103] // C += bias
v_cvt_f16_f32 v103, v[vgprValuC+103]               // convert C to fp16
buffer_store_short v103, v97, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+110], v109, v[vgprValuC+110] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+110], s[sgprBeta], v107, v[vgprValuC+110] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+110], v108, v[vgprValuC+110] // C += bias
v_cvt_f16_f32 v110, v[vgprValuC+110]               // convert C to fp16
buffer_store_short v110, v104, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+117], v116, v[vgprValuC+117] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v114, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+117], v115, v[vgprValuC+117] // C += bias
v_cvt_f16_f32 v117, v[vgprValuC+117]               // convert C to fp16
buffer_store_short v117, v111, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+122], v11, v[vgprValuC+122]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+122], s[sgprBeta], v121, v[vgprValuC+122] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+122], v10, v[vgprValuC+122]  // C += bias
v_cvt_f16_f32 v122, v[vgprValuC+122]               // convert C to fp16
buffer_store_short v122, v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+127], v18, v[vgprValuC+127]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+127], s[sgprBeta], v126, v[vgprValuC+127] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+127], v17, v[vgprValuC+127]  // C += bias
v_cvt_f16_f32 v127, v[vgprValuC+127]               // convert C to fp16
buffer_store_short v127, v123, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+132], v25, v[vgprValuC+132]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+132], s[sgprBeta], v131, v[vgprValuC+132] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+132], v24, v[vgprValuC+132]  // C += bias
v_cvt_f16_f32 v132, v[vgprValuC+132]               // convert C to fp16
buffer_store_short v132, v128, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+137], v32, v[vgprValuC+137]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+137], s[sgprBeta], v136, v[vgprValuC+137] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+137], v31, v[vgprValuC+137]  // C += bias
v_cvt_f16_f32 v137, v[vgprValuC+137]               // convert C to fp16
buffer_store_short v137, v133, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+143], v39, v[vgprValuC+143]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+143], s[sgprBeta], v142, v[vgprValuC+143] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+143], v38, v[vgprValuC+143]  // C += bias
v_cvt_f16_f32 v143, v[vgprValuC+143]               // convert C to fp16
buffer_store_short v143, v138, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+148], v46, v[vgprValuC+148]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+148], s[sgprBeta], v147, v[vgprValuC+148] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+148], v45, v[vgprValuC+148]  // C += bias
v_cvt_f16_f32 v148, v[vgprValuC+148]               // convert C to fp16
buffer_store_short v148, v144, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+153], v53, v[vgprValuC+153]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+153], s[sgprBeta], v152, v[vgprValuC+153] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+153], v52, v[vgprValuC+153]  // C += bias
v_cvt_f16_f32 v153, v[vgprValuC+153]               // convert C to fp16
buffer_store_short v153, v149, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+158], v60, v[vgprValuC+158]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+158], s[sgprBeta], v157, v[vgprValuC+158] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+158], v59, v[vgprValuC+158]  // C += bias
v_cvt_f16_f32 v158, v[vgprValuC+158]               // convert C to fp16
buffer_store_short v158, v154, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+163], v67, v[vgprValuC+163]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+163], s[sgprBeta], v162, v[vgprValuC+163] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+163], v66, v[vgprValuC+163]  // C += bias
v_cvt_f16_f32 v163, v[vgprValuC+163]               // convert C to fp16
buffer_store_short v163, v159, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+168], v74, v[vgprValuC+168]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+168], s[sgprBeta], v167, v[vgprValuC+168] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+168], v73, v[vgprValuC+168]  // C += bias
v_cvt_f16_f32 v168, v[vgprValuC+168]               // convert C to fp16
buffer_store_short v168, v164, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+173], v81, v[vgprValuC+173]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+173], s[sgprBeta], v172, v[vgprValuC+173] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+173], v80, v[vgprValuC+173]  // C += bias
v_cvt_f16_f32 v173, v[vgprValuC+173]               // convert C to fp16
buffer_store_short v173, v169, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+178], v88, v[vgprValuC+178]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+178], s[sgprBeta], v177, v[vgprValuC+178] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+178], v87, v[vgprValuC+178]  // C += bias
v_cvt_f16_f32 v178, v[vgprValuC+178]               // convert C to fp16
buffer_store_short v178, v174, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+183], v95, v[vgprValuC+183]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+183], s[sgprBeta], v182, v[vgprValuC+183] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+183], v94, v[vgprValuC+183]  // C += bias
v_cvt_f16_f32 v183, v[vgprValuC+183]               // convert C to fp16
buffer_store_short v183, v179, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+188], v102, v[vgprValuC+188] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+188], s[sgprBeta], v187, v[vgprValuC+188] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+188], v101, v[vgprValuC+188] // C += bias
v_cvt_f16_f32 v188, v[vgprValuC+188]               // convert C to fp16
buffer_store_short v188, v184, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+193], v109, v[vgprValuC+193] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+193], s[sgprBeta], v192, v[vgprValuC+193] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+193], v108, v[vgprValuC+193] // C += bias
v_cvt_f16_f32 v193, v[vgprValuC+193]               // convert C to fp16
buffer_store_short v193, v189, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+198], v116, v[vgprValuC+198] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+198], s[sgprBeta], v197, v[vgprValuC+198] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+198], v115, v[vgprValuC+198] // C += bias
v_cvt_f16_f32 v198, v[vgprValuC+198]               // convert C to fp16
buffer_store_short v198, v194, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+203], v11, v[vgprValuC+203]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+203], s[sgprBeta], v202, v[vgprValuC+203] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+203], v10, v[vgprValuC+203]  // C += bias
v_cvt_f16_f32 v203, v[vgprValuC+203]               // convert C to fp16
buffer_store_short v203, v199, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+208], v18, v[vgprValuC+208]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+208], s[sgprBeta], v207, v[vgprValuC+208] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+208], v17, v[vgprValuC+208]  // C += bias
v_cvt_f16_f32 v208, v[vgprValuC+208]               // convert C to fp16
buffer_store_short v208, v204, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #1 (d1,d0,vc1,vc0) = */
/*    (0,0,2,2:vw1); (0,0,2,3:vw1); (0,1,2,0:vw1); (0,1,2,1:vw1); (0,1,2,2:vw1); (0,1,2,3:vw1); (0,2,2,0:vw1); (0,2,2,1:vw1); (0,2,2,2:vw1); (0,2,2,3:vw1); (0,3,2,0:vw1); (0,3,2,1:vw1); (0,3,2,2:vw1); (0,3,2,3:vw1); (0,0,3,0:vw1); (0,0,3,1:vw1); (0,0,3,2:vw1); (0,0,3,3:vw1); (0,1,3,0:vw1); (0,1,3,1:vw1); (0,1,3,2:vw1); (0,1,3,3:vw1); (0,2,3,0:vw1); (0,2,3,1:vw1); (0,2,3,2:vw1); (0,2,3,3:vw1); (0,3,3,0:vw1); (0,3,3,1:vw1); (0,3,3,2:vw1); (0,3,3,3:vw1); (0,0,4,0:vw1); (0,0,4,1:vw1); (0,0,4,2:vw1); (0,0,4,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v209, BufferOOB
/* (d1,vc1,d0,vc0)=(0,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v6, v2, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v209, v6, s[56:57]               // LDC clip if OOB. offset
buffer_load_short_d16 v9, v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v4, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b32 v10, v7 offset:0                       // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v11, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v209, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v13, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v209, v13, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v4, s52
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
ds_read_b32 v17, v14 offset:0                      // load Bias
v_add_u32 v15, 1024, v14                           // add ScaleAlphaVec offset (3)
ds_read_b32 v18, v15 offset:0                      // load scaleAlpha
v_add_lshl_u32 v13, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v209, v13, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v209, v20, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s52
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
ds_read_b32 v24, v21 offset:0                      // load Bias
v_add_u32 v22, 1024, v21                           // add ScaleAlphaVec offset (3)
ds_read_b32 v25, v22 offset:0                      // load scaleAlpha
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v209, v20, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v209, v27, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s52
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
ds_read_b32 v31, v28 offset:0                      // load Bias
v_add_u32 v29, 1024, v28                           // add ScaleAlphaVec offset (3)
ds_read_b32 v32, v29 offset:0                      // load scaleAlpha
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v209, v27, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v209, v34, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s52
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
ds_read_b32 v38, v35 offset:0                      // load Bias
v_add_u32 v36, 1024, v35                           // add ScaleAlphaVec offset (3)
ds_read_b32 v39, v36 offset:0                      // load scaleAlpha
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v209, v34, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v41, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v209, v41, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s52
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
ds_read_b32 v45, v42 offset:0                      // load Bias
v_add_u32 v43, 1024, v42                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v43 offset:0                      // load scaleAlpha
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v209, v41, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v48, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v209, v48, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v51, v48, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v4, s52
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
ds_read_b32 v52, v49 offset:0                      // load Bias
v_add_u32 v50, 1024, v49                           // add ScaleAlphaVec offset (3)
ds_read_b32 v53, v50 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v209, v48, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v55, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v209, v55, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v58, v55, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v56, v4, s52
v_lshlrev_b32 v56, 0x2, v56                        // Bias address scaled by BPE
ds_read_b32 v59, v56 offset:0                      // load Bias
v_add_u32 v57, 1024, v56                           // add ScaleAlphaVec offset (3)
ds_read_b32 v60, v57 offset:0                      // load scaleAlpha
v_add_lshl_u32 v55, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v209, v55, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v62, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v209, v62, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v65, v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v4, s52
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
ds_read_b32 v66, v63 offset:0                      // load Bias
v_add_u32 v64, 1024, v63                           // add ScaleAlphaVec offset (3)
ds_read_b32 v67, v64 offset:0                      // load scaleAlpha
v_add_lshl_u32 v62, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v209, v62, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v69, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v209, v69, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v72, v69, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v4, s52
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
ds_read_b32 v73, v70 offset:0                      // load Bias
v_add_u32 v71, 1024, v70                           // add ScaleAlphaVec offset (3)
ds_read_b32 v74, v71 offset:0                      // load scaleAlpha
v_add_lshl_u32 v69, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v209, v69, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v209, v76, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s52
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
ds_read_b32 v80, v77 offset:0                      // load Bias
v_add_u32 v78, 1024, v77                           // add ScaleAlphaVec offset (3)
ds_read_b32 v81, v78 offset:0                      // load scaleAlpha
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v209, v76, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v83, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v209, v83, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v86, v83, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v84, v4, s52
v_lshlrev_b32 v84, 0x2, v84                        // Bias address scaled by BPE
ds_read_b32 v87, v84 offset:0                      // load Bias
v_add_u32 v85, 1024, v84                           // add ScaleAlphaVec offset (3)
ds_read_b32 v88, v85 offset:0                      // load scaleAlpha
v_add_lshl_u32 v83, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v209, v83, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v90, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v209, v90, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v93, v90, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v4, s52
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
ds_read_b32 v94, v91 offset:0                      // load Bias
v_add_u32 v92, 1024, v91                           // add ScaleAlphaVec offset (3)
ds_read_b32 v95, v92 offset:0                      // load scaleAlpha
v_add_lshl_u32 v90, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v209, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v97, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v209, v97, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v100, v97, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v98, v4, s52
v_lshlrev_b32 v98, 0x2, v98                        // Bias address scaled by BPE
ds_read_b32 v101, v98 offset:0                     // load Bias
v_add_u32 v99, 1024, v98                           // add ScaleAlphaVec offset (3)
ds_read_b32 v102, v99 offset:0                     // load scaleAlpha
v_add_lshl_u32 v97, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v209, v97, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v104, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v104, v209, v104, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v107, v104, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v105, v0, s52
v_lshlrev_b32 v105, 0x2, v105                      // Bias address scaled by BPE
ds_read_b32 v108, v105 offset:0                    // load Bias
v_add_u32 v106, 1024, v105                         // add ScaleAlphaVec offset (3)
ds_read_b32 v109, v106 offset:0                    // load scaleAlpha
v_add_lshl_u32 v104, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v104, v209, v104, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v111, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v111, v209, v111, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v114, v111, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v112, v4, s52
v_lshlrev_b32 v112, 0x2, v112                      // Bias address scaled by BPE
ds_read_b32 v115, v112 offset:0                    // load Bias
v_add_u32 v113, 1024, v112                         // add ScaleAlphaVec offset (3)
ds_read_b32 v116, v113 offset:0                    // load scaleAlpha
v_add_lshl_u32 v111, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v111, v209, v111, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v118, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v209, v118, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v121, v118, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v4, s52
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v120, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v209, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v123, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v123, v209, v123, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v126, v123, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v124, v4, s52
v_lshlrev_b32 v124, 0x2, v124                      // Bias address scaled by BPE
v_add_u32 v125, 1024, v124                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v123, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v123, v209, v123, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v128, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v209, v128, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v131, v128, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v129, v4, s52
v_lshlrev_b32 v129, 0x2, v129                      // Bias address scaled by BPE
v_add_u32 v130, 1024, v129                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v128, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v209, v128, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v133, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v133, v209, v133, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v136, v133, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v134, v4, s52
v_lshlrev_b32 v134, 0x2, v134                      // Bias address scaled by BPE
v_add_u32 v135, 1024, v134                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v133, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v133, v209, v133, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v138, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v209, v138, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v142, v138, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v140, v4, s52
v_lshlrev_b32 v140, 0x2, v140                      // Bias address scaled by BPE
v_add_u32 v141, 1024, v140                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v138, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v209, v138, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v144, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v144, v209, v144, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v147, v144, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v145, v4, s52
v_lshlrev_b32 v145, 0x2, v145                      // Bias address scaled by BPE
v_add_u32 v146, 1024, v145                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v144, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v144, v209, v144, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v149, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v209, v149, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v152, v149, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v150, v4, s52
v_lshlrev_b32 v150, 0x2, v150                      // Bias address scaled by BPE
v_add_u32 v151, 1024, v150                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v149, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v209, v149, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v154, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v154, v209, v154, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v157, v154, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v155, v4, s52
v_lshlrev_b32 v155, 0x2, v155                      // Bias address scaled by BPE
v_add_u32 v156, 1024, v155                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v154, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v154, v209, v154, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v159, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v209, v159, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v162, v159, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v160, v4, s52
v_lshlrev_b32 v160, 0x2, v160                      // Bias address scaled by BPE
v_add_u32 v161, 1024, v160                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v159, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v209, v159, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v164, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v164, v209, v164, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v167, v164, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v165, v4, s52
v_lshlrev_b32 v165, 0x2, v165                      // Bias address scaled by BPE
v_add_u32 v166, 1024, v165                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v164, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v164, v209, v164, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v169, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v169, v209, v169, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v172, v169, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v170, v4, s52
v_lshlrev_b32 v170, 0x2, v170                      // Bias address scaled by BPE
v_add_u32 v171, 1024, v170                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v169, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v169, v209, v169, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v174, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v174, v209, v174, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v177, v174, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v175, v4, s52
v_lshlrev_b32 v175, 0x2, v175                      // Bias address scaled by BPE
v_add_u32 v176, 1024, v175                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v174, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v174, v209, v174, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v179, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v209, v179, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v182, v179, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v180, v4, s52
v_lshlrev_b32 v180, 0x2, v180                      // Bias address scaled by BPE
v_add_u32 v181, 1024, v180                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v179, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v209, v179, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v184, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v184, v209, v184, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v187, v184, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v185, v4, s52
v_lshlrev_b32 v185, 0x2, v185                      // Bias address scaled by BPE
v_add_u32 v186, 1024, v185                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v184, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v184, v209, v184, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v189, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v189, v209, v189, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v192, v189, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v190, v0, s52
v_lshlrev_b32 v190, 0x2, v190                      // Bias address scaled by BPE
v_add_u32 v191, 1024, v190                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v189, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v189, v209, v189, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v194, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v194, v209, v194, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v197, v194, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v195, v4, s52
v_lshlrev_b32 v195, 0x2, v195                      // Bias address scaled by BPE
v_add_u32 v196, 1024, v195                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v194, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v194, v209, v194, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v199, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v199, v209, v199, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v202, v199, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v200, v4, s52
v_lshlrev_b32 v200, 0x2, v200                      // Bias address scaled by BPE
v_add_u32 v201, 1024, v200                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v199, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v199, v209, v199, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v204, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v204, v209, v204, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v207, v204, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v205, v4, s52
v_lshlrev_b32 v205, 0x2, v205                      // Bias address scaled by BPE
v_add_u32 v206, 1024, v205                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v204, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v204, v209, v204, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+12], acc34          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+19], acc35          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+26], acc36          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+33], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+40], acc38          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+47], acc39          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+54], acc40          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+61], acc41          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+68], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+75], acc43          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+82], acc44          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+89], acc45          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+96], acc46          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+103], acc47         // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+110], acc48         // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+117], acc49         // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+122], acc50         // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+127], acc51         // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+132], acc52         // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+137], acc53         // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+143], acc54         // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+148], acc55         // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+153], acc56         // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+158], acc57         // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+163], acc58         // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+168], acc59         // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+173], acc60         // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+178], acc61         // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+183], acc62         // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+188], acc63         // copy acc to vreg[63]
v_accvgpr_read_b32 v[vgprValuC+193], acc64         // copy acc to vreg[64]
v_accvgpr_read_b32 v[vgprValuC+198], acc65         // copy acc to vreg[65]
v_accvgpr_read_b32 v[vgprValuC+203], acc66         // copy acc to vreg[66]
v_accvgpr_read_b32 v[vgprValuC+208], acc67         // copy acc to vreg[67]

/* rC *= alpha batchElements=[(0, 0, 2, 2), (0, 0, 2, 3), (0, 1, 2, 0), (0, 1, 2, 1), (0, 1, 2, 2), (0, 1, 2, 3), (0, 2, 2, 0), (0, 2, 2, 1), (0, 2, 2, 2), (0, 2, 2, 3), (0, 3, 2, 0), (0, 3, 2, 1), (0, 3, 2, 2), (0, 3, 2, 3), (0, 0, 3, 0), (0, 0, 3, 1), (0, 0, 3, 2), (0, 0, 3, 3), (0, 1, 3, 0), (0, 1, 3, 1), (0, 1, 3, 2), (0, 1, 3, 3), (0, 2, 3, 0), (0, 2, 3, 1), (0, 2, 3, 2), (0, 2, 3, 3), (0, 3, 3, 0), (0, 3, 3, 1), (0, 3, 3, 2), (0, 3, 3, 3), (0, 0, 4, 0), (0, 0, 4, 1), (0, 0, 4, 2), (0, 0, 4, 3)] */
v_mul_f32 v[vgprValuC+12], s[sgprAlpha], v[vgprValuC+12] // *= alpha
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+122], s[sgprAlpha], v[vgprValuC+122] // *= alpha
v_mul_f32 v[vgprValuC+127], s[sgprAlpha], v[vgprValuC+127] // *= alpha
v_mul_f32 v[vgprValuC+132], s[sgprAlpha], v[vgprValuC+132] // *= alpha
v_mul_f32 v[vgprValuC+137], s[sgprAlpha], v[vgprValuC+137] // *= alpha
v_mul_f32 v[vgprValuC+143], s[sgprAlpha], v[vgprValuC+143] // *= alpha
v_mul_f32 v[vgprValuC+148], s[sgprAlpha], v[vgprValuC+148] // *= alpha
v_mul_f32 v[vgprValuC+153], s[sgprAlpha], v[vgprValuC+153] // *= alpha
v_mul_f32 v[vgprValuC+158], s[sgprAlpha], v[vgprValuC+158] // *= alpha
v_mul_f32 v[vgprValuC+163], s[sgprAlpha], v[vgprValuC+163] // *= alpha
v_mul_f32 v[vgprValuC+168], s[sgprAlpha], v[vgprValuC+168] // *= alpha
v_mul_f32 v[vgprValuC+173], s[sgprAlpha], v[vgprValuC+173] // *= alpha
v_mul_f32 v[vgprValuC+178], s[sgprAlpha], v[vgprValuC+178] // *= alpha
v_mul_f32 v[vgprValuC+183], s[sgprAlpha], v[vgprValuC+183] // *= alpha
v_mul_f32 v[vgprValuC+188], s[sgprAlpha], v[vgprValuC+188] // *= alpha
v_mul_f32 v[vgprValuC+193], s[sgprAlpha], v[vgprValuC+193] // *= alpha
v_mul_f32 v[vgprValuC+198], s[sgprAlpha], v[vgprValuC+198] // *= alpha
v_mul_f32 v[vgprValuC+203], s[sgprAlpha], v[vgprValuC+203] // *= alpha
v_mul_f32 v[vgprValuC+208], s[sgprAlpha], v[vgprValuC+208] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+12], v11, v[vgprValuC+12]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+12], s[sgprBeta], v9, v[vgprValuC+12] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+12], v10, v[vgprValuC+12]    // C += bias
v_cvt_f16_f32 v12, v[vgprValuC+12]                 // convert C to fp16
buffer_store_short v12, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+19], s[sgprBeta], v16, v[vgprValuC+19] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+19], v17, v[vgprValuC+19]    // C += bias
v_cvt_f16_f32 v19, v[vgprValuC+19]                 // convert C to fp16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v23, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+26], v24, v[vgprValuC+26]    // C += bias
v_cvt_f16_f32 v26, v[vgprValuC+26]                 // convert C to fp16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v30, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+33], v31, v[vgprValuC+33]    // C += bias
v_cvt_f16_f32 v33, v[vgprValuC+33]                 // convert C to fp16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v37, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+40], v38, v[vgprValuC+40]    // C += bias
v_cvt_f16_f32 v40, v[vgprValuC+40]                 // convert C to fp16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v44, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+54], v53, v[vgprValuC+54]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v51, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+54], v52, v[vgprValuC+54]    // C += bias
v_cvt_f16_f32 v54, v[vgprValuC+54]                 // convert C to fp16
buffer_store_short v54, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+61], v60, v[vgprValuC+61]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v58, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+61], v59, v[vgprValuC+61]    // C += bias
v_cvt_f16_f32 v61, v[vgprValuC+61]                 // convert C to fp16
buffer_store_short v61, v55, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+68], v67, v[vgprValuC+68]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+68], s[sgprBeta], v65, v[vgprValuC+68] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+68], v66, v[vgprValuC+68]    // C += bias
v_cvt_f16_f32 v68, v[vgprValuC+68]                 // convert C to fp16
buffer_store_short v68, v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+75], v74, v[vgprValuC+75]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v72, v[vgprValuC+75] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+75], v73, v[vgprValuC+75]    // C += bias
v_cvt_f16_f32 v75, v[vgprValuC+75]                 // convert C to fp16
buffer_store_short v75, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+82], v81, v[vgprValuC+82]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+82], s[sgprBeta], v79, v[vgprValuC+82] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+82], v80, v[vgprValuC+82]    // C += bias
v_cvt_f16_f32 v82, v[vgprValuC+82]                 // convert C to fp16
buffer_store_short v82, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+89], v88, v[vgprValuC+89]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+89], s[sgprBeta], v86, v[vgprValuC+89] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+89], v87, v[vgprValuC+89]    // C += bias
v_cvt_f16_f32 v89, v[vgprValuC+89]                 // convert C to fp16
buffer_store_short v89, v83, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+96], v95, v[vgprValuC+96]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+96], s[sgprBeta], v93, v[vgprValuC+96] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+96], v94, v[vgprValuC+96]    // C += bias
v_cvt_f16_f32 v96, v[vgprValuC+96]                 // convert C to fp16
buffer_store_short v96, v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+103], v102, v[vgprValuC+103] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+103], s[sgprBeta], v100, v[vgprValuC+103] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+103], v101, v[vgprValuC+103] // C += bias
v_cvt_f16_f32 v103, v[vgprValuC+103]               // convert C to fp16
buffer_store_short v103, v97, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+110], v109, v[vgprValuC+110] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+110], s[sgprBeta], v107, v[vgprValuC+110] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+110], v108, v[vgprValuC+110] // C += bias
v_cvt_f16_f32 v110, v[vgprValuC+110]               // convert C to fp16
buffer_store_short v110, v104, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+117], v116, v[vgprValuC+117] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v114, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+117], v115, v[vgprValuC+117] // C += bias
v_cvt_f16_f32 v117, v[vgprValuC+117]               // convert C to fp16
buffer_store_short v117, v111, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+122], v11, v[vgprValuC+122]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+122], s[sgprBeta], v121, v[vgprValuC+122] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+122], v10, v[vgprValuC+122]  // C += bias
v_cvt_f16_f32 v122, v[vgprValuC+122]               // convert C to fp16
buffer_store_short v122, v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+127], v18, v[vgprValuC+127]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+127], s[sgprBeta], v126, v[vgprValuC+127] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+127], v17, v[vgprValuC+127]  // C += bias
v_cvt_f16_f32 v127, v[vgprValuC+127]               // convert C to fp16
buffer_store_short v127, v123, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+132], v25, v[vgprValuC+132]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+132], s[sgprBeta], v131, v[vgprValuC+132] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+132], v24, v[vgprValuC+132]  // C += bias
v_cvt_f16_f32 v132, v[vgprValuC+132]               // convert C to fp16
buffer_store_short v132, v128, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+137], v32, v[vgprValuC+137]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+137], s[sgprBeta], v136, v[vgprValuC+137] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+137], v31, v[vgprValuC+137]  // C += bias
v_cvt_f16_f32 v137, v[vgprValuC+137]               // convert C to fp16
buffer_store_short v137, v133, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+143], v39, v[vgprValuC+143]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+143], s[sgprBeta], v142, v[vgprValuC+143] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+143], v38, v[vgprValuC+143]  // C += bias
v_cvt_f16_f32 v143, v[vgprValuC+143]               // convert C to fp16
buffer_store_short v143, v138, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+148], v46, v[vgprValuC+148]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+148], s[sgprBeta], v147, v[vgprValuC+148] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+148], v45, v[vgprValuC+148]  // C += bias
v_cvt_f16_f32 v148, v[vgprValuC+148]               // convert C to fp16
buffer_store_short v148, v144, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+153], v53, v[vgprValuC+153]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+153], s[sgprBeta], v152, v[vgprValuC+153] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+153], v52, v[vgprValuC+153]  // C += bias
v_cvt_f16_f32 v153, v[vgprValuC+153]               // convert C to fp16
buffer_store_short v153, v149, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+158], v60, v[vgprValuC+158]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+158], s[sgprBeta], v157, v[vgprValuC+158] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+158], v59, v[vgprValuC+158]  // C += bias
v_cvt_f16_f32 v158, v[vgprValuC+158]               // convert C to fp16
buffer_store_short v158, v154, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+163], v67, v[vgprValuC+163]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+163], s[sgprBeta], v162, v[vgprValuC+163] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+163], v66, v[vgprValuC+163]  // C += bias
v_cvt_f16_f32 v163, v[vgprValuC+163]               // convert C to fp16
buffer_store_short v163, v159, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+168], v74, v[vgprValuC+168]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+168], s[sgprBeta], v167, v[vgprValuC+168] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+168], v73, v[vgprValuC+168]  // C += bias
v_cvt_f16_f32 v168, v[vgprValuC+168]               // convert C to fp16
buffer_store_short v168, v164, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+173], v81, v[vgprValuC+173]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+173], s[sgprBeta], v172, v[vgprValuC+173] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+173], v80, v[vgprValuC+173]  // C += bias
v_cvt_f16_f32 v173, v[vgprValuC+173]               // convert C to fp16
buffer_store_short v173, v169, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+178], v88, v[vgprValuC+178]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+178], s[sgprBeta], v177, v[vgprValuC+178] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+178], v87, v[vgprValuC+178]  // C += bias
v_cvt_f16_f32 v178, v[vgprValuC+178]               // convert C to fp16
buffer_store_short v178, v174, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+183], v95, v[vgprValuC+183]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+183], s[sgprBeta], v182, v[vgprValuC+183] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+183], v94, v[vgprValuC+183]  // C += bias
v_cvt_f16_f32 v183, v[vgprValuC+183]               // convert C to fp16
buffer_store_short v183, v179, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+188], v102, v[vgprValuC+188] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+188], s[sgprBeta], v187, v[vgprValuC+188] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+188], v101, v[vgprValuC+188] // C += bias
v_cvt_f16_f32 v188, v[vgprValuC+188]               // convert C to fp16
buffer_store_short v188, v184, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+193], v109, v[vgprValuC+193] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+193], s[sgprBeta], v192, v[vgprValuC+193] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+193], v108, v[vgprValuC+193] // C += bias
v_cvt_f16_f32 v193, v[vgprValuC+193]               // convert C to fp16
buffer_store_short v193, v189, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+198], v116, v[vgprValuC+198] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+198], s[sgprBeta], v197, v[vgprValuC+198] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+198], v115, v[vgprValuC+198] // C += bias
v_cvt_f16_f32 v198, v[vgprValuC+198]               // convert C to fp16
buffer_store_short v198, v194, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+203], v11, v[vgprValuC+203]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+203], s[sgprBeta], v202, v[vgprValuC+203] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+203], v10, v[vgprValuC+203]  // C += bias
v_cvt_f16_f32 v203, v[vgprValuC+203]               // convert C to fp16
buffer_store_short v203, v199, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+208], v18, v[vgprValuC+208]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+208], s[sgprBeta], v207, v[vgprValuC+208] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+208], v17, v[vgprValuC+208]  // C += bias
v_cvt_f16_f32 v208, v[vgprValuC+208]               // convert C to fp16
buffer_store_short v208, v204, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #2 (d1,d0,vc1,vc0) = */
/*    (0,1,4,0:vw1); (0,1,4,1:vw1); (0,1,4,2:vw1); (0,1,4,3:vw1); (0,2,4,0:vw1); (0,2,4,1:vw1); (0,2,4,2:vw1); (0,2,4,3:vw1); (0,3,4,0:vw1); (0,3,4,1:vw1); (0,3,4,2:vw1); (0,3,4,3:vw1); (0,0,5,0:vw1); (0,0,5,1:vw1); (0,0,5,2:vw1); (0,0,5,3:vw1); (0,1,5,0:vw1); (0,1,5,1:vw1); (0,1,5,2:vw1); (0,1,5,3:vw1); (0,2,5,0:vw1); (0,2,5,1:vw1); (0,2,5,2:vw1); (0,2,5,3:vw1); (0,3,5,0:vw1); (0,3,5,1:vw1); (0,3,5,2:vw1); (0,3,5,3:vw1); (0,0,6,0:vw1); (0,0,6,1:vw1); (0,0,6,2:vw1); (0,0,6,3:vw1); (0,1,6,0:vw1); (0,1,6,1:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v209, BufferOOB
/* (d1,vc1,d0,vc0)=(0,4,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v6, v2, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v209, v6, s[56:57]               // LDC clip if OOB. offset
buffer_load_short_d16 v9, v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v4, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b32 v10, v7 offset:0                       // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v11, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v209, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v13, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v209, v13, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v4, s52
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
ds_read_b32 v17, v14 offset:0                      // load Bias
v_add_u32 v15, 1024, v14                           // add ScaleAlphaVec offset (3)
ds_read_b32 v18, v15 offset:0                      // load scaleAlpha
v_add_lshl_u32 v13, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v209, v13, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v209, v20, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s52
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
ds_read_b32 v24, v21 offset:0                      // load Bias
v_add_u32 v22, 1024, v21                           // add ScaleAlphaVec offset (3)
ds_read_b32 v25, v22 offset:0                      // load scaleAlpha
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v209, v20, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v209, v27, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s52
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
ds_read_b32 v31, v28 offset:0                      // load Bias
v_add_u32 v29, 1024, v28                           // add ScaleAlphaVec offset (3)
ds_read_b32 v32, v29 offset:0                      // load scaleAlpha
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v209, v27, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v209, v34, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s52
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
ds_read_b32 v38, v35 offset:0                      // load Bias
v_add_u32 v36, 1024, v35                           // add ScaleAlphaVec offset (3)
ds_read_b32 v39, v36 offset:0                      // load scaleAlpha
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v209, v34, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v41, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v209, v41, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s52
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
ds_read_b32 v45, v42 offset:0                      // load Bias
v_add_u32 v43, 1024, v42                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v43 offset:0                      // load scaleAlpha
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v209, v41, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v48, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v209, v48, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v51, v48, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v4, s52
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
ds_read_b32 v52, v49 offset:0                      // load Bias
v_add_u32 v50, 1024, v49                           // add ScaleAlphaVec offset (3)
ds_read_b32 v53, v50 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v209, v48, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v55, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v209, v55, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v58, v55, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v56, v4, s52
v_lshlrev_b32 v56, 0x2, v56                        // Bias address scaled by BPE
ds_read_b32 v59, v56 offset:0                      // load Bias
v_add_u32 v57, 1024, v56                           // add ScaleAlphaVec offset (3)
ds_read_b32 v60, v57 offset:0                      // load scaleAlpha
v_add_lshl_u32 v55, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v209, v55, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v62, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v209, v62, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v65, v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v4, s52
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
ds_read_b32 v66, v63 offset:0                      // load Bias
v_add_u32 v64, 1024, v63                           // add ScaleAlphaVec offset (3)
ds_read_b32 v67, v64 offset:0                      // load scaleAlpha
v_add_lshl_u32 v62, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v209, v62, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v69, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v209, v69, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v72, v69, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v4, s52
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
ds_read_b32 v73, v70 offset:0                      // load Bias
v_add_u32 v71, 1024, v70                           // add ScaleAlphaVec offset (3)
ds_read_b32 v74, v71 offset:0                      // load scaleAlpha
v_add_lshl_u32 v69, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v209, v69, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v209, v76, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s52
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
ds_read_b32 v80, v77 offset:0                      // load Bias
v_add_u32 v78, 1024, v77                           // add ScaleAlphaVec offset (3)
ds_read_b32 v81, v78 offset:0                      // load scaleAlpha
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v209, v76, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,4,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v83, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v209, v83, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v86, v83, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v84, v4, s52
v_lshlrev_b32 v84, 0x2, v84                        // Bias address scaled by BPE
ds_read_b32 v87, v84 offset:0                      // load Bias
v_add_u32 v85, 1024, v84                           // add ScaleAlphaVec offset (3)
ds_read_b32 v88, v85 offset:0                      // load scaleAlpha
v_add_lshl_u32 v83, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v209, v83, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v90, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v209, v90, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v93, v90, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v0, s52
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
ds_read_b32 v94, v91 offset:0                      // load Bias
v_add_u32 v92, 1024, v91                           // add ScaleAlphaVec offset (3)
ds_read_b32 v95, v92 offset:0                      // load scaleAlpha
v_add_lshl_u32 v90, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v209, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v97, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v209, v97, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v100, v97, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v98, v4, s52
v_lshlrev_b32 v98, 0x2, v98                        // Bias address scaled by BPE
ds_read_b32 v101, v98 offset:0                     // load Bias
v_add_u32 v99, 1024, v98                           // add ScaleAlphaVec offset (3)
ds_read_b32 v102, v99 offset:0                     // load scaleAlpha
v_add_lshl_u32 v97, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v209, v97, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v104, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v104, v209, v104, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v107, v104, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v105, v4, s52
v_lshlrev_b32 v105, 0x2, v105                      // Bias address scaled by BPE
ds_read_b32 v108, v105 offset:0                    // load Bias
v_add_u32 v106, 1024, v105                         // add ScaleAlphaVec offset (3)
ds_read_b32 v109, v106 offset:0                    // load scaleAlpha
v_add_lshl_u32 v104, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v104, v209, v104, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v111, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v111, v209, v111, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v114, v111, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v112, v4, s52
v_lshlrev_b32 v112, 0x2, v112                      // Bias address scaled by BPE
ds_read_b32 v115, v112 offset:0                    // load Bias
v_add_u32 v113, 1024, v112                         // add ScaleAlphaVec offset (3)
ds_read_b32 v116, v113 offset:0                    // load scaleAlpha
v_add_lshl_u32 v111, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v111, v209, v111, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v118, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v209, v118, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v121, v118, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v4, s52
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v120, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v209, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v123, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v123, v209, v123, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v126, v123, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v124, v4, s52
v_lshlrev_b32 v124, 0x2, v124                      // Bias address scaled by BPE
v_add_u32 v125, 1024, v124                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v123, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v123, v209, v123, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v128, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v209, v128, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v131, v128, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v129, v4, s52
v_lshlrev_b32 v129, 0x2, v129                      // Bias address scaled by BPE
v_add_u32 v130, 1024, v129                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v128, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v209, v128, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v133, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v133, v209, v133, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v136, v133, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v134, v4, s52
v_lshlrev_b32 v134, 0x2, v134                      // Bias address scaled by BPE
v_add_u32 v135, 1024, v134                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v133, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v133, v209, v133, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v138, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v209, v138, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v142, v138, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v140, v4, s52
v_lshlrev_b32 v140, 0x2, v140                      // Bias address scaled by BPE
v_add_u32 v141, 1024, v140                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v138, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v209, v138, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v144, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v144, v209, v144, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v147, v144, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v145, v4, s52
v_lshlrev_b32 v145, 0x2, v145                      // Bias address scaled by BPE
v_add_u32 v146, 1024, v145                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v144, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v144, v209, v144, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v149, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v209, v149, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v152, v149, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v150, v4, s52
v_lshlrev_b32 v150, 0x2, v150                      // Bias address scaled by BPE
v_add_u32 v151, 1024, v150                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v149, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v209, v149, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v154, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v154, v209, v154, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v157, v154, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v155, v4, s52
v_lshlrev_b32 v155, 0x2, v155                      // Bias address scaled by BPE
v_add_u32 v156, 1024, v155                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v154, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v154, v209, v154, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v159, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v209, v159, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v162, v159, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v160, v4, s52
v_lshlrev_b32 v160, 0x2, v160                      // Bias address scaled by BPE
v_add_u32 v161, 1024, v160                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v159, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v209, v159, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v164, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v164, v209, v164, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v167, v164, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v165, v4, s52
v_lshlrev_b32 v165, 0x2, v165                      // Bias address scaled by BPE
v_add_u32 v166, 1024, v165                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v164, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v164, v209, v164, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v169, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v169, v209, v169, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v172, v169, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v170, v4, s52
v_lshlrev_b32 v170, 0x2, v170                      // Bias address scaled by BPE
v_add_u32 v171, 1024, v170                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v169, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v169, v209, v169, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,5,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v174, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v174, v209, v174, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v177, v174, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v175, v4, s52
v_lshlrev_b32 v175, 0x2, v175                      // Bias address scaled by BPE
v_add_u32 v176, 1024, v175                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v174, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v174, v209, v174, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v179, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v209, v179, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v182, v179, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v180, v0, s52
v_lshlrev_b32 v180, 0x2, v180                      // Bias address scaled by BPE
v_add_u32 v181, 1024, v180                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v179, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v209, v179, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v184, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v184, v209, v184, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v187, v184, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v185, v4, s52
v_lshlrev_b32 v185, 0x2, v185                      // Bias address scaled by BPE
v_add_u32 v186, 1024, v185                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v184, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v184, v209, v184, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v189, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v189, v209, v189, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v192, v189, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v190, v4, s52
v_lshlrev_b32 v190, 0x2, v190                      // Bias address scaled by BPE
v_add_u32 v191, 1024, v190                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v189, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v189, v209, v189, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v194, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v194, v209, v194, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v197, v194, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v195, v4, s52
v_lshlrev_b32 v195, 0x2, v195                      // Bias address scaled by BPE
v_add_u32 v196, 1024, v195                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v194, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v194, v209, v194, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v199, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v199, v209, v199, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v202, v199, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v200, v4, s52
v_lshlrev_b32 v200, 0x2, v200                      // Bias address scaled by BPE
v_add_u32 v201, 1024, v200                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v199, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v199, v209, v199, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v204, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v204, v209, v204, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v207, v204, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v205, v4, s52
v_lshlrev_b32 v205, 0x2, v205                      // Bias address scaled by BPE
v_add_u32 v206, 1024, v205                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v204, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v204, v209, v204, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+12], acc68          // copy acc to vreg[68]
v_accvgpr_read_b32 v[vgprValuC+19], acc69          // copy acc to vreg[69]
v_accvgpr_read_b32 v[vgprValuC+26], acc70          // copy acc to vreg[70]
v_accvgpr_read_b32 v[vgprValuC+33], acc71          // copy acc to vreg[71]
v_accvgpr_read_b32 v[vgprValuC+40], acc72          // copy acc to vreg[72]
v_accvgpr_read_b32 v[vgprValuC+47], acc73          // copy acc to vreg[73]
v_accvgpr_read_b32 v[vgprValuC+54], acc74          // copy acc to vreg[74]
v_accvgpr_read_b32 v[vgprValuC+61], acc75          // copy acc to vreg[75]
v_accvgpr_read_b32 v[vgprValuC+68], acc76          // copy acc to vreg[76]
v_accvgpr_read_b32 v[vgprValuC+75], acc77          // copy acc to vreg[77]
v_accvgpr_read_b32 v[vgprValuC+82], acc78          // copy acc to vreg[78]
v_accvgpr_read_b32 v[vgprValuC+89], acc79          // copy acc to vreg[79]
v_accvgpr_read_b32 v[vgprValuC+96], acc80          // copy acc to vreg[80]
v_accvgpr_read_b32 v[vgprValuC+103], acc81         // copy acc to vreg[81]
v_accvgpr_read_b32 v[vgprValuC+110], acc82         // copy acc to vreg[82]
v_accvgpr_read_b32 v[vgprValuC+117], acc83         // copy acc to vreg[83]
v_accvgpr_read_b32 v[vgprValuC+122], acc84         // copy acc to vreg[84]
v_accvgpr_read_b32 v[vgprValuC+127], acc85         // copy acc to vreg[85]
v_accvgpr_read_b32 v[vgprValuC+132], acc86         // copy acc to vreg[86]
v_accvgpr_read_b32 v[vgprValuC+137], acc87         // copy acc to vreg[87]
v_accvgpr_read_b32 v[vgprValuC+143], acc88         // copy acc to vreg[88]
v_accvgpr_read_b32 v[vgprValuC+148], acc89         // copy acc to vreg[89]
v_accvgpr_read_b32 v[vgprValuC+153], acc90         // copy acc to vreg[90]
v_accvgpr_read_b32 v[vgprValuC+158], acc91         // copy acc to vreg[91]
v_accvgpr_read_b32 v[vgprValuC+163], acc92         // copy acc to vreg[92]
v_accvgpr_read_b32 v[vgprValuC+168], acc93         // copy acc to vreg[93]
v_accvgpr_read_b32 v[vgprValuC+173], acc94         // copy acc to vreg[94]
v_accvgpr_read_b32 v[vgprValuC+178], acc95         // copy acc to vreg[95]
v_accvgpr_read_b32 v[vgprValuC+183], acc96         // copy acc to vreg[96]
v_accvgpr_read_b32 v[vgprValuC+188], acc97         // copy acc to vreg[97]
v_accvgpr_read_b32 v[vgprValuC+193], acc98         // copy acc to vreg[98]
v_accvgpr_read_b32 v[vgprValuC+198], acc99         // copy acc to vreg[99]
v_accvgpr_read_b32 v[vgprValuC+203], acc100        // copy acc to vreg[100]
v_accvgpr_read_b32 v[vgprValuC+208], acc101        // copy acc to vreg[101]

/* rC *= alpha batchElements=[(0, 1, 4, 0), (0, 1, 4, 1), (0, 1, 4, 2), (0, 1, 4, 3), (0, 2, 4, 0), (0, 2, 4, 1), (0, 2, 4, 2), (0, 2, 4, 3), (0, 3, 4, 0), (0, 3, 4, 1), (0, 3, 4, 2), (0, 3, 4, 3), (0, 0, 5, 0), (0, 0, 5, 1), (0, 0, 5, 2), (0, 0, 5, 3), (0, 1, 5, 0), (0, 1, 5, 1), (0, 1, 5, 2), (0, 1, 5, 3), (0, 2, 5, 0), (0, 2, 5, 1), (0, 2, 5, 2), (0, 2, 5, 3), (0, 3, 5, 0), (0, 3, 5, 1), (0, 3, 5, 2), (0, 3, 5, 3), (0, 0, 6, 0), (0, 0, 6, 1), (0, 0, 6, 2), (0, 0, 6, 3), (0, 1, 6, 0), (0, 1, 6, 1)] */
v_mul_f32 v[vgprValuC+12], s[sgprAlpha], v[vgprValuC+12] // *= alpha
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+122], s[sgprAlpha], v[vgprValuC+122] // *= alpha
v_mul_f32 v[vgprValuC+127], s[sgprAlpha], v[vgprValuC+127] // *= alpha
v_mul_f32 v[vgprValuC+132], s[sgprAlpha], v[vgprValuC+132] // *= alpha
v_mul_f32 v[vgprValuC+137], s[sgprAlpha], v[vgprValuC+137] // *= alpha
v_mul_f32 v[vgprValuC+143], s[sgprAlpha], v[vgprValuC+143] // *= alpha
v_mul_f32 v[vgprValuC+148], s[sgprAlpha], v[vgprValuC+148] // *= alpha
v_mul_f32 v[vgprValuC+153], s[sgprAlpha], v[vgprValuC+153] // *= alpha
v_mul_f32 v[vgprValuC+158], s[sgprAlpha], v[vgprValuC+158] // *= alpha
v_mul_f32 v[vgprValuC+163], s[sgprAlpha], v[vgprValuC+163] // *= alpha
v_mul_f32 v[vgprValuC+168], s[sgprAlpha], v[vgprValuC+168] // *= alpha
v_mul_f32 v[vgprValuC+173], s[sgprAlpha], v[vgprValuC+173] // *= alpha
v_mul_f32 v[vgprValuC+178], s[sgprAlpha], v[vgprValuC+178] // *= alpha
v_mul_f32 v[vgprValuC+183], s[sgprAlpha], v[vgprValuC+183] // *= alpha
v_mul_f32 v[vgprValuC+188], s[sgprAlpha], v[vgprValuC+188] // *= alpha
v_mul_f32 v[vgprValuC+193], s[sgprAlpha], v[vgprValuC+193] // *= alpha
v_mul_f32 v[vgprValuC+198], s[sgprAlpha], v[vgprValuC+198] // *= alpha
v_mul_f32 v[vgprValuC+203], s[sgprAlpha], v[vgprValuC+203] // *= alpha
v_mul_f32 v[vgprValuC+208], s[sgprAlpha], v[vgprValuC+208] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+12], v11, v[vgprValuC+12]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+12], s[sgprBeta], v9, v[vgprValuC+12] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+12], v10, v[vgprValuC+12]    // C += bias
v_cvt_f16_f32 v12, v[vgprValuC+12]                 // convert C to fp16
buffer_store_short v12, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+19], s[sgprBeta], v16, v[vgprValuC+19] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+19], v17, v[vgprValuC+19]    // C += bias
v_cvt_f16_f32 v19, v[vgprValuC+19]                 // convert C to fp16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v23, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+26], v24, v[vgprValuC+26]    // C += bias
v_cvt_f16_f32 v26, v[vgprValuC+26]                 // convert C to fp16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v30, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+33], v31, v[vgprValuC+33]    // C += bias
v_cvt_f16_f32 v33, v[vgprValuC+33]                 // convert C to fp16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v37, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+40], v38, v[vgprValuC+40]    // C += bias
v_cvt_f16_f32 v40, v[vgprValuC+40]                 // convert C to fp16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v44, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+54], v53, v[vgprValuC+54]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v51, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+54], v52, v[vgprValuC+54]    // C += bias
v_cvt_f16_f32 v54, v[vgprValuC+54]                 // convert C to fp16
buffer_store_short v54, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+61], v60, v[vgprValuC+61]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v58, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+61], v59, v[vgprValuC+61]    // C += bias
v_cvt_f16_f32 v61, v[vgprValuC+61]                 // convert C to fp16
buffer_store_short v61, v55, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+68], v67, v[vgprValuC+68]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+68], s[sgprBeta], v65, v[vgprValuC+68] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+68], v66, v[vgprValuC+68]    // C += bias
v_cvt_f16_f32 v68, v[vgprValuC+68]                 // convert C to fp16
buffer_store_short v68, v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+75], v74, v[vgprValuC+75]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v72, v[vgprValuC+75] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+75], v73, v[vgprValuC+75]    // C += bias
v_cvt_f16_f32 v75, v[vgprValuC+75]                 // convert C to fp16
buffer_store_short v75, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+82], v81, v[vgprValuC+82]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+82], s[sgprBeta], v79, v[vgprValuC+82] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+82], v80, v[vgprValuC+82]    // C += bias
v_cvt_f16_f32 v82, v[vgprValuC+82]                 // convert C to fp16
buffer_store_short v82, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+89], v88, v[vgprValuC+89]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+89], s[sgprBeta], v86, v[vgprValuC+89] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+89], v87, v[vgprValuC+89]    // C += bias
v_cvt_f16_f32 v89, v[vgprValuC+89]                 // convert C to fp16
buffer_store_short v89, v83, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+96], v95, v[vgprValuC+96]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+96], s[sgprBeta], v93, v[vgprValuC+96] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+96], v94, v[vgprValuC+96]    // C += bias
v_cvt_f16_f32 v96, v[vgprValuC+96]                 // convert C to fp16
buffer_store_short v96, v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+103], v102, v[vgprValuC+103] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+103], s[sgprBeta], v100, v[vgprValuC+103] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+103], v101, v[vgprValuC+103] // C += bias
v_cvt_f16_f32 v103, v[vgprValuC+103]               // convert C to fp16
buffer_store_short v103, v97, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+110], v109, v[vgprValuC+110] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+110], s[sgprBeta], v107, v[vgprValuC+110] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+110], v108, v[vgprValuC+110] // C += bias
v_cvt_f16_f32 v110, v[vgprValuC+110]               // convert C to fp16
buffer_store_short v110, v104, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+117], v116, v[vgprValuC+117] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v114, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+117], v115, v[vgprValuC+117] // C += bias
v_cvt_f16_f32 v117, v[vgprValuC+117]               // convert C to fp16
buffer_store_short v117, v111, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+122], v11, v[vgprValuC+122]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+122], s[sgprBeta], v121, v[vgprValuC+122] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+122], v10, v[vgprValuC+122]  // C += bias
v_cvt_f16_f32 v122, v[vgprValuC+122]               // convert C to fp16
buffer_store_short v122, v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+127], v18, v[vgprValuC+127]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+127], s[sgprBeta], v126, v[vgprValuC+127] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+127], v17, v[vgprValuC+127]  // C += bias
v_cvt_f16_f32 v127, v[vgprValuC+127]               // convert C to fp16
buffer_store_short v127, v123, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+132], v25, v[vgprValuC+132]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+132], s[sgprBeta], v131, v[vgprValuC+132] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+132], v24, v[vgprValuC+132]  // C += bias
v_cvt_f16_f32 v132, v[vgprValuC+132]               // convert C to fp16
buffer_store_short v132, v128, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+137], v32, v[vgprValuC+137]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+137], s[sgprBeta], v136, v[vgprValuC+137] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+137], v31, v[vgprValuC+137]  // C += bias
v_cvt_f16_f32 v137, v[vgprValuC+137]               // convert C to fp16
buffer_store_short v137, v133, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+143], v39, v[vgprValuC+143]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+143], s[sgprBeta], v142, v[vgprValuC+143] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+143], v38, v[vgprValuC+143]  // C += bias
v_cvt_f16_f32 v143, v[vgprValuC+143]               // convert C to fp16
buffer_store_short v143, v138, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+148], v46, v[vgprValuC+148]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+148], s[sgprBeta], v147, v[vgprValuC+148] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+148], v45, v[vgprValuC+148]  // C += bias
v_cvt_f16_f32 v148, v[vgprValuC+148]               // convert C to fp16
buffer_store_short v148, v144, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+153], v53, v[vgprValuC+153]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+153], s[sgprBeta], v152, v[vgprValuC+153] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+153], v52, v[vgprValuC+153]  // C += bias
v_cvt_f16_f32 v153, v[vgprValuC+153]               // convert C to fp16
buffer_store_short v153, v149, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+158], v60, v[vgprValuC+158]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+158], s[sgprBeta], v157, v[vgprValuC+158] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+158], v59, v[vgprValuC+158]  // C += bias
v_cvt_f16_f32 v158, v[vgprValuC+158]               // convert C to fp16
buffer_store_short v158, v154, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+163], v67, v[vgprValuC+163]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+163], s[sgprBeta], v162, v[vgprValuC+163] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+163], v66, v[vgprValuC+163]  // C += bias
v_cvt_f16_f32 v163, v[vgprValuC+163]               // convert C to fp16
buffer_store_short v163, v159, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+168], v74, v[vgprValuC+168]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+168], s[sgprBeta], v167, v[vgprValuC+168] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+168], v73, v[vgprValuC+168]  // C += bias
v_cvt_f16_f32 v168, v[vgprValuC+168]               // convert C to fp16
buffer_store_short v168, v164, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+173], v81, v[vgprValuC+173]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+173], s[sgprBeta], v172, v[vgprValuC+173] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+173], v80, v[vgprValuC+173]  // C += bias
v_cvt_f16_f32 v173, v[vgprValuC+173]               // convert C to fp16
buffer_store_short v173, v169, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+178], v88, v[vgprValuC+178]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+178], s[sgprBeta], v177, v[vgprValuC+178] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+178], v87, v[vgprValuC+178]  // C += bias
v_cvt_f16_f32 v178, v[vgprValuC+178]               // convert C to fp16
buffer_store_short v178, v174, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+183], v95, v[vgprValuC+183]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+183], s[sgprBeta], v182, v[vgprValuC+183] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+183], v94, v[vgprValuC+183]  // C += bias
v_cvt_f16_f32 v183, v[vgprValuC+183]               // convert C to fp16
buffer_store_short v183, v179, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+188], v102, v[vgprValuC+188] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+188], s[sgprBeta], v187, v[vgprValuC+188] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+188], v101, v[vgprValuC+188] // C += bias
v_cvt_f16_f32 v188, v[vgprValuC+188]               // convert C to fp16
buffer_store_short v188, v184, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+193], v109, v[vgprValuC+193] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+193], s[sgprBeta], v192, v[vgprValuC+193] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+193], v108, v[vgprValuC+193] // C += bias
v_cvt_f16_f32 v193, v[vgprValuC+193]               // convert C to fp16
buffer_store_short v193, v189, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+198], v116, v[vgprValuC+198] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+198], s[sgprBeta], v197, v[vgprValuC+198] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+198], v115, v[vgprValuC+198] // C += bias
v_cvt_f16_f32 v198, v[vgprValuC+198]               // convert C to fp16
buffer_store_short v198, v194, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+203], v11, v[vgprValuC+203]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+203], s[sgprBeta], v202, v[vgprValuC+203] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+203], v10, v[vgprValuC+203]  // C += bias
v_cvt_f16_f32 v203, v[vgprValuC+203]               // convert C to fp16
buffer_store_short v203, v199, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+208], v18, v[vgprValuC+208]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+208], s[sgprBeta], v207, v[vgprValuC+208] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+208], v17, v[vgprValuC+208]  // C += bias
v_cvt_f16_f32 v208, v[vgprValuC+208]               // convert C to fp16
buffer_store_short v208, v204, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #3 (d1,d0,vc1,vc0) = */
/*    (0,1,6,2:vw1); (0,1,6,3:vw1); (0,2,6,0:vw1); (0,2,6,1:vw1); (0,2,6,2:vw1); (0,2,6,3:vw1); (0,3,6,0:vw1); (0,3,6,1:vw1); (0,3,6,2:vw1); (0,3,6,3:vw1); (0,0,7,0:vw1); (0,0,7,1:vw1); (0,0,7,2:vw1); (0,0,7,3:vw1); (0,1,7,0:vw1); (0,1,7,1:vw1); (0,1,7,2:vw1); (0,1,7,3:vw1); (0,2,7,0:vw1); (0,2,7,1:vw1); (0,2,7,2:vw1); (0,2,7,3:vw1); (0,3,7,0:vw1); (0,3,7,1:vw1); (0,3,7,2:vw1); (0,3,7,3:vw1); (1,0,0,0:vw1); (1,0,0,1:vw1); (1,0,0,2:vw1); (1,0,0,3:vw1); (1,1,0,0:vw1); (1,1,0,1:vw1); (1,1,0,2:vw1); (1,1,0,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v209, BufferOOB
/* (d1,vc1,d0,vc0)=(0,6,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v6, v2, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v209, v6, s[56:57]               // LDC clip if OOB. offset
buffer_load_short_d16 v9, v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v4, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b32 v10, v7 offset:0                       // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v11, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v209, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v13, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v209, v13, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v4, s52
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
ds_read_b32 v17, v14 offset:0                      // load Bias
v_add_u32 v15, 1024, v14                           // add ScaleAlphaVec offset (3)
ds_read_b32 v18, v15 offset:0                      // load scaleAlpha
v_add_lshl_u32 v13, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v209, v13, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v209, v20, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s52
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
ds_read_b32 v24, v21 offset:0                      // load Bias
v_add_u32 v22, 1024, v21                           // add ScaleAlphaVec offset (3)
ds_read_b32 v25, v22 offset:0                      // load scaleAlpha
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v209, v20, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v209, v27, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s52
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
ds_read_b32 v31, v28 offset:0                      // load Bias
v_add_u32 v29, 1024, v28                           // add ScaleAlphaVec offset (3)
ds_read_b32 v32, v29 offset:0                      // load scaleAlpha
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v209, v27, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v209, v34, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s52
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
ds_read_b32 v38, v35 offset:0                      // load Bias
v_add_u32 v36, 1024, v35                           // add ScaleAlphaVec offset (3)
ds_read_b32 v39, v36 offset:0                      // load scaleAlpha
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v209, v34, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v41, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v209, v41, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s52
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
ds_read_b32 v45, v42 offset:0                      // load Bias
v_add_u32 v43, 1024, v42                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v43 offset:0                      // load scaleAlpha
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v209, v41, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v48, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v209, v48, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v51, v48, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v4, s52
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
ds_read_b32 v52, v49 offset:0                      // load Bias
v_add_u32 v50, 1024, v49                           // add ScaleAlphaVec offset (3)
ds_read_b32 v53, v50 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v209, v48, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v55, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v209, v55, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v58, v55, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v56, v4, s52
v_lshlrev_b32 v56, 0x2, v56                        // Bias address scaled by BPE
ds_read_b32 v59, v56 offset:0                      // load Bias
v_add_u32 v57, 1024, v56                           // add ScaleAlphaVec offset (3)
ds_read_b32 v60, v57 offset:0                      // load scaleAlpha
v_add_lshl_u32 v55, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v209, v55, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v62, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v209, v62, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v65, v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v4, s52
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
ds_read_b32 v66, v63 offset:0                      // load Bias
v_add_u32 v64, 1024, v63                           // add ScaleAlphaVec offset (3)
ds_read_b32 v67, v64 offset:0                      // load scaleAlpha
v_add_lshl_u32 v62, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v209, v62, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,6,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v69, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v209, v69, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v72, v69, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v4, s52
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
ds_read_b32 v73, v70 offset:0                      // load Bias
v_add_u32 v71, 1024, v70                           // add ScaleAlphaVec offset (3)
ds_read_b32 v74, v71 offset:0                      // load scaleAlpha
v_add_lshl_u32 v69, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v209, v69, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v76, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v209, v76, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v0, s52
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
ds_read_b32 v80, v77 offset:0                      // load Bias
v_add_u32 v78, 1024, v77                           // add ScaleAlphaVec offset (3)
ds_read_b32 v81, v78 offset:0                      // load scaleAlpha
v_add_lshl_u32 v76, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v209, v76, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v83, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v209, v83, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v86, v83, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v84, v4, s52
v_lshlrev_b32 v84, 0x2, v84                        // Bias address scaled by BPE
ds_read_b32 v87, v84 offset:0                      // load Bias
v_add_u32 v85, 1024, v84                           // add ScaleAlphaVec offset (3)
ds_read_b32 v88, v85 offset:0                      // load scaleAlpha
v_add_lshl_u32 v83, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v209, v83, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v90, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v209, v90, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v93, v90, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v4, s52
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
ds_read_b32 v94, v91 offset:0                      // load Bias
v_add_u32 v92, 1024, v91                           // add ScaleAlphaVec offset (3)
ds_read_b32 v95, v92 offset:0                      // load scaleAlpha
v_add_lshl_u32 v90, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v209, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v97, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v209, v97, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v100, v97, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v98, v4, s52
v_lshlrev_b32 v98, 0x2, v98                        // Bias address scaled by BPE
ds_read_b32 v101, v98 offset:0                     // load Bias
v_add_u32 v99, 1024, v98                           // add ScaleAlphaVec offset (3)
ds_read_b32 v102, v99 offset:0                     // load scaleAlpha
v_add_lshl_u32 v97, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v209, v97, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v104, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v104, v209, v104, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v107, v104, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v105, v4, s52
v_lshlrev_b32 v105, 0x2, v105                      // Bias address scaled by BPE
ds_read_b32 v108, v105 offset:0                    // load Bias
v_add_u32 v106, 1024, v105                         // add ScaleAlphaVec offset (3)
ds_read_b32 v109, v106 offset:0                    // load scaleAlpha
v_add_lshl_u32 v104, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v104, v209, v104, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v111, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v111, v209, v111, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v114, v111, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v112, v4, s52
v_lshlrev_b32 v112, 0x2, v112                      // Bias address scaled by BPE
ds_read_b32 v115, v112 offset:0                    // load Bias
v_add_u32 v113, 1024, v112                         // add ScaleAlphaVec offset (3)
ds_read_b32 v116, v113 offset:0                    // load scaleAlpha
v_add_lshl_u32 v111, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v111, v209, v111, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v118, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v209, v118, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v121, v118, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v4, s52
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v120, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v209, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v123, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v123, v209, v123, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v126, v123, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v124, v4, s52
v_lshlrev_b32 v124, 0x2, v124                      // Bias address scaled by BPE
v_add_u32 v125, 1024, v124                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v123, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v123, v209, v123, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v128, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v209, v128, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v131, v128, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v129, v4, s52
v_lshlrev_b32 v129, 0x2, v129                      // Bias address scaled by BPE
v_add_u32 v130, 1024, v129                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v128, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v209, v128, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v133, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v133, v209, v133, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v136, v133, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v134, v4, s52
v_lshlrev_b32 v134, 0x2, v134                      // Bias address scaled by BPE
v_add_u32 v135, 1024, v134                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v133, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v133, v209, v133, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v138, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v209, v138, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v142, v138, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v140, v4, s52
v_lshlrev_b32 v140, 0x2, v140                      // Bias address scaled by BPE
v_add_u32 v141, 1024, v140                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v138, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v209, v138, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v144, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v144, v209, v144, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v147, v144, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v145, v4, s52
v_lshlrev_b32 v145, 0x2, v145                      // Bias address scaled by BPE
v_add_u32 v146, 1024, v145                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v144, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v144, v209, v144, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v149, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v209, v149, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v152, v149, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v150, v4, s52
v_lshlrev_b32 v150, 0x2, v150                      // Bias address scaled by BPE
v_add_u32 v151, 1024, v150                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v149, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v209, v149, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v154, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v154, v209, v154, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v157, v154, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v155, v4, s52
v_lshlrev_b32 v155, 0x2, v155                      // Bias address scaled by BPE
v_add_u32 v156, 1024, v155                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v154, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v154, v209, v154, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v159, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v209, v159, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v162, v159, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v160, v4, s52
v_lshlrev_b32 v160, 0x2, v160                      // Bias address scaled by BPE
v_add_u32 v161, 1024, v160                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v159, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v209, v159, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,7,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v164, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v164, v209, v164, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v167, v164, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v165, v4, s52
v_lshlrev_b32 v165, 0x2, v165                      // Bias address scaled by BPE
v_add_u32 v166, 1024, v165                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v164, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v164, v209, v164, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,0) */
s_mov_b32 s52, 121                                 // rowInc d1=0 vc1=0
v_add_co_u32 v1, vcc, v1, s52                      // coord1.2: coord1 += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s52, s[sgprStrideC1J], 121               // scale stride
v_add_i32 v2, v2, s52                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s52, s[sgprStrideD1J], 121               // scale stride
v_add_i32 v3, v3, s52                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v169, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v169, v209, v169, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v172, v169, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v170, v0, s52
v_lshlrev_b32 v170, 0x2, v170                      // Bias address scaled by BPE
v_add_u32 v171, 1024, v170                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v169, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v169, v209, v169, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v174, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v174, v209, v174, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v177, v174, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v175, v4, s52
v_lshlrev_b32 v175, 0x2, v175                      // Bias address scaled by BPE
v_add_u32 v176, 1024, v175                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v174, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v174, v209, v174, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v179, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v209, v179, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v182, v179, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v180, v4, s52
v_lshlrev_b32 v180, 0x2, v180                      // Bias address scaled by BPE
v_add_u32 v181, 1024, v180                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v179, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v209, v179, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v184, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v184, v209, v184, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v187, v184, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v185, v4, s52
v_lshlrev_b32 v185, 0x2, v185                      // Bias address scaled by BPE
v_add_u32 v186, 1024, v185                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v184, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v184, v209, v184, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v189, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v189, v209, v189, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v192, v189, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v190, v4, s52
v_lshlrev_b32 v190, 0x2, v190                      // Bias address scaled by BPE
v_add_u32 v191, 1024, v190                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v189, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v189, v209, v189, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v194, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v194, v209, v194, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v197, v194, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v195, v4, s52
v_lshlrev_b32 v195, 0x2, v195                      // Bias address scaled by BPE
v_add_u32 v196, 1024, v195                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v194, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v194, v209, v194, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v199, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v199, v209, v199, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v202, v199, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v200, v4, s52
v_lshlrev_b32 v200, 0x2, v200                      // Bias address scaled by BPE
v_add_u32 v201, 1024, v200                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v199, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v199, v209, v199, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v204, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v204, v209, v204, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v207, v204, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v205, v4, s52
v_lshlrev_b32 v205, 0x2, v205                      // Bias address scaled by BPE
v_add_u32 v206, 1024, v205                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v204, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v204, v209, v204, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+12], acc102         // copy acc to vreg[102]
v_accvgpr_read_b32 v[vgprValuC+19], acc103         // copy acc to vreg[103]
v_accvgpr_read_b32 v[vgprValuC+26], acc104         // copy acc to vreg[104]
v_accvgpr_read_b32 v[vgprValuC+33], acc105         // copy acc to vreg[105]
v_accvgpr_read_b32 v[vgprValuC+40], acc106         // copy acc to vreg[106]
v_accvgpr_read_b32 v[vgprValuC+47], acc107         // copy acc to vreg[107]
v_accvgpr_read_b32 v[vgprValuC+54], acc108         // copy acc to vreg[108]
v_accvgpr_read_b32 v[vgprValuC+61], acc109         // copy acc to vreg[109]
v_accvgpr_read_b32 v[vgprValuC+68], acc110         // copy acc to vreg[110]
v_accvgpr_read_b32 v[vgprValuC+75], acc111         // copy acc to vreg[111]
v_accvgpr_read_b32 v[vgprValuC+82], acc112         // copy acc to vreg[112]
v_accvgpr_read_b32 v[vgprValuC+89], acc113         // copy acc to vreg[113]
v_accvgpr_read_b32 v[vgprValuC+96], acc114         // copy acc to vreg[114]
v_accvgpr_read_b32 v[vgprValuC+103], acc115        // copy acc to vreg[115]
v_accvgpr_read_b32 v[vgprValuC+110], acc116        // copy acc to vreg[116]
v_accvgpr_read_b32 v[vgprValuC+117], acc117        // copy acc to vreg[117]
v_accvgpr_read_b32 v[vgprValuC+122], acc118        // copy acc to vreg[118]
v_accvgpr_read_b32 v[vgprValuC+127], acc119        // copy acc to vreg[119]
v_accvgpr_read_b32 v[vgprValuC+132], acc120        // copy acc to vreg[120]
v_accvgpr_read_b32 v[vgprValuC+137], acc121        // copy acc to vreg[121]
v_accvgpr_read_b32 v[vgprValuC+143], acc122        // copy acc to vreg[122]
v_accvgpr_read_b32 v[vgprValuC+148], acc123        // copy acc to vreg[123]
v_accvgpr_read_b32 v[vgprValuC+153], acc124        // copy acc to vreg[124]
v_accvgpr_read_b32 v[vgprValuC+158], acc125        // copy acc to vreg[125]
v_accvgpr_read_b32 v[vgprValuC+163], acc126        // copy acc to vreg[126]
v_accvgpr_read_b32 v[vgprValuC+168], acc127        // copy acc to vreg[127]
v_accvgpr_read_b32 v[vgprValuC+173], acc128        // copy acc to vreg[128]
v_accvgpr_read_b32 v[vgprValuC+178], acc129        // copy acc to vreg[129]
v_accvgpr_read_b32 v[vgprValuC+183], acc130        // copy acc to vreg[130]
v_accvgpr_read_b32 v[vgprValuC+188], acc131        // copy acc to vreg[131]
v_accvgpr_read_b32 v[vgprValuC+193], acc132        // copy acc to vreg[132]
v_accvgpr_read_b32 v[vgprValuC+198], acc133        // copy acc to vreg[133]
v_accvgpr_read_b32 v[vgprValuC+203], acc134        // copy acc to vreg[134]
v_accvgpr_read_b32 v[vgprValuC+208], acc135        // copy acc to vreg[135]

/* rC *= alpha batchElements=[(0, 1, 6, 2), (0, 1, 6, 3), (0, 2, 6, 0), (0, 2, 6, 1), (0, 2, 6, 2), (0, 2, 6, 3), (0, 3, 6, 0), (0, 3, 6, 1), (0, 3, 6, 2), (0, 3, 6, 3), (0, 0, 7, 0), (0, 0, 7, 1), (0, 0, 7, 2), (0, 0, 7, 3), (0, 1, 7, 0), (0, 1, 7, 1), (0, 1, 7, 2), (0, 1, 7, 3), (0, 2, 7, 0), (0, 2, 7, 1), (0, 2, 7, 2), (0, 2, 7, 3), (0, 3, 7, 0), (0, 3, 7, 1), (0, 3, 7, 2), (0, 3, 7, 3), (1, 0, 0, 0), (1, 0, 0, 1), (1, 0, 0, 2), (1, 0, 0, 3), (1, 1, 0, 0), (1, 1, 0, 1), (1, 1, 0, 2), (1, 1, 0, 3)] */
v_mul_f32 v[vgprValuC+12], s[sgprAlpha], v[vgprValuC+12] // *= alpha
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+122], s[sgprAlpha], v[vgprValuC+122] // *= alpha
v_mul_f32 v[vgprValuC+127], s[sgprAlpha], v[vgprValuC+127] // *= alpha
v_mul_f32 v[vgprValuC+132], s[sgprAlpha], v[vgprValuC+132] // *= alpha
v_mul_f32 v[vgprValuC+137], s[sgprAlpha], v[vgprValuC+137] // *= alpha
v_mul_f32 v[vgprValuC+143], s[sgprAlpha], v[vgprValuC+143] // *= alpha
v_mul_f32 v[vgprValuC+148], s[sgprAlpha], v[vgprValuC+148] // *= alpha
v_mul_f32 v[vgprValuC+153], s[sgprAlpha], v[vgprValuC+153] // *= alpha
v_mul_f32 v[vgprValuC+158], s[sgprAlpha], v[vgprValuC+158] // *= alpha
v_mul_f32 v[vgprValuC+163], s[sgprAlpha], v[vgprValuC+163] // *= alpha
v_mul_f32 v[vgprValuC+168], s[sgprAlpha], v[vgprValuC+168] // *= alpha
v_mul_f32 v[vgprValuC+173], s[sgprAlpha], v[vgprValuC+173] // *= alpha
v_mul_f32 v[vgprValuC+178], s[sgprAlpha], v[vgprValuC+178] // *= alpha
v_mul_f32 v[vgprValuC+183], s[sgprAlpha], v[vgprValuC+183] // *= alpha
v_mul_f32 v[vgprValuC+188], s[sgprAlpha], v[vgprValuC+188] // *= alpha
v_mul_f32 v[vgprValuC+193], s[sgprAlpha], v[vgprValuC+193] // *= alpha
v_mul_f32 v[vgprValuC+198], s[sgprAlpha], v[vgprValuC+198] // *= alpha
v_mul_f32 v[vgprValuC+203], s[sgprAlpha], v[vgprValuC+203] // *= alpha
v_mul_f32 v[vgprValuC+208], s[sgprAlpha], v[vgprValuC+208] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+12], v11, v[vgprValuC+12]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+12], s[sgprBeta], v9, v[vgprValuC+12] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+12], v10, v[vgprValuC+12]    // C += bias
v_cvt_f16_f32 v12, v[vgprValuC+12]                 // convert C to fp16
buffer_store_short v12, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+19], s[sgprBeta], v16, v[vgprValuC+19] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+19], v17, v[vgprValuC+19]    // C += bias
v_cvt_f16_f32 v19, v[vgprValuC+19]                 // convert C to fp16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v23, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+26], v24, v[vgprValuC+26]    // C += bias
v_cvt_f16_f32 v26, v[vgprValuC+26]                 // convert C to fp16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v30, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+33], v31, v[vgprValuC+33]    // C += bias
v_cvt_f16_f32 v33, v[vgprValuC+33]                 // convert C to fp16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v37, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+40], v38, v[vgprValuC+40]    // C += bias
v_cvt_f16_f32 v40, v[vgprValuC+40]                 // convert C to fp16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v44, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+54], v53, v[vgprValuC+54]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v51, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+54], v52, v[vgprValuC+54]    // C += bias
v_cvt_f16_f32 v54, v[vgprValuC+54]                 // convert C to fp16
buffer_store_short v54, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+61], v60, v[vgprValuC+61]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v58, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+61], v59, v[vgprValuC+61]    // C += bias
v_cvt_f16_f32 v61, v[vgprValuC+61]                 // convert C to fp16
buffer_store_short v61, v55, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+68], v67, v[vgprValuC+68]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+68], s[sgprBeta], v65, v[vgprValuC+68] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+68], v66, v[vgprValuC+68]    // C += bias
v_cvt_f16_f32 v68, v[vgprValuC+68]                 // convert C to fp16
buffer_store_short v68, v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+75], v74, v[vgprValuC+75]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v72, v[vgprValuC+75] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+75], v73, v[vgprValuC+75]    // C += bias
v_cvt_f16_f32 v75, v[vgprValuC+75]                 // convert C to fp16
buffer_store_short v75, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+82], v81, v[vgprValuC+82]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+82], s[sgprBeta], v79, v[vgprValuC+82] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+82], v80, v[vgprValuC+82]    // C += bias
v_cvt_f16_f32 v82, v[vgprValuC+82]                 // convert C to fp16
buffer_store_short v82, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+89], v88, v[vgprValuC+89]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+89], s[sgprBeta], v86, v[vgprValuC+89] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+89], v87, v[vgprValuC+89]    // C += bias
v_cvt_f16_f32 v89, v[vgprValuC+89]                 // convert C to fp16
buffer_store_short v89, v83, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+96], v95, v[vgprValuC+96]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+96], s[sgprBeta], v93, v[vgprValuC+96] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+96], v94, v[vgprValuC+96]    // C += bias
v_cvt_f16_f32 v96, v[vgprValuC+96]                 // convert C to fp16
buffer_store_short v96, v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+103], v102, v[vgprValuC+103] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+103], s[sgprBeta], v100, v[vgprValuC+103] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+103], v101, v[vgprValuC+103] // C += bias
v_cvt_f16_f32 v103, v[vgprValuC+103]               // convert C to fp16
buffer_store_short v103, v97, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+110], v109, v[vgprValuC+110] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+110], s[sgprBeta], v107, v[vgprValuC+110] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+110], v108, v[vgprValuC+110] // C += bias
v_cvt_f16_f32 v110, v[vgprValuC+110]               // convert C to fp16
buffer_store_short v110, v104, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+117], v116, v[vgprValuC+117] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v114, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+117], v115, v[vgprValuC+117] // C += bias
v_cvt_f16_f32 v117, v[vgprValuC+117]               // convert C to fp16
buffer_store_short v117, v111, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+122], v11, v[vgprValuC+122]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+122], s[sgprBeta], v121, v[vgprValuC+122] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+122], v10, v[vgprValuC+122]  // C += bias
v_cvt_f16_f32 v122, v[vgprValuC+122]               // convert C to fp16
buffer_store_short v122, v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+127], v18, v[vgprValuC+127]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+127], s[sgprBeta], v126, v[vgprValuC+127] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+127], v17, v[vgprValuC+127]  // C += bias
v_cvt_f16_f32 v127, v[vgprValuC+127]               // convert C to fp16
buffer_store_short v127, v123, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+132], v25, v[vgprValuC+132]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+132], s[sgprBeta], v131, v[vgprValuC+132] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+132], v24, v[vgprValuC+132]  // C += bias
v_cvt_f16_f32 v132, v[vgprValuC+132]               // convert C to fp16
buffer_store_short v132, v128, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+137], v32, v[vgprValuC+137]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+137], s[sgprBeta], v136, v[vgprValuC+137] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+137], v31, v[vgprValuC+137]  // C += bias
v_cvt_f16_f32 v137, v[vgprValuC+137]               // convert C to fp16
buffer_store_short v137, v133, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+143], v39, v[vgprValuC+143]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+143], s[sgprBeta], v142, v[vgprValuC+143] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+143], v38, v[vgprValuC+143]  // C += bias
v_cvt_f16_f32 v143, v[vgprValuC+143]               // convert C to fp16
buffer_store_short v143, v138, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+148], v46, v[vgprValuC+148]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+148], s[sgprBeta], v147, v[vgprValuC+148] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+148], v45, v[vgprValuC+148]  // C += bias
v_cvt_f16_f32 v148, v[vgprValuC+148]               // convert C to fp16
buffer_store_short v148, v144, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+153], v53, v[vgprValuC+153]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+153], s[sgprBeta], v152, v[vgprValuC+153] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+153], v52, v[vgprValuC+153]  // C += bias
v_cvt_f16_f32 v153, v[vgprValuC+153]               // convert C to fp16
buffer_store_short v153, v149, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+158], v60, v[vgprValuC+158]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+158], s[sgprBeta], v157, v[vgprValuC+158] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+158], v59, v[vgprValuC+158]  // C += bias
v_cvt_f16_f32 v158, v[vgprValuC+158]               // convert C to fp16
buffer_store_short v158, v154, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+163], v67, v[vgprValuC+163]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+163], s[sgprBeta], v162, v[vgprValuC+163] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+163], v66, v[vgprValuC+163]  // C += bias
v_cvt_f16_f32 v163, v[vgprValuC+163]               // convert C to fp16
buffer_store_short v163, v159, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+168], v74, v[vgprValuC+168]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+168], s[sgprBeta], v167, v[vgprValuC+168] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+168], v73, v[vgprValuC+168]  // C += bias
v_cvt_f16_f32 v168, v[vgprValuC+168]               // convert C to fp16
buffer_store_short v168, v164, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+173], v81, v[vgprValuC+173]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+173], s[sgprBeta], v172, v[vgprValuC+173] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+173], v80, v[vgprValuC+173]  // C += bias
v_cvt_f16_f32 v173, v[vgprValuC+173]               // convert C to fp16
buffer_store_short v173, v169, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+178], v88, v[vgprValuC+178]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+178], s[sgprBeta], v177, v[vgprValuC+178] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+178], v87, v[vgprValuC+178]  // C += bias
v_cvt_f16_f32 v178, v[vgprValuC+178]               // convert C to fp16
buffer_store_short v178, v174, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+183], v95, v[vgprValuC+183]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+183], s[sgprBeta], v182, v[vgprValuC+183] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+183], v94, v[vgprValuC+183]  // C += bias
v_cvt_f16_f32 v183, v[vgprValuC+183]               // convert C to fp16
buffer_store_short v183, v179, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+188], v102, v[vgprValuC+188] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+188], s[sgprBeta], v187, v[vgprValuC+188] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+188], v101, v[vgprValuC+188] // C += bias
v_cvt_f16_f32 v188, v[vgprValuC+188]               // convert C to fp16
buffer_store_short v188, v184, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+193], v109, v[vgprValuC+193] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+193], s[sgprBeta], v192, v[vgprValuC+193] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+193], v108, v[vgprValuC+193] // C += bias
v_cvt_f16_f32 v193, v[vgprValuC+193]               // convert C to fp16
buffer_store_short v193, v189, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+198], v116, v[vgprValuC+198] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+198], s[sgprBeta], v197, v[vgprValuC+198] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+198], v115, v[vgprValuC+198] // C += bias
v_cvt_f16_f32 v198, v[vgprValuC+198]               // convert C to fp16
buffer_store_short v198, v194, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+203], v11, v[vgprValuC+203]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+203], s[sgprBeta], v202, v[vgprValuC+203] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+203], v10, v[vgprValuC+203]  // C += bias
v_cvt_f16_f32 v203, v[vgprValuC+203]               // convert C to fp16
buffer_store_short v203, v199, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+208], v18, v[vgprValuC+208]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+208], s[sgprBeta], v207, v[vgprValuC+208] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+208], v17, v[vgprValuC+208]  // C += bias
v_cvt_f16_f32 v208, v[vgprValuC+208]               // convert C to fp16
buffer_store_short v208, v204, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #4 (d1,d0,vc1,vc0) = */
/*    (1,2,0,0:vw1); (1,2,0,1:vw1); (1,2,0,2:vw1); (1,2,0,3:vw1); (1,3,0,0:vw1); (1,3,0,1:vw1); (1,3,0,2:vw1); (1,3,0,3:vw1); (1,0,1,0:vw1); (1,0,1,1:vw1); (1,0,1,2:vw1); (1,0,1,3:vw1); (1,1,1,0:vw1); (1,1,1,1:vw1); (1,1,1,2:vw1); (1,1,1,3:vw1); (1,2,1,0:vw1); (1,2,1,1:vw1); (1,2,1,2:vw1); (1,2,1,3:vw1); (1,3,1,0:vw1); (1,3,1,1:vw1); (1,3,1,2:vw1); (1,3,1,3:vw1); (1,0,2,0:vw1); (1,0,2,1:vw1); (1,0,2,2:vw1); (1,0,2,3:vw1); (1,1,2,0:vw1); (1,1,2,1:vw1); (1,1,2,2:vw1); (1,1,2,3:vw1); (1,2,2,0:vw1); (1,2,2,1:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v209, BufferOOB
/* (d1,vc1,d0,vc0)=(1,0,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v6, v2, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v209, v6, s[56:57]               // LDC clip if OOB. offset
buffer_load_short_d16 v9, v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v4, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b32 v10, v7 offset:0                       // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v11, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v209, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v13, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v209, v13, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v4, s52
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
ds_read_b32 v17, v14 offset:0                      // load Bias
v_add_u32 v15, 1024, v14                           // add ScaleAlphaVec offset (3)
ds_read_b32 v18, v15 offset:0                      // load scaleAlpha
v_add_lshl_u32 v13, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v209, v13, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v209, v20, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s52
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
ds_read_b32 v24, v21 offset:0                      // load Bias
v_add_u32 v22, 1024, v21                           // add ScaleAlphaVec offset (3)
ds_read_b32 v25, v22 offset:0                      // load scaleAlpha
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v209, v20, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v209, v27, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s52
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
ds_read_b32 v31, v28 offset:0                      // load Bias
v_add_u32 v29, 1024, v28                           // add ScaleAlphaVec offset (3)
ds_read_b32 v32, v29 offset:0                      // load scaleAlpha
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v209, v27, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v209, v34, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s52
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
ds_read_b32 v38, v35 offset:0                      // load Bias
v_add_u32 v36, 1024, v35                           // add ScaleAlphaVec offset (3)
ds_read_b32 v39, v36 offset:0                      // load scaleAlpha
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v209, v34, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v41, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v209, v41, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s52
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
ds_read_b32 v45, v42 offset:0                      // load Bias
v_add_u32 v43, 1024, v42                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v43 offset:0                      // load scaleAlpha
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v209, v41, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v48, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v209, v48, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v51, v48, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v4, s52
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
ds_read_b32 v52, v49 offset:0                      // load Bias
v_add_u32 v50, 1024, v49                           // add ScaleAlphaVec offset (3)
ds_read_b32 v53, v50 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v209, v48, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v55, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v209, v55, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v58, v55, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v56, v4, s52
v_lshlrev_b32 v56, 0x2, v56                        // Bias address scaled by BPE
ds_read_b32 v59, v56 offset:0                      // load Bias
v_add_u32 v57, 1024, v56                           // add ScaleAlphaVec offset (3)
ds_read_b32 v60, v57 offset:0                      // load scaleAlpha
v_add_lshl_u32 v55, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v209, v55, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v62, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v209, v62, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v65, v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v0, s52
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
ds_read_b32 v66, v63 offset:0                      // load Bias
v_add_u32 v64, 1024, v63                           // add ScaleAlphaVec offset (3)
ds_read_b32 v67, v64 offset:0                      // load scaleAlpha
v_add_lshl_u32 v62, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v209, v62, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v69, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v209, v69, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v72, v69, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v4, s52
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
ds_read_b32 v73, v70 offset:0                      // load Bias
v_add_u32 v71, 1024, v70                           // add ScaleAlphaVec offset (3)
ds_read_b32 v74, v71 offset:0                      // load scaleAlpha
v_add_lshl_u32 v69, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v209, v69, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v209, v76, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s52
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
ds_read_b32 v80, v77 offset:0                      // load Bias
v_add_u32 v78, 1024, v77                           // add ScaleAlphaVec offset (3)
ds_read_b32 v81, v78 offset:0                      // load scaleAlpha
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v209, v76, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v83, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v209, v83, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v86, v83, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v84, v4, s52
v_lshlrev_b32 v84, 0x2, v84                        // Bias address scaled by BPE
ds_read_b32 v87, v84 offset:0                      // load Bias
v_add_u32 v85, 1024, v84                           // add ScaleAlphaVec offset (3)
ds_read_b32 v88, v85 offset:0                      // load scaleAlpha
v_add_lshl_u32 v83, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v209, v83, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v90, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v209, v90, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v93, v90, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v4, s52
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
ds_read_b32 v94, v91 offset:0                      // load Bias
v_add_u32 v92, 1024, v91                           // add ScaleAlphaVec offset (3)
ds_read_b32 v95, v92 offset:0                      // load scaleAlpha
v_add_lshl_u32 v90, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v209, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v97, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v209, v97, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v100, v97, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v98, v4, s52
v_lshlrev_b32 v98, 0x2, v98                        // Bias address scaled by BPE
ds_read_b32 v101, v98 offset:0                     // load Bias
v_add_u32 v99, 1024, v98                           // add ScaleAlphaVec offset (3)
ds_read_b32 v102, v99 offset:0                     // load scaleAlpha
v_add_lshl_u32 v97, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v209, v97, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v104, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v104, v209, v104, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v107, v104, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v105, v4, s52
v_lshlrev_b32 v105, 0x2, v105                      // Bias address scaled by BPE
ds_read_b32 v108, v105 offset:0                    // load Bias
v_add_u32 v106, 1024, v105                         // add ScaleAlphaVec offset (3)
ds_read_b32 v109, v106 offset:0                    // load scaleAlpha
v_add_lshl_u32 v104, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v104, v209, v104, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v111, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v111, v209, v111, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v114, v111, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v112, v4, s52
v_lshlrev_b32 v112, 0x2, v112                      // Bias address scaled by BPE
ds_read_b32 v115, v112 offset:0                    // load Bias
v_add_u32 v113, 1024, v112                         // add ScaleAlphaVec offset (3)
ds_read_b32 v116, v113 offset:0                    // load scaleAlpha
v_add_lshl_u32 v111, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v111, v209, v111, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v118, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v209, v118, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v121, v118, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v4, s52
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v120, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v209, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v123, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v123, v209, v123, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v126, v123, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v124, v4, s52
v_lshlrev_b32 v124, 0x2, v124                      // Bias address scaled by BPE
v_add_u32 v125, 1024, v124                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v123, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v123, v209, v123, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v128, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v209, v128, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v131, v128, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v129, v4, s52
v_lshlrev_b32 v129, 0x2, v129                      // Bias address scaled by BPE
v_add_u32 v130, 1024, v129                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v128, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v209, v128, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v133, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v133, v209, v133, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v136, v133, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v134, v4, s52
v_lshlrev_b32 v134, 0x2, v134                      // Bias address scaled by BPE
v_add_u32 v135, 1024, v134                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v133, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v133, v209, v133, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v138, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v209, v138, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v142, v138, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v140, v4, s52
v_lshlrev_b32 v140, 0x2, v140                      // Bias address scaled by BPE
v_add_u32 v141, 1024, v140                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v138, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v209, v138, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v144, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v144, v209, v144, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v147, v144, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v145, v4, s52
v_lshlrev_b32 v145, 0x2, v145                      // Bias address scaled by BPE
v_add_u32 v146, 1024, v145                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v144, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v144, v209, v144, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v149, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v209, v149, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v152, v149, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v150, v4, s52
v_lshlrev_b32 v150, 0x2, v150                      // Bias address scaled by BPE
v_add_u32 v151, 1024, v150                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v149, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v209, v149, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v154, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v154, v209, v154, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v157, v154, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v155, v4, s52
v_lshlrev_b32 v155, 0x2, v155                      // Bias address scaled by BPE
v_add_u32 v156, 1024, v155                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v154, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v154, v209, v154, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v159, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v209, v159, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v162, v159, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v160, v0, s52
v_lshlrev_b32 v160, 0x2, v160                      // Bias address scaled by BPE
v_add_u32 v161, 1024, v160                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v159, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v209, v159, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v164, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v164, v209, v164, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v167, v164, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v165, v4, s52
v_lshlrev_b32 v165, 0x2, v165                      // Bias address scaled by BPE
v_add_u32 v166, 1024, v165                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v164, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v164, v209, v164, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v169, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v169, v209, v169, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v172, v169, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v170, v4, s52
v_lshlrev_b32 v170, 0x2, v170                      // Bias address scaled by BPE
v_add_u32 v171, 1024, v170                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v169, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v169, v209, v169, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v174, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v174, v209, v174, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v177, v174, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v175, v4, s52
v_lshlrev_b32 v175, 0x2, v175                      // Bias address scaled by BPE
v_add_u32 v176, 1024, v175                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v174, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v174, v209, v174, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v179, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v209, v179, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v182, v179, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v180, v4, s52
v_lshlrev_b32 v180, 0x2, v180                      // Bias address scaled by BPE
v_add_u32 v181, 1024, v180                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v179, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v209, v179, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v184, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v184, v209, v184, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v187, v184, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v185, v4, s52
v_lshlrev_b32 v185, 0x2, v185                      // Bias address scaled by BPE
v_add_u32 v186, 1024, v185                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v184, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v184, v209, v184, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v189, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v189, v209, v189, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v192, v189, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v190, v4, s52
v_lshlrev_b32 v190, 0x2, v190                      // Bias address scaled by BPE
v_add_u32 v191, 1024, v190                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v189, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v189, v209, v189, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v194, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v194, v209, v194, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v197, v194, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v195, v4, s52
v_lshlrev_b32 v195, 0x2, v195                      // Bias address scaled by BPE
v_add_u32 v196, 1024, v195                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v194, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v194, v209, v194, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v199, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v199, v209, v199, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v202, v199, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v200, v4, s52
v_lshlrev_b32 v200, 0x2, v200                      // Bias address scaled by BPE
v_add_u32 v201, 1024, v200                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v199, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v199, v209, v199, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v204, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v204, v209, v204, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v207, v204, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v205, v4, s52
v_lshlrev_b32 v205, 0x2, v205                      // Bias address scaled by BPE
v_add_u32 v206, 1024, v205                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v204, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v204, v209, v204, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+12], acc136         // copy acc to vreg[136]
v_accvgpr_read_b32 v[vgprValuC+19], acc137         // copy acc to vreg[137]
v_accvgpr_read_b32 v[vgprValuC+26], acc138         // copy acc to vreg[138]
v_accvgpr_read_b32 v[vgprValuC+33], acc139         // copy acc to vreg[139]
v_accvgpr_read_b32 v[vgprValuC+40], acc140         // copy acc to vreg[140]
v_accvgpr_read_b32 v[vgprValuC+47], acc141         // copy acc to vreg[141]
v_accvgpr_read_b32 v[vgprValuC+54], acc142         // copy acc to vreg[142]
v_accvgpr_read_b32 v[vgprValuC+61], acc143         // copy acc to vreg[143]
v_accvgpr_read_b32 v[vgprValuC+68], acc144         // copy acc to vreg[144]
v_accvgpr_read_b32 v[vgprValuC+75], acc145         // copy acc to vreg[145]
v_accvgpr_read_b32 v[vgprValuC+82], acc146         // copy acc to vreg[146]
v_accvgpr_read_b32 v[vgprValuC+89], acc147         // copy acc to vreg[147]
v_accvgpr_read_b32 v[vgprValuC+96], acc148         // copy acc to vreg[148]
v_accvgpr_read_b32 v[vgprValuC+103], acc149        // copy acc to vreg[149]
v_accvgpr_read_b32 v[vgprValuC+110], acc150        // copy acc to vreg[150]
v_accvgpr_read_b32 v[vgprValuC+117], acc151        // copy acc to vreg[151]
v_accvgpr_read_b32 v[vgprValuC+122], acc152        // copy acc to vreg[152]
v_accvgpr_read_b32 v[vgprValuC+127], acc153        // copy acc to vreg[153]
v_accvgpr_read_b32 v[vgprValuC+132], acc154        // copy acc to vreg[154]
v_accvgpr_read_b32 v[vgprValuC+137], acc155        // copy acc to vreg[155]
v_accvgpr_read_b32 v[vgprValuC+143], acc156        // copy acc to vreg[156]
v_accvgpr_read_b32 v[vgprValuC+148], acc157        // copy acc to vreg[157]
v_accvgpr_read_b32 v[vgprValuC+153], acc158        // copy acc to vreg[158]
v_accvgpr_read_b32 v[vgprValuC+158], acc159        // copy acc to vreg[159]
v_accvgpr_read_b32 v[vgprValuC+163], acc160        // copy acc to vreg[160]
v_accvgpr_read_b32 v[vgprValuC+168], acc161        // copy acc to vreg[161]
v_accvgpr_read_b32 v[vgprValuC+173], acc162        // copy acc to vreg[162]
v_accvgpr_read_b32 v[vgprValuC+178], acc163        // copy acc to vreg[163]
v_accvgpr_read_b32 v[vgprValuC+183], acc164        // copy acc to vreg[164]
v_accvgpr_read_b32 v[vgprValuC+188], acc165        // copy acc to vreg[165]
v_accvgpr_read_b32 v[vgprValuC+193], acc166        // copy acc to vreg[166]
v_accvgpr_read_b32 v[vgprValuC+198], acc167        // copy acc to vreg[167]
v_accvgpr_read_b32 v[vgprValuC+203], acc168        // copy acc to vreg[168]
v_accvgpr_read_b32 v[vgprValuC+208], acc169        // copy acc to vreg[169]

/* rC *= alpha batchElements=[(1, 2, 0, 0), (1, 2, 0, 1), (1, 2, 0, 2), (1, 2, 0, 3), (1, 3, 0, 0), (1, 3, 0, 1), (1, 3, 0, 2), (1, 3, 0, 3), (1, 0, 1, 0), (1, 0, 1, 1), (1, 0, 1, 2), (1, 0, 1, 3), (1, 1, 1, 0), (1, 1, 1, 1), (1, 1, 1, 2), (1, 1, 1, 3), (1, 2, 1, 0), (1, 2, 1, 1), (1, 2, 1, 2), (1, 2, 1, 3), (1, 3, 1, 0), (1, 3, 1, 1), (1, 3, 1, 2), (1, 3, 1, 3), (1, 0, 2, 0), (1, 0, 2, 1), (1, 0, 2, 2), (1, 0, 2, 3), (1, 1, 2, 0), (1, 1, 2, 1), (1, 1, 2, 2), (1, 1, 2, 3), (1, 2, 2, 0), (1, 2, 2, 1)] */
v_mul_f32 v[vgprValuC+12], s[sgprAlpha], v[vgprValuC+12] // *= alpha
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+122], s[sgprAlpha], v[vgprValuC+122] // *= alpha
v_mul_f32 v[vgprValuC+127], s[sgprAlpha], v[vgprValuC+127] // *= alpha
v_mul_f32 v[vgprValuC+132], s[sgprAlpha], v[vgprValuC+132] // *= alpha
v_mul_f32 v[vgprValuC+137], s[sgprAlpha], v[vgprValuC+137] // *= alpha
v_mul_f32 v[vgprValuC+143], s[sgprAlpha], v[vgprValuC+143] // *= alpha
v_mul_f32 v[vgprValuC+148], s[sgprAlpha], v[vgprValuC+148] // *= alpha
v_mul_f32 v[vgprValuC+153], s[sgprAlpha], v[vgprValuC+153] // *= alpha
v_mul_f32 v[vgprValuC+158], s[sgprAlpha], v[vgprValuC+158] // *= alpha
v_mul_f32 v[vgprValuC+163], s[sgprAlpha], v[vgprValuC+163] // *= alpha
v_mul_f32 v[vgprValuC+168], s[sgprAlpha], v[vgprValuC+168] // *= alpha
v_mul_f32 v[vgprValuC+173], s[sgprAlpha], v[vgprValuC+173] // *= alpha
v_mul_f32 v[vgprValuC+178], s[sgprAlpha], v[vgprValuC+178] // *= alpha
v_mul_f32 v[vgprValuC+183], s[sgprAlpha], v[vgprValuC+183] // *= alpha
v_mul_f32 v[vgprValuC+188], s[sgprAlpha], v[vgprValuC+188] // *= alpha
v_mul_f32 v[vgprValuC+193], s[sgprAlpha], v[vgprValuC+193] // *= alpha
v_mul_f32 v[vgprValuC+198], s[sgprAlpha], v[vgprValuC+198] // *= alpha
v_mul_f32 v[vgprValuC+203], s[sgprAlpha], v[vgprValuC+203] // *= alpha
v_mul_f32 v[vgprValuC+208], s[sgprAlpha], v[vgprValuC+208] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+12], v11, v[vgprValuC+12]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+12], s[sgprBeta], v9, v[vgprValuC+12] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+12], v10, v[vgprValuC+12]    // C += bias
v_cvt_f16_f32 v12, v[vgprValuC+12]                 // convert C to fp16
buffer_store_short v12, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+19], s[sgprBeta], v16, v[vgprValuC+19] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+19], v17, v[vgprValuC+19]    // C += bias
v_cvt_f16_f32 v19, v[vgprValuC+19]                 // convert C to fp16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v23, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+26], v24, v[vgprValuC+26]    // C += bias
v_cvt_f16_f32 v26, v[vgprValuC+26]                 // convert C to fp16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v30, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+33], v31, v[vgprValuC+33]    // C += bias
v_cvt_f16_f32 v33, v[vgprValuC+33]                 // convert C to fp16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v37, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+40], v38, v[vgprValuC+40]    // C += bias
v_cvt_f16_f32 v40, v[vgprValuC+40]                 // convert C to fp16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v44, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+54], v53, v[vgprValuC+54]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v51, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+54], v52, v[vgprValuC+54]    // C += bias
v_cvt_f16_f32 v54, v[vgprValuC+54]                 // convert C to fp16
buffer_store_short v54, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+61], v60, v[vgprValuC+61]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v58, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+61], v59, v[vgprValuC+61]    // C += bias
v_cvt_f16_f32 v61, v[vgprValuC+61]                 // convert C to fp16
buffer_store_short v61, v55, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+68], v67, v[vgprValuC+68]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+68], s[sgprBeta], v65, v[vgprValuC+68] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+68], v66, v[vgprValuC+68]    // C += bias
v_cvt_f16_f32 v68, v[vgprValuC+68]                 // convert C to fp16
buffer_store_short v68, v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+75], v74, v[vgprValuC+75]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v72, v[vgprValuC+75] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+75], v73, v[vgprValuC+75]    // C += bias
v_cvt_f16_f32 v75, v[vgprValuC+75]                 // convert C to fp16
buffer_store_short v75, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+82], v81, v[vgprValuC+82]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+82], s[sgprBeta], v79, v[vgprValuC+82] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+82], v80, v[vgprValuC+82]    // C += bias
v_cvt_f16_f32 v82, v[vgprValuC+82]                 // convert C to fp16
buffer_store_short v82, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+89], v88, v[vgprValuC+89]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+89], s[sgprBeta], v86, v[vgprValuC+89] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+89], v87, v[vgprValuC+89]    // C += bias
v_cvt_f16_f32 v89, v[vgprValuC+89]                 // convert C to fp16
buffer_store_short v89, v83, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+96], v95, v[vgprValuC+96]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+96], s[sgprBeta], v93, v[vgprValuC+96] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+96], v94, v[vgprValuC+96]    // C += bias
v_cvt_f16_f32 v96, v[vgprValuC+96]                 // convert C to fp16
buffer_store_short v96, v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+103], v102, v[vgprValuC+103] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+103], s[sgprBeta], v100, v[vgprValuC+103] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+103], v101, v[vgprValuC+103] // C += bias
v_cvt_f16_f32 v103, v[vgprValuC+103]               // convert C to fp16
buffer_store_short v103, v97, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+110], v109, v[vgprValuC+110] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+110], s[sgprBeta], v107, v[vgprValuC+110] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+110], v108, v[vgprValuC+110] // C += bias
v_cvt_f16_f32 v110, v[vgprValuC+110]               // convert C to fp16
buffer_store_short v110, v104, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+117], v116, v[vgprValuC+117] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v114, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+117], v115, v[vgprValuC+117] // C += bias
v_cvt_f16_f32 v117, v[vgprValuC+117]               // convert C to fp16
buffer_store_short v117, v111, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+122], v11, v[vgprValuC+122]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+122], s[sgprBeta], v121, v[vgprValuC+122] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+122], v10, v[vgprValuC+122]  // C += bias
v_cvt_f16_f32 v122, v[vgprValuC+122]               // convert C to fp16
buffer_store_short v122, v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+127], v18, v[vgprValuC+127]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+127], s[sgprBeta], v126, v[vgprValuC+127] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+127], v17, v[vgprValuC+127]  // C += bias
v_cvt_f16_f32 v127, v[vgprValuC+127]               // convert C to fp16
buffer_store_short v127, v123, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+132], v25, v[vgprValuC+132]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+132], s[sgprBeta], v131, v[vgprValuC+132] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+132], v24, v[vgprValuC+132]  // C += bias
v_cvt_f16_f32 v132, v[vgprValuC+132]               // convert C to fp16
buffer_store_short v132, v128, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+137], v32, v[vgprValuC+137]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+137], s[sgprBeta], v136, v[vgprValuC+137] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+137], v31, v[vgprValuC+137]  // C += bias
v_cvt_f16_f32 v137, v[vgprValuC+137]               // convert C to fp16
buffer_store_short v137, v133, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+143], v39, v[vgprValuC+143]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+143], s[sgprBeta], v142, v[vgprValuC+143] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+143], v38, v[vgprValuC+143]  // C += bias
v_cvt_f16_f32 v143, v[vgprValuC+143]               // convert C to fp16
buffer_store_short v143, v138, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+148], v46, v[vgprValuC+148]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+148], s[sgprBeta], v147, v[vgprValuC+148] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+148], v45, v[vgprValuC+148]  // C += bias
v_cvt_f16_f32 v148, v[vgprValuC+148]               // convert C to fp16
buffer_store_short v148, v144, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+153], v53, v[vgprValuC+153]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+153], s[sgprBeta], v152, v[vgprValuC+153] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+153], v52, v[vgprValuC+153]  // C += bias
v_cvt_f16_f32 v153, v[vgprValuC+153]               // convert C to fp16
buffer_store_short v153, v149, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+158], v60, v[vgprValuC+158]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+158], s[sgprBeta], v157, v[vgprValuC+158] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+158], v59, v[vgprValuC+158]  // C += bias
v_cvt_f16_f32 v158, v[vgprValuC+158]               // convert C to fp16
buffer_store_short v158, v154, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+163], v67, v[vgprValuC+163]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+163], s[sgprBeta], v162, v[vgprValuC+163] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+163], v66, v[vgprValuC+163]  // C += bias
v_cvt_f16_f32 v163, v[vgprValuC+163]               // convert C to fp16
buffer_store_short v163, v159, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+168], v74, v[vgprValuC+168]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+168], s[sgprBeta], v167, v[vgprValuC+168] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+168], v73, v[vgprValuC+168]  // C += bias
v_cvt_f16_f32 v168, v[vgprValuC+168]               // convert C to fp16
buffer_store_short v168, v164, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+173], v81, v[vgprValuC+173]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+173], s[sgprBeta], v172, v[vgprValuC+173] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+173], v80, v[vgprValuC+173]  // C += bias
v_cvt_f16_f32 v173, v[vgprValuC+173]               // convert C to fp16
buffer_store_short v173, v169, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+178], v88, v[vgprValuC+178]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+178], s[sgprBeta], v177, v[vgprValuC+178] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+178], v87, v[vgprValuC+178]  // C += bias
v_cvt_f16_f32 v178, v[vgprValuC+178]               // convert C to fp16
buffer_store_short v178, v174, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+183], v95, v[vgprValuC+183]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+183], s[sgprBeta], v182, v[vgprValuC+183] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+183], v94, v[vgprValuC+183]  // C += bias
v_cvt_f16_f32 v183, v[vgprValuC+183]               // convert C to fp16
buffer_store_short v183, v179, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+188], v102, v[vgprValuC+188] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+188], s[sgprBeta], v187, v[vgprValuC+188] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+188], v101, v[vgprValuC+188] // C += bias
v_cvt_f16_f32 v188, v[vgprValuC+188]               // convert C to fp16
buffer_store_short v188, v184, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+193], v109, v[vgprValuC+193] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+193], s[sgprBeta], v192, v[vgprValuC+193] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+193], v108, v[vgprValuC+193] // C += bias
v_cvt_f16_f32 v193, v[vgprValuC+193]               // convert C to fp16
buffer_store_short v193, v189, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+198], v116, v[vgprValuC+198] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+198], s[sgprBeta], v197, v[vgprValuC+198] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+198], v115, v[vgprValuC+198] // C += bias
v_cvt_f16_f32 v198, v[vgprValuC+198]               // convert C to fp16
buffer_store_short v198, v194, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+203], v11, v[vgprValuC+203]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+203], s[sgprBeta], v202, v[vgprValuC+203] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+203], v10, v[vgprValuC+203]  // C += bias
v_cvt_f16_f32 v203, v[vgprValuC+203]               // convert C to fp16
buffer_store_short v203, v199, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+208], v18, v[vgprValuC+208]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+208], s[sgprBeta], v207, v[vgprValuC+208] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+208], v17, v[vgprValuC+208]  // C += bias
v_cvt_f16_f32 v208, v[vgprValuC+208]               // convert C to fp16
buffer_store_short v208, v204, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #5 (d1,d0,vc1,vc0) = */
/*    (1,2,2,2:vw1); (1,2,2,3:vw1); (1,3,2,0:vw1); (1,3,2,1:vw1); (1,3,2,2:vw1); (1,3,2,3:vw1); (1,0,3,0:vw1); (1,0,3,1:vw1); (1,0,3,2:vw1); (1,0,3,3:vw1); (1,1,3,0:vw1); (1,1,3,1:vw1); (1,1,3,2:vw1); (1,1,3,3:vw1); (1,2,3,0:vw1); (1,2,3,1:vw1); (1,2,3,2:vw1); (1,2,3,3:vw1); (1,3,3,0:vw1); (1,3,3,1:vw1); (1,3,3,2:vw1); (1,3,3,3:vw1); (1,0,4,0:vw1); (1,0,4,1:vw1); (1,0,4,2:vw1); (1,0,4,3:vw1); (1,1,4,0:vw1); (1,1,4,1:vw1); (1,1,4,2:vw1); (1,1,4,3:vw1); (1,2,4,0:vw1); (1,2,4,1:vw1); (1,2,4,2:vw1); (1,2,4,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v209, BufferOOB
/* (d1,vc1,d0,vc0)=(1,2,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v6, v2, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v209, v6, s[56:57]               // LDC clip if OOB. offset
buffer_load_short_d16 v9, v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v4, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b32 v10, v7 offset:0                       // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v11, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v209, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v13, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v209, v13, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v4, s52
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
ds_read_b32 v17, v14 offset:0                      // load Bias
v_add_u32 v15, 1024, v14                           // add ScaleAlphaVec offset (3)
ds_read_b32 v18, v15 offset:0                      // load scaleAlpha
v_add_lshl_u32 v13, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v209, v13, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v209, v20, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s52
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
ds_read_b32 v24, v21 offset:0                      // load Bias
v_add_u32 v22, 1024, v21                           // add ScaleAlphaVec offset (3)
ds_read_b32 v25, v22 offset:0                      // load scaleAlpha
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v209, v20, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v209, v27, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s52
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
ds_read_b32 v31, v28 offset:0                      // load Bias
v_add_u32 v29, 1024, v28                           // add ScaleAlphaVec offset (3)
ds_read_b32 v32, v29 offset:0                      // load scaleAlpha
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v209, v27, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v209, v34, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s52
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
ds_read_b32 v38, v35 offset:0                      // load Bias
v_add_u32 v36, 1024, v35                           // add ScaleAlphaVec offset (3)
ds_read_b32 v39, v36 offset:0                      // load scaleAlpha
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v209, v34, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v41, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v209, v41, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s52
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
ds_read_b32 v45, v42 offset:0                      // load Bias
v_add_u32 v43, 1024, v42                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v43 offset:0                      // load scaleAlpha
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v209, v41, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v48, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v209, v48, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v51, v48, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v0, s52
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
ds_read_b32 v52, v49 offset:0                      // load Bias
v_add_u32 v50, 1024, v49                           // add ScaleAlphaVec offset (3)
ds_read_b32 v53, v50 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v209, v48, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v55, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v209, v55, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v58, v55, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v56, v4, s52
v_lshlrev_b32 v56, 0x2, v56                        // Bias address scaled by BPE
ds_read_b32 v59, v56 offset:0                      // load Bias
v_add_u32 v57, 1024, v56                           // add ScaleAlphaVec offset (3)
ds_read_b32 v60, v57 offset:0                      // load scaleAlpha
v_add_lshl_u32 v55, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v209, v55, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v62, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v209, v62, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v65, v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v4, s52
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
ds_read_b32 v66, v63 offset:0                      // load Bias
v_add_u32 v64, 1024, v63                           // add ScaleAlphaVec offset (3)
ds_read_b32 v67, v64 offset:0                      // load scaleAlpha
v_add_lshl_u32 v62, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v209, v62, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v69, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v209, v69, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v72, v69, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v4, s52
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
ds_read_b32 v73, v70 offset:0                      // load Bias
v_add_u32 v71, 1024, v70                           // add ScaleAlphaVec offset (3)
ds_read_b32 v74, v71 offset:0                      // load scaleAlpha
v_add_lshl_u32 v69, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v209, v69, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v209, v76, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s52
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
ds_read_b32 v80, v77 offset:0                      // load Bias
v_add_u32 v78, 1024, v77                           // add ScaleAlphaVec offset (3)
ds_read_b32 v81, v78 offset:0                      // load scaleAlpha
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v209, v76, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v83, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v209, v83, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v86, v83, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v84, v4, s52
v_lshlrev_b32 v84, 0x2, v84                        // Bias address scaled by BPE
ds_read_b32 v87, v84 offset:0                      // load Bias
v_add_u32 v85, 1024, v84                           // add ScaleAlphaVec offset (3)
ds_read_b32 v88, v85 offset:0                      // load scaleAlpha
v_add_lshl_u32 v83, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v209, v83, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v90, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v209, v90, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v93, v90, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v4, s52
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
ds_read_b32 v94, v91 offset:0                      // load Bias
v_add_u32 v92, 1024, v91                           // add ScaleAlphaVec offset (3)
ds_read_b32 v95, v92 offset:0                      // load scaleAlpha
v_add_lshl_u32 v90, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v209, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v97, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v209, v97, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v100, v97, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v98, v4, s52
v_lshlrev_b32 v98, 0x2, v98                        // Bias address scaled by BPE
ds_read_b32 v101, v98 offset:0                     // load Bias
v_add_u32 v99, 1024, v98                           // add ScaleAlphaVec offset (3)
ds_read_b32 v102, v99 offset:0                     // load scaleAlpha
v_add_lshl_u32 v97, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v209, v97, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v104, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v104, v209, v104, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v107, v104, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v105, v4, s52
v_lshlrev_b32 v105, 0x2, v105                      // Bias address scaled by BPE
ds_read_b32 v108, v105 offset:0                    // load Bias
v_add_u32 v106, 1024, v105                         // add ScaleAlphaVec offset (3)
ds_read_b32 v109, v106 offset:0                    // load scaleAlpha
v_add_lshl_u32 v104, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v104, v209, v104, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v111, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v111, v209, v111, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v114, v111, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v112, v4, s52
v_lshlrev_b32 v112, 0x2, v112                      // Bias address scaled by BPE
ds_read_b32 v115, v112 offset:0                    // load Bias
v_add_u32 v113, 1024, v112                         // add ScaleAlphaVec offset (3)
ds_read_b32 v116, v113 offset:0                    // load scaleAlpha
v_add_lshl_u32 v111, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v111, v209, v111, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v118, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v209, v118, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v121, v118, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v4, s52
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v120, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v209, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v123, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v123, v209, v123, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v126, v123, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v124, v4, s52
v_lshlrev_b32 v124, 0x2, v124                      // Bias address scaled by BPE
v_add_u32 v125, 1024, v124                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v123, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v123, v209, v123, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v128, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v209, v128, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v131, v128, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v129, v4, s52
v_lshlrev_b32 v129, 0x2, v129                      // Bias address scaled by BPE
v_add_u32 v130, 1024, v129                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v128, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v209, v128, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v133, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v133, v209, v133, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v136, v133, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v134, v4, s52
v_lshlrev_b32 v134, 0x2, v134                      // Bias address scaled by BPE
v_add_u32 v135, 1024, v134                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v133, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v133, v209, v133, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v138, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v209, v138, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v142, v138, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v140, v4, s52
v_lshlrev_b32 v140, 0x2, v140                      // Bias address scaled by BPE
v_add_u32 v141, 1024, v140                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v138, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v209, v138, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v144, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v144, v209, v144, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v147, v144, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v145, v4, s52
v_lshlrev_b32 v145, 0x2, v145                      // Bias address scaled by BPE
v_add_u32 v146, 1024, v145                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v144, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v144, v209, v144, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v149, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v209, v149, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v152, v149, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v150, v0, s52
v_lshlrev_b32 v150, 0x2, v150                      // Bias address scaled by BPE
v_add_u32 v151, 1024, v150                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v149, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v209, v149, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v154, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v154, v209, v154, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v157, v154, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v155, v4, s52
v_lshlrev_b32 v155, 0x2, v155                      // Bias address scaled by BPE
v_add_u32 v156, 1024, v155                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v154, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v154, v209, v154, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v159, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v209, v159, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v162, v159, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v160, v4, s52
v_lshlrev_b32 v160, 0x2, v160                      // Bias address scaled by BPE
v_add_u32 v161, 1024, v160                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v159, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v209, v159, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v164, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v164, v209, v164, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v167, v164, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v165, v4, s52
v_lshlrev_b32 v165, 0x2, v165                      // Bias address scaled by BPE
v_add_u32 v166, 1024, v165                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v164, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v164, v209, v164, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v169, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v169, v209, v169, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v172, v169, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v170, v4, s52
v_lshlrev_b32 v170, 0x2, v170                      // Bias address scaled by BPE
v_add_u32 v171, 1024, v170                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v169, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v169, v209, v169, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v174, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v174, v209, v174, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v177, v174, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v175, v4, s52
v_lshlrev_b32 v175, 0x2, v175                      // Bias address scaled by BPE
v_add_u32 v176, 1024, v175                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v174, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v174, v209, v174, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v179, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v209, v179, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v182, v179, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v180, v4, s52
v_lshlrev_b32 v180, 0x2, v180                      // Bias address scaled by BPE
v_add_u32 v181, 1024, v180                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v179, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v209, v179, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v184, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v184, v209, v184, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v187, v184, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v185, v4, s52
v_lshlrev_b32 v185, 0x2, v185                      // Bias address scaled by BPE
v_add_u32 v186, 1024, v185                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v184, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v184, v209, v184, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v189, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v189, v209, v189, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v192, v189, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v190, v4, s52
v_lshlrev_b32 v190, 0x2, v190                      // Bias address scaled by BPE
v_add_u32 v191, 1024, v190                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v189, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v189, v209, v189, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v194, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v194, v209, v194, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v197, v194, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v195, v4, s52
v_lshlrev_b32 v195, 0x2, v195                      // Bias address scaled by BPE
v_add_u32 v196, 1024, v195                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v194, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v194, v209, v194, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v199, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v199, v209, v199, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v202, v199, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v200, v4, s52
v_lshlrev_b32 v200, 0x2, v200                      // Bias address scaled by BPE
v_add_u32 v201, 1024, v200                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v199, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v199, v209, v199, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v204, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v204, v209, v204, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v207, v204, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v205, v4, s52
v_lshlrev_b32 v205, 0x2, v205                      // Bias address scaled by BPE
v_add_u32 v206, 1024, v205                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v204, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v204, v209, v204, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+12], acc170         // copy acc to vreg[170]
v_accvgpr_read_b32 v[vgprValuC+19], acc171         // copy acc to vreg[171]
v_accvgpr_read_b32 v[vgprValuC+26], acc172         // copy acc to vreg[172]
v_accvgpr_read_b32 v[vgprValuC+33], acc173         // copy acc to vreg[173]
v_accvgpr_read_b32 v[vgprValuC+40], acc174         // copy acc to vreg[174]
v_accvgpr_read_b32 v[vgprValuC+47], acc175         // copy acc to vreg[175]
v_accvgpr_read_b32 v[vgprValuC+54], acc176         // copy acc to vreg[176]
v_accvgpr_read_b32 v[vgprValuC+61], acc177         // copy acc to vreg[177]
v_accvgpr_read_b32 v[vgprValuC+68], acc178         // copy acc to vreg[178]
v_accvgpr_read_b32 v[vgprValuC+75], acc179         // copy acc to vreg[179]
v_accvgpr_read_b32 v[vgprValuC+82], acc180         // copy acc to vreg[180]
v_accvgpr_read_b32 v[vgprValuC+89], acc181         // copy acc to vreg[181]
v_accvgpr_read_b32 v[vgprValuC+96], acc182         // copy acc to vreg[182]
v_accvgpr_read_b32 v[vgprValuC+103], acc183        // copy acc to vreg[183]
v_accvgpr_read_b32 v[vgprValuC+110], acc184        // copy acc to vreg[184]
v_accvgpr_read_b32 v[vgprValuC+117], acc185        // copy acc to vreg[185]
v_accvgpr_read_b32 v[vgprValuC+122], acc186        // copy acc to vreg[186]
v_accvgpr_read_b32 v[vgprValuC+127], acc187        // copy acc to vreg[187]
v_accvgpr_read_b32 v[vgprValuC+132], acc188        // copy acc to vreg[188]
v_accvgpr_read_b32 v[vgprValuC+137], acc189        // copy acc to vreg[189]
v_accvgpr_read_b32 v[vgprValuC+143], acc190        // copy acc to vreg[190]
v_accvgpr_read_b32 v[vgprValuC+148], acc191        // copy acc to vreg[191]
v_accvgpr_read_b32 v[vgprValuC+153], acc192        // copy acc to vreg[192]
v_accvgpr_read_b32 v[vgprValuC+158], acc193        // copy acc to vreg[193]
v_accvgpr_read_b32 v[vgprValuC+163], acc194        // copy acc to vreg[194]
v_accvgpr_read_b32 v[vgprValuC+168], acc195        // copy acc to vreg[195]
v_accvgpr_read_b32 v[vgprValuC+173], acc196        // copy acc to vreg[196]
v_accvgpr_read_b32 v[vgprValuC+178], acc197        // copy acc to vreg[197]
v_accvgpr_read_b32 v[vgprValuC+183], acc198        // copy acc to vreg[198]
v_accvgpr_read_b32 v[vgprValuC+188], acc199        // copy acc to vreg[199]
v_accvgpr_read_b32 v[vgprValuC+193], acc200        // copy acc to vreg[200]
v_accvgpr_read_b32 v[vgprValuC+198], acc201        // copy acc to vreg[201]
v_accvgpr_read_b32 v[vgprValuC+203], acc202        // copy acc to vreg[202]
v_accvgpr_read_b32 v[vgprValuC+208], acc203        // copy acc to vreg[203]

/* rC *= alpha batchElements=[(1, 2, 2, 2), (1, 2, 2, 3), (1, 3, 2, 0), (1, 3, 2, 1), (1, 3, 2, 2), (1, 3, 2, 3), (1, 0, 3, 0), (1, 0, 3, 1), (1, 0, 3, 2), (1, 0, 3, 3), (1, 1, 3, 0), (1, 1, 3, 1), (1, 1, 3, 2), (1, 1, 3, 3), (1, 2, 3, 0), (1, 2, 3, 1), (1, 2, 3, 2), (1, 2, 3, 3), (1, 3, 3, 0), (1, 3, 3, 1), (1, 3, 3, 2), (1, 3, 3, 3), (1, 0, 4, 0), (1, 0, 4, 1), (1, 0, 4, 2), (1, 0, 4, 3), (1, 1, 4, 0), (1, 1, 4, 1), (1, 1, 4, 2), (1, 1, 4, 3), (1, 2, 4, 0), (1, 2, 4, 1), (1, 2, 4, 2), (1, 2, 4, 3)] */
v_mul_f32 v[vgprValuC+12], s[sgprAlpha], v[vgprValuC+12] // *= alpha
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+122], s[sgprAlpha], v[vgprValuC+122] // *= alpha
v_mul_f32 v[vgprValuC+127], s[sgprAlpha], v[vgprValuC+127] // *= alpha
v_mul_f32 v[vgprValuC+132], s[sgprAlpha], v[vgprValuC+132] // *= alpha
v_mul_f32 v[vgprValuC+137], s[sgprAlpha], v[vgprValuC+137] // *= alpha
v_mul_f32 v[vgprValuC+143], s[sgprAlpha], v[vgprValuC+143] // *= alpha
v_mul_f32 v[vgprValuC+148], s[sgprAlpha], v[vgprValuC+148] // *= alpha
v_mul_f32 v[vgprValuC+153], s[sgprAlpha], v[vgprValuC+153] // *= alpha
v_mul_f32 v[vgprValuC+158], s[sgprAlpha], v[vgprValuC+158] // *= alpha
v_mul_f32 v[vgprValuC+163], s[sgprAlpha], v[vgprValuC+163] // *= alpha
v_mul_f32 v[vgprValuC+168], s[sgprAlpha], v[vgprValuC+168] // *= alpha
v_mul_f32 v[vgprValuC+173], s[sgprAlpha], v[vgprValuC+173] // *= alpha
v_mul_f32 v[vgprValuC+178], s[sgprAlpha], v[vgprValuC+178] // *= alpha
v_mul_f32 v[vgprValuC+183], s[sgprAlpha], v[vgprValuC+183] // *= alpha
v_mul_f32 v[vgprValuC+188], s[sgprAlpha], v[vgprValuC+188] // *= alpha
v_mul_f32 v[vgprValuC+193], s[sgprAlpha], v[vgprValuC+193] // *= alpha
v_mul_f32 v[vgprValuC+198], s[sgprAlpha], v[vgprValuC+198] // *= alpha
v_mul_f32 v[vgprValuC+203], s[sgprAlpha], v[vgprValuC+203] // *= alpha
v_mul_f32 v[vgprValuC+208], s[sgprAlpha], v[vgprValuC+208] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+12], v11, v[vgprValuC+12]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+12], s[sgprBeta], v9, v[vgprValuC+12] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+12], v10, v[vgprValuC+12]    // C += bias
v_cvt_f16_f32 v12, v[vgprValuC+12]                 // convert C to fp16
buffer_store_short v12, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+19], s[sgprBeta], v16, v[vgprValuC+19] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+19], v17, v[vgprValuC+19]    // C += bias
v_cvt_f16_f32 v19, v[vgprValuC+19]                 // convert C to fp16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v23, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+26], v24, v[vgprValuC+26]    // C += bias
v_cvt_f16_f32 v26, v[vgprValuC+26]                 // convert C to fp16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v30, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+33], v31, v[vgprValuC+33]    // C += bias
v_cvt_f16_f32 v33, v[vgprValuC+33]                 // convert C to fp16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v37, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+40], v38, v[vgprValuC+40]    // C += bias
v_cvt_f16_f32 v40, v[vgprValuC+40]                 // convert C to fp16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v44, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+54], v53, v[vgprValuC+54]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v51, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+54], v52, v[vgprValuC+54]    // C += bias
v_cvt_f16_f32 v54, v[vgprValuC+54]                 // convert C to fp16
buffer_store_short v54, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+61], v60, v[vgprValuC+61]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v58, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+61], v59, v[vgprValuC+61]    // C += bias
v_cvt_f16_f32 v61, v[vgprValuC+61]                 // convert C to fp16
buffer_store_short v61, v55, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+68], v67, v[vgprValuC+68]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+68], s[sgprBeta], v65, v[vgprValuC+68] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+68], v66, v[vgprValuC+68]    // C += bias
v_cvt_f16_f32 v68, v[vgprValuC+68]                 // convert C to fp16
buffer_store_short v68, v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+75], v74, v[vgprValuC+75]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v72, v[vgprValuC+75] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+75], v73, v[vgprValuC+75]    // C += bias
v_cvt_f16_f32 v75, v[vgprValuC+75]                 // convert C to fp16
buffer_store_short v75, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+82], v81, v[vgprValuC+82]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+82], s[sgprBeta], v79, v[vgprValuC+82] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+82], v80, v[vgprValuC+82]    // C += bias
v_cvt_f16_f32 v82, v[vgprValuC+82]                 // convert C to fp16
buffer_store_short v82, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+89], v88, v[vgprValuC+89]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+89], s[sgprBeta], v86, v[vgprValuC+89] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+89], v87, v[vgprValuC+89]    // C += bias
v_cvt_f16_f32 v89, v[vgprValuC+89]                 // convert C to fp16
buffer_store_short v89, v83, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+96], v95, v[vgprValuC+96]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+96], s[sgprBeta], v93, v[vgprValuC+96] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+96], v94, v[vgprValuC+96]    // C += bias
v_cvt_f16_f32 v96, v[vgprValuC+96]                 // convert C to fp16
buffer_store_short v96, v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+103], v102, v[vgprValuC+103] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+103], s[sgprBeta], v100, v[vgprValuC+103] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+103], v101, v[vgprValuC+103] // C += bias
v_cvt_f16_f32 v103, v[vgprValuC+103]               // convert C to fp16
buffer_store_short v103, v97, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+110], v109, v[vgprValuC+110] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+110], s[sgprBeta], v107, v[vgprValuC+110] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+110], v108, v[vgprValuC+110] // C += bias
v_cvt_f16_f32 v110, v[vgprValuC+110]               // convert C to fp16
buffer_store_short v110, v104, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+117], v116, v[vgprValuC+117] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v114, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+117], v115, v[vgprValuC+117] // C += bias
v_cvt_f16_f32 v117, v[vgprValuC+117]               // convert C to fp16
buffer_store_short v117, v111, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+122], v11, v[vgprValuC+122]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+122], s[sgprBeta], v121, v[vgprValuC+122] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+122], v10, v[vgprValuC+122]  // C += bias
v_cvt_f16_f32 v122, v[vgprValuC+122]               // convert C to fp16
buffer_store_short v122, v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+127], v18, v[vgprValuC+127]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+127], s[sgprBeta], v126, v[vgprValuC+127] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+127], v17, v[vgprValuC+127]  // C += bias
v_cvt_f16_f32 v127, v[vgprValuC+127]               // convert C to fp16
buffer_store_short v127, v123, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+132], v25, v[vgprValuC+132]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+132], s[sgprBeta], v131, v[vgprValuC+132] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+132], v24, v[vgprValuC+132]  // C += bias
v_cvt_f16_f32 v132, v[vgprValuC+132]               // convert C to fp16
buffer_store_short v132, v128, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+137], v32, v[vgprValuC+137]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+137], s[sgprBeta], v136, v[vgprValuC+137] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+137], v31, v[vgprValuC+137]  // C += bias
v_cvt_f16_f32 v137, v[vgprValuC+137]               // convert C to fp16
buffer_store_short v137, v133, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+143], v39, v[vgprValuC+143]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+143], s[sgprBeta], v142, v[vgprValuC+143] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+143], v38, v[vgprValuC+143]  // C += bias
v_cvt_f16_f32 v143, v[vgprValuC+143]               // convert C to fp16
buffer_store_short v143, v138, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+148], v46, v[vgprValuC+148]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+148], s[sgprBeta], v147, v[vgprValuC+148] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+148], v45, v[vgprValuC+148]  // C += bias
v_cvt_f16_f32 v148, v[vgprValuC+148]               // convert C to fp16
buffer_store_short v148, v144, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+153], v53, v[vgprValuC+153]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+153], s[sgprBeta], v152, v[vgprValuC+153] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+153], v52, v[vgprValuC+153]  // C += bias
v_cvt_f16_f32 v153, v[vgprValuC+153]               // convert C to fp16
buffer_store_short v153, v149, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+158], v60, v[vgprValuC+158]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+158], s[sgprBeta], v157, v[vgprValuC+158] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+158], v59, v[vgprValuC+158]  // C += bias
v_cvt_f16_f32 v158, v[vgprValuC+158]               // convert C to fp16
buffer_store_short v158, v154, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+163], v67, v[vgprValuC+163]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+163], s[sgprBeta], v162, v[vgprValuC+163] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+163], v66, v[vgprValuC+163]  // C += bias
v_cvt_f16_f32 v163, v[vgprValuC+163]               // convert C to fp16
buffer_store_short v163, v159, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+168], v74, v[vgprValuC+168]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+168], s[sgprBeta], v167, v[vgprValuC+168] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+168], v73, v[vgprValuC+168]  // C += bias
v_cvt_f16_f32 v168, v[vgprValuC+168]               // convert C to fp16
buffer_store_short v168, v164, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+173], v81, v[vgprValuC+173]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+173], s[sgprBeta], v172, v[vgprValuC+173] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+173], v80, v[vgprValuC+173]  // C += bias
v_cvt_f16_f32 v173, v[vgprValuC+173]               // convert C to fp16
buffer_store_short v173, v169, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+178], v88, v[vgprValuC+178]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+178], s[sgprBeta], v177, v[vgprValuC+178] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+178], v87, v[vgprValuC+178]  // C += bias
v_cvt_f16_f32 v178, v[vgprValuC+178]               // convert C to fp16
buffer_store_short v178, v174, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+183], v95, v[vgprValuC+183]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+183], s[sgprBeta], v182, v[vgprValuC+183] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+183], v94, v[vgprValuC+183]  // C += bias
v_cvt_f16_f32 v183, v[vgprValuC+183]               // convert C to fp16
buffer_store_short v183, v179, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+188], v102, v[vgprValuC+188] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+188], s[sgprBeta], v187, v[vgprValuC+188] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+188], v101, v[vgprValuC+188] // C += bias
v_cvt_f16_f32 v188, v[vgprValuC+188]               // convert C to fp16
buffer_store_short v188, v184, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+193], v109, v[vgprValuC+193] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+193], s[sgprBeta], v192, v[vgprValuC+193] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+193], v108, v[vgprValuC+193] // C += bias
v_cvt_f16_f32 v193, v[vgprValuC+193]               // convert C to fp16
buffer_store_short v193, v189, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+198], v116, v[vgprValuC+198] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+198], s[sgprBeta], v197, v[vgprValuC+198] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+198], v115, v[vgprValuC+198] // C += bias
v_cvt_f16_f32 v198, v[vgprValuC+198]               // convert C to fp16
buffer_store_short v198, v194, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+203], v11, v[vgprValuC+203]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+203], s[sgprBeta], v202, v[vgprValuC+203] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+203], v10, v[vgprValuC+203]  // C += bias
v_cvt_f16_f32 v203, v[vgprValuC+203]               // convert C to fp16
buffer_store_short v203, v199, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+208], v18, v[vgprValuC+208]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+208], s[sgprBeta], v207, v[vgprValuC+208] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+208], v17, v[vgprValuC+208]  // C += bias
v_cvt_f16_f32 v208, v[vgprValuC+208]               // convert C to fp16
buffer_store_short v208, v204, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #6 (d1,d0,vc1,vc0) = */
/*    (1,3,4,0:vw1); (1,3,4,1:vw1); (1,3,4,2:vw1); (1,3,4,3:vw1); (1,0,5,0:vw1); (1,0,5,1:vw1); (1,0,5,2:vw1); (1,0,5,3:vw1); (1,1,5,0:vw1); (1,1,5,1:vw1); (1,1,5,2:vw1); (1,1,5,3:vw1); (1,2,5,0:vw1); (1,2,5,1:vw1); (1,2,5,2:vw1); (1,2,5,3:vw1); (1,3,5,0:vw1); (1,3,5,1:vw1); (1,3,5,2:vw1); (1,3,5,3:vw1); (1,0,6,0:vw1); (1,0,6,1:vw1); (1,0,6,2:vw1); (1,0,6,3:vw1); (1,1,6,0:vw1); (1,1,6,1:vw1); (1,1,6,2:vw1); (1,1,6,3:vw1); (1,2,6,0:vw1); (1,2,6,1:vw1); (1,2,6,2:vw1); (1,2,6,3:vw1); (1,3,6,0:vw1); (1,3,6,1:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v209, BufferOOB
/* (d1,vc1,d0,vc0)=(1,4,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v6, v2, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v209, v6, s[56:57]               // LDC clip if OOB. offset
buffer_load_short_d16 v9, v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v4, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b32 v10, v7 offset:0                       // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v11, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v209, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v13, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v209, v13, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v4, s52
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
ds_read_b32 v17, v14 offset:0                      // load Bias
v_add_u32 v15, 1024, v14                           // add ScaleAlphaVec offset (3)
ds_read_b32 v18, v15 offset:0                      // load scaleAlpha
v_add_lshl_u32 v13, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v209, v13, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v209, v20, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s52
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
ds_read_b32 v24, v21 offset:0                      // load Bias
v_add_u32 v22, 1024, v21                           // add ScaleAlphaVec offset (3)
ds_read_b32 v25, v22 offset:0                      // load scaleAlpha
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v209, v20, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,4,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v209, v27, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s52
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
ds_read_b32 v31, v28 offset:0                      // load Bias
v_add_u32 v29, 1024, v28                           // add ScaleAlphaVec offset (3)
ds_read_b32 v32, v29 offset:0                      // load scaleAlpha
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v209, v27, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v34, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v209, v34, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v0, s52
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
ds_read_b32 v38, v35 offset:0                      // load Bias
v_add_u32 v36, 1024, v35                           // add ScaleAlphaVec offset (3)
ds_read_b32 v39, v36 offset:0                      // load scaleAlpha
v_add_lshl_u32 v34, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v209, v34, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v41, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v209, v41, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s52
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
ds_read_b32 v45, v42 offset:0                      // load Bias
v_add_u32 v43, 1024, v42                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v43 offset:0                      // load scaleAlpha
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v209, v41, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v48, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v209, v48, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v51, v48, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v4, s52
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
ds_read_b32 v52, v49 offset:0                      // load Bias
v_add_u32 v50, 1024, v49                           // add ScaleAlphaVec offset (3)
ds_read_b32 v53, v50 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v209, v48, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v55, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v209, v55, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v58, v55, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v56, v4, s52
v_lshlrev_b32 v56, 0x2, v56                        // Bias address scaled by BPE
ds_read_b32 v59, v56 offset:0                      // load Bias
v_add_u32 v57, 1024, v56                           // add ScaleAlphaVec offset (3)
ds_read_b32 v60, v57 offset:0                      // load scaleAlpha
v_add_lshl_u32 v55, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v209, v55, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v62, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v209, v62, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v65, v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v4, s52
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
ds_read_b32 v66, v63 offset:0                      // load Bias
v_add_u32 v64, 1024, v63                           // add ScaleAlphaVec offset (3)
ds_read_b32 v67, v64 offset:0                      // load scaleAlpha
v_add_lshl_u32 v62, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v209, v62, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v69, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v209, v69, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v72, v69, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v4, s52
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
ds_read_b32 v73, v70 offset:0                      // load Bias
v_add_u32 v71, 1024, v70                           // add ScaleAlphaVec offset (3)
ds_read_b32 v74, v71 offset:0                      // load scaleAlpha
v_add_lshl_u32 v69, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v209, v69, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v209, v76, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s52
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
ds_read_b32 v80, v77 offset:0                      // load Bias
v_add_u32 v78, 1024, v77                           // add ScaleAlphaVec offset (3)
ds_read_b32 v81, v78 offset:0                      // load scaleAlpha
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v209, v76, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v83, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v209, v83, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v86, v83, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v84, v4, s52
v_lshlrev_b32 v84, 0x2, v84                        // Bias address scaled by BPE
ds_read_b32 v87, v84 offset:0                      // load Bias
v_add_u32 v85, 1024, v84                           // add ScaleAlphaVec offset (3)
ds_read_b32 v88, v85 offset:0                      // load scaleAlpha
v_add_lshl_u32 v83, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v209, v83, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v90, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v209, v90, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v93, v90, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v4, s52
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
ds_read_b32 v94, v91 offset:0                      // load Bias
v_add_u32 v92, 1024, v91                           // add ScaleAlphaVec offset (3)
ds_read_b32 v95, v92 offset:0                      // load scaleAlpha
v_add_lshl_u32 v90, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v209, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v97, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v209, v97, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v100, v97, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v98, v4, s52
v_lshlrev_b32 v98, 0x2, v98                        // Bias address scaled by BPE
ds_read_b32 v101, v98 offset:0                     // load Bias
v_add_u32 v99, 1024, v98                           // add ScaleAlphaVec offset (3)
ds_read_b32 v102, v99 offset:0                     // load scaleAlpha
v_add_lshl_u32 v97, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v209, v97, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v104, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v104, v209, v104, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v107, v104, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v105, v4, s52
v_lshlrev_b32 v105, 0x2, v105                      // Bias address scaled by BPE
ds_read_b32 v108, v105 offset:0                    // load Bias
v_add_u32 v106, 1024, v105                         // add ScaleAlphaVec offset (3)
ds_read_b32 v109, v106 offset:0                    // load scaleAlpha
v_add_lshl_u32 v104, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v104, v209, v104, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v111, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v111, v209, v111, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v114, v111, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v112, v4, s52
v_lshlrev_b32 v112, 0x2, v112                      // Bias address scaled by BPE
ds_read_b32 v115, v112 offset:0                    // load Bias
v_add_u32 v113, 1024, v112                         // add ScaleAlphaVec offset (3)
ds_read_b32 v116, v113 offset:0                    // load scaleAlpha
v_add_lshl_u32 v111, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v111, v209, v111, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v118, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v209, v118, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v121, v118, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v4, s52
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v120, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v209, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v123, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v123, v209, v123, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v126, v123, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v124, v4, s52
v_lshlrev_b32 v124, 0x2, v124                      // Bias address scaled by BPE
v_add_u32 v125, 1024, v124                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v123, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v123, v209, v123, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v128, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v209, v128, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v131, v128, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v129, v4, s52
v_lshlrev_b32 v129, 0x2, v129                      // Bias address scaled by BPE
v_add_u32 v130, 1024, v129                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v128, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v128, v209, v128, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,5,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v133, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v133, v209, v133, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v136, v133, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v134, v4, s52
v_lshlrev_b32 v134, 0x2, v134                      // Bias address scaled by BPE
v_add_u32 v135, 1024, v134                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v133, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v133, v209, v133, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v138, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v209, v138, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v142, v138, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v140, v0, s52
v_lshlrev_b32 v140, 0x2, v140                      // Bias address scaled by BPE
v_add_u32 v141, 1024, v140                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v138, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v138, v209, v138, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v144, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v144, v209, v144, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v147, v144, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v145, v4, s52
v_lshlrev_b32 v145, 0x2, v145                      // Bias address scaled by BPE
v_add_u32 v146, 1024, v145                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v144, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v144, v209, v144, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v149, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v209, v149, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v152, v149, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v150, v4, s52
v_lshlrev_b32 v150, 0x2, v150                      // Bias address scaled by BPE
v_add_u32 v151, 1024, v150                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v149, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v149, v209, v149, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v154, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v154, v209, v154, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v157, v154, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v155, v4, s52
v_lshlrev_b32 v155, 0x2, v155                      // Bias address scaled by BPE
v_add_u32 v156, 1024, v155                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v154, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v154, v209, v154, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v159, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v209, v159, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v162, v159, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v160, v4, s52
v_lshlrev_b32 v160, 0x2, v160                      // Bias address scaled by BPE
v_add_u32 v161, 1024, v160                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v159, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v159, v209, v159, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v164, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v164, v209, v164, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v167, v164, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v165, v4, s52
v_lshlrev_b32 v165, 0x2, v165                      // Bias address scaled by BPE
v_add_u32 v166, 1024, v165                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v164, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v164, v209, v164, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v169, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v169, v209, v169, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v172, v169, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v170, v4, s52
v_lshlrev_b32 v170, 0x2, v170                      // Bias address scaled by BPE
v_add_u32 v171, 1024, v170                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v169, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v169, v209, v169, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v174, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v174, v209, v174, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v177, v174, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v175, v4, s52
v_lshlrev_b32 v175, 0x2, v175                      // Bias address scaled by BPE
v_add_u32 v176, 1024, v175                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v174, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v174, v209, v174, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v179, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v209, v179, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v182, v179, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v180, v4, s52
v_lshlrev_b32 v180, 0x2, v180                      // Bias address scaled by BPE
v_add_u32 v181, 1024, v180                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v179, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v179, v209, v179, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v184, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v184, v209, v184, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v187, v184, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v185, v4, s52
v_lshlrev_b32 v185, 0x2, v185                      // Bias address scaled by BPE
v_add_u32 v186, 1024, v185                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v184, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v184, v209, v184, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v189, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v189, v209, v189, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v192, v189, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v190, v4, s52
v_lshlrev_b32 v190, 0x2, v190                      // Bias address scaled by BPE
v_add_u32 v191, 1024, v190                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v189, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v189, v209, v189, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v194, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v194, v209, v194, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v197, v194, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v195, v4, s52
v_lshlrev_b32 v195, 0x2, v195                      // Bias address scaled by BPE
v_add_u32 v196, 1024, v195                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v194, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v194, v209, v194, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v199, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v199, v209, v199, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v202, v199, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v200, v4, s52
v_lshlrev_b32 v200, 0x2, v200                      // Bias address scaled by BPE
v_add_u32 v201, 1024, v200                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v199, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v199, v209, v199, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v204, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v204, v209, v204, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v207, v204, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v205, v4, s52
v_lshlrev_b32 v205, 0x2, v205                      // Bias address scaled by BPE
v_add_u32 v206, 1024, v205                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v204, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v204, v209, v204, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+12], acc204         // copy acc to vreg[204]
v_accvgpr_read_b32 v[vgprValuC+19], acc205         // copy acc to vreg[205]
v_accvgpr_read_b32 v[vgprValuC+26], acc206         // copy acc to vreg[206]
v_accvgpr_read_b32 v[vgprValuC+33], acc207         // copy acc to vreg[207]
v_accvgpr_read_b32 v[vgprValuC+40], acc208         // copy acc to vreg[208]
v_accvgpr_read_b32 v[vgprValuC+47], acc209         // copy acc to vreg[209]
v_accvgpr_read_b32 v[vgprValuC+54], acc210         // copy acc to vreg[210]
v_accvgpr_read_b32 v[vgprValuC+61], acc211         // copy acc to vreg[211]
v_accvgpr_read_b32 v[vgprValuC+68], acc212         // copy acc to vreg[212]
v_accvgpr_read_b32 v[vgprValuC+75], acc213         // copy acc to vreg[213]
v_accvgpr_read_b32 v[vgprValuC+82], acc214         // copy acc to vreg[214]
v_accvgpr_read_b32 v[vgprValuC+89], acc215         // copy acc to vreg[215]
v_accvgpr_read_b32 v[vgprValuC+96], acc216         // copy acc to vreg[216]
v_accvgpr_read_b32 v[vgprValuC+103], acc217        // copy acc to vreg[217]
v_accvgpr_read_b32 v[vgprValuC+110], acc218        // copy acc to vreg[218]
v_accvgpr_read_b32 v[vgprValuC+117], acc219        // copy acc to vreg[219]
v_accvgpr_read_b32 v[vgprValuC+122], acc220        // copy acc to vreg[220]
v_accvgpr_read_b32 v[vgprValuC+127], acc221        // copy acc to vreg[221]
v_accvgpr_read_b32 v[vgprValuC+132], acc222        // copy acc to vreg[222]
v_accvgpr_read_b32 v[vgprValuC+137], acc223        // copy acc to vreg[223]
v_accvgpr_read_b32 v[vgprValuC+143], acc224        // copy acc to vreg[224]
v_accvgpr_read_b32 v[vgprValuC+148], acc225        // copy acc to vreg[225]
v_accvgpr_read_b32 v[vgprValuC+153], acc226        // copy acc to vreg[226]
v_accvgpr_read_b32 v[vgprValuC+158], acc227        // copy acc to vreg[227]
v_accvgpr_read_b32 v[vgprValuC+163], acc228        // copy acc to vreg[228]
v_accvgpr_read_b32 v[vgprValuC+168], acc229        // copy acc to vreg[229]
v_accvgpr_read_b32 v[vgprValuC+173], acc230        // copy acc to vreg[230]
v_accvgpr_read_b32 v[vgprValuC+178], acc231        // copy acc to vreg[231]
v_accvgpr_read_b32 v[vgprValuC+183], acc232        // copy acc to vreg[232]
v_accvgpr_read_b32 v[vgprValuC+188], acc233        // copy acc to vreg[233]
v_accvgpr_read_b32 v[vgprValuC+193], acc234        // copy acc to vreg[234]
v_accvgpr_read_b32 v[vgprValuC+198], acc235        // copy acc to vreg[235]
v_accvgpr_read_b32 v[vgprValuC+203], acc236        // copy acc to vreg[236]
v_accvgpr_read_b32 v[vgprValuC+208], acc237        // copy acc to vreg[237]

/* rC *= alpha batchElements=[(1, 3, 4, 0), (1, 3, 4, 1), (1, 3, 4, 2), (1, 3, 4, 3), (1, 0, 5, 0), (1, 0, 5, 1), (1, 0, 5, 2), (1, 0, 5, 3), (1, 1, 5, 0), (1, 1, 5, 1), (1, 1, 5, 2), (1, 1, 5, 3), (1, 2, 5, 0), (1, 2, 5, 1), (1, 2, 5, 2), (1, 2, 5, 3), (1, 3, 5, 0), (1, 3, 5, 1), (1, 3, 5, 2), (1, 3, 5, 3), (1, 0, 6, 0), (1, 0, 6, 1), (1, 0, 6, 2), (1, 0, 6, 3), (1, 1, 6, 0), (1, 1, 6, 1), (1, 1, 6, 2), (1, 1, 6, 3), (1, 2, 6, 0), (1, 2, 6, 1), (1, 2, 6, 2), (1, 2, 6, 3), (1, 3, 6, 0), (1, 3, 6, 1)] */
v_mul_f32 v[vgprValuC+12], s[sgprAlpha], v[vgprValuC+12] // *= alpha
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+122], s[sgprAlpha], v[vgprValuC+122] // *= alpha
v_mul_f32 v[vgprValuC+127], s[sgprAlpha], v[vgprValuC+127] // *= alpha
v_mul_f32 v[vgprValuC+132], s[sgprAlpha], v[vgprValuC+132] // *= alpha
v_mul_f32 v[vgprValuC+137], s[sgprAlpha], v[vgprValuC+137] // *= alpha
v_mul_f32 v[vgprValuC+143], s[sgprAlpha], v[vgprValuC+143] // *= alpha
v_mul_f32 v[vgprValuC+148], s[sgprAlpha], v[vgprValuC+148] // *= alpha
v_mul_f32 v[vgprValuC+153], s[sgprAlpha], v[vgprValuC+153] // *= alpha
v_mul_f32 v[vgprValuC+158], s[sgprAlpha], v[vgprValuC+158] // *= alpha
v_mul_f32 v[vgprValuC+163], s[sgprAlpha], v[vgprValuC+163] // *= alpha
v_mul_f32 v[vgprValuC+168], s[sgprAlpha], v[vgprValuC+168] // *= alpha
v_mul_f32 v[vgprValuC+173], s[sgprAlpha], v[vgprValuC+173] // *= alpha
v_mul_f32 v[vgprValuC+178], s[sgprAlpha], v[vgprValuC+178] // *= alpha
v_mul_f32 v[vgprValuC+183], s[sgprAlpha], v[vgprValuC+183] // *= alpha
v_mul_f32 v[vgprValuC+188], s[sgprAlpha], v[vgprValuC+188] // *= alpha
v_mul_f32 v[vgprValuC+193], s[sgprAlpha], v[vgprValuC+193] // *= alpha
v_mul_f32 v[vgprValuC+198], s[sgprAlpha], v[vgprValuC+198] // *= alpha
v_mul_f32 v[vgprValuC+203], s[sgprAlpha], v[vgprValuC+203] // *= alpha
v_mul_f32 v[vgprValuC+208], s[sgprAlpha], v[vgprValuC+208] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+12], v11, v[vgprValuC+12]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+12], s[sgprBeta], v9, v[vgprValuC+12] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+12], v10, v[vgprValuC+12]    // C += bias
v_cvt_f16_f32 v12, v[vgprValuC+12]                 // convert C to fp16
buffer_store_short v12, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+19], s[sgprBeta], v16, v[vgprValuC+19] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+19], v17, v[vgprValuC+19]    // C += bias
v_cvt_f16_f32 v19, v[vgprValuC+19]                 // convert C to fp16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v23, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+26], v24, v[vgprValuC+26]    // C += bias
v_cvt_f16_f32 v26, v[vgprValuC+26]                 // convert C to fp16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v30, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+33], v31, v[vgprValuC+33]    // C += bias
v_cvt_f16_f32 v33, v[vgprValuC+33]                 // convert C to fp16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v37, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+40], v38, v[vgprValuC+40]    // C += bias
v_cvt_f16_f32 v40, v[vgprValuC+40]                 // convert C to fp16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v44, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+54], v53, v[vgprValuC+54]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v51, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+54], v52, v[vgprValuC+54]    // C += bias
v_cvt_f16_f32 v54, v[vgprValuC+54]                 // convert C to fp16
buffer_store_short v54, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+61], v60, v[vgprValuC+61]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v58, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+61], v59, v[vgprValuC+61]    // C += bias
v_cvt_f16_f32 v61, v[vgprValuC+61]                 // convert C to fp16
buffer_store_short v61, v55, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+68], v67, v[vgprValuC+68]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+68], s[sgprBeta], v65, v[vgprValuC+68] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+68], v66, v[vgprValuC+68]    // C += bias
v_cvt_f16_f32 v68, v[vgprValuC+68]                 // convert C to fp16
buffer_store_short v68, v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+75], v74, v[vgprValuC+75]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v72, v[vgprValuC+75] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+75], v73, v[vgprValuC+75]    // C += bias
v_cvt_f16_f32 v75, v[vgprValuC+75]                 // convert C to fp16
buffer_store_short v75, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+82], v81, v[vgprValuC+82]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+82], s[sgprBeta], v79, v[vgprValuC+82] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+82], v80, v[vgprValuC+82]    // C += bias
v_cvt_f16_f32 v82, v[vgprValuC+82]                 // convert C to fp16
buffer_store_short v82, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+89], v88, v[vgprValuC+89]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+89], s[sgprBeta], v86, v[vgprValuC+89] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+89], v87, v[vgprValuC+89]    // C += bias
v_cvt_f16_f32 v89, v[vgprValuC+89]                 // convert C to fp16
buffer_store_short v89, v83, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+96], v95, v[vgprValuC+96]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+96], s[sgprBeta], v93, v[vgprValuC+96] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+96], v94, v[vgprValuC+96]    // C += bias
v_cvt_f16_f32 v96, v[vgprValuC+96]                 // convert C to fp16
buffer_store_short v96, v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+103], v102, v[vgprValuC+103] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+103], s[sgprBeta], v100, v[vgprValuC+103] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+103], v101, v[vgprValuC+103] // C += bias
v_cvt_f16_f32 v103, v[vgprValuC+103]               // convert C to fp16
buffer_store_short v103, v97, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+110], v109, v[vgprValuC+110] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+110], s[sgprBeta], v107, v[vgprValuC+110] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+110], v108, v[vgprValuC+110] // C += bias
v_cvt_f16_f32 v110, v[vgprValuC+110]               // convert C to fp16
buffer_store_short v110, v104, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+117], v116, v[vgprValuC+117] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v114, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+117], v115, v[vgprValuC+117] // C += bias
v_cvt_f16_f32 v117, v[vgprValuC+117]               // convert C to fp16
buffer_store_short v117, v111, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+122], v11, v[vgprValuC+122]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+122], s[sgprBeta], v121, v[vgprValuC+122] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+122], v10, v[vgprValuC+122]  // C += bias
v_cvt_f16_f32 v122, v[vgprValuC+122]               // convert C to fp16
buffer_store_short v122, v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+127], v18, v[vgprValuC+127]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+127], s[sgprBeta], v126, v[vgprValuC+127] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+127], v17, v[vgprValuC+127]  // C += bias
v_cvt_f16_f32 v127, v[vgprValuC+127]               // convert C to fp16
buffer_store_short v127, v123, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+132], v25, v[vgprValuC+132]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+132], s[sgprBeta], v131, v[vgprValuC+132] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+132], v24, v[vgprValuC+132]  // C += bias
v_cvt_f16_f32 v132, v[vgprValuC+132]               // convert C to fp16
buffer_store_short v132, v128, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+137], v32, v[vgprValuC+137]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+137], s[sgprBeta], v136, v[vgprValuC+137] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+137], v31, v[vgprValuC+137]  // C += bias
v_cvt_f16_f32 v137, v[vgprValuC+137]               // convert C to fp16
buffer_store_short v137, v133, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+143], v39, v[vgprValuC+143]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+143], s[sgprBeta], v142, v[vgprValuC+143] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+143], v38, v[vgprValuC+143]  // C += bias
v_cvt_f16_f32 v143, v[vgprValuC+143]               // convert C to fp16
buffer_store_short v143, v138, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+148], v46, v[vgprValuC+148]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+148], s[sgprBeta], v147, v[vgprValuC+148] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+148], v45, v[vgprValuC+148]  // C += bias
v_cvt_f16_f32 v148, v[vgprValuC+148]               // convert C to fp16
buffer_store_short v148, v144, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+153], v53, v[vgprValuC+153]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+153], s[sgprBeta], v152, v[vgprValuC+153] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+153], v52, v[vgprValuC+153]  // C += bias
v_cvt_f16_f32 v153, v[vgprValuC+153]               // convert C to fp16
buffer_store_short v153, v149, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+158], v60, v[vgprValuC+158]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+158], s[sgprBeta], v157, v[vgprValuC+158] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+158], v59, v[vgprValuC+158]  // C += bias
v_cvt_f16_f32 v158, v[vgprValuC+158]               // convert C to fp16
buffer_store_short v158, v154, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+163], v67, v[vgprValuC+163]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+163], s[sgprBeta], v162, v[vgprValuC+163] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+163], v66, v[vgprValuC+163]  // C += bias
v_cvt_f16_f32 v163, v[vgprValuC+163]               // convert C to fp16
buffer_store_short v163, v159, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+168], v74, v[vgprValuC+168]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+168], s[sgprBeta], v167, v[vgprValuC+168] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+168], v73, v[vgprValuC+168]  // C += bias
v_cvt_f16_f32 v168, v[vgprValuC+168]               // convert C to fp16
buffer_store_short v168, v164, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+173], v81, v[vgprValuC+173]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+173], s[sgprBeta], v172, v[vgprValuC+173] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+173], v80, v[vgprValuC+173]  // C += bias
v_cvt_f16_f32 v173, v[vgprValuC+173]               // convert C to fp16
buffer_store_short v173, v169, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+178], v88, v[vgprValuC+178]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+178], s[sgprBeta], v177, v[vgprValuC+178] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+178], v87, v[vgprValuC+178]  // C += bias
v_cvt_f16_f32 v178, v[vgprValuC+178]               // convert C to fp16
buffer_store_short v178, v174, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+183], v95, v[vgprValuC+183]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+183], s[sgprBeta], v182, v[vgprValuC+183] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+183], v94, v[vgprValuC+183]  // C += bias
v_cvt_f16_f32 v183, v[vgprValuC+183]               // convert C to fp16
buffer_store_short v183, v179, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+188], v102, v[vgprValuC+188] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+188], s[sgprBeta], v187, v[vgprValuC+188] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+188], v101, v[vgprValuC+188] // C += bias
v_cvt_f16_f32 v188, v[vgprValuC+188]               // convert C to fp16
buffer_store_short v188, v184, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+193], v109, v[vgprValuC+193] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+193], s[sgprBeta], v192, v[vgprValuC+193] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+193], v108, v[vgprValuC+193] // C += bias
v_cvt_f16_f32 v193, v[vgprValuC+193]               // convert C to fp16
buffer_store_short v193, v189, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+198], v116, v[vgprValuC+198] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+198], s[sgprBeta], v197, v[vgprValuC+198] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+198], v115, v[vgprValuC+198] // C += bias
v_cvt_f16_f32 v198, v[vgprValuC+198]               // convert C to fp16
buffer_store_short v198, v194, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+203], v11, v[vgprValuC+203]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+203], s[sgprBeta], v202, v[vgprValuC+203] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+203], v10, v[vgprValuC+203]  // C += bias
v_cvt_f16_f32 v203, v[vgprValuC+203]               // convert C to fp16
buffer_store_short v203, v199, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+208], v18, v[vgprValuC+208]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+208], s[sgprBeta], v207, v[vgprValuC+208] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+208], v17, v[vgprValuC+208]  // C += bias
v_cvt_f16_f32 v208, v[vgprValuC+208]               // convert C to fp16
buffer_store_short v208, v204, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 factorDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #7 (d1,d0,vc1,vc0) = */
/*    (1,3,6,2:vw1); (1,3,6,3:vw1); (1,0,7,0:vw1); (1,0,7,1:vw1); (1,0,7,2:vw1); (1,0,7,3:vw1); (1,1,7,0:vw1); (1,1,7,1:vw1); (1,1,7,2:vw1); (1,1,7,3:vw1); (1,2,7,0:vw1); (1,2,7,1:vw1); (1,2,7,2:vw1); (1,2,7,3:vw1); (1,3,7,0:vw1); (1,3,7,1:vw1); (1,3,7,2:vw1); (1,3,7,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v128, BufferOOB
/* (d1,vc1,d0,vc0)=(1,6,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v6, v2, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v128, v6, s[56:57]               // LDC clip if OOB. offset
buffer_load_short_d16 v9, v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v7, v4, s52
v_lshlrev_b32 v7, 0x2, v7                          // Bias address scaled by BPE
ds_read_b32 v10, v7 offset:0                       // load Bias
v_add_u32 v8, 1024, v7                             // add ScaleAlphaVec offset (3)
ds_read_b32 v11, v8 offset:0                       // load scaleAlpha
v_add_lshl_u32 v6, v3, v4, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v128, v6, s[56:57]               // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,6,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v13, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v128, v13, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v4, s52
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
ds_read_b32 v17, v14 offset:0                      // load Bias
v_add_u32 v15, 1024, v14                           // add ScaleAlphaVec offset (3)
ds_read_b32 v18, v15 offset:0                      // load scaleAlpha
v_add_lshl_u32 v13, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v128, v13, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[52:53], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v20, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v128, v20, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v0, s52
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
ds_read_b32 v24, v21 offset:0                      // load Bias
v_add_u32 v22, 1024, v21                           // add ScaleAlphaVec offset (3)
ds_read_b32 v25, v22 offset:0                      // load scaleAlpha
v_add_lshl_u32 v20, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v128, v20, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v128, v27, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s52
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
ds_read_b32 v31, v28 offset:0                      // load Bias
v_add_u32 v29, 1024, v28                           // add ScaleAlphaVec offset (3)
ds_read_b32 v32, v29 offset:0                      // load scaleAlpha
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v128, v27, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v128, v34, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s52
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
ds_read_b32 v38, v35 offset:0                      // load Bias
v_add_u32 v36, 1024, v35                           // add ScaleAlphaVec offset (3)
ds_read_b32 v39, v36 offset:0                      // load scaleAlpha
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v128, v34, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v41, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v128, v41, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s52
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
ds_read_b32 v45, v42 offset:0                      // load Bias
v_add_u32 v43, 1024, v42                           // add ScaleAlphaVec offset (3)
ds_read_b32 v46, v43 offset:0                      // load scaleAlpha
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v128, v41, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,1,0) */
v_add_co_u32 v4, vcc, v0, 64                       // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v48, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v128, v48, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v51, v48, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v4, s52
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
ds_read_b32 v52, v49 offset:0                      // load Bias
v_add_u32 v50, 1024, v49                           // add ScaleAlphaVec offset (3)
ds_read_b32 v53, v50 offset:0                      // load scaleAlpha
v_add_lshl_u32 v48, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v128, v48, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,1,1) */
s_mov_b32 s52, 65                                  // coordOffset0 d0=1 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v55, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v128, v55, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v58, v55, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v56, v4, s52
v_lshlrev_b32 v56, 0x2, v56                        // Bias address scaled by BPE
ds_read_b32 v59, v56 offset:0                      // load Bias
v_add_u32 v57, 1024, v56                           // add ScaleAlphaVec offset (3)
ds_read_b32 v60, v57 offset:0                      // load scaleAlpha
v_add_lshl_u32 v55, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v55, v128, v55, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,1,2) */
s_mov_b32 s52, 66                                  // coordOffset0 d0=1 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v62, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v128, v62, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v65, v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v4, s52
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
ds_read_b32 v66, v63 offset:0                      // load Bias
v_add_u32 v64, 1024, v63                           // add ScaleAlphaVec offset (3)
ds_read_b32 v67, v64 offset:0                      // load scaleAlpha
v_add_lshl_u32 v62, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v128, v62, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,1,3) */
s_mov_b32 s52, 67                                  // coordOffset0 d0=1 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v69, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v128, v69, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v72, v69, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v4, s52
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
ds_read_b32 v73, v70 offset:0                      // load Bias
v_add_u32 v71, 1024, v70                           // add ScaleAlphaVec offset (3)
ds_read_b32 v74, v71 offset:0                      // load scaleAlpha
v_add_lshl_u32 v69, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v128, v69, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,2,0) */
s_mov_b32 s52, 128                                 // coordOffset0 d0=2 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v128, v76, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s52
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
ds_read_b32 v80, v77 offset:0                      // load Bias
v_add_u32 v78, 1024, v77                           // add ScaleAlphaVec offset (3)
ds_read_b32 v81, v78 offset:0                      // load scaleAlpha
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v128, v76, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,2,1) */
s_mov_b32 s52, 129                                 // coordOffset0 d0=2 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v83, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v128, v83, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v86, v83, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v84, v4, s52
v_lshlrev_b32 v84, 0x2, v84                        // Bias address scaled by BPE
ds_read_b32 v87, v84 offset:0                      // load Bias
v_add_u32 v85, 1024, v84                           // add ScaleAlphaVec offset (3)
ds_read_b32 v88, v85 offset:0                      // load scaleAlpha
v_add_lshl_u32 v83, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v83, v128, v83, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,2,2) */
s_mov_b32 s52, 130                                 // coordOffset0 d0=2 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v90, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v128, v90, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16 v93, v90, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v91, v4, s52
v_lshlrev_b32 v91, 0x2, v91                        // Bias address scaled by BPE
ds_read_b32 v94, v91 offset:0                      // load Bias
v_add_u32 v92, 1024, v91                           // add ScaleAlphaVec offset (3)
ds_read_b32 v95, v92 offset:0                      // load scaleAlpha
v_add_lshl_u32 v90, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v90, v128, v90, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,2,3) */
s_mov_b32 s52, 131                                 // coordOffset0 d0=2 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v97, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v128, v97, s[56:57]             // LDC clip if OOB. offset
buffer_load_short_d16_hi v100, v97, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v98, v4, s52
v_lshlrev_b32 v98, 0x2, v98                        // Bias address scaled by BPE
ds_read_b32 v101, v98 offset:0                     // load Bias
v_add_u32 v99, 1024, v98                           // add ScaleAlphaVec offset (3)
ds_read_b32 v102, v99 offset:0                     // load scaleAlpha
v_add_lshl_u32 v97, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v97, v128, v97, s[56:57]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,3,0) */
s_mov_b32 s52, 192                                 // coordOffset0 d0=3 vc0=0
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v104, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v104, v128, v104, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v107, v104, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v105, v4, s52
v_lshlrev_b32 v105, 0x2, v105                      // Bias address scaled by BPE
ds_read_b32 v108, v105 offset:0                    // load Bias
v_add_u32 v106, 1024, v105                         // add ScaleAlphaVec offset (3)
ds_read_b32 v109, v106 offset:0                    // load scaleAlpha
v_add_lshl_u32 v104, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v104, v128, v104, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,3,1) */
s_mov_b32 s52, 193                                 // coordOffset0 d0=3 vc0=1
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v111, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v111, v128, v111, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v114, v111, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v112, v4, s52
v_lshlrev_b32 v112, 0x2, v112                      // Bias address scaled by BPE
ds_read_b32 v115, v112 offset:0                    // load Bias
v_add_u32 v113, 1024, v112                         // add ScaleAlphaVec offset (3)
ds_read_b32 v116, v113 offset:0                    // load scaleAlpha
v_add_lshl_u32 v111, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v111, v128, v111, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,3,2) */
s_mov_b32 s52, 194                                 // coordOffset0 d0=3 vc0=2
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v118, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v128, v118, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16 v121, v118, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v119, v4, s52
v_lshlrev_b32 v119, 0x2, v119                      // Bias address scaled by BPE
v_add_u32 v120, 1024, v119                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v118, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v118, v128, v118, s[56:57]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,7,3,3) */
s_mov_b32 s52, 195                                 // coordOffset0 d0=3 vc0=3
v_add_co_u32 v4, vcc, v0, s52                      // coord0.2: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[52:53], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[56:57], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[56:57], s[52:53], s[56:57]             // in0 && in1
v_add_lshl_u32 v123, v2, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v123, v128, v123, s[56:57]           // LDC clip if OOB. offset
buffer_load_short_d16_hi v126, v123, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s52, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v124, v4, s52
v_lshlrev_b32 v124, 0x2, v124                      // Bias address scaled by BPE
v_add_u32 v125, 1024, v124                         // add ScaleAlphaVec offset (3)
v_add_lshl_u32 v123, v3, v4, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v123, v128, v123, s[56:57]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+12], acc238         // copy acc to vreg[238]
v_accvgpr_read_b32 v[vgprValuC+19], acc239         // copy acc to vreg[239]
v_accvgpr_read_b32 v[vgprValuC+26], acc240         // copy acc to vreg[240]
v_accvgpr_read_b32 v[vgprValuC+33], acc241         // copy acc to vreg[241]
v_accvgpr_read_b32 v[vgprValuC+40], acc242         // copy acc to vreg[242]
v_accvgpr_read_b32 v[vgprValuC+47], acc243         // copy acc to vreg[243]
v_accvgpr_read_b32 v[vgprValuC+54], acc244         // copy acc to vreg[244]
v_accvgpr_read_b32 v[vgprValuC+61], acc245         // copy acc to vreg[245]
v_accvgpr_read_b32 v[vgprValuC+68], acc246         // copy acc to vreg[246]
v_accvgpr_read_b32 v[vgprValuC+75], acc247         // copy acc to vreg[247]
v_accvgpr_read_b32 v[vgprValuC+82], acc248         // copy acc to vreg[248]
v_accvgpr_read_b32 v[vgprValuC+89], acc249         // copy acc to vreg[249]
v_accvgpr_read_b32 v[vgprValuC+96], acc250         // copy acc to vreg[250]
v_accvgpr_read_b32 v[vgprValuC+103], acc251        // copy acc to vreg[251]
v_accvgpr_read_b32 v[vgprValuC+110], acc252        // copy acc to vreg[252]
v_accvgpr_read_b32 v[vgprValuC+117], acc253        // copy acc to vreg[253]
v_accvgpr_read_b32 v[vgprValuC+122], acc254        // copy acc to vreg[254]
v_accvgpr_read_b32 v[vgprValuC+127], acc255        // copy acc to vreg[255]

/* rC *= alpha batchElements=[(1, 3, 6, 2), (1, 3, 6, 3), (1, 0, 7, 0), (1, 0, 7, 1), (1, 0, 7, 2), (1, 0, 7, 3), (1, 1, 7, 0), (1, 1, 7, 1), (1, 1, 7, 2), (1, 1, 7, 3), (1, 2, 7, 0), (1, 2, 7, 1), (1, 2, 7, 2), (1, 2, 7, 3), (1, 3, 7, 0), (1, 3, 7, 1), (1, 3, 7, 2), (1, 3, 7, 3)] */
v_mul_f32 v[vgprValuC+12], s[sgprAlpha], v[vgprValuC+12] // *= alpha
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+122], s[sgprAlpha], v[vgprValuC+122] // *= alpha
v_mul_f32 v[vgprValuC+127], s[sgprAlpha], v[vgprValuC+127] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mul_f32 v[vgprValuC+12], v11, v[vgprValuC+12]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+12], s[sgprBeta], v9, v[vgprValuC+12] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+12], v10, v[vgprValuC+12]    // C += bias
v_cvt_f16_f32 v12, v[vgprValuC+12]                 // convert C to fp16
buffer_store_short v12, v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+19], s[sgprBeta], v16, v[vgprValuC+19] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+19], v17, v[vgprValuC+19]    // C += bias
v_cvt_f16_f32 v19, v[vgprValuC+19]                 // convert C to fp16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v23, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+26], v24, v[vgprValuC+26]    // C += bias
v_cvt_f16_f32 v26, v[vgprValuC+26]                 // convert C to fp16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v30, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+33], v31, v[vgprValuC+33]    // C += bias
v_cvt_f16_f32 v33, v[vgprValuC+33]                 // convert C to fp16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v37, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+40], v38, v[vgprValuC+40]    // C += bias
v_cvt_f16_f32 v40, v[vgprValuC+40]                 // convert C to fp16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+47], v46, v[vgprValuC+47]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v44, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+47], v45, v[vgprValuC+47]    // C += bias
v_cvt_f16_f32 v47, v[vgprValuC+47]                 // convert C to fp16
buffer_store_short v47, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+54], v53, v[vgprValuC+54]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v51, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+54], v52, v[vgprValuC+54]    // C += bias
v_cvt_f16_f32 v54, v[vgprValuC+54]                 // convert C to fp16
buffer_store_short v54, v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+61], v60, v[vgprValuC+61]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v58, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+61], v59, v[vgprValuC+61]    // C += bias
v_cvt_f16_f32 v61, v[vgprValuC+61]                 // convert C to fp16
buffer_store_short v61, v55, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+68], v67, v[vgprValuC+68]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+68], s[sgprBeta], v65, v[vgprValuC+68] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+68], v66, v[vgprValuC+68]    // C += bias
v_cvt_f16_f32 v68, v[vgprValuC+68]                 // convert C to fp16
buffer_store_short v68, v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+75], v74, v[vgprValuC+75]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+75], s[sgprBeta], v72, v[vgprValuC+75] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+75], v73, v[vgprValuC+75]    // C += bias
v_cvt_f16_f32 v75, v[vgprValuC+75]                 // convert C to fp16
buffer_store_short v75, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+82], v81, v[vgprValuC+82]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+82], s[sgprBeta], v79, v[vgprValuC+82] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+82], v80, v[vgprValuC+82]    // C += bias
v_cvt_f16_f32 v82, v[vgprValuC+82]                 // convert C to fp16
buffer_store_short v82, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+89], v88, v[vgprValuC+89]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+89], s[sgprBeta], v86, v[vgprValuC+89] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+89], v87, v[vgprValuC+89]    // C += bias
v_cvt_f16_f32 v89, v[vgprValuC+89]                 // convert C to fp16
buffer_store_short v89, v83, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+96], v95, v[vgprValuC+96]    // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+96], s[sgprBeta], v93, v[vgprValuC+96] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+96], v94, v[vgprValuC+96]    // C += bias
v_cvt_f16_f32 v96, v[vgprValuC+96]                 // convert C to fp16
buffer_store_short v96, v90, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+103], v102, v[vgprValuC+103] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+103], s[sgprBeta], v100, v[vgprValuC+103] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+103], v101, v[vgprValuC+103] // C += bias
v_cvt_f16_f32 v103, v[vgprValuC+103]               // convert C to fp16
buffer_store_short v103, v97, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+110], v109, v[vgprValuC+110] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+110], s[sgprBeta], v107, v[vgprValuC+110] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+110], v108, v[vgprValuC+110] // C += bias
v_cvt_f16_f32 v110, v[vgprValuC+110]               // convert C to fp16
buffer_store_short v110, v104, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+117], v116, v[vgprValuC+117] // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+117], s[sgprBeta], v114, v[vgprValuC+117] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+117], v115, v[vgprValuC+117] // C += bias
v_cvt_f16_f32 v117, v[vgprValuC+117]               // convert C to fp16
buffer_store_short v117, v111, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+122], v11, v[vgprValuC+122]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+122], s[sgprBeta], v121, v[vgprValuC+122] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+122], v10, v[vgprValuC+122]  // C += bias
v_cvt_f16_f32 v122, v[vgprValuC+122]               // convert C to fp16
buffer_store_short v122, v118, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_mul_f32 v[vgprValuC+127], v18, v[vgprValuC+127]  // *= ScaleAlphaVecVMul
v_fma_mix_f32 v[vgprValuC+127], s[sgprBeta], v126, v[vgprValuC+127] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_add_f32 v[vgprValuC+127], v17, v[vgprValuC+127]  // C += bias
v_cvt_f16_f32 v127, v[vgprValuC+127]               // convert C to fp16
buffer_store_short v127, v123, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_2                            // jump to end
label_GW_End_2:
label_KernelEnd:
s_endpgm                                           // Kernel End
label_ASM_End:  /// The end of the kernel
